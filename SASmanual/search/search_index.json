{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"I hope this site help you with your SAS programming. Contact me if you want any content to be added to this brief (hopefully not so brief in the future) manual. I will be glad to include it. Enjoy! :) If this is your starting point with SAS programming, maybe these readings could be useful Getting Started with SAS Programming Working with SAS Programs SAS proceedings papers repository SAS seminars by the UCLA Statistical Consulting Group Interesting configuration tips and tricks Print out the available SAS packages according to your license and the expiration dates: 1 2 PROC SETINIT; RUN; Comments: 1 2 /* comment */ * comment statement; When you name a process flow Autoexec , SAS Enterprise Guide prompts you to run the process flow when you open the project. This makes it easy to recreate your data when you return to the project. How to compare SAS programs in SAS Enterprise Guide Send an email with some coding Shortcuts Link Shortcut Function F3 Run selection or run all if there's nothing selected Ctrl + I Beautify code (proper indentation) Ctrl + Shift + U Convert to uppercase Ctrl + Shift + L Convert to lowercase Ctrl + / Wrap selection (or current line) in a comment Ctrl + Shift + / Unwrap selection (or current line) from a comment Ctrl + G Go to line (prompts for a line number) Ctrl + [, Ctrl + ] Move caret to matching parenthesis/brace Alt + [, Alt + ] Move caret to matching DO/END keyword Check these websites 5 keyboard shortcuts in SAS that will change your life Create folder from code Link Create folders directly in the chosen directory: 1 X MD \"&path.\\folder\" ; Create the folders in the study directory from the library declaration: 1 2 options dlcreatedir ; libname newfolder \"&path.\\newfolder\" ;","title":"Gettin' Started!"},{"location":"#shortcuts","text":"Shortcut Function F3 Run selection or run all if there's nothing selected Ctrl + I Beautify code (proper indentation) Ctrl + Shift + U Convert to uppercase Ctrl + Shift + L Convert to lowercase Ctrl + / Wrap selection (or current line) in a comment Ctrl + Shift + / Unwrap selection (or current line) from a comment Ctrl + G Go to line (prompts for a line number) Ctrl + [, Ctrl + ] Move caret to matching parenthesis/brace Alt + [, Alt + ] Move caret to matching DO/END keyword Check these websites 5 keyboard shortcuts in SAS that will change your life","title":"Shortcuts"},{"location":"#create-folder-from-code","text":"Create folders directly in the chosen directory: 1 X MD \"&path.\\folder\" ; Create the folders in the study directory from the library declaration: 1 2 options dlcreatedir ; libname newfolder \"&path.\\newfolder\" ;","title":"Create folder from code"},{"location":"sasvi/","text":"Check these websites VI Tutorials Home Page Link At the Home Page you can: Click the icon beside \"Home\" in the banner to access your applications using a side panel Add application shortcuts to your Home page (you can customize the color and the name) Create customized collections of documents/projects that you can then share Check recent projects in the Recent tile Check your favorite projects at the Favorites list (you can Edit $\\rightarrow$ Add a new favorite to the list or include it when you inspect the project by activating the star symbol) Check your customize list of links in the Links tile (you can Edit $\\rightarrow$ Add to include a new one) Change your application settings at the top right corner Data Explorer Link Report Designer Link These are the tipical step that a report author might walk through while creating a report in the designer: Choose your data source Modify data item properties Drag one or more report objects onto the canvas Assign data items to report object roles Apply any needed or desired filters Update properties for the report Update properties and styles for report objects If desired, add new sections to the report If required, add interactions between the report objects Save your report The minimun steps to create a basic report are 1, 3, 4 and 10. Data Preparation Link Report Viewer Link","title":"SAS Visual Analytics"},{"location":"sasvi/#home-page","text":"At the Home Page you can: Click the icon beside \"Home\" in the banner to access your applications using a side panel Add application shortcuts to your Home page (you can customize the color and the name) Create customized collections of documents/projects that you can then share Check recent projects in the Recent tile Check your favorite projects at the Favorites list (you can Edit $\\rightarrow$ Add a new favorite to the list or include it when you inspect the project by activating the star symbol) Check your customize list of links in the Links tile (you can Edit $\\rightarrow$ Add to include a new one) Change your application settings at the top right corner","title":"Home Page"},{"location":"sasvi/#data-explorer","text":"","title":"Data Explorer"},{"location":"sasvi/#report-designer","text":"These are the tipical step that a report author might walk through while creating a report in the designer: Choose your data source Modify data item properties Drag one or more report objects onto the canvas Assign data items to report object roles Apply any needed or desired filters Update properties for the report Update properties and styles for report objects If desired, add new sections to the report If required, add interactions between the report objects Save your report The minimun steps to create a basic report are 1, 3, 4 and 10.","title":"Report Designer"},{"location":"sasvi/#data-preparation","text":"","title":"Data Preparation"},{"location":"sasvi/#report-viewer","text":"","title":"Report Viewer"},{"location":"wizard/","text":"Despu\u00e9s de crear un c\u00f3digo de recodificaci\u00f3n con el wizard click derecho en el workflow, create as code, template, save code DATA Step Debugger Link Check these websites Demo: DATA Step Debugging in Enterprise Guide Using the DATA step debugger in SAS Enterprise Guide Step through Your DATA Step: Introducing the DATA Step Debugger in SAS Enterprise Guide An Animated Guide: The SAS Data Step Debugger This tool is for debugging DATA step code. It can't be used to debug PROC SQL or PROC IML or SAS macro programs. It can't be used to debug DATA steps that read data from CARDS or DATALINES . That's an unfortunate limitation, but it's a side effect of the way the DATA step debug mode works with client applications like SAS Enterprise Guide. To workaround this limitation you can load your data in a separate step and then debug your more complex DATA step logic in a subsequent step. When a variable changes its value, it's colored red. If the value hasn't changed it will remain black. If you want the DATA step to break processing when a certain variable changes value, check the Watch box for that variable. You can set and clear line-specific breakpoints by clicking in the left space next to the line number. In the Debug Console window you can introduce more complex breakpoints through commands: BREAK suspends program execution at an executable statement CALCULATE evaluates a debugger expression and displays the result DELETE deletes breakpoints or the watch status of variables in the DATA step DESCRIBE displays the attributes of one or more variables ENTER assigns one or more debugger commands to the ENTER key EXAMINE displays the value of one or more variables GO starts or resumes execution of the DATA step HELP displays information about debugger commands JUMP restarts execution of a suspended program LIST displays all occurrences of the item that is listed in the argument QUIT terminates a debugger session SET assigns a new value to a specified variable STEP executes statements one at a time in the active program SWAP switches control between the SOURCE window and the LOG window TRACE controls whether the debugger displays a continuous record of the DATA step execution WATCH suspends execution when the value of a specified variable changes Examples break 8 when (running_price > 100) will break on line 8 when the value of running_price exceeds 100 break 8 after 5 will break on line 8 after 5 passes through the DATA step","title":"Using Wizard Menus"},{"location":"wizard/#data-step-debugger","text":"Check these websites Demo: DATA Step Debugging in Enterprise Guide Using the DATA step debugger in SAS Enterprise Guide Step through Your DATA Step: Introducing the DATA Step Debugger in SAS Enterprise Guide An Animated Guide: The SAS Data Step Debugger This tool is for debugging DATA step code. It can't be used to debug PROC SQL or PROC IML or SAS macro programs. It can't be used to debug DATA steps that read data from CARDS or DATALINES . That's an unfortunate limitation, but it's a side effect of the way the DATA step debug mode works with client applications like SAS Enterprise Guide. To workaround this limitation you can load your data in a separate step and then debug your more complex DATA step logic in a subsequent step. When a variable changes its value, it's colored red. If the value hasn't changed it will remain black. If you want the DATA step to break processing when a certain variable changes value, check the Watch box for that variable. You can set and clear line-specific breakpoints by clicking in the left space next to the line number. In the Debug Console window you can introduce more complex breakpoints through commands: BREAK suspends program execution at an executable statement CALCULATE evaluates a debugger expression and displays the result DELETE deletes breakpoints or the watch status of variables in the DATA step DESCRIBE displays the attributes of one or more variables ENTER assigns one or more debugger commands to the ENTER key EXAMINE displays the value of one or more variables GO starts or resumes execution of the DATA step HELP displays information about debugger commands JUMP restarts execution of a suspended program LIST displays all occurrences of the item that is listed in the argument QUIT terminates a debugger session SET assigns a new value to a specified variable STEP executes statements one at a time in the active program SWAP switches control between the SOURCE window and the LOG window TRACE controls whether the debugger displays a continuous record of the DATA step execution WATCH suspends execution when the value of a specified variable changes Examples break 8 when (running_price > 100) will break on line 8 when the value of running_price exceeds 100 break 8 after 5 will break on line 8 after 5 passes through the DATA step","title":"DATA Step Debugger"},{"location":"R-with-Shiny/custom/","text":"Shiny comes with a list of functions saved under tags that allow us to access HTML tags and use them to add static (as opposed to reactive) content to our apps. The tags objects in Shiny is a list of a 110 simple functions for constructing HMTL documents. Each of the elements in this list is a function that maps to an HTML tag. 1 2 tags $ b ( \"This is my first app\" ) < b > This is my first app </ b > The most common used tags are wrapped in their own functions and you can use them without the tags$ list. These are functions like a() for anchor text, br() for line break, code() for displaying code in monospace form and the heading functions ( h1() , h2() , etc.). The following app does a bunch of things: allows users to customize the plot, subsets and samples the data, and reports simple summary statistics and displays the data table. HTML tags are used to add headers and visual separators to make it a bit more clear, at a first glance, what's happening in the app. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 library ( shiny ) library ( ggplot2 ) library ( stringr ) library ( dplyr ) library ( DT ) library ( tools ) load ( url ( \"http://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata\" )) # Define UI for application that plots features of movies ui <- fluidPage ( # Sidebar layout with a input and output definitions sidebarLayout ( # Inputs sidebarPanel ( h3 ( \"Plotting\" ), # Third level header: Plotting # Select variable for y-axis selectInput ( inputId = \"y\" , label = \"Y-axis:\" , choices = c ( \"IMDB rating\" = \"imdb_rating\" , \"IMDB number of votes\" = \"imdb_num_votes\" , \"Critics Score\" = \"critics_score\" , \"Audience Score\" = \"audience_score\" , \"Runtime\" = \"runtime\" ), selected = \"audience_score\" ), # Select variable for x-axis selectInput ( inputId = \"x\" , label = \"X-axis:\" , choices = c ( \"IMDB rating\" = \"imdb_rating\" , \"IMDB number of votes\" = \"imdb_num_votes\" , \"Critics Score\" = \"critics_score\" , \"Audience Score\" = \"audience_score\" , \"Runtime\" = \"runtime\" ), selected = \"critics_score\" ), # Select variable for color selectInput ( inputId = \"z\" , label = \"Color by:\" , choices = c ( \"Title Type\" = \"title_type\" , \"Genre\" = \"genre\" , \"MPAA Rating\" = \"mpaa_rating\" , \"Critics Rating\" = \"critics_rating\" , \"Audience Rating\" = \"audience_rating\" ), selected = \"mpaa_rating\" ), hr (), # Horizontal line for visual separation # Set alpha level sliderInput ( inputId = \"alpha\" , label = \"Alpha:\" , min = 0 , max = 1 , value = 0.5 ), # Set point size sliderInput ( inputId = \"size\" , label = \"Size:\" , min = 0 , max = 5 , value = 2 ), # Enter text for plot title textInput ( inputId = \"plot_title\" , label = \"Plot title\" , placeholder = \"Enter text to be used as plot title\" ), hr (), # Horizontal line for visual separation h3 ( \"Sampling and subsetting\" ), # Third level header: Sampling and subsetting # Select which types of movies to plot checkboxGroupInput ( inputId = \"selected_type\" , label = \"Select movie type(s):\" , choices = c ( \"Documentary\" , \"Feature Film\" , \"TV Movie\" ), selected = \"Feature Film\" ), # Select sample size numericInput ( inputId = \"n_samp\" , label = \"Sample size:\" , min = 1 , max = nrow ( movies ), value = 50 ), hr (), # Horizontal line for visual separation # Show data table checkboxInput ( inputId = \"show_data\" , label = \"Show data table\" , value = TRUE ) ), # Output: mainPanel ( # Show scatterplot h3 ( \"Scatterplot\" ), # Third level header: Scatterplot plotOutput ( outputId = \"scatterplot\" ), br (), # Single line break for a little bit of visual separation # Print number of obs plotted h4 ( uiOutput ( outputId = \"n\" )), # Fourth level header br (), br (), # Two line breaks for a little bit of visual separation # Show data table h3 ( \"Data table\" ), # Third level header: Data table DT :: dataTableOutput ( outputId = \"moviestable\" ) ) ) ) # Define server function required to create the scatterplot server <- function ( input , output , session ) { # Create a subset of data filtering for selected title types movies_subset <- reactive ({ req ( input $ selected_type ) # ensure availablity of value before proceeding filter ( movies , title_type %in% input $ selected_type ) }) # Update the maximum allowed n_samp for selected type movies observe ({ updateNumericInput ( session , inputId = \"n_samp\" , value = min ( 50 , nrow ( movies_subset ())), max = nrow ( movies_subset ()) ) }) # Create new df that is n_samp obs from selected type movies movies_sample <- reactive ({ req ( input $ n_samp ) # ensure availablity of value before proceeding sample_n ( movies_subset (), input $ n_samp ) }) # Create scatterplot object the plotOutput function is expecting output $ scatterplot <- renderPlot ({ ggplot ( data = movies_sample (), aes_string ( x = input $ x , y = input $ y , color = input $ z )) + geom_point ( alpha = input $ alpha , size = input $ size ) + labs ( x = toTitleCase ( str_replace_all ( input $ x , \"_\" , \" \" )), y = toTitleCase ( str_replace_all ( input $ y , \"_\" , \" \" )), color = toTitleCase ( str_replace_all ( input $ z , \"_\" , \" \" )), title = toTitleCase ( input $ plot_title )) }) # Print number of movies plotted output $ n <- renderUI ({ types <- movies_sample () $ title_type %>% factor ( levels = input $ selected_type ) counts <- table ( types ) HTML ( paste ( \"There are\" , counts , input $ selected_type , \"movies in this dataset. <br>\" )) }) # Print data table if checked output $ moviestable <- DT :: renderDataTable ( if ( input $ show_data ){ DT :: datatable ( data = movies_sample () [ , 1 : 7 ] , options = list ( pageLength = 10 ), rownames = FALSE ) } ) } # Create Shiny app object shinyApp ( ui = ui , server = server )","title":"Customizing Appearance"},{"location":"R-with-Shiny/input-output/","text":"Reactive Flow Link Suppose you have a slinderInput in your app with the inputId=\"alpha\" . The value of this input is stored in input$alpha so when the user moves around the slider the value of the alpha input is updated in th einput list. Reactivity automatically occurs when an input value is used to render an output object. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 library ( shiny ) library ( ggplot2 ) load ( url ( \"http://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata\" )) # Define UI for application that plots features of movies ui <- fluidPage ( # Sidebar layout with a input and output definitions sidebarLayout ( # Inputs sidebarPanel ( # Select variable for y-axis selectInput ( inputId = \"y\" , label = \"Y-axis:\" , choices = c ( \"imdb_rating\" , \"imdb_num_votes\" , \"critics_score\" , \"audience_score\" , \"runtime\" ), selected = \"audience_score\" ), # Select variable for x-axis selectInput ( inputId = \"x\" , label = \"X-axis:\" , choices = c ( \"imdb_rating\" , \"imdb_num_votes\" , \"critics_score\" , \"audience_score\" , \"runtime\" ), selected = \"critics_score\" ), # Set alpha level sliderInput ( inputId = \"alpha\" , label = \"Alpha:\" , min = 0 , max = 1 , value = 0.5 ) ), # Outputs mainPanel ( plotOutput ( outputId = \"scatterplot\" ) ) ) ) # Define server function required to create the scatterplot server <- function ( input , output ) { # Create scatterplot object the plotOutput function is expecting output $ scatterplot <- renderPlot ({ ggplot ( data = movies , aes_string ( x = input $ x , y = input $ y )) + geom_point ( alpha = input $ alpha ) }) } # Create the Shiny app object shinyApp ( ui = ui , server = server ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 library ( shiny ) library ( ggplot2 ) load ( url ( \"http://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata\" )) # Define UI for application that plots features of movies ui <- fluidPage ( # Sidebar layout with a input and output definitions sidebarLayout ( # Inputs sidebarPanel ( # Select variable for y-axis selectInput ( inputId = \"y\" , label = \"Y-axis:\" , choices = c ( \"imdb_rating\" , \"imdb_num_votes\" , \"critics_score\" , \"audience_score\" , \"runtime\" ), selected = \"audience_score\" ), # Select variable for x-axis selectInput ( inputId = \"x\" , label = \"X-axis:\" , choices = c ( \"imdb_rating\" , \"imdb_num_votes\" , \"critics_score\" , \"audience_score\" , \"runtime\" ), selected = \"critics_score\" ) ), # Outputs mainPanel ( plotOutput ( outputId = \"scatterplot\" ), plotOutput ( outputId = \"densityplot\" , height = 200 ) ) ) ) # Define server function required to create the scatterplot server <- function ( input , output ) { # Create scatterplot output $ scatterplot <- renderPlot ({ ggplot ( data = movies , aes_string ( x = input $ x , y = input $ y )) + geom_point () }) # Create densityplot output $ densityplot <- renderPlot ({ ggplot ( data = movies , aes_string ( x = input $ x )) + geom_density () }) } # Create the Shiny app object shinyApp ( ui = ui , server = server ) UI Inputs Link Shiny provides a wide selection of input widgets: checkboxInput Link Add a checkbox input to specify whether the data plotted should be shown in a data table. UI : Add an input widget that the user can interact with to check/uncheck the box UI : Add an output defining where the data table should appear Server : Add a reactive expression that creates the data table if the checkbox is checked 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 library ( shiny ) library ( dplyr ) library ( DT ) load ( url ( \"http://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata\" )) n_total <- nrow ( movies ) # Define UI for application that plots features of movies ui <- fluidPage ( # Sidebar layout with a input and output definitions sidebarLayout ( # Inputs sidebarPanel ( # Text instructions HTML ( paste ( \"Enter a value between 1 and\" , n_total )), # Numeric input for sample size numericInput ( inputId = \"n\" , label = \"Sample size:\" , min = 1 , max = n_total , value = 30 , step = 1 ) ), # Output: Show data table mainPanel ( DT :: dataTableOutput ( outputId = \"moviestable\" ) ) ) ) # Define server function required to create the scatterplot server <- function ( input , output ) { # Create data table output $ moviestable <- DT :: renderDataTable ({ req ( input $ n ) movies_sample <- movies %>% sample_n ( input $ n ) %>% select ( title : studio ) DT :: datatable ( data = movies_sample , options = list ( pageLength = 10 ), rownames = FALSE ) }) } # Create a Shiny app object shinyApp ( ui = ui , server = server ) The req() function If you delete the numeric value from the checkbox, you will encounter an error: Error: size is not a numeric or integer vector . In order to avoid such errors, which users of your app could very easily encounter, we need to hold back the output from being calculated if the input is missing. The req function is the simplest and best way to do this, it ensures that values are available (\"truthy\") before proceeding with a calculation or action. If any of the given values is not truthy, the operation is stopped by raising a \"silent\" exception (not logged by Shiny, nor displayed in the Shiny app's UI). selectInput : Multiple Selection Link The following app can be used to display movies from selected studios. There are 211 unique studios represented in this dataset, we need a better way to select than to scroll through such a long list, and we address that with the selectize option, which will suggest names of studios as you type them. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 library ( shiny ) library ( ggplot2 ) library ( dplyr ) library ( DT ) load ( url ( \"http://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata\" )) all_studios <- sort ( unique ( movies $ studio )) # UI ui <- fluidPage ( sidebarLayout ( # Input(s) sidebarPanel ( selectInput ( inputId = \"studio\" , label = \"Select studio:\" , choices = all_studios , selected = \"20th Century Fox\" , multiple = TRUE , selectize = TRUE ) ), # Output(s) mainPanel ( DT :: dataTableOutput ( outputId = \"moviestable\" ) ) ) ) # Server server <- function ( input , output ) { # Create data table output $ moviestable <- DT :: renderDataTable ({ req ( input $ studio ) movies_from_selected_studios <- movies %>% filter ( studio %in% input $ studio ) %>% select ( title : studio ) DT :: datatable ( data = movies_from_selected_studios , options = list ( pageLength = 10 ), rownames = FALSE ) }) } # Create a Shiny app object shinyApp ( ui = ui , server = server ) dateRangeInput Link The following app is coded to show the selected movies between two given dates using dateRangeInput . This input will yield a vector ( input$date ) of length two, the first element is the start date and the second is the end date. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 library ( shiny ) library ( dplyr ) library ( ggplot2 ) load ( url ( \"http://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata\" )) min_date <- min ( movies $ thtr_rel_date ) max_date <- max ( movies $ thtr_rel_date ) # UI ui <- fluidPage ( sidebarLayout ( # Input(s) sidebarPanel ( # Explanatory text HTML ( paste0 ( \"Movies released between the following dates will be plotted. Pick dates between \" , min_date , \" and \" , max_date , \".\" )), # Break for visual separation br (), br (), # Date input dateRangeInput ( inputId = \"date\" , label = \"Select dates:\" , start = \"2013-01-01\" , end = \"2014-01-01\" , startview = \"year\" , min = min_date , max = max_date ) ), # Output(s) mainPanel ( plotOutput ( outputId = \"scatterplot\" ) ) ) ) # Server server <- function ( input , output ) { # Create the plot output $ scatterplot <- renderPlot ({ req ( input $ date ) movies_selected_date <- movies %>% filter ( thtr_rel_date >= as.POSIXct ( input $ date[1] ) & thtr_rel_date <= as.POSIXct ( input $ date[2] )) ggplot ( data = movies_selected_date , aes ( x = critics_score , y = audience_score , color = mpaa_rating )) + geom_point () }) } # Create a Shiny app object shinyApp ( ui = ui , server = server ) Rendering Functions Link Shiny provides a wide selection of input widgets, each of which works with a render function: renderTable Link Add a table beneath the plot displaying summary statistics for a new variable: score_ratio = audience_score / critics_score . Calculate the new variable UI : Add an input widget that the user can interact with to check boxes for selected title types UI : Add an output defining where the summary table should appear Server : Add a reactive expression that creates the summary table 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 ( ... ) # Create new variable: # ratio of critics and audience scores movies <- movies %>% mutate ( score_ratio = audience_score / critics_score ) ( ... ) # Subset for title types checkboxGroupInput ( inputId = \"selected_title_type\" , label = \"Select title type:\" , choices = levels ( movies $ title_type ), selected = levels ( movies $ title_type )) ( ... ) mainPanel ( # Show scatterplot plotOutput ( outputId = \"scatterplot\" ), # Show data table tableOutput ( outputId = \"summarytable\" ) ) ( ... ) output $ summarytable <- renderTable ( { movies %>% filter ( title_type %in% input $ selected_title_type ) %>% group_by ( mpaa_rating ) %>% summarise ( Mean = mean ( score_ratio ), SD = sd ( score_ratio ), n = n ())}, striped = TRUE , spacing = \"l\" , align = \"lccr\" , digits = 4 , width = \"90%\" , caption = \"Score ratio (audience / critics' scores) summary statistics by MPAA rating.\" ) ( ... ) renderText Link In this app the user selects x and y variables for the scatterplot and also a textOutput which prints the correlation between the two selected variables as well as some informational text. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 library ( shiny ) library ( ggplot2 ) load ( url ( \"http://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata\" )) # UI ui <- fluidPage ( sidebarLayout ( # Input(s) sidebarPanel ( # Select variable for y-axis selectInput ( inputId = \"y\" , label = \"Y-axis:\" , choices = c ( \"imdb_rating\" , \"imdb_num_votes\" , \"critics_score\" , \"audience_score\" , \"runtime\" ), selected = \"audience_score\" ), # Select variable for x-axis selectInput ( inputId = \"x\" , label = \"X-axis:\" , choices = c ( \"imdb_rating\" , \"imdb_num_votes\" , \"critics_score\" , \"audience_score\" , \"runtime\" ), selected = \"critics_score\" ) ), # Outputs mainPanel ( plotOutput ( outputId = \"scatterplot\" ), textOutput ( outputId = \"correlation\" ) ) ) ) # Server server <- function ( input , output ) { # Create scatterplot object the plotOutput function is expecting output $ scatterplot <- renderPlot ({ ggplot ( data = movies , aes_string ( x = input $ x , y = input $ y )) + geom_point () }) # Create text output stating the correlation between the two ploted output $ correlation <- renderText ({ r <- round ( cor ( movies[ , input $ x] , movies[ , input $ y] , use = \"pairwise\" ), 3 ) paste0 ( \"Correlation = \" , r , \". Note: If the relationship between the two variables is not linear, the correlation coefficient will not be meaningful.\" ) }) } # Create a Shiny app object shinyApp ( ui = ui , server = server ) Recap of Output/Rendering Functions Link Shiny has a variety of render* functions with corresponding *Ourput functions to create and display outputs render* functions can take on multiple arguments, the first being the expression for the desired output The expression in the render* function should be wrapped in curly braces UI Outputs Link plotOutput Link Select points on the plot via brushing, and report the selected points in a data table underneath the plot. Brushing means that the user will be able to draw a rectangle in the plotting area and drag it around. UI : Add functionality to plotOutput to select points via brushing UI : Add an output defining where the data table should appear Server : Add a reactive expression that creates the data table for the selected points 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ( ... ) # Show scatterplot with brushing capability plotOutput ( outputId = \"scatterplot\" , brush = \"plot_brush\" ) ( ... ) # Show data table DT :: dataTableOutput ( outputId = \"moviestable\" ) ( ... ) # Print data table output $ moviestable <- DT :: renderDataTable ({ brushedPoints ( movies , input $ plot_brush ) %>% select ( title , audience_score , critics_score ) }) ( ... ) In addition to brushing, users can also interact with plots by hovering over them as in the following example. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 # Load packages library ( shiny ) library ( ggplot2 ) library ( tidyverse ) library ( DT ) # Load data load ( url ( \"http://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata\" )) # Define UI for application that plots features of movies ui <- fluidPage ( br (), # Sidebar layout with a input and output definitions sidebarLayout ( # Inputs sidebarPanel ( # Select variable for y-axis selectInput ( inputId = \"y\" , label = \"Y-axis:\" , choices = c ( \"imdb_rating\" , \"imdb_num_votes\" , \"critics_score\" , \"audience_score\" , \"runtime\" ), selected = \"audience_score\" ), # Select variable for x-axis selectInput ( inputId = \"x\" , label = \"X-axis:\" , choices = c ( \"imdb_rating\" , \"imdb_num_votes\" , \"critics_score\" , \"audience_score\" , \"runtime\" ), selected = \"critics_score\" ) ), # Output: mainPanel ( # Show scatterplot with brushing capability plotOutput ( outputId = \"scatterplot\" , hover = \"plot_hover\" ), # Show data table dataTableOutput ( outputId = \"moviestable\" ), br () ) ) ) # Define server function required to create the scatterplot server <- function ( input , output ) { # Create scatterplot object the plotOutput function is expecting output $ scatterplot <- renderPlot ({ ggplot ( data = movies , aes_string ( x = input $ x , y = input $ y )) + geom_point () }) # Create data table output $ moviestable <- DT :: renderDataTable ({ nearPoints ( movies , input $ plot_hover ) %>% select ( title , audience_score , critics_score ) }) } # Create a Shiny app object shinyApp ( ui = ui , server = server ) verbatimtextOutput Link 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 library ( shiny ) library ( dplyr ) library ( ggplot2 ) load ( url ( \"http://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata\" )) # UI ui <- fluidPage ( sidebarLayout ( # Input(s) sidebarPanel ( # Select variable for y-axis selectInput ( inputId = \"y\" , label = \"Y-axis:\" , choices = c ( \"imdb_rating\" , \"imdb_num_votes\" , \"critics_score\" , \"audience_score\" , \"runtime\" ), selected = \"audience_score\" ), # Select variable for x-axis selectInput ( inputId = \"x\" , label = \"X-axis:\" , choices = c ( \"imdb_rating\" , \"imdb_num_votes\" , \"critics_score\" , \"audience_score\" , \"runtime\" ), selected = \"critics_score\" ) ), # Output(s) mainPanel ( plotOutput ( outputId = \"scatterplot\" ), textOutput ( outputId = \"avg_x\" ), # avg of x textOutput ( outputId = \"avg_y\" ), # avg of y verbatimTextOutput ( outputId = \"lmoutput\" ) # regression output ) ) ) # Server server <- function ( input , output ) { # Create scatterplot output $ scatterplot <- renderPlot ({ ggplot ( data = movies , aes_string ( x = input $ x , y = input $ y )) + geom_point () }) # Calculate average of x output $ avg_x <- renderText ({ avg_x <- movies %>% pull ( input $ x ) %>% mean () %>% round ( 2 ) paste ( \"Average\" , input $ x , \"=\" , avg_x ) }) # Calculate average of y output $ avg_y <- renderText ({ avg_y <- movies %>% pull ( input $ y ) %>% mean () %>% round ( 2 ) paste ( \"Average\" , input $ y , \"=\" , avg_y ) }) # Create regression output output $ lmoutput <- renderPrint ({ x <- movies %>% pull ( input $ x ) y <- movies %>% pull ( input $ y ) summ <- summary ( lm ( y ~ x , data = movies )) print ( summ , digits = 3 , signif.stars = FALSE ) }) } # Create a Shiny app object shinyApp ( ui = ui , server = server ) htmlOutput Link In the previous example the app reported averages of selected x and y variables as two separate outputs. An alternative approach would be to combine them into a single, multi-line output. For this purpose, in the next example values calculated in app chunk in the paste() command are used to create customized HTML output with specified formatting obtaining the same result. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 library ( shiny ) library ( dplyr ) library ( ggplot2 ) load ( url ( \"http://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata\" )) # UI ui <- fluidPage ( sidebarLayout ( # Input(s) sidebarPanel ( # Select variable for y-axis selectInput ( inputId = \"y\" , label = \"Y-axis:\" , choices = c ( \"imdb_rating\" , \"imdb_num_votes\" , \"critics_score\" , \"audience_score\" , \"runtime\" ), selected = \"audience_score\" ), # Select variable for x-axis selectInput ( inputId = \"x\" , label = \"X-axis:\" , choices = c ( \"imdb_rating\" , \"imdb_num_votes\" , \"critics_score\" , \"audience_score\" , \"runtime\" ), selected = \"critics_score\" ) ), # Output(s) mainPanel ( plotOutput ( outputId = \"scatterplot\" ), htmlOutput ( outputId = \"avgs\" ), verbatimTextOutput ( outputId = \"lmoutput\" ) # regression output ) ) ) # Server server <- function ( input , output ) { # Create scatterplot output $ scatterplot <- renderPlot ({ ggplot ( data = movies , aes_string ( x = input $ x , y = input $ y )) + geom_point () }) # Calculate average of x output $ avgs <- renderUI ({ avg_x <- movies %>% pull ( input $ x ) %>% mean () %>% round ( 2 ) str_x <- paste ( \"Average\" , input $ x , \"=\" , avg_x ) avg_y <- movies %>% pull ( input $ y ) %>% mean () %>% round ( 2 ) str_y <- paste ( \"Average\" , input $ y , \"=\" , avg_y ) HTML ( paste ( str_x , str_y , sep = '<br/>' )) }) # Create regression output output $ lmoutput <- renderPrint ({ x <- movies %>% pull ( input $ x ) y <- movies %>% pull ( input $ y ) print ( summary ( lm ( y ~ x , data = movies )), digits = 3 , signif.stars = FALSE ) }) } # Create a Shiny app object shinyApp ( ui = ui , server = server ) Download data with downloadButton Link In this app you get to specify the file type and the variables included in the file you will download. For downloading from a Shiny app we use the downloadHandler function in the server and downloadButton or downloadLink function in the UI. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 library ( shiny ) library ( dplyr ) library ( readr ) load ( url ( \"http://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata\" )) # UI ui <- fluidPage ( sidebarLayout ( # Input(s) sidebarPanel ( # Select filetype radioButtons ( inputId = \"filetype\" , label = \"Select filetype:\" , choices = c ( \"csv\" , \"tsv\" ), selected = \"csv\" ), # Select variables to download checkboxGroupInput ( inputId = \"selected_var\" , label = \"Select variables:\" , choices = names ( movies ), selected = c ( \"title\" )) ), # Output(s) mainPanel ( HTML ( \"Select filetype and variables, then hit 'Download data'.\" ), downloadButton ( \"download_data\" , \"Download data\" ) ) ) ) # Server server <- function ( input , output ) { # Download file output $ download_data <- downloadHandler ( filename = function () { paste0 ( \"movies.\" , input $ filetype ) }, content = function ( file ) { if ( input $ filetype == \"csv\" ){ write_csv ( movies %>% select ( input $ selected_var ), file ) } if ( input $ filetype == \"tsv\" ){ write_tsv ( movies %>% select ( input $ selected_var ), file ) } } ) } # Create a Shiny app object shinyApp ( ui = ui , server = server )","title":"Inputs, Outputs, and Rendering Functions"},{"location":"R-with-Shiny/input-output/#reactive-flow","text":"Suppose you have a slinderInput in your app with the inputId=\"alpha\" . The value of this input is stored in input$alpha so when the user moves around the slider the value of the alpha input is updated in th einput list. Reactivity automatically occurs when an input value is used to render an output object. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 library ( shiny ) library ( ggplot2 ) load ( url ( \"http://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata\" )) # Define UI for application that plots features of movies ui <- fluidPage ( # Sidebar layout with a input and output definitions sidebarLayout ( # Inputs sidebarPanel ( # Select variable for y-axis selectInput ( inputId = \"y\" , label = \"Y-axis:\" , choices = c ( \"imdb_rating\" , \"imdb_num_votes\" , \"critics_score\" , \"audience_score\" , \"runtime\" ), selected = \"audience_score\" ), # Select variable for x-axis selectInput ( inputId = \"x\" , label = \"X-axis:\" , choices = c ( \"imdb_rating\" , \"imdb_num_votes\" , \"critics_score\" , \"audience_score\" , \"runtime\" ), selected = \"critics_score\" ), # Set alpha level sliderInput ( inputId = \"alpha\" , label = \"Alpha:\" , min = 0 , max = 1 , value = 0.5 ) ), # Outputs mainPanel ( plotOutput ( outputId = \"scatterplot\" ) ) ) ) # Define server function required to create the scatterplot server <- function ( input , output ) { # Create scatterplot object the plotOutput function is expecting output $ scatterplot <- renderPlot ({ ggplot ( data = movies , aes_string ( x = input $ x , y = input $ y )) + geom_point ( alpha = input $ alpha ) }) } # Create the Shiny app object shinyApp ( ui = ui , server = server ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 library ( shiny ) library ( ggplot2 ) load ( url ( \"http://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata\" )) # Define UI for application that plots features of movies ui <- fluidPage ( # Sidebar layout with a input and output definitions sidebarLayout ( # Inputs sidebarPanel ( # Select variable for y-axis selectInput ( inputId = \"y\" , label = \"Y-axis:\" , choices = c ( \"imdb_rating\" , \"imdb_num_votes\" , \"critics_score\" , \"audience_score\" , \"runtime\" ), selected = \"audience_score\" ), # Select variable for x-axis selectInput ( inputId = \"x\" , label = \"X-axis:\" , choices = c ( \"imdb_rating\" , \"imdb_num_votes\" , \"critics_score\" , \"audience_score\" , \"runtime\" ), selected = \"critics_score\" ) ), # Outputs mainPanel ( plotOutput ( outputId = \"scatterplot\" ), plotOutput ( outputId = \"densityplot\" , height = 200 ) ) ) ) # Define server function required to create the scatterplot server <- function ( input , output ) { # Create scatterplot output $ scatterplot <- renderPlot ({ ggplot ( data = movies , aes_string ( x = input $ x , y = input $ y )) + geom_point () }) # Create densityplot output $ densityplot <- renderPlot ({ ggplot ( data = movies , aes_string ( x = input $ x )) + geom_density () }) } # Create the Shiny app object shinyApp ( ui = ui , server = server )","title":"Reactive Flow"},{"location":"R-with-Shiny/input-output/#ui-inputs","text":"Shiny provides a wide selection of input widgets:","title":"UI Inputs"},{"location":"R-with-Shiny/input-output/#checkboxinput","text":"Add a checkbox input to specify whether the data plotted should be shown in a data table. UI : Add an input widget that the user can interact with to check/uncheck the box UI : Add an output defining where the data table should appear Server : Add a reactive expression that creates the data table if the checkbox is checked 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 library ( shiny ) library ( dplyr ) library ( DT ) load ( url ( \"http://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata\" )) n_total <- nrow ( movies ) # Define UI for application that plots features of movies ui <- fluidPage ( # Sidebar layout with a input and output definitions sidebarLayout ( # Inputs sidebarPanel ( # Text instructions HTML ( paste ( \"Enter a value between 1 and\" , n_total )), # Numeric input for sample size numericInput ( inputId = \"n\" , label = \"Sample size:\" , min = 1 , max = n_total , value = 30 , step = 1 ) ), # Output: Show data table mainPanel ( DT :: dataTableOutput ( outputId = \"moviestable\" ) ) ) ) # Define server function required to create the scatterplot server <- function ( input , output ) { # Create data table output $ moviestable <- DT :: renderDataTable ({ req ( input $ n ) movies_sample <- movies %>% sample_n ( input $ n ) %>% select ( title : studio ) DT :: datatable ( data = movies_sample , options = list ( pageLength = 10 ), rownames = FALSE ) }) } # Create a Shiny app object shinyApp ( ui = ui , server = server ) The req() function If you delete the numeric value from the checkbox, you will encounter an error: Error: size is not a numeric or integer vector . In order to avoid such errors, which users of your app could very easily encounter, we need to hold back the output from being calculated if the input is missing. The req function is the simplest and best way to do this, it ensures that values are available (\"truthy\") before proceeding with a calculation or action. If any of the given values is not truthy, the operation is stopped by raising a \"silent\" exception (not logged by Shiny, nor displayed in the Shiny app's UI).","title":"checkboxInput"},{"location":"R-with-Shiny/input-output/#selectinput-multiple-selection","text":"The following app can be used to display movies from selected studios. There are 211 unique studios represented in this dataset, we need a better way to select than to scroll through such a long list, and we address that with the selectize option, which will suggest names of studios as you type them. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 library ( shiny ) library ( ggplot2 ) library ( dplyr ) library ( DT ) load ( url ( \"http://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata\" )) all_studios <- sort ( unique ( movies $ studio )) # UI ui <- fluidPage ( sidebarLayout ( # Input(s) sidebarPanel ( selectInput ( inputId = \"studio\" , label = \"Select studio:\" , choices = all_studios , selected = \"20th Century Fox\" , multiple = TRUE , selectize = TRUE ) ), # Output(s) mainPanel ( DT :: dataTableOutput ( outputId = \"moviestable\" ) ) ) ) # Server server <- function ( input , output ) { # Create data table output $ moviestable <- DT :: renderDataTable ({ req ( input $ studio ) movies_from_selected_studios <- movies %>% filter ( studio %in% input $ studio ) %>% select ( title : studio ) DT :: datatable ( data = movies_from_selected_studios , options = list ( pageLength = 10 ), rownames = FALSE ) }) } # Create a Shiny app object shinyApp ( ui = ui , server = server )","title":"selectInput: Multiple Selection"},{"location":"R-with-Shiny/input-output/#daterangeinput","text":"The following app is coded to show the selected movies between two given dates using dateRangeInput . This input will yield a vector ( input$date ) of length two, the first element is the start date and the second is the end date. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 library ( shiny ) library ( dplyr ) library ( ggplot2 ) load ( url ( \"http://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata\" )) min_date <- min ( movies $ thtr_rel_date ) max_date <- max ( movies $ thtr_rel_date ) # UI ui <- fluidPage ( sidebarLayout ( # Input(s) sidebarPanel ( # Explanatory text HTML ( paste0 ( \"Movies released between the following dates will be plotted. Pick dates between \" , min_date , \" and \" , max_date , \".\" )), # Break for visual separation br (), br (), # Date input dateRangeInput ( inputId = \"date\" , label = \"Select dates:\" , start = \"2013-01-01\" , end = \"2014-01-01\" , startview = \"year\" , min = min_date , max = max_date ) ), # Output(s) mainPanel ( plotOutput ( outputId = \"scatterplot\" ) ) ) ) # Server server <- function ( input , output ) { # Create the plot output $ scatterplot <- renderPlot ({ req ( input $ date ) movies_selected_date <- movies %>% filter ( thtr_rel_date >= as.POSIXct ( input $ date[1] ) & thtr_rel_date <= as.POSIXct ( input $ date[2] )) ggplot ( data = movies_selected_date , aes ( x = critics_score , y = audience_score , color = mpaa_rating )) + geom_point () }) } # Create a Shiny app object shinyApp ( ui = ui , server = server )","title":"dateRangeInput"},{"location":"R-with-Shiny/input-output/#rendering-functions","text":"Shiny provides a wide selection of input widgets, each of which works with a render function:","title":"Rendering Functions"},{"location":"R-with-Shiny/input-output/#rendertable","text":"Add a table beneath the plot displaying summary statistics for a new variable: score_ratio = audience_score / critics_score . Calculate the new variable UI : Add an input widget that the user can interact with to check boxes for selected title types UI : Add an output defining where the summary table should appear Server : Add a reactive expression that creates the summary table 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 ( ... ) # Create new variable: # ratio of critics and audience scores movies <- movies %>% mutate ( score_ratio = audience_score / critics_score ) ( ... ) # Subset for title types checkboxGroupInput ( inputId = \"selected_title_type\" , label = \"Select title type:\" , choices = levels ( movies $ title_type ), selected = levels ( movies $ title_type )) ( ... ) mainPanel ( # Show scatterplot plotOutput ( outputId = \"scatterplot\" ), # Show data table tableOutput ( outputId = \"summarytable\" ) ) ( ... ) output $ summarytable <- renderTable ( { movies %>% filter ( title_type %in% input $ selected_title_type ) %>% group_by ( mpaa_rating ) %>% summarise ( Mean = mean ( score_ratio ), SD = sd ( score_ratio ), n = n ())}, striped = TRUE , spacing = \"l\" , align = \"lccr\" , digits = 4 , width = \"90%\" , caption = \"Score ratio (audience / critics' scores) summary statistics by MPAA rating.\" ) ( ... )","title":"renderTable"},{"location":"R-with-Shiny/input-output/#rendertext","text":"In this app the user selects x and y variables for the scatterplot and also a textOutput which prints the correlation between the two selected variables as well as some informational text. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 library ( shiny ) library ( ggplot2 ) load ( url ( \"http://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata\" )) # UI ui <- fluidPage ( sidebarLayout ( # Input(s) sidebarPanel ( # Select variable for y-axis selectInput ( inputId = \"y\" , label = \"Y-axis:\" , choices = c ( \"imdb_rating\" , \"imdb_num_votes\" , \"critics_score\" , \"audience_score\" , \"runtime\" ), selected = \"audience_score\" ), # Select variable for x-axis selectInput ( inputId = \"x\" , label = \"X-axis:\" , choices = c ( \"imdb_rating\" , \"imdb_num_votes\" , \"critics_score\" , \"audience_score\" , \"runtime\" ), selected = \"critics_score\" ) ), # Outputs mainPanel ( plotOutput ( outputId = \"scatterplot\" ), textOutput ( outputId = \"correlation\" ) ) ) ) # Server server <- function ( input , output ) { # Create scatterplot object the plotOutput function is expecting output $ scatterplot <- renderPlot ({ ggplot ( data = movies , aes_string ( x = input $ x , y = input $ y )) + geom_point () }) # Create text output stating the correlation between the two ploted output $ correlation <- renderText ({ r <- round ( cor ( movies[ , input $ x] , movies[ , input $ y] , use = \"pairwise\" ), 3 ) paste0 ( \"Correlation = \" , r , \". Note: If the relationship between the two variables is not linear, the correlation coefficient will not be meaningful.\" ) }) } # Create a Shiny app object shinyApp ( ui = ui , server = server )","title":"renderText"},{"location":"R-with-Shiny/input-output/#recap-of-outputrendering-functions","text":"Shiny has a variety of render* functions with corresponding *Ourput functions to create and display outputs render* functions can take on multiple arguments, the first being the expression for the desired output The expression in the render* function should be wrapped in curly braces","title":"Recap of Output/Rendering Functions"},{"location":"R-with-Shiny/input-output/#ui-outputs","text":"","title":"UI Outputs"},{"location":"R-with-Shiny/input-output/#plotoutput","text":"Select points on the plot via brushing, and report the selected points in a data table underneath the plot. Brushing means that the user will be able to draw a rectangle in the plotting area and drag it around. UI : Add functionality to plotOutput to select points via brushing UI : Add an output defining where the data table should appear Server : Add a reactive expression that creates the data table for the selected points 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ( ... ) # Show scatterplot with brushing capability plotOutput ( outputId = \"scatterplot\" , brush = \"plot_brush\" ) ( ... ) # Show data table DT :: dataTableOutput ( outputId = \"moviestable\" ) ( ... ) # Print data table output $ moviestable <- DT :: renderDataTable ({ brushedPoints ( movies , input $ plot_brush ) %>% select ( title , audience_score , critics_score ) }) ( ... ) In addition to brushing, users can also interact with plots by hovering over them as in the following example. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 # Load packages library ( shiny ) library ( ggplot2 ) library ( tidyverse ) library ( DT ) # Load data load ( url ( \"http://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata\" )) # Define UI for application that plots features of movies ui <- fluidPage ( br (), # Sidebar layout with a input and output definitions sidebarLayout ( # Inputs sidebarPanel ( # Select variable for y-axis selectInput ( inputId = \"y\" , label = \"Y-axis:\" , choices = c ( \"imdb_rating\" , \"imdb_num_votes\" , \"critics_score\" , \"audience_score\" , \"runtime\" ), selected = \"audience_score\" ), # Select variable for x-axis selectInput ( inputId = \"x\" , label = \"X-axis:\" , choices = c ( \"imdb_rating\" , \"imdb_num_votes\" , \"critics_score\" , \"audience_score\" , \"runtime\" ), selected = \"critics_score\" ) ), # Output: mainPanel ( # Show scatterplot with brushing capability plotOutput ( outputId = \"scatterplot\" , hover = \"plot_hover\" ), # Show data table dataTableOutput ( outputId = \"moviestable\" ), br () ) ) ) # Define server function required to create the scatterplot server <- function ( input , output ) { # Create scatterplot object the plotOutput function is expecting output $ scatterplot <- renderPlot ({ ggplot ( data = movies , aes_string ( x = input $ x , y = input $ y )) + geom_point () }) # Create data table output $ moviestable <- DT :: renderDataTable ({ nearPoints ( movies , input $ plot_hover ) %>% select ( title , audience_score , critics_score ) }) } # Create a Shiny app object shinyApp ( ui = ui , server = server )","title":"plotOutput"},{"location":"R-with-Shiny/input-output/#verbatimtextoutput","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 library ( shiny ) library ( dplyr ) library ( ggplot2 ) load ( url ( \"http://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata\" )) # UI ui <- fluidPage ( sidebarLayout ( # Input(s) sidebarPanel ( # Select variable for y-axis selectInput ( inputId = \"y\" , label = \"Y-axis:\" , choices = c ( \"imdb_rating\" , \"imdb_num_votes\" , \"critics_score\" , \"audience_score\" , \"runtime\" ), selected = \"audience_score\" ), # Select variable for x-axis selectInput ( inputId = \"x\" , label = \"X-axis:\" , choices = c ( \"imdb_rating\" , \"imdb_num_votes\" , \"critics_score\" , \"audience_score\" , \"runtime\" ), selected = \"critics_score\" ) ), # Output(s) mainPanel ( plotOutput ( outputId = \"scatterplot\" ), textOutput ( outputId = \"avg_x\" ), # avg of x textOutput ( outputId = \"avg_y\" ), # avg of y verbatimTextOutput ( outputId = \"lmoutput\" ) # regression output ) ) ) # Server server <- function ( input , output ) { # Create scatterplot output $ scatterplot <- renderPlot ({ ggplot ( data = movies , aes_string ( x = input $ x , y = input $ y )) + geom_point () }) # Calculate average of x output $ avg_x <- renderText ({ avg_x <- movies %>% pull ( input $ x ) %>% mean () %>% round ( 2 ) paste ( \"Average\" , input $ x , \"=\" , avg_x ) }) # Calculate average of y output $ avg_y <- renderText ({ avg_y <- movies %>% pull ( input $ y ) %>% mean () %>% round ( 2 ) paste ( \"Average\" , input $ y , \"=\" , avg_y ) }) # Create regression output output $ lmoutput <- renderPrint ({ x <- movies %>% pull ( input $ x ) y <- movies %>% pull ( input $ y ) summ <- summary ( lm ( y ~ x , data = movies )) print ( summ , digits = 3 , signif.stars = FALSE ) }) } # Create a Shiny app object shinyApp ( ui = ui , server = server )","title":"verbatimtextOutput"},{"location":"R-with-Shiny/input-output/#htmloutput","text":"In the previous example the app reported averages of selected x and y variables as two separate outputs. An alternative approach would be to combine them into a single, multi-line output. For this purpose, in the next example values calculated in app chunk in the paste() command are used to create customized HTML output with specified formatting obtaining the same result. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 library ( shiny ) library ( dplyr ) library ( ggplot2 ) load ( url ( \"http://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata\" )) # UI ui <- fluidPage ( sidebarLayout ( # Input(s) sidebarPanel ( # Select variable for y-axis selectInput ( inputId = \"y\" , label = \"Y-axis:\" , choices = c ( \"imdb_rating\" , \"imdb_num_votes\" , \"critics_score\" , \"audience_score\" , \"runtime\" ), selected = \"audience_score\" ), # Select variable for x-axis selectInput ( inputId = \"x\" , label = \"X-axis:\" , choices = c ( \"imdb_rating\" , \"imdb_num_votes\" , \"critics_score\" , \"audience_score\" , \"runtime\" ), selected = \"critics_score\" ) ), # Output(s) mainPanel ( plotOutput ( outputId = \"scatterplot\" ), htmlOutput ( outputId = \"avgs\" ), verbatimTextOutput ( outputId = \"lmoutput\" ) # regression output ) ) ) # Server server <- function ( input , output ) { # Create scatterplot output $ scatterplot <- renderPlot ({ ggplot ( data = movies , aes_string ( x = input $ x , y = input $ y )) + geom_point () }) # Calculate average of x output $ avgs <- renderUI ({ avg_x <- movies %>% pull ( input $ x ) %>% mean () %>% round ( 2 ) str_x <- paste ( \"Average\" , input $ x , \"=\" , avg_x ) avg_y <- movies %>% pull ( input $ y ) %>% mean () %>% round ( 2 ) str_y <- paste ( \"Average\" , input $ y , \"=\" , avg_y ) HTML ( paste ( str_x , str_y , sep = '<br/>' )) }) # Create regression output output $ lmoutput <- renderPrint ({ x <- movies %>% pull ( input $ x ) y <- movies %>% pull ( input $ y ) print ( summary ( lm ( y ~ x , data = movies )), digits = 3 , signif.stars = FALSE ) }) } # Create a Shiny app object shinyApp ( ui = ui , server = server )","title":"htmlOutput"},{"location":"R-with-Shiny/input-output/#download-data-with-downloadbutton","text":"In this app you get to specify the file type and the variables included in the file you will download. For downloading from a Shiny app we use the downloadHandler function in the server and downloadButton or downloadLink function in the UI. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 library ( shiny ) library ( dplyr ) library ( readr ) load ( url ( \"http://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata\" )) # UI ui <- fluidPage ( sidebarLayout ( # Input(s) sidebarPanel ( # Select filetype radioButtons ( inputId = \"filetype\" , label = \"Select filetype:\" , choices = c ( \"csv\" , \"tsv\" ), selected = \"csv\" ), # Select variables to download checkboxGroupInput ( inputId = \"selected_var\" , label = \"Select variables:\" , choices = names ( movies ), selected = c ( \"title\" )) ), # Output(s) mainPanel ( HTML ( \"Select filetype and variables, then hit 'Download data'.\" ), downloadButton ( \"download_data\" , \"Download data\" ) ) ) ) # Server server <- function ( input , output ) { # Download file output $ download_data <- downloadHandler ( filename = function () { paste0 ( \"movies.\" , input $ filetype ) }, content = function ( file ) { if ( input $ filetype == \"csv\" ){ write_csv ( movies %>% select ( input $ selected_var ), file ) } if ( input $ filetype == \"tsv\" ){ write_tsv ( movies %>% select ( input $ selected_var ), file ) } } ) } # Create a Shiny app object shinyApp ( ui = ui , server = server )","title":"Download data with downloadButton"},{"location":"R-with-Shiny/introduction/","text":"What is Shiny? Link Shiny is an R package that makes it easy to build interactive web apps straight from R. You can host standalone apps on a webpage or embed them in R Markdown documents or build dashboards. You can also extend your Shiny apps with CSS themes, htmlwidgets, and JavaScript actions. Every Shiny app has a webpage that the user visits, and behind this webpage there is a computer that serves this webpage by running R. When running your app locally, the computer serving your app is your own computer. When your app is deployed, the computer serving your app is a web server. General Tips Link Always run the entire script, not just up to the point where you're developing code Sometimes the best way to see what's wrong it is to run the app and review the error Watch out for commas! Anatomy of a Shiny App Link We start by loading any necessary packages one of which is necessarily Shiny (we also load the data before the ui and server definitions so that it can be used in both) Then we lay out the user interface with the UI object that controls the appearance of our app We define the server function that contains instructions needed to build the app We end each Shiny app script with a call to the shinyApp() function that puts these two components together to create the Shiny app object 1 2 3 4 5 6 7 8 9 10 11 library ( shiny ) load ( url ( \"http://your-data\" )) # Define UI for application ui <- fluidPage () # Define server function server <- function ( input , output ) {} # Create a Shiny app object shinyApp ( ui = ui , server = server ) Loading Data Link The first step in the following example is to load the libraries and data to be used. 1 2 3 library ( shiny ) library ( ggplot2 ) load ( url ( \"http://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata\" )) User Interface Link The user interface, that we'll refer to as the UI going forward, defines and lays out the inputs of your app where users can make their selections. It also lays out the outputs. At the outermost layer of out UI definition we begin with the fluidPage function. This function creates a fluid page layout consisting of rows and columns . Rows make sure that elements in them appear on the same line and columns within these rows define how much horizontal space each element should occupy. Fluid pages scale their components in realtime to fill all available browser width, which means the app developer don't need to worry about defininf relative widths for individual app components. We define the layout of our app. Shiny includes a number of options for laying out the components of an application. The default layout is a layout with a sidebar that you can define with the sidebarLayout function. Under the hood, Shiny implements layout features available in Bootstrap 2, which is a popular HTML/CSS framework, so no prior experience with Bootstrap is necessary. We define out sidebar panel that will contain the input controls in the following example. There are two dropdown menus created with the selectInput function. The final component of our UI is the mainPanel . In the example, the main panel contains only one component, a plot output. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 # Define UI for application that plots features of movies ui <- fluidPage ( # Sidebar layout with a input and output definitions sidebarLayout ( # Inputs sidebarPanel ( # Select variable for y-axis selectInput ( inputId = \"y\" , label = \"Y-axis:\" , choices = c ( \"IMDB rating\" = \"imdb_rating\" , \"IMDB number of votes\" = \"imdb_num_votes\" , \"Critics score\" = \"critics_score\" , \"Audience score\" = \"audience_score\" , \"Runtime\" = \"runtime\" ), selected = \"audience_score\" ), # Select variable for x-axis selectInput ( inputId = \"x\" , label = \"X-axis:\" , choices = c ( \"IMDB rating\" = \"imdb_rating\" , \"IMDB number of votes\" = \"imdb_num_votes\" , \"Critics score\" = \"critics_score\" , \"Audience score\" = \"audience_score\" , \"Runtime\" = \"runtime\" ), selected = \"critics_score\" ), # Select variable for color selectInput ( inputId = \"z\" , label = \"Color by:\" , choices = c ( \"Title type\" = \"title_type\" , \"Genre\" = \"genre\" , \"MPAA rating\" = \"mpaa_rating\" , \"Critics rating\" = \"critics_rating\" , \"Audience rating\" = \"audience_rating\" ), selected = \"mpaa_rating\" ) ), # Output mainPanel ( plotOutput ( outputId = \"scatterplot\" ) ) ) ) Check these websites To learn more about various layouts check the Application Layout Guide . Server Function Link The server function calculates outputs and performs any other calculations needed for the outputs. At the outermost layer we define our server function which takes two arguments: an input and an output . Both of these are named lists. The server function accesses inputs selected by the user to perform computations and specifies how outputs laid out in the UI should be updated. The server function can take on one more argument, session , which is an environment that can be used to access information and functionality relating to the session. In the following example of server function has only one output, a plot, so it contains the logic necessary to build this plot. The renderPlot function specifies how the plot output should be updated through some ggplot2 code. The definition of the variables comes from the input list that is built in the UI. 1 2 3 4 5 6 7 8 9 10 # Define server function required to create the scatterplot server <- function ( input , output ) { # Create the scatterplot object the plotOutput function is expecting output $ scatterplot <- renderPlot ({ ggplot ( data = movies , aes_string ( x = input $ x , y = input $ y , color = input $ z )) + geom_point () }) } There are three rules of building server functions: Always save objects to display to the named output list, in other words, something of the form output$plot-to-display Build objects to display with one of the render functions ( render*() ), like we built our plot with renderPlor Use input values from the named input list, with output$plot-to-display Just like various inputs, Shiny also provides a wide selection of output types, each of which works with a render function. render*() function *Output() function DT::renderDataTable() dataTableOutput() renderImage() imageOutput() renderPlot() plotOutput() renderPrint() verbatimTextOutput() renderTable() tableOutput() renderText() textOutput() renderUI() uiOutput() or htmlOutput() Tip It's easy to build interactive applications with Shiny, but to get the most out of it, you'll need to understand the reactive programming scheme used by Shiny: it automatically updates outputs, such as plots, when inputs that go into them change. Building the Shiny app object Link The last component of each Shiny app is a call to the application named shinyApp function, which puts the UI and the server pieces together to create a Shiny app object. 1 2 # Create a Shiny app object shinyApp ( ui = ui , server = server )","title":"Introduction and Shiny Basics"},{"location":"R-with-Shiny/introduction/#what-is-shiny","text":"Shiny is an R package that makes it easy to build interactive web apps straight from R. You can host standalone apps on a webpage or embed them in R Markdown documents or build dashboards. You can also extend your Shiny apps with CSS themes, htmlwidgets, and JavaScript actions. Every Shiny app has a webpage that the user visits, and behind this webpage there is a computer that serves this webpage by running R. When running your app locally, the computer serving your app is your own computer. When your app is deployed, the computer serving your app is a web server.","title":"What is Shiny?"},{"location":"R-with-Shiny/introduction/#general-tips","text":"Always run the entire script, not just up to the point where you're developing code Sometimes the best way to see what's wrong it is to run the app and review the error Watch out for commas!","title":"General Tips"},{"location":"R-with-Shiny/introduction/#anatomy-of-a-shiny-app","text":"We start by loading any necessary packages one of which is necessarily Shiny (we also load the data before the ui and server definitions so that it can be used in both) Then we lay out the user interface with the UI object that controls the appearance of our app We define the server function that contains instructions needed to build the app We end each Shiny app script with a call to the shinyApp() function that puts these two components together to create the Shiny app object 1 2 3 4 5 6 7 8 9 10 11 library ( shiny ) load ( url ( \"http://your-data\" )) # Define UI for application ui <- fluidPage () # Define server function server <- function ( input , output ) {} # Create a Shiny app object shinyApp ( ui = ui , server = server )","title":"Anatomy of a Shiny App"},{"location":"R-with-Shiny/introduction/#loading-data","text":"The first step in the following example is to load the libraries and data to be used. 1 2 3 library ( shiny ) library ( ggplot2 ) load ( url ( \"http://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata\" ))","title":"Loading Data"},{"location":"R-with-Shiny/introduction/#user-interface","text":"The user interface, that we'll refer to as the UI going forward, defines and lays out the inputs of your app where users can make their selections. It also lays out the outputs. At the outermost layer of out UI definition we begin with the fluidPage function. This function creates a fluid page layout consisting of rows and columns . Rows make sure that elements in them appear on the same line and columns within these rows define how much horizontal space each element should occupy. Fluid pages scale their components in realtime to fill all available browser width, which means the app developer don't need to worry about defininf relative widths for individual app components. We define the layout of our app. Shiny includes a number of options for laying out the components of an application. The default layout is a layout with a sidebar that you can define with the sidebarLayout function. Under the hood, Shiny implements layout features available in Bootstrap 2, which is a popular HTML/CSS framework, so no prior experience with Bootstrap is necessary. We define out sidebar panel that will contain the input controls in the following example. There are two dropdown menus created with the selectInput function. The final component of our UI is the mainPanel . In the example, the main panel contains only one component, a plot output. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 # Define UI for application that plots features of movies ui <- fluidPage ( # Sidebar layout with a input and output definitions sidebarLayout ( # Inputs sidebarPanel ( # Select variable for y-axis selectInput ( inputId = \"y\" , label = \"Y-axis:\" , choices = c ( \"IMDB rating\" = \"imdb_rating\" , \"IMDB number of votes\" = \"imdb_num_votes\" , \"Critics score\" = \"critics_score\" , \"Audience score\" = \"audience_score\" , \"Runtime\" = \"runtime\" ), selected = \"audience_score\" ), # Select variable for x-axis selectInput ( inputId = \"x\" , label = \"X-axis:\" , choices = c ( \"IMDB rating\" = \"imdb_rating\" , \"IMDB number of votes\" = \"imdb_num_votes\" , \"Critics score\" = \"critics_score\" , \"Audience score\" = \"audience_score\" , \"Runtime\" = \"runtime\" ), selected = \"critics_score\" ), # Select variable for color selectInput ( inputId = \"z\" , label = \"Color by:\" , choices = c ( \"Title type\" = \"title_type\" , \"Genre\" = \"genre\" , \"MPAA rating\" = \"mpaa_rating\" , \"Critics rating\" = \"critics_rating\" , \"Audience rating\" = \"audience_rating\" ), selected = \"mpaa_rating\" ) ), # Output mainPanel ( plotOutput ( outputId = \"scatterplot\" ) ) ) ) Check these websites To learn more about various layouts check the Application Layout Guide .","title":"User Interface"},{"location":"R-with-Shiny/introduction/#server-function","text":"The server function calculates outputs and performs any other calculations needed for the outputs. At the outermost layer we define our server function which takes two arguments: an input and an output . Both of these are named lists. The server function accesses inputs selected by the user to perform computations and specifies how outputs laid out in the UI should be updated. The server function can take on one more argument, session , which is an environment that can be used to access information and functionality relating to the session. In the following example of server function has only one output, a plot, so it contains the logic necessary to build this plot. The renderPlot function specifies how the plot output should be updated through some ggplot2 code. The definition of the variables comes from the input list that is built in the UI. 1 2 3 4 5 6 7 8 9 10 # Define server function required to create the scatterplot server <- function ( input , output ) { # Create the scatterplot object the plotOutput function is expecting output $ scatterplot <- renderPlot ({ ggplot ( data = movies , aes_string ( x = input $ x , y = input $ y , color = input $ z )) + geom_point () }) } There are three rules of building server functions: Always save objects to display to the named output list, in other words, something of the form output$plot-to-display Build objects to display with one of the render functions ( render*() ), like we built our plot with renderPlor Use input values from the named input list, with output$plot-to-display Just like various inputs, Shiny also provides a wide selection of output types, each of which works with a render function. render*() function *Output() function DT::renderDataTable() dataTableOutput() renderImage() imageOutput() renderPlot() plotOutput() renderPrint() verbatimTextOutput() renderTable() tableOutput() renderText() textOutput() renderUI() uiOutput() or htmlOutput() Tip It's easy to build interactive applications with Shiny, but to get the most out of it, you'll need to understand the reactive programming scheme used by Shiny: it automatically updates outputs, such as plots, when inputs that go into them change.","title":"Server Function"},{"location":"R-with-Shiny/introduction/#building-the-shiny-app-object","text":"The last component of each Shiny app is a call to the application named shinyApp function, which puts the UI and the server pieces together to create a Shiny app object. 1 2 # Create a Shiny app object shinyApp ( ui = ui , server = server )","title":"Building the Shiny app object"},{"location":"R-with-Shiny/reactive/","text":"Reactive Elements Link There are three kinds of objects in reactive programming: Reactive Sources : user input that comes through a browser interface, typically. Reactive Endpoints : something that appears in the user's browser window, such as a plot or a table of values. A reactive source can be connected to multiple endpoints, and vice versa. Reactive Conductors : reactive component between a source and an endpoint. It can be a dependetn (child) and have dependents (parent) while sources can only be parents and endpoints can only be children We can create a reactive data set using the reactive() function which creates a cached expression that knows it is out of date when input changes. Remember to check the availability of the predefined input with the req() function before doing any calculations that depends on it and surround the expression with curly braces. When you refer to a reactive data set you need to use parentheses after its name, that is, a cached expression, meaning that it only rerun when its inputs change. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 library ( shiny ) library ( dplyr ) library ( readr ) load ( url ( \"http://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata\" )) # UI ui <- fluidPage ( sidebarLayout ( # Input(s) sidebarPanel ( # Select filetype radioButtons ( inputId = \"filetype\" , label = \"Select filetype:\" , choices = c ( \"csv\" , \"tsv\" ), selected = \"csv\" ), # Select variables to download checkboxGroupInput ( inputId = \"selected_var\" , label = \"Select variables:\" , choices = names ( movies ), selected = c ( \"title\" )) ), # Output(s) mainPanel ( DT :: dataTableOutput ( outputId = \"moviestable\" ), downloadButton ( \"download_data\" , \"Download data\" ) ) ) ) # Server server <- function ( input , output ) { # Create reactive data frame movies_selected <- reactive ({ req ( input $ selected_var ) movies %>% select ( input $ selected_var ) }) # Create data table output $ moviestable <- DT :: renderDataTable ({ req ( input $ selected_var ) DT :: datatable ( data = movies_selected (), options = list ( pageLength = 10 ), rownames = FALSE ) }) # Download file output $ download_data <- downloadHandler ( filename = function () { paste0 ( \"movies.\" , input $ filetype ) }, content = function ( file ) { if ( input $ filetype == \"csv\" ){ write_csv ( movies_selected (), file ) } if ( input $ filetype == \"tsv\" ){ write_tsv ( movies_selected (), file ) } } ) } # Create a Shiny app object shinyApp ( ui = ui , server = server ) Tip The obvious choice for creating a text output would be renderText but if you want to get a little fancier including some HTML to use some text decoration, like bolding and line breaks in the text output, we need a rendering function that generates HTML, which is renderUI . Why Using Reactives? Link By using a reactive expression for the subsetted data frame, we were able to get away with subsetting once and then using the result twice. In general, reactive conductors let you not repeat yourself (i.e. avoid copy-and-paste code) and decompose large, complex calculations into smaller pieces to make them more understandable. This benefits are similar to decomposing a large complex R script into a series of small functions that build on each other. Functions vs Reactives Link Each time you call a function, R will revaluate it. However, reactive expressions are lazy, they only get executed when their input changes. This means that even if you call a reactive expression multiple times, it only re-executes when its input(s) change(s). Using many reactive expressions in your app can create a complicated dependency structure. The reactlog is a graphical representation of this dependency structure, and it also gives you very detailed information about what's happening under the hood as Shiny evaluates your application. To view the reactlog : In a fresh R session and run options(shiny.reactlog = TRUE) Then, launch your app as you normally would In the app, pres Ctrl+F3 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 library ( shiny ) library ( ggplot2 ) library ( dplyr ) library ( tools ) load ( url ( \"http://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata\" )) # Define UI for application that plots features of movies ui <- fluidPage ( # Application title titlePanel ( \"Movie browser\" ), # Sidebar layout with a input and output definitions sidebarLayout ( # Inputs(s) sidebarPanel ( # Select variable for y-axis selectInput ( inputId = \"y\" , label = \"Y-axis:\" , choices = c ( \"IMDB rating\" = \"imdb_rating\" , \"IMDB number of votes\" = \"imdb_num_votes\" , \"Critics Score\" = \"critics_score\" , \"Audience Score\" = \"audience_score\" , \"Runtime\" = \"runtime\" ), selected = \"audience_score\" ), # Select variable for x-axis selectInput ( inputId = \"x\" , label = \"X-axis:\" , choices = c ( \"IMDB rating\" = \"imdb_rating\" , \"IMDB number of votes\" = \"imdb_num_votes\" , \"Critics Score\" = \"critics_score\" , \"Audience Score\" = \"audience_score\" , \"Runtime\" = \"runtime\" ), selected = \"critics_score\" ), # Select variable for color selectInput ( inputId = \"z\" , label = \"Color by:\" , choices = c ( \"Title Type\" = \"title_type\" , \"Genre\" = \"genre\" , \"MPAA Rating\" = \"mpaa_rating\" , \"Critics Rating\" = \"critics_rating\" , \"Audience Rating\" = \"audience_rating\" ), selected = \"mpaa_rating\" ), # Enter text for plot title textInput ( inputId = \"plot_title\" , label = \"Plot title\" , placeholder = \"Enter text for plot title\" ), # Select which types of movies to plot checkboxGroupInput ( inputId = \"selected_type\" , label = \"Select movie type(s):\" , choices = c ( \"Documentary\" , \"Feature Film\" , \"TV Movie\" ), selected = \"Feature Film\" ) ), # Output(s) mainPanel ( plotOutput ( outputId = \"scatterplot\" ), textOutput ( outputId = \"description\" ) ) ) ) # Server server <- function ( input , output ) { # Create a subset of data filtering for selected title types movies_subset <- reactive ({ req ( input $ selected_type ) filter ( movies , title_type %in% input $ selected_type ) }) # Convert plot_title toTitleCase pretty_plot_title <- reactive ({ toTitleCase ( input $ plot_title ) }) # Create scatterplot object the plotOutput function is expecting output $ scatterplot <- renderPlot ({ ggplot ( data = movies_subset (), aes_string ( x = input $ x , y = input $ y , color = input $ z )) + geom_point () + labs ( title = pretty_plot_title ()) }) # Create descriptive text output $ description <- renderText ({ paste0 ( \"The plot above titled '\" , pretty_plot_title (), \"' visualizes the relationship between \" , input $ x , \" and \" , input $ y , \", conditional on \" , input $ z , \".\" ) }) } # Create the Shiny app object shinyApp ( ui = ui , server = server ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 library ( shiny ) library ( ggplot2 ) library ( dplyr ) load ( url ( \"http://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata\" )) # UI ui <- fluidPage ( sidebarLayout ( # Input(s) sidebarPanel ( # Select variable for y-axis selectInput ( inputId = \"y\" , label = \"Y-axis:\" , choices = c ( \"IMDB rating\" = \"imdb_rating\" , \"IMDB number of votes\" = \"imdb_num_votes\" , \"Critics Score\" = \"critics_score\" , \"Audience Score\" = \"audience_score\" , \"Runtime\" = \"runtime\" ), selected = \"audience_score\" ), # Select variable for x-axis selectInput ( inputId = \"x\" , label = \"X-axis:\" , choices = c ( \"IMDB rating\" = \"imdb_rating\" , \"IMDB number of votes\" = \"imdb_num_votes\" , \"Critics Score\" = \"critics_score\" , \"Audience Score\" = \"audience_score\" , \"Runtime\" = \"runtime\" ), selected = \"critics_score\" ), # Select variable for color selectInput ( inputId = \"z\" , label = \"Color by:\" , choices = c ( \"Title Type\" = \"title_type\" , \"Genre\" = \"genre\" , \"MPAA Rating\" = \"mpaa_rating\" , \"Critics Rating\" = \"critics_rating\" , \"Audience Rating\" = \"audience_rating\" ), selected = \"mpaa_rating\" ), # Select which types of movies to plot checkboxGroupInput ( inputId = \"selected_type\" , label = \"Select movie type(s):\" , choices = c ( \"Documentary\" , \"Feature Film\" , \"TV Movie\" ), selected = \"Feature Film\" ), # Select sample size numericInput ( inputId = \"n_samp\" , label = \"Sample size:\" , min = 1 , max = nrow ( movies ), value = 3 ) ), # Output(s) mainPanel ( plotOutput ( outputId = \"scatterplot\" ), uiOutput ( outputId = \"n\" ) ) ) ) # Server server <- function ( input , output ) { # Create a subset of data filtering for selected title types movies_subset <- reactive ({ req ( input $ selected_type ) filter ( movies , title_type %in% input $ selected_type ) }) # Create new df that is n_samp obs from selected type movies movies_sample <- reactive ({ req ( input $ n_samp ) sample_n ( movies_subset (), input $ n_samp ) }) # Create scatterplot object the plotOutput function is expecting output $ scatterplot <- renderPlot ({ ggplot ( data = movies_sample (), aes_string ( x = input $ x , y = input $ y , color = input $ z )) + geom_point () }) # Print number of movies plotted output $ n <- renderUI ({ types <- movies_sample () $ title_type %>% factor ( levels = input $ selected_type ) counts <- table ( types ) HTML ( paste ( \"There are\" , counts , input $ selected_type , \"movies plotted in the plot above. <br>\" )) }) } # Create a Shiny app object shinyApp ( ui = ui , server = server ) Reactives and observers Link Here we discuss implementations of the three different types of reactive objects. As we go through the different implementations, it's recommended to think back to where they appear on the reactive flow chart. Implementation of Reactive Sources : An implementation of reactive sources is reactiveValues() . One example of this is user inputs. The input object is a reactive value that looks like a list and contains many individual reactive values that are set by input from the web browser. Implementation of Reactive : The implementation of reactive conductors is a reactive() expression that you can create with the reactive function. An example is the reactive data frame subsets created in the previous example. Reactive expressions can access reactive values or other reactive expressions and they return a value. They are useful for caching the results of any procedure that happens in response to user input. Implementation of Reactive : The implementation of reactive endpoints is observe() . For example, an output object is a reactive observer. Actually, under the hood, a render function returns a reactive expression, and when you assing this reactive expression to an output value, Shiny automatically creates an observer that uses the reactive expression. Observers can access reactive sources and reactive expressions, but they don't return a value. Instead they are used for their side effects, which typically involves sending data to the web browser. Reactives vs Observers Link Similarities: they both store expressions that can be executed Differences: Reactive expressions return values, but observers don't Observers (and endpoints in general) eagerly respond to changes in their dependences, but reactive expressions (and conductors in general) do not Reactive expressions must not have side effects, while observers are only useful for their side effects Most importantly: The reactive() function is used when calculating values, without side effects The observe() function is used to perform actions, with side effects Do not use an observe() function when calculating a value, and especially don't use reactive() for performing actions with side effects reactive() observe() Purpose Calculations Actions Side effects Forbidden Allowed Stop-Trigger-Delay Link Isolating Reactions Link Suppose your app has an input widget where users can enter text for the title of the plot. However, you only want the title to update if any ot the other inputs that go into the plot change. You can achieve this by isolating the plot title such that when input$x or input$y changes, the plot, along with the title, will update. But when only the title input changes, the plot will not update. 1 2 3 4 5 output $ scatterplot <- renderPlot ({ ggplot ( data = movies_subset (), aes_string ( x = input $ x , y = input $ y )) + geom_point () + labs ( title = isolate ({ input $ plot_title }) ) }) isolate() is then used to stop a reaction. Triggering Reactions Link Why might one want to explicitly trigger a reaction? Somethimes you might want to wait for a specific action to be taken from the user, like clicking an actionButton , before calculating an expression or taking an action. A reactive value or expression that is used to trigger other calculations in this way is called an event . These events can be the first argument in the observeEvent function. This arguments can be a simple reactive value like an input, a call to a reactive expression, or a complex expresion provided wrapped in curly braces. The second argument is the expression to call whenever the first argument is invalidated. This is similar to saying if event expression happens, call handler expression. 1 observeEvent ( eventExpr , handlerExpr , ... ) Suppose your app allows for taking a random sample of the data based on a sample size numeric input. Suppose also that you want to add functionality for the userd to download the random sample they generated if they press an action button requesting to do so. In the UI we create an action button and in the server we condition the observeEvent on the inputId of that action button. This way R knows to call the expression given in the second argument of observeEvent when the user presses the action button. And finally we can delay reactions with eventReactive , which takes similar arguments as observeEvent . 1 2 3 4 5 6 7 8 9 10 # UI actionButton ( inputId = \"write_csv\" , label = \"Write CSV\" ) # Server observeEvent ( input $ write_csv , { filename <- paste0 ( \"movies_\" , str_replace_all ( Sys.time (), \":|\\ \" , \"_\" ), \".csv\" ) write_csv ( movies_sample (), path = filename ) } Suppose your goal is to change how users take random samples in your app (you only want them to get a new sample when an action button that says \"get new sample\" is pressed, not when other things like numeric input defining the size of the sample changes). In the event reactive function, the first argument is the input associated with the action button and the second argument is the sampling code. Then we add one more argument, ignoreNull , which tells R what to do (or what not to do) when the event expression evaluates to Null. For example, what should the app do when the app is first launched and the user has not even interacted with the app yet? If this is set to FALSE, the app will initially perform the action or calculation and then the user can re-initiate it. 1 2 3 4 5 6 7 8 9 10 # UI actionButton ( inputId = \"get_new_sample\" , label = \"Get new sample\" ) # Server movies_sample <- eventReactive ( input $ get_new_sample , { req ( input $ n_samp ) sample_n ( movies_subset (), input $ n_samp ) }, ignoreNULL = FALSE ) observeEvent() and eventReactive() look and feel very similar. They have the same syntax, same arguments, but they're actually not the same at all! observeEvent() is to perform an action in response to an event eventReactive() is used to create a calculated value that only updates in response to an event This pair of functions also seem similar to the observe/reactive pair, however, the main differences between them is that observe() and reactive() functions automatically trigger on whatever they access while observeEvent() and eventReactive() functions need to be explicitly told what triggers them. In the following example isolate() is used to prevent the plot title to be updated until some of the other inputs are updated: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 library ( shiny ) library ( ggplot2 ) library ( tools ) load ( url ( \"http://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata\" )) # UI ui <- fluidPage ( sidebarLayout ( # Input sidebarPanel ( # Select variable for y-axis selectInput ( inputId = \"y\" , label = \"Y-axis:\" , choices = c ( \"IMDB rating\" = \"imdb_rating\" , \"IMDB number of votes\" = \"imdb_num_votes\" , \"Critics Score\" = \"critics_score\" , \"Audience Score\" = \"audience_score\" , \"Runtime\" = \"runtime\" ), selected = \"audience_score\" ), # Select variable for x-axis selectInput ( inputId = \"x\" , label = \"X-axis:\" , choices = c ( \"IMDB rating\" = \"imdb_rating\" , \"IMDB number of votes\" = \"imdb_num_votes\" , \"Critics Score\" = \"critics_score\" , \"Audience Score\" = \"audience_score\" , \"Runtime\" = \"runtime\" ), selected = \"critics_score\" ), # Select variable for color selectInput ( inputId = \"z\" , label = \"Color by:\" , choices = c ( \"Title Type\" = \"title_type\" , \"Genre\" = \"genre\" , \"MPAA Rating\" = \"mpaa_rating\" , \"Critics Rating\" = \"critics_rating\" , \"Audience Rating\" = \"audience_rating\" ), selected = \"mpaa_rating\" ), # Set alpha level sliderInput ( inputId = \"alpha\" , label = \"Alpha:\" , min = 0 , max = 1 , value = 0.5 ), # Set point size sliderInput ( inputId = \"size\" , label = \"Size:\" , min = 0 , max = 5 , value = 2 ), # Enter text for plot title textInput ( inputId = \"plot_title\" , label = \"Plot title\" , placeholder = \"Enter text to be used as plot title\" ) ), # Output: mainPanel ( plotOutput ( outputId = \"scatterplot\" ) ) ) ) # Define server function required to create the scatterplot- server <- function ( input , output , session ) { # Create scatterplot object the plotOutput function is expecting output $ scatterplot <- renderPlot ({ ggplot ( data = movies , aes_string ( x = input $ x , y = input $ y , color = input $ z )) + geom_point ( alpha = input $ alpha , size = input $ size ) + labs ( title = isolate ({ toTitleCase ( input $ plot_title )}) ) }) } # Create a Shiny app object shinyApp ( ui = ui , server = server ) In the following example an eventReactive() function is used to wait until an actionButton() is clicked to update the plot title: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 library ( shiny ) library ( ggplot2 ) library ( tools ) load ( url ( \"http://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata\" )) # UI ui <- fluidPage ( sidebarLayout ( # Input sidebarPanel ( # Select variable for y-axis selectInput ( inputId = \"y\" , label = \"Y-axis:\" , choices = c ( \"IMDB rating\" = \"imdb_rating\" , \"IMDB number of votes\" = \"imdb_num_votes\" , \"Critics Score\" = \"critics_score\" , \"Audience Score\" = \"audience_score\" , \"Runtime\" = \"runtime\" ), selected = \"audience_score\" ), # Select variable for x-axis selectInput ( inputId = \"x\" , label = \"X-axis:\" , choices = c ( \"IMDB rating\" = \"imdb_rating\" , \"IMDB number of votes\" = \"imdb_num_votes\" , \"Critics Score\" = \"critics_score\" , \"Audience Score\" = \"audience_score\" , \"Runtime\" = \"runtime\" ), selected = \"critics_score\" ), # Select variable for color selectInput ( inputId = \"z\" , label = \"Color by:\" , choices = c ( \"Title Type\" = \"title_type\" , \"Genre\" = \"genre\" , \"MPAA Rating\" = \"mpaa_rating\" , \"Critics Rating\" = \"critics_rating\" , \"Audience Rating\" = \"audience_rating\" ), selected = \"mpaa_rating\" ), # Set alpha level sliderInput ( inputId = \"alpha\" , label = \"Alpha:\" , min = 0 , max = 1 , value = 0.5 ), # Set point size sliderInput ( inputId = \"size\" , label = \"Size:\" , min = 0 , max = 5 , value = 2 ), # Enter text for plot title textInput ( inputId = \"plot_title\" , label = \"Plot title\" , placeholder = \"Enter text to be used as plot title\" ), # Action button for plot title actionButton ( inputId = \"update_plot_title\" , label = \"Update plot title\" ) ), # Output: mainPanel ( plotOutput ( outputId = \"scatterplot\" ) ) ) ) # Define server function required to create the scatterplot- server <- function ( input , output , session ) { # New plot title new_plot_title <- eventReactive ( input $ update_plot_title , { toTitleCase ( input $ plot_title ) } ) # Create scatterplot object the plotOutput function is expecting output $ scatterplot <- renderPlot ({ ggplot ( data = movies , aes_string ( x = input $ x , y = input $ y , color = input $ z )) + geom_point ( alpha = input $ alpha , size = input $ size ) + labs ( title = new_plot_title () ) }) } # Create a Shiny app object shinyApp ( ui = ui , server = server ) In the following example, observeEvent() is used to make things happen when an action button is clicked: a message is printed to the console stating how many records are shown and a table is generated with those records. The table output is only printed when action button is clicked, but not when other inputs that go into the creation of that output changes thanks to the isolate() function. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 library ( shiny ) load ( url ( \"http://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata\" )) # UI ui <- fluidPage ( sidebarLayout ( # Input sidebarPanel ( # Numeric input for number of rows to show numericInput ( inputId = \"n_rows\" , label = \"How many rows do you want to see?\" , value = 10 ), # Action button to show actionButton ( inputId = \"button\" , label = \"Show\" ) ), # Output: mainPanel ( tableOutput ( outputId = \"datatable\" ) ) ) ) # Define server function required to create the scatterplot- server <- function ( input , output , session ) { # Print a message to the console every time button is pressed observeEvent ( input $ button , { cat ( \"Showing\" , input $ n_rows , \"rows\\n\" ) }) # Take a reactive dependency on input$button, # but not on any of the stuff inside the function df <- eventReactive ( input $ button , { head ( movies , input $ n_rows ) }) output $ datatable <- renderTable ({ df () }) } # Create a Shiny app object shinyApp ( ui = ui , server = server ) Reactivity Summary Link Reactives are equivalent to no argument functions. Think about them as functions, think about them as variables that can depend on user input and other reactives. Reactives are for reactive values and expressions, observers are for their side effects. Do not define a reactive() inside a render*() function. Be careful with missing parentheses when calling reactive expressions. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 library ( shiny ) ui <- fluidPage ( titlePanel ( \"Add 2\" ), sidebarLayout ( sidebarPanel ( sliderInput ( \"x\" , \"Select x\" , min = 1 , max = 50 , value = 30 ) ), mainPanel ( textOutput ( \"x_updated\" ) ) ) ) add_2 <- function ( x ) { x + 2 } server <- function ( input , output ) { current_x <- reactive ({ add_2 ( input $ x )}) output $ x_updated <- renderText ({ current_x () }) } shinyApp ( ui , server )","title":"Reactive Programming"},{"location":"R-with-Shiny/reactive/#reactive-elements","text":"There are three kinds of objects in reactive programming: Reactive Sources : user input that comes through a browser interface, typically. Reactive Endpoints : something that appears in the user's browser window, such as a plot or a table of values. A reactive source can be connected to multiple endpoints, and vice versa. Reactive Conductors : reactive component between a source and an endpoint. It can be a dependetn (child) and have dependents (parent) while sources can only be parents and endpoints can only be children We can create a reactive data set using the reactive() function which creates a cached expression that knows it is out of date when input changes. Remember to check the availability of the predefined input with the req() function before doing any calculations that depends on it and surround the expression with curly braces. When you refer to a reactive data set you need to use parentheses after its name, that is, a cached expression, meaning that it only rerun when its inputs change. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 library ( shiny ) library ( dplyr ) library ( readr ) load ( url ( \"http://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata\" )) # UI ui <- fluidPage ( sidebarLayout ( # Input(s) sidebarPanel ( # Select filetype radioButtons ( inputId = \"filetype\" , label = \"Select filetype:\" , choices = c ( \"csv\" , \"tsv\" ), selected = \"csv\" ), # Select variables to download checkboxGroupInput ( inputId = \"selected_var\" , label = \"Select variables:\" , choices = names ( movies ), selected = c ( \"title\" )) ), # Output(s) mainPanel ( DT :: dataTableOutput ( outputId = \"moviestable\" ), downloadButton ( \"download_data\" , \"Download data\" ) ) ) ) # Server server <- function ( input , output ) { # Create reactive data frame movies_selected <- reactive ({ req ( input $ selected_var ) movies %>% select ( input $ selected_var ) }) # Create data table output $ moviestable <- DT :: renderDataTable ({ req ( input $ selected_var ) DT :: datatable ( data = movies_selected (), options = list ( pageLength = 10 ), rownames = FALSE ) }) # Download file output $ download_data <- downloadHandler ( filename = function () { paste0 ( \"movies.\" , input $ filetype ) }, content = function ( file ) { if ( input $ filetype == \"csv\" ){ write_csv ( movies_selected (), file ) } if ( input $ filetype == \"tsv\" ){ write_tsv ( movies_selected (), file ) } } ) } # Create a Shiny app object shinyApp ( ui = ui , server = server ) Tip The obvious choice for creating a text output would be renderText but if you want to get a little fancier including some HTML to use some text decoration, like bolding and line breaks in the text output, we need a rendering function that generates HTML, which is renderUI .","title":"Reactive Elements"},{"location":"R-with-Shiny/reactive/#why-using-reactives","text":"By using a reactive expression for the subsetted data frame, we were able to get away with subsetting once and then using the result twice. In general, reactive conductors let you not repeat yourself (i.e. avoid copy-and-paste code) and decompose large, complex calculations into smaller pieces to make them more understandable. This benefits are similar to decomposing a large complex R script into a series of small functions that build on each other.","title":"Why Using Reactives?"},{"location":"R-with-Shiny/reactive/#functions-vs-reactives","text":"Each time you call a function, R will revaluate it. However, reactive expressions are lazy, they only get executed when their input changes. This means that even if you call a reactive expression multiple times, it only re-executes when its input(s) change(s). Using many reactive expressions in your app can create a complicated dependency structure. The reactlog is a graphical representation of this dependency structure, and it also gives you very detailed information about what's happening under the hood as Shiny evaluates your application. To view the reactlog : In a fresh R session and run options(shiny.reactlog = TRUE) Then, launch your app as you normally would In the app, pres Ctrl+F3 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 library ( shiny ) library ( ggplot2 ) library ( dplyr ) library ( tools ) load ( url ( \"http://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata\" )) # Define UI for application that plots features of movies ui <- fluidPage ( # Application title titlePanel ( \"Movie browser\" ), # Sidebar layout with a input and output definitions sidebarLayout ( # Inputs(s) sidebarPanel ( # Select variable for y-axis selectInput ( inputId = \"y\" , label = \"Y-axis:\" , choices = c ( \"IMDB rating\" = \"imdb_rating\" , \"IMDB number of votes\" = \"imdb_num_votes\" , \"Critics Score\" = \"critics_score\" , \"Audience Score\" = \"audience_score\" , \"Runtime\" = \"runtime\" ), selected = \"audience_score\" ), # Select variable for x-axis selectInput ( inputId = \"x\" , label = \"X-axis:\" , choices = c ( \"IMDB rating\" = \"imdb_rating\" , \"IMDB number of votes\" = \"imdb_num_votes\" , \"Critics Score\" = \"critics_score\" , \"Audience Score\" = \"audience_score\" , \"Runtime\" = \"runtime\" ), selected = \"critics_score\" ), # Select variable for color selectInput ( inputId = \"z\" , label = \"Color by:\" , choices = c ( \"Title Type\" = \"title_type\" , \"Genre\" = \"genre\" , \"MPAA Rating\" = \"mpaa_rating\" , \"Critics Rating\" = \"critics_rating\" , \"Audience Rating\" = \"audience_rating\" ), selected = \"mpaa_rating\" ), # Enter text for plot title textInput ( inputId = \"plot_title\" , label = \"Plot title\" , placeholder = \"Enter text for plot title\" ), # Select which types of movies to plot checkboxGroupInput ( inputId = \"selected_type\" , label = \"Select movie type(s):\" , choices = c ( \"Documentary\" , \"Feature Film\" , \"TV Movie\" ), selected = \"Feature Film\" ) ), # Output(s) mainPanel ( plotOutput ( outputId = \"scatterplot\" ), textOutput ( outputId = \"description\" ) ) ) ) # Server server <- function ( input , output ) { # Create a subset of data filtering for selected title types movies_subset <- reactive ({ req ( input $ selected_type ) filter ( movies , title_type %in% input $ selected_type ) }) # Convert plot_title toTitleCase pretty_plot_title <- reactive ({ toTitleCase ( input $ plot_title ) }) # Create scatterplot object the plotOutput function is expecting output $ scatterplot <- renderPlot ({ ggplot ( data = movies_subset (), aes_string ( x = input $ x , y = input $ y , color = input $ z )) + geom_point () + labs ( title = pretty_plot_title ()) }) # Create descriptive text output $ description <- renderText ({ paste0 ( \"The plot above titled '\" , pretty_plot_title (), \"' visualizes the relationship between \" , input $ x , \" and \" , input $ y , \", conditional on \" , input $ z , \".\" ) }) } # Create the Shiny app object shinyApp ( ui = ui , server = server ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 library ( shiny ) library ( ggplot2 ) library ( dplyr ) load ( url ( \"http://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata\" )) # UI ui <- fluidPage ( sidebarLayout ( # Input(s) sidebarPanel ( # Select variable for y-axis selectInput ( inputId = \"y\" , label = \"Y-axis:\" , choices = c ( \"IMDB rating\" = \"imdb_rating\" , \"IMDB number of votes\" = \"imdb_num_votes\" , \"Critics Score\" = \"critics_score\" , \"Audience Score\" = \"audience_score\" , \"Runtime\" = \"runtime\" ), selected = \"audience_score\" ), # Select variable for x-axis selectInput ( inputId = \"x\" , label = \"X-axis:\" , choices = c ( \"IMDB rating\" = \"imdb_rating\" , \"IMDB number of votes\" = \"imdb_num_votes\" , \"Critics Score\" = \"critics_score\" , \"Audience Score\" = \"audience_score\" , \"Runtime\" = \"runtime\" ), selected = \"critics_score\" ), # Select variable for color selectInput ( inputId = \"z\" , label = \"Color by:\" , choices = c ( \"Title Type\" = \"title_type\" , \"Genre\" = \"genre\" , \"MPAA Rating\" = \"mpaa_rating\" , \"Critics Rating\" = \"critics_rating\" , \"Audience Rating\" = \"audience_rating\" ), selected = \"mpaa_rating\" ), # Select which types of movies to plot checkboxGroupInput ( inputId = \"selected_type\" , label = \"Select movie type(s):\" , choices = c ( \"Documentary\" , \"Feature Film\" , \"TV Movie\" ), selected = \"Feature Film\" ), # Select sample size numericInput ( inputId = \"n_samp\" , label = \"Sample size:\" , min = 1 , max = nrow ( movies ), value = 3 ) ), # Output(s) mainPanel ( plotOutput ( outputId = \"scatterplot\" ), uiOutput ( outputId = \"n\" ) ) ) ) # Server server <- function ( input , output ) { # Create a subset of data filtering for selected title types movies_subset <- reactive ({ req ( input $ selected_type ) filter ( movies , title_type %in% input $ selected_type ) }) # Create new df that is n_samp obs from selected type movies movies_sample <- reactive ({ req ( input $ n_samp ) sample_n ( movies_subset (), input $ n_samp ) }) # Create scatterplot object the plotOutput function is expecting output $ scatterplot <- renderPlot ({ ggplot ( data = movies_sample (), aes_string ( x = input $ x , y = input $ y , color = input $ z )) + geom_point () }) # Print number of movies plotted output $ n <- renderUI ({ types <- movies_sample () $ title_type %>% factor ( levels = input $ selected_type ) counts <- table ( types ) HTML ( paste ( \"There are\" , counts , input $ selected_type , \"movies plotted in the plot above. <br>\" )) }) } # Create a Shiny app object shinyApp ( ui = ui , server = server )","title":"Functions vs Reactives"},{"location":"R-with-Shiny/reactive/#reactives-and-observers","text":"Here we discuss implementations of the three different types of reactive objects. As we go through the different implementations, it's recommended to think back to where they appear on the reactive flow chart. Implementation of Reactive Sources : An implementation of reactive sources is reactiveValues() . One example of this is user inputs. The input object is a reactive value that looks like a list and contains many individual reactive values that are set by input from the web browser. Implementation of Reactive : The implementation of reactive conductors is a reactive() expression that you can create with the reactive function. An example is the reactive data frame subsets created in the previous example. Reactive expressions can access reactive values or other reactive expressions and they return a value. They are useful for caching the results of any procedure that happens in response to user input. Implementation of Reactive : The implementation of reactive endpoints is observe() . For example, an output object is a reactive observer. Actually, under the hood, a render function returns a reactive expression, and when you assing this reactive expression to an output value, Shiny automatically creates an observer that uses the reactive expression. Observers can access reactive sources and reactive expressions, but they don't return a value. Instead they are used for their side effects, which typically involves sending data to the web browser.","title":"Reactives and observers"},{"location":"R-with-Shiny/reactive/#reactives-vs-observers","text":"Similarities: they both store expressions that can be executed Differences: Reactive expressions return values, but observers don't Observers (and endpoints in general) eagerly respond to changes in their dependences, but reactive expressions (and conductors in general) do not Reactive expressions must not have side effects, while observers are only useful for their side effects Most importantly: The reactive() function is used when calculating values, without side effects The observe() function is used to perform actions, with side effects Do not use an observe() function when calculating a value, and especially don't use reactive() for performing actions with side effects reactive() observe() Purpose Calculations Actions Side effects Forbidden Allowed","title":"Reactives vs Observers"},{"location":"R-with-Shiny/reactive/#stop-trigger-delay","text":"","title":"Stop-Trigger-Delay"},{"location":"R-with-Shiny/reactive/#isolating-reactions","text":"Suppose your app has an input widget where users can enter text for the title of the plot. However, you only want the title to update if any ot the other inputs that go into the plot change. You can achieve this by isolating the plot title such that when input$x or input$y changes, the plot, along with the title, will update. But when only the title input changes, the plot will not update. 1 2 3 4 5 output $ scatterplot <- renderPlot ({ ggplot ( data = movies_subset (), aes_string ( x = input $ x , y = input $ y )) + geom_point () + labs ( title = isolate ({ input $ plot_title }) ) }) isolate() is then used to stop a reaction.","title":"Isolating Reactions"},{"location":"R-with-Shiny/reactive/#triggering-reactions","text":"Why might one want to explicitly trigger a reaction? Somethimes you might want to wait for a specific action to be taken from the user, like clicking an actionButton , before calculating an expression or taking an action. A reactive value or expression that is used to trigger other calculations in this way is called an event . These events can be the first argument in the observeEvent function. This arguments can be a simple reactive value like an input, a call to a reactive expression, or a complex expresion provided wrapped in curly braces. The second argument is the expression to call whenever the first argument is invalidated. This is similar to saying if event expression happens, call handler expression. 1 observeEvent ( eventExpr , handlerExpr , ... ) Suppose your app allows for taking a random sample of the data based on a sample size numeric input. Suppose also that you want to add functionality for the userd to download the random sample they generated if they press an action button requesting to do so. In the UI we create an action button and in the server we condition the observeEvent on the inputId of that action button. This way R knows to call the expression given in the second argument of observeEvent when the user presses the action button. And finally we can delay reactions with eventReactive , which takes similar arguments as observeEvent . 1 2 3 4 5 6 7 8 9 10 # UI actionButton ( inputId = \"write_csv\" , label = \"Write CSV\" ) # Server observeEvent ( input $ write_csv , { filename <- paste0 ( \"movies_\" , str_replace_all ( Sys.time (), \":|\\ \" , \"_\" ), \".csv\" ) write_csv ( movies_sample (), path = filename ) } Suppose your goal is to change how users take random samples in your app (you only want them to get a new sample when an action button that says \"get new sample\" is pressed, not when other things like numeric input defining the size of the sample changes). In the event reactive function, the first argument is the input associated with the action button and the second argument is the sampling code. Then we add one more argument, ignoreNull , which tells R what to do (or what not to do) when the event expression evaluates to Null. For example, what should the app do when the app is first launched and the user has not even interacted with the app yet? If this is set to FALSE, the app will initially perform the action or calculation and then the user can re-initiate it. 1 2 3 4 5 6 7 8 9 10 # UI actionButton ( inputId = \"get_new_sample\" , label = \"Get new sample\" ) # Server movies_sample <- eventReactive ( input $ get_new_sample , { req ( input $ n_samp ) sample_n ( movies_subset (), input $ n_samp ) }, ignoreNULL = FALSE ) observeEvent() and eventReactive() look and feel very similar. They have the same syntax, same arguments, but they're actually not the same at all! observeEvent() is to perform an action in response to an event eventReactive() is used to create a calculated value that only updates in response to an event This pair of functions also seem similar to the observe/reactive pair, however, the main differences between them is that observe() and reactive() functions automatically trigger on whatever they access while observeEvent() and eventReactive() functions need to be explicitly told what triggers them. In the following example isolate() is used to prevent the plot title to be updated until some of the other inputs are updated: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 library ( shiny ) library ( ggplot2 ) library ( tools ) load ( url ( \"http://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata\" )) # UI ui <- fluidPage ( sidebarLayout ( # Input sidebarPanel ( # Select variable for y-axis selectInput ( inputId = \"y\" , label = \"Y-axis:\" , choices = c ( \"IMDB rating\" = \"imdb_rating\" , \"IMDB number of votes\" = \"imdb_num_votes\" , \"Critics Score\" = \"critics_score\" , \"Audience Score\" = \"audience_score\" , \"Runtime\" = \"runtime\" ), selected = \"audience_score\" ), # Select variable for x-axis selectInput ( inputId = \"x\" , label = \"X-axis:\" , choices = c ( \"IMDB rating\" = \"imdb_rating\" , \"IMDB number of votes\" = \"imdb_num_votes\" , \"Critics Score\" = \"critics_score\" , \"Audience Score\" = \"audience_score\" , \"Runtime\" = \"runtime\" ), selected = \"critics_score\" ), # Select variable for color selectInput ( inputId = \"z\" , label = \"Color by:\" , choices = c ( \"Title Type\" = \"title_type\" , \"Genre\" = \"genre\" , \"MPAA Rating\" = \"mpaa_rating\" , \"Critics Rating\" = \"critics_rating\" , \"Audience Rating\" = \"audience_rating\" ), selected = \"mpaa_rating\" ), # Set alpha level sliderInput ( inputId = \"alpha\" , label = \"Alpha:\" , min = 0 , max = 1 , value = 0.5 ), # Set point size sliderInput ( inputId = \"size\" , label = \"Size:\" , min = 0 , max = 5 , value = 2 ), # Enter text for plot title textInput ( inputId = \"plot_title\" , label = \"Plot title\" , placeholder = \"Enter text to be used as plot title\" ) ), # Output: mainPanel ( plotOutput ( outputId = \"scatterplot\" ) ) ) ) # Define server function required to create the scatterplot- server <- function ( input , output , session ) { # Create scatterplot object the plotOutput function is expecting output $ scatterplot <- renderPlot ({ ggplot ( data = movies , aes_string ( x = input $ x , y = input $ y , color = input $ z )) + geom_point ( alpha = input $ alpha , size = input $ size ) + labs ( title = isolate ({ toTitleCase ( input $ plot_title )}) ) }) } # Create a Shiny app object shinyApp ( ui = ui , server = server ) In the following example an eventReactive() function is used to wait until an actionButton() is clicked to update the plot title: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 library ( shiny ) library ( ggplot2 ) library ( tools ) load ( url ( \"http://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata\" )) # UI ui <- fluidPage ( sidebarLayout ( # Input sidebarPanel ( # Select variable for y-axis selectInput ( inputId = \"y\" , label = \"Y-axis:\" , choices = c ( \"IMDB rating\" = \"imdb_rating\" , \"IMDB number of votes\" = \"imdb_num_votes\" , \"Critics Score\" = \"critics_score\" , \"Audience Score\" = \"audience_score\" , \"Runtime\" = \"runtime\" ), selected = \"audience_score\" ), # Select variable for x-axis selectInput ( inputId = \"x\" , label = \"X-axis:\" , choices = c ( \"IMDB rating\" = \"imdb_rating\" , \"IMDB number of votes\" = \"imdb_num_votes\" , \"Critics Score\" = \"critics_score\" , \"Audience Score\" = \"audience_score\" , \"Runtime\" = \"runtime\" ), selected = \"critics_score\" ), # Select variable for color selectInput ( inputId = \"z\" , label = \"Color by:\" , choices = c ( \"Title Type\" = \"title_type\" , \"Genre\" = \"genre\" , \"MPAA Rating\" = \"mpaa_rating\" , \"Critics Rating\" = \"critics_rating\" , \"Audience Rating\" = \"audience_rating\" ), selected = \"mpaa_rating\" ), # Set alpha level sliderInput ( inputId = \"alpha\" , label = \"Alpha:\" , min = 0 , max = 1 , value = 0.5 ), # Set point size sliderInput ( inputId = \"size\" , label = \"Size:\" , min = 0 , max = 5 , value = 2 ), # Enter text for plot title textInput ( inputId = \"plot_title\" , label = \"Plot title\" , placeholder = \"Enter text to be used as plot title\" ), # Action button for plot title actionButton ( inputId = \"update_plot_title\" , label = \"Update plot title\" ) ), # Output: mainPanel ( plotOutput ( outputId = \"scatterplot\" ) ) ) ) # Define server function required to create the scatterplot- server <- function ( input , output , session ) { # New plot title new_plot_title <- eventReactive ( input $ update_plot_title , { toTitleCase ( input $ plot_title ) } ) # Create scatterplot object the plotOutput function is expecting output $ scatterplot <- renderPlot ({ ggplot ( data = movies , aes_string ( x = input $ x , y = input $ y , color = input $ z )) + geom_point ( alpha = input $ alpha , size = input $ size ) + labs ( title = new_plot_title () ) }) } # Create a Shiny app object shinyApp ( ui = ui , server = server ) In the following example, observeEvent() is used to make things happen when an action button is clicked: a message is printed to the console stating how many records are shown and a table is generated with those records. The table output is only printed when action button is clicked, but not when other inputs that go into the creation of that output changes thanks to the isolate() function. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 library ( shiny ) load ( url ( \"http://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata\" )) # UI ui <- fluidPage ( sidebarLayout ( # Input sidebarPanel ( # Numeric input for number of rows to show numericInput ( inputId = \"n_rows\" , label = \"How many rows do you want to see?\" , value = 10 ), # Action button to show actionButton ( inputId = \"button\" , label = \"Show\" ) ), # Output: mainPanel ( tableOutput ( outputId = \"datatable\" ) ) ) ) # Define server function required to create the scatterplot- server <- function ( input , output , session ) { # Print a message to the console every time button is pressed observeEvent ( input $ button , { cat ( \"Showing\" , input $ n_rows , \"rows\\n\" ) }) # Take a reactive dependency on input$button, # but not on any of the stuff inside the function df <- eventReactive ( input $ button , { head ( movies , input $ n_rows ) }) output $ datatable <- renderTable ({ df () }) } # Create a Shiny app object shinyApp ( ui = ui , server = server )","title":"Triggering Reactions"},{"location":"R-with-Shiny/reactive/#reactivity-summary","text":"Reactives are equivalent to no argument functions. Think about them as functions, think about them as variables that can depend on user input and other reactives. Reactives are for reactive values and expressions, observers are for their side effects. Do not define a reactive() inside a render*() function. Be careful with missing parentheses when calling reactive expressions. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 library ( shiny ) ui <- fluidPage ( titlePanel ( \"Add 2\" ), sidebarLayout ( sidebarPanel ( sliderInput ( \"x\" , \"Select x\" , min = 1 , max = 50 , value = 30 ) ), mainPanel ( textOutput ( \"x_updated\" ) ) ) ) add_2 <- function ( x ) { x + 2 } server <- function ( input , output ) { current_x <- reactive ({ add_2 ( input $ x )}) output $ x_updated <- renderText ({ current_x () }) } shinyApp ( ui , server )","title":"Reactivity Summary"},{"location":"SAS-CDI/intro-CDI/","text":"Series of Short Videos on the SAS\u00ae Clinical Data Integration Solution (CDI) Defining a Clinical Study Registering Source or Raw Data Defining SDTM Domain Metadata Mapping Source Data to SDTM Domains Creating Job Templates","title":"Introduction to CDI"},{"location":"SAS-CDI/intro-CDISC/","text":"Check these websites CDISC Official Site CDISC Standards Macro-Supported Metadata-Driven Process for Mapping SDTM VISIT and VISITNUM \u00bfQu\u00e9 es CDISC? Link Diferencias entre CDASH, SDTM/SEND y ADAM Link CDASH Link Clinical Data Acquisition Standards Harmonization Lograr que los datos sean recogidos de manera m\u00e1s homog\u00e9nea. No est\u00e1 pensado para presentar los datos sino para recogerlos. Est\u00e1ndar para la recogida de datos. No hay que inspirarse en SDTM (estructura vertical) para dise\u00f1ar la base de datos sino en CDASH, y luego ya se har\u00e1 una transformaci\u00f3n a SDTM. SDTM Link Study Data Tabulation Model Depu\u00e9s de la transformaci\u00f3n se tiene que reportar sobre el CRF anotado una nueva anotaci\u00f3n con las variables SDTM. Esta nueva anotaci\u00f3n puede requerir estructuras condicionales para poder reconocer las nuevas variables. ADaM Link Analysis Data Model Define los est\u00e1ndares de data sets y metadata de cara a un an\u00e1lisis y presentaci\u00f3n de resultados. Permite una generaci\u00f3n eficiente, replicaci\u00f3n y revisi\u00f3n de los datos usados para realizar an\u00e1lisis estad\u00edsticos en ensayos cl\u00ednicos. SEND Link Standard for Exchange of Nonclinical Data Una implementaci\u00f3n menos restrictiva de SDTM para estudios no cl\u00ednicos que tienen m\u00e1s variabilidad. \u00bfQu\u00e9 es el CDI? Link SAS Clinical Data Integration. Te permite hacer un mapeo de tu base de datos original y la documentaci\u00f3n de la transformaci\u00f3n se genera autom\u00e1ticamente. Librer\u00edas Link Jobs Link Dominios Link Conjuntos de variables. Los est\u00e1ndares definen qu\u00e9 variables tiene cada uno, cuales son obligatorias, opcionales o recomendables y cuales se deben enviar. En algunas te tienes que limitar a una estructura restringida y tienes que meter la informaci\u00f3n extra en un dominio suplementario que va ligado al principal, pero otros dominios son un poco m\u00e1s flexibles. Hay informaci\u00f3n que te puede cuadrar en varios dominios, ah\u00ed entra ya la interpretaci\u00f3n del sponsor o del que mapea la base de datos. El est\u00e1ndar no est\u00e1 totalmente cerrado y hay cosas que hay que decidir y ser\u00e1 necesario justificar las decisiones. Pinacle21/OpenCDISC Link Validar que tus data sets cumplen todas las reglas y est\u00e1ndares que impone CDISC. Te presenta un informe en excel de los errores. Diferentes tipos de dominios Link Dominios de prop\u00f3sito especial Link CO (Comments) DM (Demographics): una de las pocas tablas que es plana, se recogen variables que siempre est\u00e1n en todos los estudios que tienen nombre propio. SE (Subject Elements): especifica las visitas que va a tener el paciente a nivel de elemento, el esquema de visitas que deber\u00eda tener el paciente. SV (Subject Visits): fechas de visitas General Observation Classes Link Intervenciones: dominios que recogen acciones o intervenciones sobre los pacientes: CM (Concomitant and Prior Medications), EX (Exposure), EC (Exposure as Collected), PR (Procedures), SU (Substance Use) Eventos: dominios donde se recogen los acontecimientos que suceden sobre el paciente: AE (Adverse Events), CE (Clinical Events), DS (Disposition), DV (Protocol Deviations), HO (Helthcare Encounters), MH (Medical History) Findings: todo el resto de la informaci\u00f3n del estudio. Experimental Design Scheduling of Assessments Trial Summary Eligilility Caso pr\u00e1ctico Link Extract from OC Transform in SAS CDI Load SDTM Domains Validate SDTM Domains Implementaci\u00f3n Link Requerimientos: se define el estudio, la librer\u00eda, los data sets originales y se carga la versi\u00f3n de los est\u00e1ndares que se van a usar (SDTM, ADaM) que se quedar\u00e1 registrado en la metadata del estudio. Pasos: registrar los data sets originales, crear un nuevo job, identificar las variables y data sets que van a componer cada dominio, hacer la transformaci\u00f3n pertinente, validar los dominios. Tipos de variables Link Required: obligatorias y no permiten missing ni duplicados Expected: hay que incluirlas pero pueden no estar informadas Permissible: si tiene datos en alg\u00fan registro se supone que hay que reportarla, aunque no saben que la tienes recogida porque no es required, con lo cual a efectos pr\u00e1cticos las puedes quitar si quieres Validaci\u00f3n de los dominios Link Define.xml compliance Link SAS CDI genera este fichero autom\u00e1ticamente agrupando toda la informaci\u00f3n necesaria para enviar a las agencias regulatorias con hiperv\u00ednculos entre secci\u00f3n. El archivo Define.xml contiene toda la informaci\u00f3n anterior.","title":"What is CDISC?"},{"location":"SAS-CDI/intro-CDISC/#que-es-cdisc","text":"","title":"\u00bfQu\u00e9 es CDISC?"},{"location":"SAS-CDI/intro-CDISC/#diferencias-entre-cdash-sdtmsend-y-adam","text":"","title":"Diferencias entre CDASH, SDTM/SEND y ADAM"},{"location":"SAS-CDI/intro-CDISC/#cdash","text":"Clinical Data Acquisition Standards Harmonization Lograr que los datos sean recogidos de manera m\u00e1s homog\u00e9nea. No est\u00e1 pensado para presentar los datos sino para recogerlos. Est\u00e1ndar para la recogida de datos. No hay que inspirarse en SDTM (estructura vertical) para dise\u00f1ar la base de datos sino en CDASH, y luego ya se har\u00e1 una transformaci\u00f3n a SDTM.","title":"CDASH"},{"location":"SAS-CDI/intro-CDISC/#sdtm","text":"Study Data Tabulation Model Depu\u00e9s de la transformaci\u00f3n se tiene que reportar sobre el CRF anotado una nueva anotaci\u00f3n con las variables SDTM. Esta nueva anotaci\u00f3n puede requerir estructuras condicionales para poder reconocer las nuevas variables.","title":"SDTM"},{"location":"SAS-CDI/intro-CDISC/#adam","text":"Analysis Data Model Define los est\u00e1ndares de data sets y metadata de cara a un an\u00e1lisis y presentaci\u00f3n de resultados. Permite una generaci\u00f3n eficiente, replicaci\u00f3n y revisi\u00f3n de los datos usados para realizar an\u00e1lisis estad\u00edsticos en ensayos cl\u00ednicos.","title":"ADaM"},{"location":"SAS-CDI/intro-CDISC/#send","text":"Standard for Exchange of Nonclinical Data Una implementaci\u00f3n menos restrictiva de SDTM para estudios no cl\u00ednicos que tienen m\u00e1s variabilidad.","title":"SEND"},{"location":"SAS-CDI/intro-CDISC/#que-es-el-cdi","text":"SAS Clinical Data Integration. Te permite hacer un mapeo de tu base de datos original y la documentaci\u00f3n de la transformaci\u00f3n se genera autom\u00e1ticamente.","title":"\u00bfQu\u00e9 es el CDI?"},{"location":"SAS-CDI/intro-CDISC/#librerias","text":"","title":"Librer\u00edas"},{"location":"SAS-CDI/intro-CDISC/#jobs","text":"","title":"Jobs"},{"location":"SAS-CDI/intro-CDISC/#dominios","text":"Conjuntos de variables. Los est\u00e1ndares definen qu\u00e9 variables tiene cada uno, cuales son obligatorias, opcionales o recomendables y cuales se deben enviar. En algunas te tienes que limitar a una estructura restringida y tienes que meter la informaci\u00f3n extra en un dominio suplementario que va ligado al principal, pero otros dominios son un poco m\u00e1s flexibles. Hay informaci\u00f3n que te puede cuadrar en varios dominios, ah\u00ed entra ya la interpretaci\u00f3n del sponsor o del que mapea la base de datos. El est\u00e1ndar no est\u00e1 totalmente cerrado y hay cosas que hay que decidir y ser\u00e1 necesario justificar las decisiones.","title":"Dominios"},{"location":"SAS-CDI/intro-CDISC/#pinacle21opencdisc","text":"Validar que tus data sets cumplen todas las reglas y est\u00e1ndares que impone CDISC. Te presenta un informe en excel de los errores.","title":"Pinacle21/OpenCDISC"},{"location":"SAS-CDI/intro-CDISC/#diferentes-tipos-de-dominios","text":"","title":"Diferentes tipos de dominios"},{"location":"SAS-CDI/intro-CDISC/#dominios-de-proposito-especial","text":"CO (Comments) DM (Demographics): una de las pocas tablas que es plana, se recogen variables que siempre est\u00e1n en todos los estudios que tienen nombre propio. SE (Subject Elements): especifica las visitas que va a tener el paciente a nivel de elemento, el esquema de visitas que deber\u00eda tener el paciente. SV (Subject Visits): fechas de visitas","title":"Dominios de prop\u00f3sito especial"},{"location":"SAS-CDI/intro-CDISC/#general-observation-classes","text":"Intervenciones: dominios que recogen acciones o intervenciones sobre los pacientes: CM (Concomitant and Prior Medications), EX (Exposure), EC (Exposure as Collected), PR (Procedures), SU (Substance Use) Eventos: dominios donde se recogen los acontecimientos que suceden sobre el paciente: AE (Adverse Events), CE (Clinical Events), DS (Disposition), DV (Protocol Deviations), HO (Helthcare Encounters), MH (Medical History) Findings: todo el resto de la informaci\u00f3n del estudio. Experimental Design Scheduling of Assessments Trial Summary Eligilility","title":"General Observation Classes"},{"location":"SAS-CDI/intro-CDISC/#caso-practico","text":"Extract from OC Transform in SAS CDI Load SDTM Domains Validate SDTM Domains","title":"Caso pr\u00e1ctico"},{"location":"SAS-CDI/intro-CDISC/#implementacion","text":"Requerimientos: se define el estudio, la librer\u00eda, los data sets originales y se carga la versi\u00f3n de los est\u00e1ndares que se van a usar (SDTM, ADaM) que se quedar\u00e1 registrado en la metadata del estudio. Pasos: registrar los data sets originales, crear un nuevo job, identificar las variables y data sets que van a componer cada dominio, hacer la transformaci\u00f3n pertinente, validar los dominios.","title":"Implementaci\u00f3n"},{"location":"SAS-CDI/intro-CDISC/#tipos-de-variables","text":"Required: obligatorias y no permiten missing ni duplicados Expected: hay que incluirlas pero pueden no estar informadas Permissible: si tiene datos en alg\u00fan registro se supone que hay que reportarla, aunque no saben que la tienes recogida porque no es required, con lo cual a efectos pr\u00e1cticos las puedes quitar si quieres","title":"Tipos de variables"},{"location":"SAS-CDI/intro-CDISC/#validacion-de-los-dominios","text":"","title":"Validaci\u00f3n de los dominios"},{"location":"SAS-CDI/intro-CDISC/#definexml-compliance","text":"SAS CDI genera este fichero autom\u00e1ticamente agrupando toda la informaci\u00f3n necesaria para enviar a las agencias regulatorias con hiperv\u00ednculos entre secci\u00f3n. El archivo Define.xml contiene toda la informaci\u00f3n anterior.","title":"Define.xml compliance"},{"location":"SAS-CDI/sdtm/","text":"CDISC (Clinical Data Interchanging Standards Consortium) standards are to support the acquisition, exchange, submission and archival of clinical trial and research data. SDTM (Study Data Tabulation Model) for CRFs (Case Report Forms) was recommended for U.S. Food and Drug Administration (FDA) regulatory submissions since 2004. Although the SDTM Implementation Guide gives a standardized and predefined collection of submission metadata \"domains\" containing extensive variable collections, transforming CRFs to SDTM files for FDA submission is still a very hard and time-consuming task. Check these websites Using CDISC Standards with an MDR for EDC to Submission Traceability (2019) Streamline process: To Generate SDTM Program by Automation (2019) CDISC SDTM - An Automated Approach (2018) AUTOSDTM: A Macro to Automatically Map CDASH Data to SDTM (2017) SDTM Automation with Standard CRF Pages (2016) CDISC Transformer: a metadata-based transformation tool for clinical trial and research data into CDISC standards (2011) Confessions of a Clinical Programmer: Creating SDTM Domains with SAS Target Domain Link Category/Class Domain abbreviation Domain name Special Purpose DM Demographics CO Comments SE Subject Elements SV Subject Visits Interventions CM Concomitant Medications EX Exposure SU Substance Use Events AE Adverse Events DS Disposition MH Medical History DV Protocol Deviations CE Clinical Events Findings EG ECG Test Results IE Inclusion/Exclusion Exception LB Laboratory Tests Results PE Physical Examination QS Questionnaires SC Subject Characteristics VS Vital Signs DA Drug Accountability MB Microbiology Specimen MS Microbiology Susceptibility PC Pharmacokinetic Concentrations PP Pharmacokinetic Parameters FA Findings About Events https://www.semanticscholar.org/paper/Practical-Methods-for-Creating-CDISC-SDTM-Domain-Graebner/cfa34869f92bec4f7c9c58505b64ef5201ea0dee/figure/0 There is no common rule for converting clinical trial data to CDISC standard-compliant data. Without automation, they require labor intensive or time-consuming processes such as re-entering data and manually mapping the data to the standard models. SDTM Model Concepts and Terms Link Note that Core variable together with the other two shaded columns CDISC Notes and References are not sent to FDA. Three categories of variables are specified in the Core column: Core variable Meaning Description Req Required Required variables must always be include in the dataset and cannot be null for any record. They are basic to the identification of a data record and are necessary to make record meaningful (key variables and topic variables). Exp Expected Expected variables may contain some null values and still are included in the dataset even when no data has been collected. In this case, a comment can be included in define.xml to state that data was not collected. Perm Permissible The sponsor can decide whether a Permissible variable should be included as a column when all values for that variable are null. http://pharma-sas.com/sdtm-model-concepts-and-terms/","title":"SDTM Basics"},{"location":"SAS-CDI/sdtm/#target-domain","text":"Category/Class Domain abbreviation Domain name Special Purpose DM Demographics CO Comments SE Subject Elements SV Subject Visits Interventions CM Concomitant Medications EX Exposure SU Substance Use Events AE Adverse Events DS Disposition MH Medical History DV Protocol Deviations CE Clinical Events Findings EG ECG Test Results IE Inclusion/Exclusion Exception LB Laboratory Tests Results PE Physical Examination QS Questionnaires SC Subject Characteristics VS Vital Signs DA Drug Accountability MB Microbiology Specimen MS Microbiology Susceptibility PC Pharmacokinetic Concentrations PP Pharmacokinetic Parameters FA Findings About Events https://www.semanticscholar.org/paper/Practical-Methods-for-Creating-CDISC-SDTM-Domain-Graebner/cfa34869f92bec4f7c9c58505b64ef5201ea0dee/figure/0 There is no common rule for converting clinical trial data to CDISC standard-compliant data. Without automation, they require labor intensive or time-consuming processes such as re-entering data and manually mapping the data to the standard models.","title":"Target Domain"},{"location":"SAS-CDI/sdtm/#sdtm-model-concepts-and-terms","text":"Note that Core variable together with the other two shaded columns CDISC Notes and References are not sent to FDA. Three categories of variables are specified in the Core column: Core variable Meaning Description Req Required Required variables must always be include in the dataset and cannot be null for any record. They are basic to the identification of a data record and are necessary to make record meaningful (key variables and topic variables). Exp Expected Expected variables may contain some null values and still are included in the dataset even when no data has been collected. In this case, a comment can be included in define.xml to state that data was not collected. Perm Permissible The sponsor can decide whether a Permissible variable should be included as a column when all values for that variable are null. http://pharma-sas.com/sdtm-model-concepts-and-terms/","title":"SDTM Model Concepts and Terms"},{"location":"essentials/accessing/","text":"Chapter summary in SAS Accessing SAS libraries Link libref is a library reference name (a shortcut to the physical location). There are three rules for valid librefs: A length of one to eight characters Begin with a letter or underscore The remaining characters are letters, numbers, or underscores Valid variable names begin with a letter or underscore, and continue with letters, numbers, or underscores. The VALIDVARNAME system option specifies the rules for valid SAS variable names that can be created and processed during a SAS session: 1 OPTIONS VALIDVARNAME = V7 ( default ) | UPCASE | ANY ; libref.data-set-name : data set reference two-level name data-set-name : when the data set belongs to a temporary library, you can optionally use a one-level name (SAS assumes that it is contained in the work library, which is the default) The LIBNAME statement associates the libref with the physical location of the library/data for the current SAS session 1 2 3 4 5 6 LIBNAME libref - name ' SAS - library - folder - path ' < options > ; /* Example */ %let path =/ folders / myfolders / ecprg193 ; libname orion \"&path\" ; Erase the association between SAS and a custom library Link 1 LIBNAME libref - name CLEAR ; Remove data sets and libraries Link 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 * Delete data sets ; * ------------------ ; PROC DELETE DATA = data1 data2 data3 ; RUN ; * Delete full library ; * --------------------- ; PROC DATASETS LIB = library-name MEMTYPE = DATA KILL NOLIST ; RUN ; QUIT ; * Delete specific data sets ; * --------------------------- ; PROC DATASETS LIB = library-name NOWARN NOLIST NODETAILS ; DELETE prefix-data-set : other-data-set ; RUN ; QUIT ; Copy data sets and libraries Link 1 2 3 4 5 6 7 8 9 10 * Copy full library ; * -------------------; PROC COPY IN = old - library OUT = new - library ; RUN ; * Copy specific data sets ; * -------------------------; PROC COPY IN = old - library OUT = new - library MEMTYPE = DATA ; SELECT data - set1 data - set2 ; RUN ; Rename data sets Link 1 2 3 PROC DATASETS LIBRARY = library - name ; CHANGE data - set - name1 = data - set - new - name1 data - set - name2 = data - set - new - name2 ; RUN ; Check the contents of a library Link 1 2 PROC CONTENTS DATA = libref . _ALL_ ; RUN ; To hide the descriptors of all data sets in the library (it could generate a very long report) you can add the option nods (only compatible with the keybord _all_ ): 1 2 PROC CONTENTS DATA = libref . _ALL_ NODS ; RUN ; Print the full data set Link 1 2 PROC PRINT DATA = SAS - data - set ; RUN ; Examining SAS Data Sets Link Parts of a library (SAS notation): Table = data set Column = variable Row = observation PROC CONTENTS Link The descriptor portion contains information about the attributes of the data set (metadata), including the variable names. It is show in three tables: Table 1: general information about the data set (name, creation date/time, etc.) Table 2: operating environment information, file location, etc. Table 3: alphabetic list of variables in the data set and their attributes PROC PRINT Link The data portion contains the data values, stored in variables (numeric/character) Numeric values: right-aligned digits 0-9, minus sign, single decimal point, scientific notation (E) Character values: left-aligned; letters, numbers, special characters and blanks Missing values: blank for character variables and period for numeric ones. To change this default behaviour use MISSING='new-character' Values length: for character variables 1 byte = 1 character, numeric variables have 8 bytes of storage by default (16-17 significant digits) Other attributes: format , informat , label","title":"Accessing Data"},{"location":"essentials/accessing/#accessing-sas-libraries","text":"libref is a library reference name (a shortcut to the physical location). There are three rules for valid librefs: A length of one to eight characters Begin with a letter or underscore The remaining characters are letters, numbers, or underscores Valid variable names begin with a letter or underscore, and continue with letters, numbers, or underscores. The VALIDVARNAME system option specifies the rules for valid SAS variable names that can be created and processed during a SAS session: 1 OPTIONS VALIDVARNAME = V7 ( default ) | UPCASE | ANY ; libref.data-set-name : data set reference two-level name data-set-name : when the data set belongs to a temporary library, you can optionally use a one-level name (SAS assumes that it is contained in the work library, which is the default) The LIBNAME statement associates the libref with the physical location of the library/data for the current SAS session 1 2 3 4 5 6 LIBNAME libref - name ' SAS - library - folder - path ' < options > ; /* Example */ %let path =/ folders / myfolders / ecprg193 ; libname orion \"&path\" ;","title":"Accessing SAS libraries"},{"location":"essentials/accessing/#erase-the-association-between-sas-and-a-custom-library","text":"1 LIBNAME libref - name CLEAR ;","title":"Erase the association between SAS and a custom library"},{"location":"essentials/accessing/#remove-data-sets-and-libraries","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 * Delete data sets ; * ------------------ ; PROC DELETE DATA = data1 data2 data3 ; RUN ; * Delete full library ; * --------------------- ; PROC DATASETS LIB = library-name MEMTYPE = DATA KILL NOLIST ; RUN ; QUIT ; * Delete specific data sets ; * --------------------------- ; PROC DATASETS LIB = library-name NOWARN NOLIST NODETAILS ; DELETE prefix-data-set : other-data-set ; RUN ; QUIT ;","title":"Remove data sets and libraries"},{"location":"essentials/accessing/#copy-data-sets-and-libraries","text":"1 2 3 4 5 6 7 8 9 10 * Copy full library ; * -------------------; PROC COPY IN = old - library OUT = new - library ; RUN ; * Copy specific data sets ; * -------------------------; PROC COPY IN = old - library OUT = new - library MEMTYPE = DATA ; SELECT data - set1 data - set2 ; RUN ;","title":"Copy data sets and libraries"},{"location":"essentials/accessing/#rename-data-sets","text":"1 2 3 PROC DATASETS LIBRARY = library - name ; CHANGE data - set - name1 = data - set - new - name1 data - set - name2 = data - set - new - name2 ; RUN ;","title":"Rename data sets"},{"location":"essentials/accessing/#check-the-contents-of-a-library","text":"1 2 PROC CONTENTS DATA = libref . _ALL_ ; RUN ; To hide the descriptors of all data sets in the library (it could generate a very long report) you can add the option nods (only compatible with the keybord _all_ ): 1 2 PROC CONTENTS DATA = libref . _ALL_ NODS ; RUN ;","title":"Check the contents of a library"},{"location":"essentials/accessing/#print-the-full-data-set","text":"1 2 PROC PRINT DATA = SAS - data - set ; RUN ;","title":"Print the full data set"},{"location":"essentials/accessing/#examining-sas-data-sets","text":"Parts of a library (SAS notation): Table = data set Column = variable Row = observation","title":"Examining SAS Data Sets"},{"location":"essentials/accessing/#proc-contents","text":"The descriptor portion contains information about the attributes of the data set (metadata), including the variable names. It is show in three tables: Table 1: general information about the data set (name, creation date/time, etc.) Table 2: operating environment information, file location, etc. Table 3: alphabetic list of variables in the data set and their attributes","title":"PROC CONTENTS"},{"location":"essentials/accessing/#proc-print","text":"The data portion contains the data values, stored in variables (numeric/character) Numeric values: right-aligned digits 0-9, minus sign, single decimal point, scientific notation (E) Character values: left-aligned; letters, numbers, special characters and blanks Missing values: blank for character variables and period for numeric ones. To change this default behaviour use MISSING='new-character' Values length: for character variables 1 byte = 1 character, numeric variables have 8 bytes of storage by default (16-17 significant digits) Other attributes: format , informat , label","title":"PROC PRINT"},{"location":"essentials/combination/","text":"Chapter summary in SAS Concatenating Data Sets (Vertical Combination) Link 1 2 3 DATA SAS - data - set ; SET SAS - data - set1 SAS - data - set2 ...; RUN ; Combine two different variables that are actually the same one 1 2 3 DATA SAS - data - set ; SET SAS - data - set1 ( RENAME = ( old - name1 = new - name1 old - name2 = new - name2 )) SAS - data - set2 ...; RUN ; The name change affects the PDV and the output data set, but has no effect on the input data set The variable attributes are assigned from the first data set in the SET statement You will get an error in the DATA step if a variable is defined with different data types in the files that you are trying to concatenate Merging SAS Data Sets (Horizontal Combination) Link In a one-to-one relationship, a single observation in one data set is related to one, and only one, observation in another data set based on the values of one or more common variables In a one-to-many relationship, a single observation in one data set is related to one or more observations in another data set In a many-to-one relationship, multiple observations in one data set are related to one observation in another data set In a many-to-many relationship, multiple observations in one data set are related to multiple observations in another data set Sometimes the data sets have non-matches : at least one observation in one of the data sets is unrelated to any observation in another data set based on the values of one or more common variables Merging SAS Data Sets One-to-One Link The match-merging is a process based on the values of common variables Data sets are merged in the order that they appear in the MERGE statement You may need to SORT the files by the BY-variable(s) before merging the files 1 2 3 4 5 DATA SAS - data - set ; MERGE SAS - data - set1 ( RENAME = ( old - name1 = new - name1 ...)) SAS - data - set2 ...; BY < DESCENDING > BY - variable ( s ); < additional SAS statements > RUN ; Merging SAS Data Sets One-to-Many Link 1 2 3 4 5 DATA SAS - data - set ; MERGE SAS - data - set1 SAS - data - set2 ...; BY < DESCENDING > BY - variable ( s ); < additional SAS statements > RUN ; Performing a merge without a BY statement merges the observations based on their positions, this is almost never done intentionally and can lead to unexpected results MERGENOBY ( = NOWARN (default) | WARN | ERROR ) controls whether a message is issued when MERGE processing occurs without an associated BY statement When you reverse the order of the data sets in the MERGE statement, the results are the same, but the order of the variables is different. SAS performs a many-to-one merge One-line-to-Many Merge If you have a data set that is just one line that you would like to joint to all the observations of a different data set you can do it in a single DATA step: DATA result-data-set; SET multiple-observations-data-set; IF _N_ EQ 1 THEN DO; SET single-line-data-set; END; RUN; If you want to perform a cross join which is functionally the same as a Cartesian product join instead you can do it in a single PROC SQL : PROC SQL; CREATE TABLE result-data-set AS SELECT var1, var2 FROM data-set1 AS alias1 CROSS JOIN data-set2 AS alias2; QUIT; Merging SAS Data Sets that Have Non-Matches Link 1 2 3 4 5 DATA SAS - data - set ; MERGE SAS - data - set1 SAS - data - set2 ...; BY < DESCENDING > BY - variable ( s ); < additional SAS statements > RUN ; After the merging, the output data set contains both matches and non-matches You want the new data set to contain only the observations that match across the input data sets, and not those ones that are missing in one of the data sets that you are merging 1 2 3 4 5 6 DATA SAS - data - set ; MERGE SAS - data - set1 ( IN = variable1 ) SAS - data - set2 ( IN = variable2 ) ...; BY < DESCENDING > BY - variable ( s ); < additional SAS statements > RUN ; When you specify the IN option after an input data set in the MERGE statement, SAS creates a temporary numeric variable that indicates whether the data set contributed data to the current observation (0 = it did not contribute to the current observation, 1 = it did contribute). These variables are only available during execution . 1 2 3 4 5 6 7 DATA SAS - data - set ; MERGE SAS - data - set1 ( IN = variable1 ) SAS - data - set2 ( IN = variable2 ) ... ; BY < DESCENDING > BY - variable ( s ) ; IF variable1 = 1 and variable2 = 1 ; /* write only matches */ < additional SAS statements > RUN ; Matches 1 2 IF variable1 = 1 and variable2 = 1 IF variable1 and variable2 Non-matches from either data set 1 2 IF variable1 = 0 or not variable2 = 0 IF not variable1 or not variable2 E.g.: 1 2 3 4 5 6 7 DATA SAS - new - data - set1 SAS - new - data - set2 ; MERGE SAS - data - set1 ( in = var1 ) SAS - data - set2 ( in = var2 ) ; BY BY - variable ( s ) ; IF var2 THEN OUTPUT SAS - new - data - set1 ; ELSE IF var1 and not var2 THEN OUTPUT SAS - new - data - set2 ; KEEP variable1 variable2 variable5 variable8 ; RUN ; Merging SAS Data Sets Many-to-Many Link With the macros makewide.sas and makelong.sas you can Make one of your data sets wide Perform a one-to-many merge with the other data set Make your resultant data set long to obtain the required result","title":"Combining SAS Data Sets"},{"location":"essentials/combination/#concatenating-data-sets-vertical-combination","text":"1 2 3 DATA SAS - data - set ; SET SAS - data - set1 SAS - data - set2 ...; RUN ; Combine two different variables that are actually the same one 1 2 3 DATA SAS - data - set ; SET SAS - data - set1 ( RENAME = ( old - name1 = new - name1 old - name2 = new - name2 )) SAS - data - set2 ...; RUN ; The name change affects the PDV and the output data set, but has no effect on the input data set The variable attributes are assigned from the first data set in the SET statement You will get an error in the DATA step if a variable is defined with different data types in the files that you are trying to concatenate","title":"Concatenating Data Sets (Vertical Combination)"},{"location":"essentials/combination/#merging-sas-data-sets-horizontal-combination","text":"In a one-to-one relationship, a single observation in one data set is related to one, and only one, observation in another data set based on the values of one or more common variables In a one-to-many relationship, a single observation in one data set is related to one or more observations in another data set In a many-to-one relationship, multiple observations in one data set are related to one observation in another data set In a many-to-many relationship, multiple observations in one data set are related to multiple observations in another data set Sometimes the data sets have non-matches : at least one observation in one of the data sets is unrelated to any observation in another data set based on the values of one or more common variables","title":"Merging SAS Data Sets (Horizontal Combination)"},{"location":"essentials/combination/#merging-sas-data-sets-one-to-one","text":"The match-merging is a process based on the values of common variables Data sets are merged in the order that they appear in the MERGE statement You may need to SORT the files by the BY-variable(s) before merging the files 1 2 3 4 5 DATA SAS - data - set ; MERGE SAS - data - set1 ( RENAME = ( old - name1 = new - name1 ...)) SAS - data - set2 ...; BY < DESCENDING > BY - variable ( s ); < additional SAS statements > RUN ;","title":"Merging SAS Data Sets One-to-One"},{"location":"essentials/combination/#merging-sas-data-sets-one-to-many","text":"1 2 3 4 5 DATA SAS - data - set ; MERGE SAS - data - set1 SAS - data - set2 ...; BY < DESCENDING > BY - variable ( s ); < additional SAS statements > RUN ; Performing a merge without a BY statement merges the observations based on their positions, this is almost never done intentionally and can lead to unexpected results MERGENOBY ( = NOWARN (default) | WARN | ERROR ) controls whether a message is issued when MERGE processing occurs without an associated BY statement When you reverse the order of the data sets in the MERGE statement, the results are the same, but the order of the variables is different. SAS performs a many-to-one merge One-line-to-Many Merge If you have a data set that is just one line that you would like to joint to all the observations of a different data set you can do it in a single DATA step: DATA result-data-set; SET multiple-observations-data-set; IF _N_ EQ 1 THEN DO; SET single-line-data-set; END; RUN; If you want to perform a cross join which is functionally the same as a Cartesian product join instead you can do it in a single PROC SQL : PROC SQL; CREATE TABLE result-data-set AS SELECT var1, var2 FROM data-set1 AS alias1 CROSS JOIN data-set2 AS alias2; QUIT;","title":"Merging SAS Data Sets One-to-Many"},{"location":"essentials/combination/#merging-sas-data-sets-that-have-non-matches","text":"1 2 3 4 5 DATA SAS - data - set ; MERGE SAS - data - set1 SAS - data - set2 ...; BY < DESCENDING > BY - variable ( s ); < additional SAS statements > RUN ; After the merging, the output data set contains both matches and non-matches You want the new data set to contain only the observations that match across the input data sets, and not those ones that are missing in one of the data sets that you are merging 1 2 3 4 5 6 DATA SAS - data - set ; MERGE SAS - data - set1 ( IN = variable1 ) SAS - data - set2 ( IN = variable2 ) ...; BY < DESCENDING > BY - variable ( s ); < additional SAS statements > RUN ; When you specify the IN option after an input data set in the MERGE statement, SAS creates a temporary numeric variable that indicates whether the data set contributed data to the current observation (0 = it did not contribute to the current observation, 1 = it did contribute). These variables are only available during execution . 1 2 3 4 5 6 7 DATA SAS - data - set ; MERGE SAS - data - set1 ( IN = variable1 ) SAS - data - set2 ( IN = variable2 ) ... ; BY < DESCENDING > BY - variable ( s ) ; IF variable1 = 1 and variable2 = 1 ; /* write only matches */ < additional SAS statements > RUN ; Matches 1 2 IF variable1 = 1 and variable2 = 1 IF variable1 and variable2 Non-matches from either data set 1 2 IF variable1 = 0 or not variable2 = 0 IF not variable1 or not variable2 E.g.: 1 2 3 4 5 6 7 DATA SAS - new - data - set1 SAS - new - data - set2 ; MERGE SAS - data - set1 ( in = var1 ) SAS - data - set2 ( in = var2 ) ; BY BY - variable ( s ) ; IF var2 THEN OUTPUT SAS - new - data - set1 ; ELSE IF var1 and not var2 THEN OUTPUT SAS - new - data - set2 ; KEEP variable1 variable2 variable5 variable8 ; RUN ;","title":"Merging SAS Data Sets that Have Non-Matches"},{"location":"essentials/combination/#merging-sas-data-sets-many-to-many","text":"With the macros makewide.sas and makelong.sas you can Make one of your data sets wide Perform a one-to-many merge with the other data set Make your resultant data set long to obtain the required result","title":"Merging SAS Data Sets Many-to-Many"},{"location":"essentials/formatting/","text":"Chapter summary in SAS Using SAS Formats Link 1 2 3 4 PROC PRINT DATA = SAS - data - base ; FORMAT variable1 variable2 format ; FORMAT variable3 format3 variable4 format4 ; RUN ; Format definition Link <$>format<w>.<d> <$> = character format format = format name <w> = total width (includes special characters, commas, decimal point and decimal places) . = required syntax (dot) <d> = decimal places (numeric format) SAS formats Link Dictionary of formats $w. = writes standard character data $QUOTE. = writes a string in quotation marks w.d = writes standard numeric data COMMAw.d = writes numeric values with a comma that separates every three digits and a period that separates the decimal fraction DOLLARw.d = writes numeric values with a leading dollar sign, a comma that separates every three digits and a period that separates the decimal fraction COMMAXw.d = writes numeric values with a period that separates every three digits and a coma that separates the decimal fraction EUROXw.d = writes numeric values with a leading euro symbol, a period that separates every three digits and a comma that separates the decimal fraction DOSEF. = you can see the actual variable level values in the output rather than some indexes $UPCASE. = writes a string in uppercase If you want to uppercase only the first letter of words there is not a format but a function that you could use to transform your value: 1 var_propercase = PROPCASE ( var_uppercase ); SAS date values Link MMDDYY<w>. | DDMMYY<w>. | MONYY<w>. | DATE<w>. | WEEKDATE. w = 6: only date numbers w = 8: date numbers with / separators (just the last 2 digits of year) w = 10: date numbers with / separators (full 4-digit year) Note Dates before 01/01/1960 (0 value) will appear as negative numbers. Warning If you ever have to deal with hours (4-character strings with the military hour) you better create an auxiliary character variable with a : in between hours and minutes or translate it into seconds (numeric) before applying an HOURw.d (time interval in hours and its fractions) or HHMMw.d (time in HH:MM appearance) format. Creating and Applying User-Defined Formats Link PROC FORMAT Link 1 2 3 4 PROC FORMAT ; VALUE < $ > format - name value - or - range1 = 'formatted-value1' value - or - range2 = 'formatted-value2' ; RUN ; 1 2 3 PROC PRINT DATA = SAS - data - set ; FORMAT variable1 < $ > format - name .; RUN ; A format name can have a maximum of 32 characters The name of a format that applies to character values must begin with a dollar sign followed by a letter or underscore The name of a format that applies to numeric values must begin with a letter or underscore A format name cannot end in a number All remaining characters can be letters, underscores or numbers A user defined format name cannot be the name of a SAS format Each value-range set has three parts: value-or-range : specifies one or more values to be formatted (it can be a value, a range or a list of values) = : equal sign formatted-value : the formatted value you want to display instead of the stored value/s (it is allways a character string no matter wheter the format applies to character values or numeric values) 1 2 3 4 5 6 7 8 9 PROC FORMAT LIBRARY = my - format - library ; /* To save the custom formats */ VALUE string ' A ' - ' H ' = ' First ' ' I ' , ' J ' , ' K ' = ' Middle ' OTHER = ' End ' ; /* Non-specified values */ VALUE tiers low -< 50000 = ' Tier1 ' /* 50000 not included */ 50000 -< 100000 = ' Tier2 ' /* 100000 not included */ 100000 - high = ' Tier3 ' . = ' Missing value ' ; RUN ; Note If you omit the LIBRARY option, then formats and informats are stored in the work.formats catalog. If you do not include the keyword OTHER , then SAS applies the format only to values that match the value-range sets that you specify and the rest of values are displayed as they are stored in the data set. You can only use the < symbol to define a non-inclusive range. 1 OPTIONS FMTSEARCH = ( libref1 libref2 ... librefn ); The FMTSEARCH system option controls the order in which format catalogs are searched until the desired member is found. The WORK.FORMATS catalog is always searched first, unless it appears in the FMTSEARCH list. Format maximum length The maximum lenght of a custom format is defined by the length of its longer label. If you need to increase it you can create a larger dummy element in the format or change the format attributes (see this example ). Creating a Format from a SAS Dataset Link 1 2 3 4 5 6 7 8 DATA formatdataset ; SET originaldataset ; RETAIN fmtname '$custom_format_name' TYPE 'C' ; RENAME index_variable = start label_variable = label ; RUN ; PROC FORMAT CNTLIN = formatdataset ; RUN ; Check these websites Creating a Format from Raw Data or a SAS Dataset PROC FORMAT 's PICTURE Statement Link LOW-HIGH ensures that all values are included in the range. The MULT= statement option specifies that each value is multiplied by 1.61. The PREFIX= statement adds a US dollar sign to any number that you format. The picture contains six digit selectors, five for the salary and one for the dollar sign prefix. 1 2 3 4 5 6 PROC FORMAT ; PICTURE pct ( round ) low - high = '0009.9%)' ( mult = 10 prefix = '(' ); PICTURE pctl ( round ) low - high = '0000.00%)' ( mult = 100 prefix = '(' ); PICTURE numero low - high = \"0000000)\" ( prefix = '(N=' ); PICTURE uscurrency low - high = '000,000' ( mult = 1 . 61 prefix = '$' ); RUN ; Examples Link How to Order Categorical Variables Link Check these websites There Is No APPEND Option On PROC FORMAT . What Can You Do? You first create a format that you will apply to an auxiliary variable: 1 2 3 4 5 6 value SmFmt 1 = 'Non-smoker' 2 = 'Light (1-5)' 3 = 'Moderate (6-15)' 4 = 'Heavy (16-25)' 5 = 'Very Heavy (> 25)' ; run ; Then you create a data set view rather than a data set in order to save storage space (which might be important for large data sets) on which you define your auxiliary variable with the predefined format: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 data Heart / view = Heart ; format Smoking_Cat SmFmt . ; set sashelp . heart ; counter = _n_ ; keep counter Status Sex AgeAtStart Height Weight Diastolic Systolic Smoking_Cat ; select ( Smoking_Status ) ; when ( ' Non-smoker ' ) Smoking_Cat = 1 ; when ( ' Light (1-5) ' ) Smoking_Cat = 2 ; when ( ' Moderate (6-15) ' ) Smoking_Cat = 3 ; when ( ' Heavy (16-25) ' ) Smoking_Cat = 4 ; when ( ' Very Heavy (> 25) ' ) Smoking_Cat = 5 ; when ( ' ' ) Smoking_Cat = . ; end ; run ; If you then use a PROC REPORT to display your results, the order of appearance will be the numeric order of your auxiliary variable. By using this technique, you can specify any order for the categories of a contingency table. How to Modify an Existing Format Link We first load the existing format from the catalog into a data set: 1 2 3 4 5 6 7 8 9 10 % LET fmtname = % QSYSFUNC ( COMPRESS ( & fmtnamepoint ., % STR ( % . ))) ; PROC FORMAT LIBRARY = WORK CNTLOUT = tmpfmt1 ; SELECT & fmtname . ; RUN ; DATA tmpfmt2 ; SET tmpfmt1 ; KEEP START END LABEL FMTNAME ; RUN ; We create the new format entry and add it to the existing format list: 1 2 3 4 5 6 7 8 9 10 11 12 13 DATA updatefmt ; LENGTH FMTNAME $32 . START $16 . END $16 . LABEL $102 . ; FMTNAME = \" &TIMEVARFORMAT. \" ; LABEL = ' Format label ' ; END = ' end-valule ' ; START = ' start-value ' ; RUN ; DATA newfmt ; SET updatefmt tmpfmt2 ; END = TRIM ( LEFT ( END )) ; START = TRIM ( LEFT ( START )) ; RUN ; Now we load the updated format to the format catalog: 1 2 3 4 ODS SELECT NONE ; PROC FORMAT LIBRARY = WORK CNTLIN = newfmt FMTLIB ; RUN ; ODS SELECT ALL ; If you need to recover the original format after some operations you just load the original data set: 1 2 3 4 ODS SELECT NONE ; PROC FORMAT LIBRARY = WORK CNTLIN = tmpfmt1 FMTLIB ; RUN ; ODS SELECT ALL ; Introducing Special Characters in a Format Definition Link 1 2 3 4 5 6 7 8 9 10 11 12 13 PROC FORMAT ; VALUE cea 1 = '>5 ~{unicode mu}g/L' 2 = '<=5 ~{unicode mu}g/L' ; RUN ; ODS SCAPECHAR = '~' ; PROC PRINT DATA = SAS - data - set ; FORMAT cea cea . VAR pt cea ; RUN ;","title":"Formatting Data Values"},{"location":"essentials/formatting/#using-sas-formats","text":"1 2 3 4 PROC PRINT DATA = SAS - data - base ; FORMAT variable1 variable2 format ; FORMAT variable3 format3 variable4 format4 ; RUN ;","title":"Using SAS Formats"},{"location":"essentials/formatting/#format-definition","text":"<$>format<w>.<d> <$> = character format format = format name <w> = total width (includes special characters, commas, decimal point and decimal places) . = required syntax (dot) <d> = decimal places (numeric format)","title":"Format definition"},{"location":"essentials/formatting/#sas-formats","text":"Dictionary of formats $w. = writes standard character data $QUOTE. = writes a string in quotation marks w.d = writes standard numeric data COMMAw.d = writes numeric values with a comma that separates every three digits and a period that separates the decimal fraction DOLLARw.d = writes numeric values with a leading dollar sign, a comma that separates every three digits and a period that separates the decimal fraction COMMAXw.d = writes numeric values with a period that separates every three digits and a coma that separates the decimal fraction EUROXw.d = writes numeric values with a leading euro symbol, a period that separates every three digits and a comma that separates the decimal fraction DOSEF. = you can see the actual variable level values in the output rather than some indexes $UPCASE. = writes a string in uppercase If you want to uppercase only the first letter of words there is not a format but a function that you could use to transform your value: 1 var_propercase = PROPCASE ( var_uppercase );","title":"SAS formats"},{"location":"essentials/formatting/#sas-date-values","text":"MMDDYY<w>. | DDMMYY<w>. | MONYY<w>. | DATE<w>. | WEEKDATE. w = 6: only date numbers w = 8: date numbers with / separators (just the last 2 digits of year) w = 10: date numbers with / separators (full 4-digit year) Note Dates before 01/01/1960 (0 value) will appear as negative numbers. Warning If you ever have to deal with hours (4-character strings with the military hour) you better create an auxiliary character variable with a : in between hours and minutes or translate it into seconds (numeric) before applying an HOURw.d (time interval in hours and its fractions) or HHMMw.d (time in HH:MM appearance) format.","title":"SAS date values"},{"location":"essentials/formatting/#creating-and-applying-user-defined-formats","text":"","title":"Creating and Applying User-Defined Formats"},{"location":"essentials/formatting/#proc-format","text":"1 2 3 4 PROC FORMAT ; VALUE < $ > format - name value - or - range1 = 'formatted-value1' value - or - range2 = 'formatted-value2' ; RUN ; 1 2 3 PROC PRINT DATA = SAS - data - set ; FORMAT variable1 < $ > format - name .; RUN ; A format name can have a maximum of 32 characters The name of a format that applies to character values must begin with a dollar sign followed by a letter or underscore The name of a format that applies to numeric values must begin with a letter or underscore A format name cannot end in a number All remaining characters can be letters, underscores or numbers A user defined format name cannot be the name of a SAS format Each value-range set has three parts: value-or-range : specifies one or more values to be formatted (it can be a value, a range or a list of values) = : equal sign formatted-value : the formatted value you want to display instead of the stored value/s (it is allways a character string no matter wheter the format applies to character values or numeric values) 1 2 3 4 5 6 7 8 9 PROC FORMAT LIBRARY = my - format - library ; /* To save the custom formats */ VALUE string ' A ' - ' H ' = ' First ' ' I ' , ' J ' , ' K ' = ' Middle ' OTHER = ' End ' ; /* Non-specified values */ VALUE tiers low -< 50000 = ' Tier1 ' /* 50000 not included */ 50000 -< 100000 = ' Tier2 ' /* 100000 not included */ 100000 - high = ' Tier3 ' . = ' Missing value ' ; RUN ; Note If you omit the LIBRARY option, then formats and informats are stored in the work.formats catalog. If you do not include the keyword OTHER , then SAS applies the format only to values that match the value-range sets that you specify and the rest of values are displayed as they are stored in the data set. You can only use the < symbol to define a non-inclusive range. 1 OPTIONS FMTSEARCH = ( libref1 libref2 ... librefn ); The FMTSEARCH system option controls the order in which format catalogs are searched until the desired member is found. The WORK.FORMATS catalog is always searched first, unless it appears in the FMTSEARCH list. Format maximum length The maximum lenght of a custom format is defined by the length of its longer label. If you need to increase it you can create a larger dummy element in the format or change the format attributes (see this example ).","title":"PROC FORMAT"},{"location":"essentials/formatting/#creating-a-format-from-a-sas-dataset","text":"1 2 3 4 5 6 7 8 DATA formatdataset ; SET originaldataset ; RETAIN fmtname '$custom_format_name' TYPE 'C' ; RENAME index_variable = start label_variable = label ; RUN ; PROC FORMAT CNTLIN = formatdataset ; RUN ; Check these websites Creating a Format from Raw Data or a SAS Dataset","title":"Creating a Format from a SAS Dataset"},{"location":"essentials/formatting/#proc-formats-picture-statement","text":"LOW-HIGH ensures that all values are included in the range. The MULT= statement option specifies that each value is multiplied by 1.61. The PREFIX= statement adds a US dollar sign to any number that you format. The picture contains six digit selectors, five for the salary and one for the dollar sign prefix. 1 2 3 4 5 6 PROC FORMAT ; PICTURE pct ( round ) low - high = '0009.9%)' ( mult = 10 prefix = '(' ); PICTURE pctl ( round ) low - high = '0000.00%)' ( mult = 100 prefix = '(' ); PICTURE numero low - high = \"0000000)\" ( prefix = '(N=' ); PICTURE uscurrency low - high = '000,000' ( mult = 1 . 61 prefix = '$' ); RUN ;","title":"PROC FORMAT's PICTURE Statement"},{"location":"essentials/formatting/#examples","text":"","title":"Examples"},{"location":"essentials/formatting/#how-to-order-categorical-variables","text":"Check these websites There Is No APPEND Option On PROC FORMAT . What Can You Do? You first create a format that you will apply to an auxiliary variable: 1 2 3 4 5 6 value SmFmt 1 = 'Non-smoker' 2 = 'Light (1-5)' 3 = 'Moderate (6-15)' 4 = 'Heavy (16-25)' 5 = 'Very Heavy (> 25)' ; run ; Then you create a data set view rather than a data set in order to save storage space (which might be important for large data sets) on which you define your auxiliary variable with the predefined format: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 data Heart / view = Heart ; format Smoking_Cat SmFmt . ; set sashelp . heart ; counter = _n_ ; keep counter Status Sex AgeAtStart Height Weight Diastolic Systolic Smoking_Cat ; select ( Smoking_Status ) ; when ( ' Non-smoker ' ) Smoking_Cat = 1 ; when ( ' Light (1-5) ' ) Smoking_Cat = 2 ; when ( ' Moderate (6-15) ' ) Smoking_Cat = 3 ; when ( ' Heavy (16-25) ' ) Smoking_Cat = 4 ; when ( ' Very Heavy (> 25) ' ) Smoking_Cat = 5 ; when ( ' ' ) Smoking_Cat = . ; end ; run ; If you then use a PROC REPORT to display your results, the order of appearance will be the numeric order of your auxiliary variable. By using this technique, you can specify any order for the categories of a contingency table.","title":"How to Order Categorical Variables"},{"location":"essentials/formatting/#how-to-modify-an-existing-format","text":"We first load the existing format from the catalog into a data set: 1 2 3 4 5 6 7 8 9 10 % LET fmtname = % QSYSFUNC ( COMPRESS ( & fmtnamepoint ., % STR ( % . ))) ; PROC FORMAT LIBRARY = WORK CNTLOUT = tmpfmt1 ; SELECT & fmtname . ; RUN ; DATA tmpfmt2 ; SET tmpfmt1 ; KEEP START END LABEL FMTNAME ; RUN ; We create the new format entry and add it to the existing format list: 1 2 3 4 5 6 7 8 9 10 11 12 13 DATA updatefmt ; LENGTH FMTNAME $32 . START $16 . END $16 . LABEL $102 . ; FMTNAME = \" &TIMEVARFORMAT. \" ; LABEL = ' Format label ' ; END = ' end-valule ' ; START = ' start-value ' ; RUN ; DATA newfmt ; SET updatefmt tmpfmt2 ; END = TRIM ( LEFT ( END )) ; START = TRIM ( LEFT ( START )) ; RUN ; Now we load the updated format to the format catalog: 1 2 3 4 ODS SELECT NONE ; PROC FORMAT LIBRARY = WORK CNTLIN = newfmt FMTLIB ; RUN ; ODS SELECT ALL ; If you need to recover the original format after some operations you just load the original data set: 1 2 3 4 ODS SELECT NONE ; PROC FORMAT LIBRARY = WORK CNTLIN = tmpfmt1 FMTLIB ; RUN ; ODS SELECT ALL ;","title":"How to Modify an Existing Format"},{"location":"essentials/formatting/#introducing-special-characters-in-a-format-definition","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 PROC FORMAT ; VALUE cea 1 = '>5 ~{unicode mu}g/L' 2 = '<=5 ~{unicode mu}g/L' ; RUN ; ODS SCAPECHAR = '~' ; PROC PRINT DATA = SAS - data - set ; FORMAT cea cea . VAR pt cea ; RUN ;","title":"Introducing Special Characters in a Format Definition"},{"location":"essentials/manipulation/","text":"Chapter summary in SAS Using SAS Functions Link CATX Concatenation Function Link 1 2 CATX ( ' ' , First_Name , Last_Name ) CATX ( '-' , of array_name ( * )) The CATX function removes leading and trailing blanks , inserts delimiters , and returns a concatenated character string. In the code, you first specify a character string that is used as a delimiter between concatenated items. Tip Similar functions are: CAT : concatenates variables similar to the concatenation operator || CATT (concatenate with TRIM ) : similar to the CAT function, but it removes the trailing spaces before concatenating the variables CATS (concatenate with STRIP ) : similar to the CAT function, but it removes both the leading and trailing spaces before concatenating the variables CATQ (concatenate with quotes) : concatenates variables by using a delimiter to separate items and by adding quotation marks to strings that contain the delimiter Function Equivalent Code `CAT(OF X1-X4) X1||X2||X3||X4 CATT(OF X1-X4) TRIM(X1)||TRIM(X2)||TRIM(X3)||TRIM(X4) CATS(OF X1-X4) TRIM(LEFT(X1))||TRIM(LEFT(X2))||TRIM(LEFT(X3))||TRIM(LEFT(X4)) CATX(SP, OF X1-X4) TRIM(LEFT(X1))||SP||TRIM(LEFT(X2))||SP||TRIM(LEFT(X3))||SP||TRIM(LEFT(X4)) Notice that, although no length IS set for the resultant concatenated variable, the longer values that came later in the step are not truncated. Where the length has not been set, the CAT functions will set a length of 200 . This is another significant advantage over using the || operator. Items, or arguments, can be numeric instead of character; numeric arguments are converted to character using BESTw. format. Search for Required Values Link Searching for a single character value (at least one 'Y' answer in a ten-questions questionnaire): 1 2 3 4 DATA yes ; SET answers ; IF FIND ( CAT ( of a1 - a10 ) , ' Y ' ) ; RUN ; Searching for multiple character values (at least two 'Y' answer in a ten-questions questionnaire): 1 2 3 4 DATA yes ; SET answers ; IF COUNTC ( CAT ( OF a1 - a10 ) , ' Y ' ) GE 2 ; RUN ; Searching for numeric values (at least one question answered with the value 5 in a ten-questions questionnaire): 1 2 3 4 DATA yes ; SET answers ; IF FIND ( CAT ( OF a : ) , ' 5 ' ) ; RUN ; Searching for a single numeric pattern (variable whose value starts with '250'): 1 2 3 4 DATA diagnostic - results ; SET diagnostic - tests ; IF FIND ( CATX ( ' * ' , OF diag1 - diag5 ) , ' 250 ' ) ne 0 ; RUN ; Searching for multiple numeric patterns (variable whose value starts with '250' or '493'): 1 2 3 4 5 6 7 8 DATA diagnostic - results ; LENGTH string $ 100 ; SET diagnostic - tests ; string = CATX ( '*' , OF diag1 - diag5 ); diabetes = ( FIND ( string , '250' ) ne 0 ); asthma = ( FIND ( string , '493' ) ne 0 ); DROP string ; RUN ; Search for characters in numeric variables : 1 2 3 4 5 6 7 DATA final - SAS - data - set ; SET origin - SAS - data - set ; ARRAY arrayname ( 3 ) item1 - item3 ; DO _N_ = 1 TO 3 UNTIL ( search eq 1 ) ; search = ( CAT ( arrayname ( _N_ )) IN : ( ' 171 ' ' 172 ' )) ; END ; RUN ; DATE Function Link 1 2 3 4 5 6 7 8 YEAR ( SAS - date ) QTR ( SAS - date ) MONTH ( SAS - date ) DAY ( SAS - date ) WEEKDAY ( SAS - date ) TODAY () /* Obtain the current date and convert to SAS-date (no argument) */ DATE () /* Obtain the current date and convert to SAS-date (no argument) */ MDY ( month , day , year ) The arguments must be numeric values (except from TODAY() and DATE() functions) You can subtract dates: Agein2012=(Bday2012-Birth_Date)/365.25; INTCK Time Interval Function Link 1 INTCK ( 'year' , Hire_Date , '01JAN2012' d ) The INTCK function returns the number of interval boundaries of a given kind that lie between the two dates, times, or datetime values. In the code, you first specify the interval value. What happens when you define a new variable from another that you are gonna DROP in this DATA statement? The DROP statement is a compile-time-only statement. SAS sets a drop flag for the dropped variables, but the variables are in the PDV and, therefore, are available for processing. SUM Summation Function Link 1 SUM ( argument1 , argument2 , ...) The arguments must be numeric values The SUM function ignores missing values, so if an argument has a missing value, the result of the SUM function is the sum of the nonmissing values If you add two values by + , if one of them is missing, the result will be a missing value, which makes the SUM function a better choice Conditional Processing Link IF-THEN-ELSE Conditional Structures Link 1 2 3 IF expression THEN statement ; ELSE IF expression THEN statement ; ELSE statement ; In the conditional expressions involving strings watch out for possible mixed case values where the condition may not be met: country = UPCASE(country); to avoid problems What do you mean 0.73 doesn't equal 0.73? When comparing numeric values, you may get unexpected results when the values of your variables seem to be the same (but actually they are not). The key to making sure these issues do not introduce erroneous results lies in understanding how numeric values are actually stored in data sets and in memory. This is referred to as numeric representation and precision . Read this article for more information: Numeric Representation and Precision in SAS and Why it Matters . Main solutions to this problem: The ROUND function: A = round(A, 0.01); Character versions of numeric variables: CharA = put(A, best.); Options in procedures: PROC COMPARE BASE=baseds COMPARE=compds CRITERION=0.0000001; Executing Multiple Statements Link 1 2 3 4 5 6 7 8 IF expression THEN DO ; executable statements ; END ; ELSE IF expression THEN DO ; executable statements ; END ; In the DATA step, the first reference to a variable determines its length. The first reference to a new variable can be in a LENGTH statement, an assignment statement, or another statement such as an INPUT statement. After a variable is created in the PDV, the length of the variable's first value doesn't matter. To avoid truncation in a variable defined inside a conditional structure you can: Define the longer string as the first condition Add some blanks at the end of shorter strings to fit the longer one Define the length explicitly before any other reference to the variable SELECT Group Link 1 2 3 4 5 6 7 8 9 10 11 12 13 14 SELECT ( Gender ) ; WHEN ( ' F ' ) DO ; Gift1 = ' Scarf ' ; Gift2 = ' Pedometer ' ; END ; WHEN ( ' M ' ) DO ; Gift1 = ' Gloves ' ; Gift2 = ' Money Clip ' ; END ; OTHERWISE DO ; Gift1 = ' Coffee ' ; Gift2 = ' Calendar ' ; END ; END ; The SELECT statement executes one of several statements or groups of statements The SELECT statement begins a SELECT group. They contain WHEN statements that identify SAS statements that are executed when a particular condition is true Use at least one WHEN statement in a SELECT group An optional OTHERWISE statement specifies a statement to be executed if no WHEN condition is met An END statement ends a SELECT group Avoiding Duplicates Link The dataset period has more than one register per patient and the calculation of time periods between each one and the reference. 1 2 3 PROC SORT DATA = period ; BY pt periodmax periodvisit ; RUN ; If you want to keep only highest/lowest value (last/first value after the sorting), you need to use the last. / .first functions. 1 2 3 4 5 DATA maxperiod ; SET period ; BY pt ; IF last . pt ; RUN ;","title":"Manipulating Data"},{"location":"essentials/manipulation/#using-sas-functions","text":"","title":"Using SAS Functions"},{"location":"essentials/manipulation/#catx-concatenation-function","text":"1 2 CATX ( ' ' , First_Name , Last_Name ) CATX ( '-' , of array_name ( * )) The CATX function removes leading and trailing blanks , inserts delimiters , and returns a concatenated character string. In the code, you first specify a character string that is used as a delimiter between concatenated items. Tip Similar functions are: CAT : concatenates variables similar to the concatenation operator || CATT (concatenate with TRIM ) : similar to the CAT function, but it removes the trailing spaces before concatenating the variables CATS (concatenate with STRIP ) : similar to the CAT function, but it removes both the leading and trailing spaces before concatenating the variables CATQ (concatenate with quotes) : concatenates variables by using a delimiter to separate items and by adding quotation marks to strings that contain the delimiter Function Equivalent Code `CAT(OF X1-X4) X1||X2||X3||X4 CATT(OF X1-X4) TRIM(X1)||TRIM(X2)||TRIM(X3)||TRIM(X4) CATS(OF X1-X4) TRIM(LEFT(X1))||TRIM(LEFT(X2))||TRIM(LEFT(X3))||TRIM(LEFT(X4)) CATX(SP, OF X1-X4) TRIM(LEFT(X1))||SP||TRIM(LEFT(X2))||SP||TRIM(LEFT(X3))||SP||TRIM(LEFT(X4)) Notice that, although no length IS set for the resultant concatenated variable, the longer values that came later in the step are not truncated. Where the length has not been set, the CAT functions will set a length of 200 . This is another significant advantage over using the || operator. Items, or arguments, can be numeric instead of character; numeric arguments are converted to character using BESTw. format.","title":"CATX Concatenation Function"},{"location":"essentials/manipulation/#search-for-required-values","text":"Searching for a single character value (at least one 'Y' answer in a ten-questions questionnaire): 1 2 3 4 DATA yes ; SET answers ; IF FIND ( CAT ( of a1 - a10 ) , ' Y ' ) ; RUN ; Searching for multiple character values (at least two 'Y' answer in a ten-questions questionnaire): 1 2 3 4 DATA yes ; SET answers ; IF COUNTC ( CAT ( OF a1 - a10 ) , ' Y ' ) GE 2 ; RUN ; Searching for numeric values (at least one question answered with the value 5 in a ten-questions questionnaire): 1 2 3 4 DATA yes ; SET answers ; IF FIND ( CAT ( OF a : ) , ' 5 ' ) ; RUN ; Searching for a single numeric pattern (variable whose value starts with '250'): 1 2 3 4 DATA diagnostic - results ; SET diagnostic - tests ; IF FIND ( CATX ( ' * ' , OF diag1 - diag5 ) , ' 250 ' ) ne 0 ; RUN ; Searching for multiple numeric patterns (variable whose value starts with '250' or '493'): 1 2 3 4 5 6 7 8 DATA diagnostic - results ; LENGTH string $ 100 ; SET diagnostic - tests ; string = CATX ( '*' , OF diag1 - diag5 ); diabetes = ( FIND ( string , '250' ) ne 0 ); asthma = ( FIND ( string , '493' ) ne 0 ); DROP string ; RUN ; Search for characters in numeric variables : 1 2 3 4 5 6 7 DATA final - SAS - data - set ; SET origin - SAS - data - set ; ARRAY arrayname ( 3 ) item1 - item3 ; DO _N_ = 1 TO 3 UNTIL ( search eq 1 ) ; search = ( CAT ( arrayname ( _N_ )) IN : ( ' 171 ' ' 172 ' )) ; END ; RUN ;","title":"Search for Required Values"},{"location":"essentials/manipulation/#date-function","text":"1 2 3 4 5 6 7 8 YEAR ( SAS - date ) QTR ( SAS - date ) MONTH ( SAS - date ) DAY ( SAS - date ) WEEKDAY ( SAS - date ) TODAY () /* Obtain the current date and convert to SAS-date (no argument) */ DATE () /* Obtain the current date and convert to SAS-date (no argument) */ MDY ( month , day , year ) The arguments must be numeric values (except from TODAY() and DATE() functions) You can subtract dates: Agein2012=(Bday2012-Birth_Date)/365.25;","title":"DATE Function"},{"location":"essentials/manipulation/#intck-time-interval-function","text":"1 INTCK ( 'year' , Hire_Date , '01JAN2012' d ) The INTCK function returns the number of interval boundaries of a given kind that lie between the two dates, times, or datetime values. In the code, you first specify the interval value. What happens when you define a new variable from another that you are gonna DROP in this DATA statement? The DROP statement is a compile-time-only statement. SAS sets a drop flag for the dropped variables, but the variables are in the PDV and, therefore, are available for processing.","title":"INTCK Time Interval Function"},{"location":"essentials/manipulation/#sum-summation-function","text":"1 SUM ( argument1 , argument2 , ...) The arguments must be numeric values The SUM function ignores missing values, so if an argument has a missing value, the result of the SUM function is the sum of the nonmissing values If you add two values by + , if one of them is missing, the result will be a missing value, which makes the SUM function a better choice","title":"SUM Summation Function"},{"location":"essentials/manipulation/#conditional-processing","text":"","title":"Conditional Processing"},{"location":"essentials/manipulation/#if-then-else-conditional-structures","text":"1 2 3 IF expression THEN statement ; ELSE IF expression THEN statement ; ELSE statement ; In the conditional expressions involving strings watch out for possible mixed case values where the condition may not be met: country = UPCASE(country); to avoid problems What do you mean 0.73 doesn't equal 0.73? When comparing numeric values, you may get unexpected results when the values of your variables seem to be the same (but actually they are not). The key to making sure these issues do not introduce erroneous results lies in understanding how numeric values are actually stored in data sets and in memory. This is referred to as numeric representation and precision . Read this article for more information: Numeric Representation and Precision in SAS and Why it Matters . Main solutions to this problem: The ROUND function: A = round(A, 0.01); Character versions of numeric variables: CharA = put(A, best.); Options in procedures: PROC COMPARE BASE=baseds COMPARE=compds CRITERION=0.0000001;","title":"IF-THEN-ELSE Conditional Structures"},{"location":"essentials/manipulation/#executing-multiple-statements","text":"1 2 3 4 5 6 7 8 IF expression THEN DO ; executable statements ; END ; ELSE IF expression THEN DO ; executable statements ; END ; In the DATA step, the first reference to a variable determines its length. The first reference to a new variable can be in a LENGTH statement, an assignment statement, or another statement such as an INPUT statement. After a variable is created in the PDV, the length of the variable's first value doesn't matter. To avoid truncation in a variable defined inside a conditional structure you can: Define the longer string as the first condition Add some blanks at the end of shorter strings to fit the longer one Define the length explicitly before any other reference to the variable","title":"Executing Multiple Statements"},{"location":"essentials/manipulation/#select-group","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 SELECT ( Gender ) ; WHEN ( ' F ' ) DO ; Gift1 = ' Scarf ' ; Gift2 = ' Pedometer ' ; END ; WHEN ( ' M ' ) DO ; Gift1 = ' Gloves ' ; Gift2 = ' Money Clip ' ; END ; OTHERWISE DO ; Gift1 = ' Coffee ' ; Gift2 = ' Calendar ' ; END ; END ; The SELECT statement executes one of several statements or groups of statements The SELECT statement begins a SELECT group. They contain WHEN statements that identify SAS statements that are executed when a particular condition is true Use at least one WHEN statement in a SELECT group An optional OTHERWISE statement specifies a statement to be executed if no WHEN condition is met An END statement ends a SELECT group","title":"SELECT Group"},{"location":"essentials/manipulation/#avoiding-duplicates","text":"The dataset period has more than one register per patient and the calculation of time periods between each one and the reference. 1 2 3 PROC SORT DATA = period ; BY pt periodmax periodvisit ; RUN ; If you want to keep only highest/lowest value (last/first value after the sorting), you need to use the last. / .first functions. 1 2 3 4 5 DATA maxperiod ; SET period ; BY pt ; IF last . pt ; RUN ;","title":"Avoiding Duplicates"},{"location":"essentials/reading-raw/","text":"Chapter summary in SAS Introduction to Reading Raw Data Files Link Raw data files are not software specific A delimited raw data file is an external text file in which the values are separated by spaces or other special characters. A list input will be used to work with delimited raw data files that contain standard and/or nonstandard data Standard data is data that SAS can read without any special instructions Nonstandard data includes values like dates or numeric values that include special characters like dollar signs (extra instructions needed) You cannot use a WHERE statement when the input data is a raw data file instead of a SAS data set Reading Standard Delimited Data Link 1 2 3 4 5 6 7 8 9 10 11 12 13 DATA output - SAS - data - set - name ; LENGTH variable ( s ) < $ > length ; INFILE 'raw-data-file-name' DLM = 'delimiter' ; INPUT variable1 < $ > variable2 < $ > ... variableN < $ > ; RUN ; /* Example */ DATA work . sales1 ; LENGTH First_Name Last_Name $ 12 Gender $ 1 ; INFILE '&path/sales.csv' DLM = ',' ; INPUT Employee_ID Gender $ Salary $ Job_Title $ Country $ ; RUN ; With list input , the default length for all variables is 8 bytes SAS uses an input buffer only if the input data is a raw data file The variable names will appear in the report as stated in the LENGTH statement (watch out the uppercase/lowercase) The LENGTH statement must precede the INPUT statement in order to correctly set the length of the variable The variables not specified in the LENGTH statement will appear at the end of the table. If you want to keep the original order you should include all variables even if you want them to have the defaul length (8) Reading Nonstandard Delimited Data Link You can use a modified list input to read all of the fields from a raw data file (including nonstandard variables) Informats are similar to formats except that formats provide instruction on how to write a value while informats provide instruction on how to read a value The colon format modifier (:) causes SAS to read up to the delimiter 1 2 3 4 5 6 INPUT variable < $ > variable < : informat > ; /* Example */ : date . : mmddyy . COMMA./DOLLAR. : reads nonstandard numeric data and removes embedded commas, blanks, dollar sign, percent signs and dashes COMMAX./DOLLARX. : reads nonstandard numeric data and removes embedded non-numeric characters; reverses the roles of the decima point and the comma EUROX. : reads nonstandard numeric data and removes embedded non-numeric characters in European currency $CHAR. : reads character values and preserves leading blanks $UPCASE. : reads character values and converts them to uppercase 1 2 3 4 5 6 7 8 9 10 DATA ( ... ) ; INFILE DATALINES DLM = ' , ' ; /* only if datalines are delimited */ INPUT ( ... ) ; DATALINES ; < instream data > ; INPUT ( ... ) ; DATALINES ; < instream data > ; The null statement ( ; ) indicates the end of the input data You precede the instream data with the DATALINES statement and follow it with a null statement The instream data should be the last part of the DATA step except for a null statement 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 /* Example */ DATA work . managers ; infile datalines dlm = '/' ; input ID First : $ 12 . Last : $ 12 . Gender $ Salary : comma . Title : $ 25 . HireDate : date . ; datalines ; 120102 / Tom / Zhou / M / 108 , 255 / Sales Manager / 01 Jun1993 120103 / Wilson / Dawes / M / 87 , 975 / Sales Manager / 01 Jan1978 120261 / Harry / Highpoint / M / 243 , 190 / Chief Sales Officer / 01 Aug1991 121143 / Louis / Favaron / M / 95 , 090 / Senior Sales Manager / 01 Jul2001 121144 / Renee / Capachietti / F / 83 , 505 / Sales Manager / 01 Nov1995 121145 / Dennis / Lansberry / M / 84 , 260 / Sales Manager / 01 Apr1980 ; title 'Orion Star Management Team' ; proc print data = work . managers noobs ; format HireDate mmddyy10 . ; run ; title ; Validating Data Link When SAS encounters a data error, it prints messages and a ruler in the log and assigns a missing value to the affected variable. Then SAS continues processing. Missing Values between Delimiters (Consecutive Delimiters) Link 1 INFILE 'raw-data-file-name' < DLM => DSD ; The DSD option sets the default delimiter to a comma, treats consecutive delimiters as missing values and enables SAS to read values with embedded delimiters if the value is surrounded by quotation marks Missing Values at the End of a Line Link 1 INFILE 'raw-data-file-name' MISSOVER ; With the MISSOVER option, if SAS reaches the end of a record without finding values for all fields, variables without values are set to missing.","title":"Reading Raw Data Files"},{"location":"essentials/reading-raw/#introduction-to-reading-raw-data-files","text":"Raw data files are not software specific A delimited raw data file is an external text file in which the values are separated by spaces or other special characters. A list input will be used to work with delimited raw data files that contain standard and/or nonstandard data Standard data is data that SAS can read without any special instructions Nonstandard data includes values like dates or numeric values that include special characters like dollar signs (extra instructions needed) You cannot use a WHERE statement when the input data is a raw data file instead of a SAS data set","title":"Introduction to Reading Raw Data Files"},{"location":"essentials/reading-raw/#reading-standard-delimited-data","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 DATA output - SAS - data - set - name ; LENGTH variable ( s ) < $ > length ; INFILE 'raw-data-file-name' DLM = 'delimiter' ; INPUT variable1 < $ > variable2 < $ > ... variableN < $ > ; RUN ; /* Example */ DATA work . sales1 ; LENGTH First_Name Last_Name $ 12 Gender $ 1 ; INFILE '&path/sales.csv' DLM = ',' ; INPUT Employee_ID Gender $ Salary $ Job_Title $ Country $ ; RUN ; With list input , the default length for all variables is 8 bytes SAS uses an input buffer only if the input data is a raw data file The variable names will appear in the report as stated in the LENGTH statement (watch out the uppercase/lowercase) The LENGTH statement must precede the INPUT statement in order to correctly set the length of the variable The variables not specified in the LENGTH statement will appear at the end of the table. If you want to keep the original order you should include all variables even if you want them to have the defaul length (8)","title":"Reading Standard Delimited Data"},{"location":"essentials/reading-raw/#reading-nonstandard-delimited-data","text":"You can use a modified list input to read all of the fields from a raw data file (including nonstandard variables) Informats are similar to formats except that formats provide instruction on how to write a value while informats provide instruction on how to read a value The colon format modifier (:) causes SAS to read up to the delimiter 1 2 3 4 5 6 INPUT variable < $ > variable < : informat > ; /* Example */ : date . : mmddyy . COMMA./DOLLAR. : reads nonstandard numeric data and removes embedded commas, blanks, dollar sign, percent signs and dashes COMMAX./DOLLARX. : reads nonstandard numeric data and removes embedded non-numeric characters; reverses the roles of the decima point and the comma EUROX. : reads nonstandard numeric data and removes embedded non-numeric characters in European currency $CHAR. : reads character values and preserves leading blanks $UPCASE. : reads character values and converts them to uppercase 1 2 3 4 5 6 7 8 9 10 DATA ( ... ) ; INFILE DATALINES DLM = ' , ' ; /* only if datalines are delimited */ INPUT ( ... ) ; DATALINES ; < instream data > ; INPUT ( ... ) ; DATALINES ; < instream data > ; The null statement ( ; ) indicates the end of the input data You precede the instream data with the DATALINES statement and follow it with a null statement The instream data should be the last part of the DATA step except for a null statement 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 /* Example */ DATA work . managers ; infile datalines dlm = '/' ; input ID First : $ 12 . Last : $ 12 . Gender $ Salary : comma . Title : $ 25 . HireDate : date . ; datalines ; 120102 / Tom / Zhou / M / 108 , 255 / Sales Manager / 01 Jun1993 120103 / Wilson / Dawes / M / 87 , 975 / Sales Manager / 01 Jan1978 120261 / Harry / Highpoint / M / 243 , 190 / Chief Sales Officer / 01 Aug1991 121143 / Louis / Favaron / M / 95 , 090 / Senior Sales Manager / 01 Jul2001 121144 / Renee / Capachietti / F / 83 , 505 / Sales Manager / 01 Nov1995 121145 / Dennis / Lansberry / M / 84 , 260 / Sales Manager / 01 Apr1980 ; title 'Orion Star Management Team' ; proc print data = work . managers noobs ; format HireDate mmddyy10 . ; run ; title ;","title":"Reading Nonstandard Delimited Data"},{"location":"essentials/reading-raw/#validating-data","text":"When SAS encounters a data error, it prints messages and a ruler in the log and assigns a missing value to the affected variable. Then SAS continues processing.","title":"Validating Data"},{"location":"essentials/reading-raw/#missing-values-between-delimiters-consecutive-delimiters","text":"1 INFILE 'raw-data-file-name' < DLM => DSD ; The DSD option sets the default delimiter to a comma, treats consecutive delimiters as missing values and enables SAS to read values with embedded delimiters if the value is surrounded by quotation marks","title":"Missing Values between Delimiters (Consecutive Delimiters)"},{"location":"essentials/reading-raw/#missing-values-at-the-end-of-a-line","text":"1 INFILE 'raw-data-file-name' MISSOVER ; With the MISSOVER option, if SAS reaches the end of a record without finding values for all fields, variables without values are set to missing.","title":"Missing Values at the End of a Line"},{"location":"essentials/reading-sas/","text":"Chapter summary in SAS Subsetting using the WHERE statement Link To create a new data set that is a subset of a previous data set: 1 2 3 4 5 6 DATA output - SAS - data - set ; SET input - SAS - data - set ; WHERE where - expression ; variable_name WHERE where - expression ; variable_name = expression ; /* new variable creation */ RUN ; Note If a missing value is involved in an arithmetic calculation the result will be a missing value too New variables being created in the DATA step and not contained in the original data set cannot be used in a WHERE statement Other WHERE options Link Here's how to set a filter for WHERE a variable IS MISSING Customizing a SAS Data Set Link How to select a subset of the variables/observations of the original data set: 1 2 3 4 5 DATA output - SAS - data - set ; SET input - SAS - data - set ; DROP variable - list ; /* variables to exclude */ KEEP variable - list ; /* variables to include */ RUN ; How SAS processes the DATA step Link Compilation phase SAS scan each DATA step statement for syntax errors and converts the program into machine code if everything's alright. SAS also creates the program data vector ( PDV ) in memory to hold the current observation. _N_ : iteration number of the DATA step _ERROR_ : its value is 0 is there are no errors (1 if there are some) SAS creates the descriptor portion of the new data set (takes the original one, adds the new variables and flags the variables to be dropped). Execution phase SAS initializes the PDV to missing SAS reads and processes the observations from the input data set SAS creates observations in the data portion of the output data set (an implicit output/implicit return loop over all the observations that continues until EOF) Subsetting using the IF statement Link 1 2 3 4 DATA output - SAS - data - set ; SET input - SAS - data - set ; IF expression ; RUN ; When the expression is false, SAS excludes the observation from the output data set and continues processing While original values can be managed with a WHERE statement as well as an IF statement, for new variable conditionals only IF can be used You should subset as early as possible in your program for more efficient processing (a WHERE before an IF can make the processing more efficient). In a PROC step IF statements are NOT allowed IF THEN analogue to CONTAINS : 1 IF FIND ( variable_name , ' pattern ' ) THEN ( ... ) ; Subsetting IF-THEN/DELETE statement Link 1 2 3 4 DATA output - SAS - data - set ; SET input - SAS - data - set ; IF expression1 or expression2 THEN DELETE ; RUN ; The IF-THEN/DELETE statement eliminates the observations where the conditions are not met (on the contrary of what the IF does) The DELETE statement stops processing the current observation. It is often used in a THEN clause of an IF-THEN statement or as part of a conditionally executed DO group. Tip You can remove all the observations with at least one missing value using this condition inside a DATA step: if cmiss(of _all_) then delete; Create different data sets from one Link 1 2 3 4 5 6 DATA data1 data2 data3 ; SET original_data ; IF ( condition1 ) THEN OUTPUT prueba1 ; IF ( condition2 ) THEN OUTPUT prueba2 ; IF ( condition3 ) THEN OUTPUT prueba3 ; RUN ; Available operations Link Addition of several variables: Total=sum(var1, var2, var3) Count of nonmissing values: Nonmissing=n(var1, var2, var3) Using PROC SQL to GROUP BY variables Link PROC SQL is a wonderful tool for summarizing or aggregating data. When you use a GROUP BY clause, you also use an aggregate function in the SELECT clause or in a HAVING clause to instruct PROC SQL in how to summarize the data for each group: 1 2 3 4 5 6 7 PROC SQL ; select T , range ( survival ) as RangeSurvival , sqrt ( sum ( sdf_stderr ** 2 )) as Squares , range ( survival ) / sqrt ( sum ( sdf_stderr ** 2 )) as z , probnorm ( abs ( range ( survival ) / sqrt ( sum ( sdf_stderr ** 2 )))) as pz , 2 * ( 1 - probnorm ( abs ( range ( survival ) / sqrt ( sum ( sdf_stderr ** 2 ))))) as pvalue from BTM_param where T > 0 group by T ; QUIT ; Adding Permanent Attributes Link Permanent variable labels Link 1 2 3 4 5 DATA output - SAS - data - set ; SET input - SAS - data - set ; LABEL variable1 = 'label1' variable2 = 'label2' ; RUN ; 1 2 PROC PRINT DATA = output - SAS - data - set label ; RUN ; If you use the LABEL statement in the PROC step the labels are temporary while if you use it in the DATA step, SAS permanently associates the labels to the variables Labels and formats that you specify in PROC steps override the permanent labels in the current step. However, the permanent labels are not changed. Permanent variable formats Link 1 2 3 4 5 DATA output - SAS - data - set ; SET input - SAS - data - set ; FORMAT variable1 format1 variable2 format2 ; RUN ;","title":"Reading SAS Data Sets"},{"location":"essentials/reading-sas/#subsetting-using-the-where-statement","text":"To create a new data set that is a subset of a previous data set: 1 2 3 4 5 6 DATA output - SAS - data - set ; SET input - SAS - data - set ; WHERE where - expression ; variable_name WHERE where - expression ; variable_name = expression ; /* new variable creation */ RUN ; Note If a missing value is involved in an arithmetic calculation the result will be a missing value too New variables being created in the DATA step and not contained in the original data set cannot be used in a WHERE statement","title":"Subsetting using the WHERE statement"},{"location":"essentials/reading-sas/#other-where-options","text":"Here's how to set a filter for WHERE a variable IS MISSING","title":"Other WHERE options"},{"location":"essentials/reading-sas/#customizing-a-sas-data-set","text":"How to select a subset of the variables/observations of the original data set: 1 2 3 4 5 DATA output - SAS - data - set ; SET input - SAS - data - set ; DROP variable - list ; /* variables to exclude */ KEEP variable - list ; /* variables to include */ RUN ;","title":"Customizing a SAS Data Set"},{"location":"essentials/reading-sas/#how-sas-processes-the-data-step","text":"Compilation phase SAS scan each DATA step statement for syntax errors and converts the program into machine code if everything's alright. SAS also creates the program data vector ( PDV ) in memory to hold the current observation. _N_ : iteration number of the DATA step _ERROR_ : its value is 0 is there are no errors (1 if there are some) SAS creates the descriptor portion of the new data set (takes the original one, adds the new variables and flags the variables to be dropped). Execution phase SAS initializes the PDV to missing SAS reads and processes the observations from the input data set SAS creates observations in the data portion of the output data set (an implicit output/implicit return loop over all the observations that continues until EOF)","title":"How SAS processes the DATA step"},{"location":"essentials/reading-sas/#subsetting-using-the-if-statement","text":"1 2 3 4 DATA output - SAS - data - set ; SET input - SAS - data - set ; IF expression ; RUN ; When the expression is false, SAS excludes the observation from the output data set and continues processing While original values can be managed with a WHERE statement as well as an IF statement, for new variable conditionals only IF can be used You should subset as early as possible in your program for more efficient processing (a WHERE before an IF can make the processing more efficient). In a PROC step IF statements are NOT allowed IF THEN analogue to CONTAINS : 1 IF FIND ( variable_name , ' pattern ' ) THEN ( ... ) ;","title":"Subsetting using the IF statement"},{"location":"essentials/reading-sas/#subsetting-if-thendelete-statement","text":"1 2 3 4 DATA output - SAS - data - set ; SET input - SAS - data - set ; IF expression1 or expression2 THEN DELETE ; RUN ; The IF-THEN/DELETE statement eliminates the observations where the conditions are not met (on the contrary of what the IF does) The DELETE statement stops processing the current observation. It is often used in a THEN clause of an IF-THEN statement or as part of a conditionally executed DO group. Tip You can remove all the observations with at least one missing value using this condition inside a DATA step: if cmiss(of _all_) then delete;","title":"Subsetting IF-THEN/DELETE statement"},{"location":"essentials/reading-sas/#create-different-data-sets-from-one","text":"1 2 3 4 5 6 DATA data1 data2 data3 ; SET original_data ; IF ( condition1 ) THEN OUTPUT prueba1 ; IF ( condition2 ) THEN OUTPUT prueba2 ; IF ( condition3 ) THEN OUTPUT prueba3 ; RUN ;","title":"Create different data sets from one"},{"location":"essentials/reading-sas/#available-operations","text":"Addition of several variables: Total=sum(var1, var2, var3) Count of nonmissing values: Nonmissing=n(var1, var2, var3)","title":"Available operations"},{"location":"essentials/reading-sas/#using-proc-sql-to-group-by-variables","text":"PROC SQL is a wonderful tool for summarizing or aggregating data. When you use a GROUP BY clause, you also use an aggregate function in the SELECT clause or in a HAVING clause to instruct PROC SQL in how to summarize the data for each group: 1 2 3 4 5 6 7 PROC SQL ; select T , range ( survival ) as RangeSurvival , sqrt ( sum ( sdf_stderr ** 2 )) as Squares , range ( survival ) / sqrt ( sum ( sdf_stderr ** 2 )) as z , probnorm ( abs ( range ( survival ) / sqrt ( sum ( sdf_stderr ** 2 )))) as pz , 2 * ( 1 - probnorm ( abs ( range ( survival ) / sqrt ( sum ( sdf_stderr ** 2 ))))) as pvalue from BTM_param where T > 0 group by T ; QUIT ;","title":"Using PROC SQL to GROUP BY variables"},{"location":"essentials/reading-sas/#adding-permanent-attributes","text":"","title":"Adding Permanent Attributes"},{"location":"essentials/reading-sas/#permanent-variable-labels","text":"1 2 3 4 5 DATA output - SAS - data - set ; SET input - SAS - data - set ; LABEL variable1 = 'label1' variable2 = 'label2' ; RUN ; 1 2 PROC PRINT DATA = output - SAS - data - set label ; RUN ; If you use the LABEL statement in the PROC step the labels are temporary while if you use it in the DATA step, SAS permanently associates the labels to the variables Labels and formats that you specify in PROC steps override the permanent labels in the current step. However, the permanent labels are not changed.","title":"Permanent variable labels"},{"location":"essentials/reading-sas/#permanent-variable-formats","text":"1 2 3 4 5 DATA output - SAS - data - set ; SET input - SAS - data - set ; FORMAT variable1 format1 variable2 format2 ; RUN ;","title":"Permanent variable formats"},{"location":"essentials/reading-spreadsheets/","text":"Chapter summary in SAS Reading Spreadsheet Data Link SAS/ACCESS LIBNAME statement (read/write/update data): 1 LIBNAME libref < engine > < PATH => \"workbook-name\" < options > ; E.g.: Default engine: LIBNAME orionx excel \"&path/sales.xls\" PC Files server engine: LIBNAME orionx pcfiles PATH=\"&path/sales.xls\" <\\engine> : excel (if both SAS and Office are 32/64 bits), pcfiles (if the value is different) The icon of the library will be different (a globe) indicating that the data is outside SAS The members whose name ends with a $ are the spreadsheets while the others are named ranges . In case it has the $ , you need to refer to that Excel worksheet in a special way to account for that special character (SAS name literal): libref.'worksheetname$'n You can use the VALIDVARNAME = v7 option in SAS Enterprise Guide to cause it to behave the same as in the SAS window environment Is important to disassociate the library: the workbook cannot be opened in Excel meanwhile (SAS puts a lock on the Excel file when the libref is assigned): LIBNAME libref CLEAR; Import the xlsx data: 1 2 3 4 LIBNAME x XLSX \"/folders/myfolders/reading_test.xlsx\" ; DATA SAS - dataset ; SET x . sheetname ; RUN ; 1 2 3 4 5 6 PROC IMPORT DATAFILE = \"/folders/myfolders/reading_test.xlsx\" OUT = work . myexcel DBMS = xlsx REPLACE SHEET = sheetname ; RUN ; Reading Database Data Link 1 LIBNAME libref engine < SAS / ACCESS options > ; engine : oracle or BD2 SAS/ACCESS options : USER, PASSWORD/PW, PATH (specifies the Oracle driver, node and database), SCHEMA (enables you to read database objects such as tables and views)","title":"Reading Spreadsheet and Database Data"},{"location":"essentials/reading-spreadsheets/#reading-spreadsheet-data","text":"SAS/ACCESS LIBNAME statement (read/write/update data): 1 LIBNAME libref < engine > < PATH => \"workbook-name\" < options > ; E.g.: Default engine: LIBNAME orionx excel \"&path/sales.xls\" PC Files server engine: LIBNAME orionx pcfiles PATH=\"&path/sales.xls\" <\\engine> : excel (if both SAS and Office are 32/64 bits), pcfiles (if the value is different) The icon of the library will be different (a globe) indicating that the data is outside SAS The members whose name ends with a $ are the spreadsheets while the others are named ranges . In case it has the $ , you need to refer to that Excel worksheet in a special way to account for that special character (SAS name literal): libref.'worksheetname$'n You can use the VALIDVARNAME = v7 option in SAS Enterprise Guide to cause it to behave the same as in the SAS window environment Is important to disassociate the library: the workbook cannot be opened in Excel meanwhile (SAS puts a lock on the Excel file when the libref is assigned): LIBNAME libref CLEAR; Import the xlsx data: 1 2 3 4 LIBNAME x XLSX \"/folders/myfolders/reading_test.xlsx\" ; DATA SAS - dataset ; SET x . sheetname ; RUN ; 1 2 3 4 5 6 PROC IMPORT DATAFILE = \"/folders/myfolders/reading_test.xlsx\" OUT = work . myexcel DBMS = xlsx REPLACE SHEET = sheetname ; RUN ;","title":"Reading Spreadsheet Data"},{"location":"essentials/reading-spreadsheets/#reading-database-data","text":"1 LIBNAME libref engine < SAS / ACCESS options > ; engine : oracle or BD2 SAS/ACCESS options : USER, PASSWORD/PW, PATH (specifies the Oracle driver, node and database), SCHEMA (enables you to read database objects such as tables and views)","title":"Reading Database Data"},{"location":"essentials/reporting/","text":"Chapter summary in SAS Subsetting Report Data Link 1 2 3 4 5 6 7 8 9 10 PROC PRINT DATA = SAS - data - set ( OBS = 3 ) NOOBS ; /* OBS=3 prints only 3 elements | NOOBS hides the 'Obs' */ VAR variable1 variable2 variable3 ; /* prints out only this variables in the report */ SUM variable1 variable2 ; /* adds an extra line at the end with the total */ WHERE variable3 < 1000 ; variable3<1000; /* operators: < > <= >= = ^= in + - / * ** & | ~ ^ ? */ WHERE variable4 in ( ' Child ' , ' Elder ' ) ; /* only the last WHERE condition is applied */ WHERE variable1 = 20 AND variable4 CONTAINS ' case-sensitive-substring ' ; /* CONTAINS = ? */ IDWHERE ANYALPHA ( variable ) NE 0 /* only values containing at least a letter */ ID variable1 /* replaces the 'Obs' column by a selected variable values */ BY variable3variable3 /* separate in different tables for different variable values (sort first) */ RUN ; Special WHERE operators: BETWEEN x AND y : an inclusive range WHERE SAME AND : augment a previous where expression (both applied) IS NULL : a missing value IS MISSING : a missing value LIKE : matches a pattern (% = any number of characters, _ = one character). E.g.: 'T_m%' The SOUNDS-LIKE (=\\*) operator selects observations that contain a spelling variation of a specified word or words. This operator uses the Soundex algorithm to compare the variable value and the operand. ANYVALUE is an interesting function that searches a character string for an alphabetic character, and returns the first position at which the character is found Note To compare with a SAS date value you need to express is as a SAS date constant: 'DDMM<\\YY>YY'D . Sorting and Grouping Report Data Link 1 2 3 4 5 PROC SORT DATA = SAS - data - set OUT = new - SAS - data - set NODUPKEY ; /* optional */ DUPOUT = work . duplicates ; /* optional */ BY ASCENDING variable1 - to - be - sorted DESCENDING variable2 - to - be - sorted ; /* optional (ASCENDING is the default order)*/ RUN ; The NODUPKEY option deletes observations with duplicate BY values DUPOUT writes duplicate observations to a separate output data set Enhancing Reports Link Check this section for more details and examples on reporting results. Titles and footnotes Link 1 2 3 4 5 6 TITLEline 'text' ; FOOTNOTEline 'text' ; TITLE1 'text1' ; TITLE1 'text1_change' ; /* Change title text and also cancels all footnotes with higher numbers */ TITLE ; /* Cancel (erase) all titles */ The lines specifies the line (1-10) on which the title/footnote will appear (line = 1 is the default value) The title/footnote will remain until you change it, cancel it or you end your SAS session Labels Link Assigning temporary labels to display in the report instead of the variable names: 1 2 3 4 5 PROC PRINT DATA = SAS - data - set LABEL ; LABEL variable1 = 'new variable1 name' variable2 = 'new variable2 name' ; LABEL variable3 = 'new variable3 name' ; RUN ; You need to add the LABEL option at the PROC PRINT definition to display the labels The LABEL lengths can go up to 256 characters long You can specify several labels in one LABEL statement or use a separate LABEL statement for each variable The SPLIT option Link 1 2 3 PROC PRINT DATA = SAS - data - set SPLIT = '*' ; LABEL variable1 = 'variable label line 1*variable label line 1' ; RUN ; When you use SPLIT you no longer need to add the LABEL option to get the labels printed out The SPLIT option introduces a line break at the label text whenever it finds the specified character ( * )","title":"Producing Detailed Reports"},{"location":"essentials/reporting/#subsetting-report-data","text":"1 2 3 4 5 6 7 8 9 10 PROC PRINT DATA = SAS - data - set ( OBS = 3 ) NOOBS ; /* OBS=3 prints only 3 elements | NOOBS hides the 'Obs' */ VAR variable1 variable2 variable3 ; /* prints out only this variables in the report */ SUM variable1 variable2 ; /* adds an extra line at the end with the total */ WHERE variable3 < 1000 ; variable3<1000; /* operators: < > <= >= = ^= in + - / * ** & | ~ ^ ? */ WHERE variable4 in ( ' Child ' , ' Elder ' ) ; /* only the last WHERE condition is applied */ WHERE variable1 = 20 AND variable4 CONTAINS ' case-sensitive-substring ' ; /* CONTAINS = ? */ IDWHERE ANYALPHA ( variable ) NE 0 /* only values containing at least a letter */ ID variable1 /* replaces the 'Obs' column by a selected variable values */ BY variable3variable3 /* separate in different tables for different variable values (sort first) */ RUN ; Special WHERE operators: BETWEEN x AND y : an inclusive range WHERE SAME AND : augment a previous where expression (both applied) IS NULL : a missing value IS MISSING : a missing value LIKE : matches a pattern (% = any number of characters, _ = one character). E.g.: 'T_m%' The SOUNDS-LIKE (=\\*) operator selects observations that contain a spelling variation of a specified word or words. This operator uses the Soundex algorithm to compare the variable value and the operand. ANYVALUE is an interesting function that searches a character string for an alphabetic character, and returns the first position at which the character is found Note To compare with a SAS date value you need to express is as a SAS date constant: 'DDMM<\\YY>YY'D .","title":"Subsetting Report Data"},{"location":"essentials/reporting/#sorting-and-grouping-report-data","text":"1 2 3 4 5 PROC SORT DATA = SAS - data - set OUT = new - SAS - data - set NODUPKEY ; /* optional */ DUPOUT = work . duplicates ; /* optional */ BY ASCENDING variable1 - to - be - sorted DESCENDING variable2 - to - be - sorted ; /* optional (ASCENDING is the default order)*/ RUN ; The NODUPKEY option deletes observations with duplicate BY values DUPOUT writes duplicate observations to a separate output data set","title":"Sorting and Grouping Report Data"},{"location":"essentials/reporting/#enhancing-reports","text":"Check this section for more details and examples on reporting results.","title":"Enhancing Reports"},{"location":"essentials/reporting/#titles-and-footnotes","text":"1 2 3 4 5 6 TITLEline 'text' ; FOOTNOTEline 'text' ; TITLE1 'text1' ; TITLE1 'text1_change' ; /* Change title text and also cancels all footnotes with higher numbers */ TITLE ; /* Cancel (erase) all titles */ The lines specifies the line (1-10) on which the title/footnote will appear (line = 1 is the default value) The title/footnote will remain until you change it, cancel it or you end your SAS session","title":"Titles and footnotes"},{"location":"essentials/reporting/#labels","text":"Assigning temporary labels to display in the report instead of the variable names: 1 2 3 4 5 PROC PRINT DATA = SAS - data - set LABEL ; LABEL variable1 = 'new variable1 name' variable2 = 'new variable2 name' ; LABEL variable3 = 'new variable3 name' ; RUN ; You need to add the LABEL option at the PROC PRINT definition to display the labels The LABEL lengths can go up to 256 characters long You can specify several labels in one LABEL statement or use a separate LABEL statement for each variable","title":"Labels"},{"location":"essentials/reporting/#the-split-option","text":"1 2 3 PROC PRINT DATA = SAS - data - set SPLIT = '*' ; LABEL variable1 = 'variable label line 1*variable label line 1' ; RUN ; When you use SPLIT you no longer need to add the LABEL option to get the labels printed out The SPLIT option introduces a line break at the label text whenever it finds the specified character ( * )","title":"The SPLIT option"},{"location":"essentials/summary/","text":"Chapter summary in SAS PROC FREQ to Create Summary Reports Link When you're summarizing data, there's no need to show a frequency distribution for variables that have a large number of distinct values Frequency distributions work best with variables whose values meet two criteria: variable with categorical values and values are best summarized by counts instead of averages Variables that have continuous numerical values, such as dollar amounts and dates, will need to be grouped into categories by applying formats inside the PROC FREQ step (substitute an specific range of those values by a tag) 1 2 3 4 PROC FREQ DATA=SAS-data-set <option (s) > ; TABLE(S) variable(s) < / option(s)>; <additional statements > RUN; PROC FREQ produces frequency tables that report the distribution of any or all variable values in a SAS data set In the TABLE(S) statement you specify the frequency tables to produce Each unique variable's value displayed in the 1 st column of the output is called a level of the variable One-way Frequency Tables Link To create one-way frequency tables you specify one or more variable names separated by space. Warning If you omit the TABLE statement, SAS produces a one-way tables for every variable in the data set, which could be very messy if you have a lot of variables. 1 2 3 4 5 6 7 8 9 10 PROC SORT DATA = SAS - data - set OUT = SAS - data - set - sorted ; BY variable_sorted ; RUN ; PROC FREQ DATA = SAS - data - set - sorted ORDER = FREQ < option ( s ) > ; TABLES variable / NOCUM NOPERCENT OUT = custom - output - name ; BY variable_sorted ; < additional statements > RUN ; NOCUM option supresses the display of the cummulative frequency and cummulative percent values NOPERCENT option supresses the display of all percentages ORDER=FREQ option orders the output in descending frequency order OUT= option saves the output data set with a custom name BY option produces a frequency table for each value of variable_sorted (the data set must be sorted by the variable named in the statement) Crosstabulation Tables Link Sometimes it is useful to view a single table with statistics for each distintic combination of values of the selected variables The simplest crosstabulation table is a two-way table 1 2 3 PROC FREQ DATA = SAS - data - set ; TABLES variable_rows * variable_columns / NOFREQ NOPERCENT NOROW NOCOL ; RUN ; Information contained in crosstabulation tables (legend): Frequency : indicates the number of observations with the unique combination of values represented in that cell Percent : indicates the cell's percentage of the total frequency Row Pct : cell's percentage of the total frequency for its row Col Pct : cell's percentage of the total frequency for its column LIST option format: the first two columns specify each possible combination of the two variables; it displays the same statistics as the default one-way frequency table CROSSLIST option format: it displays the same statistics as the default crosstabulation table Formatting Variables in PROC FREQ Link The FORMAT= option allows you to format the frequency value (to any SAS numeric format or a user-defined numeric format while its length is not more than 24) and to change the width of the column (e.g. to allow variable labels to fit in one line). 1 2 3 4 5 PROC FREQ DATA = SAS - data - set ; TABLES variable1 * variable2 / FORMAT = < w > .; FORMAT variable1 $ format - name .; RUN ; The FORMAT= option applies only to crosstabulation tables displayed in the default format. It doesn't apply to crosstabulation tables produced with the LIST / CROSSLIST option. Using the MEANS and UNIVARIATE Procedures Link PROC MEANS produces summary reports with descriptive statistics and you can create statistics for groups of observations It automatically displays output in a report and you can also save the output in a SAS data set It reports the number of nonmissing values of the analysis variable (N), and the mean , the standard deviation and minimum / maximum values of every numeric variable in the data set The variables in the CLASS statement are called classification variables or class variables (they typically have few discrete values) Each combination of class variable values is called a class level The data set doesn't need to be sorted or indexed by the class variables N Obs reports the number of observations with each unique combination of class variables, whether or not there are missing values (if these N Obs are identical to N , there are no missing values in you data set) 1 2 3 4 PROC MEANS DATA = SAS - data - set < statistic ( s ) > ; VAR analysis - variable ( s ); CLASS classification - variable ( s ); RUN ; To write the report in a new data set (including total addition): 1 2 3 4 5 PROC MEANS DATA = SAS - data - set NOPRINT NWAY ; OUTPUT OUT = SAS - new - data - set SUM = addition - new - variable ; VAR analysis - variable ( s ); CLASS classification - variable ( s ); RUN ; Format options: MAXDEC=number (default format = BESTw. ) NONOBS FW=number : specifies that the field width for all columns is number PRINTALLTYPES : displays statistics for all requested combination of class variables Sums and counts by group ODS EXCLUDE ALL; PROC MEANS DATA=SAS-data-set SUM; CLASS classification-variable; VAR var1 var2 var3 var4; ODS OUTPUT SUMMARY=name-output-data-set; RUN; ODS EXCLUDE NONE; How to Use these Procedures for Data Validation Link PROC FREQ Link You can use a PROC FREQ step with the TABLES statement to detect invalud numeric and character data by looking at distinct values. The FREQ procedure lists all discrete values for a variable and reports its missing values . 1 2 3 PROC FREQ DATA = SAS - data - set < ORDER = FREQ > ; TABLES variable ; RUN ; You can check for non-expected variable's values You can check for missing values You can find duplicated values The table showing the Number of Variable Levels can indicate whether a variable contains duplicate/missing/non-expected values: 1 2 3 PROC FREQ DATA = SAS - data - set NLEVELS ; TABLES variable / NOPRINT ; RUN ; You can use a WHERE statement to print out only the invalid values to be checked: 1 2 3 4 5 6 PROC PRINT DATA = SAS - data - set ; WHERE gender NOT IN ( 'F' , 'M' ) OR job_title IS NULL OR salary NOT BETWEEN 24000 AND 500000 OR employee IS MISSING ; RUN ; You can output the tables to a new data set instead of displaying it: 1 2 3 PROC FREQ DATA = SAS - data - set NOPRINT ; TABLE variable / OUT = SAS - new - data - set ; RUN ; PROC MEANS Link NMISS option displays the number of observations with missing values The MIN / MAX values can be useful to check if the data is within a range To get the MIN / MAX values per group and the number of observations per group follow the next example: 1 2 3 4 5 PROC MEANS DATA = SAS - data - set MIN MAX ; CLASS grouping - variable ; VAR evaluation - variable ; ODS OUTPUT SUMMARY = SAS - output - data - set ; RUN ; PROC UNIVARIATE Link PROC UNIVARIATE is a procedure that is useful for detecting data outliers that also produces summary reports of descriptive statistics . 1 2 3 4 5 6 7 PROC UNIVARIATE DATA=SAS-data-set; VAR variable(s); ID variable_to_relate; HISTOGRAM variables </options> ; PROBPLOT variables </options> ; INSET keywords </options> ; RUN; If you omit the VAR statement, all numeric variables in the data set are analyzed The Extreme Observations table contains useful information to locate outliers: it displays the 5 lowest/highest values by default along with the corresponding observation number. The ID statement specifies that SAS will use this variable as a label in the table of extreme observations and as an identifier for any extreme. To specify the number of listed observations you can use NEXTROBS= HISTOGRAM / PROBPLOT options: normal(mu=est sigma=est) creates a normal curve overlay to the histogram using the estimates of the population mean and standard deviation INSET writes a legend for the graph. / position=ne moves the INSET to the north-east corner of the graph. To include in the report only one of the automatically produced tables: Check the specific table name in the LOG information using ODS TRACE ON : 1 2 3 4 5 ODS TRACE ON ; PROC UNIVARIATE DATA = SAS - data - set ; VAR variable ( s ); RUN ; ODS TRACE OFF ; Select the wanted table with ODS SELECT : 1 2 3 4 ODS SELECT ExtremeObs ; PROC UNIVARIATE DATA = SAS - data - set ; VAR variable ( s ); RUN ; Summary of Validation Procedures Link Using the SAS Output Delivery System ( ODS ) Link 1 2 3 ODS destination FILE = \"filename\" < options > ; < SAS code to generate the report > ODS destination CLOSE ; You can have multiple destinations open and execute multiple procedures All generated output will be sent to every open destination You might not be able to view the file, or the most updated file, outside of SAS until you close the destination E.g.: 1 2 3 4 5 6 7 8 9 ODS pdf FILE = \"C:/output/test.pdf\" ; (...) ODS pdf CLOSE ; ODS csvall FILE = \"C:/output/test.cvs\" ; ODS rtf FILE = \"C:/output/test.rtf\" ; (...) ODS csvall CLOSE ; ODS rtf CLOSE ; Allowed File Formats and Their Corresponding Destinations Link You can also export a database to a different format: Export to *.csv : 1 2 3 4 5 PROC EXPORT DATA = sashelp . class OUTFILE = 'c:\\temp\\sashelp class.csv' DBMS = CSV REPLACE ; RUN ; Export to *.dat : 1 2 3 4 5 data _null_ ; set library . SAS-data-set ; file 'C:\\your-custom-path\\your-file-name.dat' ; put variable1 variable2 variable3 ; run ;","title":"Creating Summary Reports"},{"location":"essentials/summary/#proc-freq-to-create-summary-reports","text":"When you're summarizing data, there's no need to show a frequency distribution for variables that have a large number of distinct values Frequency distributions work best with variables whose values meet two criteria: variable with categorical values and values are best summarized by counts instead of averages Variables that have continuous numerical values, such as dollar amounts and dates, will need to be grouped into categories by applying formats inside the PROC FREQ step (substitute an specific range of those values by a tag) 1 2 3 4 PROC FREQ DATA=SAS-data-set <option (s) > ; TABLE(S) variable(s) < / option(s)>; <additional statements > RUN; PROC FREQ produces frequency tables that report the distribution of any or all variable values in a SAS data set In the TABLE(S) statement you specify the frequency tables to produce Each unique variable's value displayed in the 1 st column of the output is called a level of the variable","title":"PROC FREQ to Create Summary Reports"},{"location":"essentials/summary/#one-way-frequency-tables","text":"To create one-way frequency tables you specify one or more variable names separated by space. Warning If you omit the TABLE statement, SAS produces a one-way tables for every variable in the data set, which could be very messy if you have a lot of variables. 1 2 3 4 5 6 7 8 9 10 PROC SORT DATA = SAS - data - set OUT = SAS - data - set - sorted ; BY variable_sorted ; RUN ; PROC FREQ DATA = SAS - data - set - sorted ORDER = FREQ < option ( s ) > ; TABLES variable / NOCUM NOPERCENT OUT = custom - output - name ; BY variable_sorted ; < additional statements > RUN ; NOCUM option supresses the display of the cummulative frequency and cummulative percent values NOPERCENT option supresses the display of all percentages ORDER=FREQ option orders the output in descending frequency order OUT= option saves the output data set with a custom name BY option produces a frequency table for each value of variable_sorted (the data set must be sorted by the variable named in the statement)","title":"One-way Frequency Tables"},{"location":"essentials/summary/#crosstabulation-tables","text":"Sometimes it is useful to view a single table with statistics for each distintic combination of values of the selected variables The simplest crosstabulation table is a two-way table 1 2 3 PROC FREQ DATA = SAS - data - set ; TABLES variable_rows * variable_columns / NOFREQ NOPERCENT NOROW NOCOL ; RUN ; Information contained in crosstabulation tables (legend): Frequency : indicates the number of observations with the unique combination of values represented in that cell Percent : indicates the cell's percentage of the total frequency Row Pct : cell's percentage of the total frequency for its row Col Pct : cell's percentage of the total frequency for its column LIST option format: the first two columns specify each possible combination of the two variables; it displays the same statistics as the default one-way frequency table CROSSLIST option format: it displays the same statistics as the default crosstabulation table","title":"Crosstabulation Tables"},{"location":"essentials/summary/#formatting-variables-in-proc-freq","text":"The FORMAT= option allows you to format the frequency value (to any SAS numeric format or a user-defined numeric format while its length is not more than 24) and to change the width of the column (e.g. to allow variable labels to fit in one line). 1 2 3 4 5 PROC FREQ DATA = SAS - data - set ; TABLES variable1 * variable2 / FORMAT = < w > .; FORMAT variable1 $ format - name .; RUN ; The FORMAT= option applies only to crosstabulation tables displayed in the default format. It doesn't apply to crosstabulation tables produced with the LIST / CROSSLIST option.","title":"Formatting Variables in PROC FREQ"},{"location":"essentials/summary/#using-the-means-and-univariate-procedures","text":"PROC MEANS produces summary reports with descriptive statistics and you can create statistics for groups of observations It automatically displays output in a report and you can also save the output in a SAS data set It reports the number of nonmissing values of the analysis variable (N), and the mean , the standard deviation and minimum / maximum values of every numeric variable in the data set The variables in the CLASS statement are called classification variables or class variables (they typically have few discrete values) Each combination of class variable values is called a class level The data set doesn't need to be sorted or indexed by the class variables N Obs reports the number of observations with each unique combination of class variables, whether or not there are missing values (if these N Obs are identical to N , there are no missing values in you data set) 1 2 3 4 PROC MEANS DATA = SAS - data - set < statistic ( s ) > ; VAR analysis - variable ( s ); CLASS classification - variable ( s ); RUN ; To write the report in a new data set (including total addition): 1 2 3 4 5 PROC MEANS DATA = SAS - data - set NOPRINT NWAY ; OUTPUT OUT = SAS - new - data - set SUM = addition - new - variable ; VAR analysis - variable ( s ); CLASS classification - variable ( s ); RUN ; Format options: MAXDEC=number (default format = BESTw. ) NONOBS FW=number : specifies that the field width for all columns is number PRINTALLTYPES : displays statistics for all requested combination of class variables Sums and counts by group ODS EXCLUDE ALL; PROC MEANS DATA=SAS-data-set SUM; CLASS classification-variable; VAR var1 var2 var3 var4; ODS OUTPUT SUMMARY=name-output-data-set; RUN; ODS EXCLUDE NONE;","title":"Using the MEANS and UNIVARIATE Procedures"},{"location":"essentials/summary/#how-to-use-these-procedures-for-data-validation","text":"","title":"How to Use these Procedures for Data Validation"},{"location":"essentials/summary/#proc-freq","text":"You can use a PROC FREQ step with the TABLES statement to detect invalud numeric and character data by looking at distinct values. The FREQ procedure lists all discrete values for a variable and reports its missing values . 1 2 3 PROC FREQ DATA = SAS - data - set < ORDER = FREQ > ; TABLES variable ; RUN ; You can check for non-expected variable's values You can check for missing values You can find duplicated values The table showing the Number of Variable Levels can indicate whether a variable contains duplicate/missing/non-expected values: 1 2 3 PROC FREQ DATA = SAS - data - set NLEVELS ; TABLES variable / NOPRINT ; RUN ; You can use a WHERE statement to print out only the invalid values to be checked: 1 2 3 4 5 6 PROC PRINT DATA = SAS - data - set ; WHERE gender NOT IN ( 'F' , 'M' ) OR job_title IS NULL OR salary NOT BETWEEN 24000 AND 500000 OR employee IS MISSING ; RUN ; You can output the tables to a new data set instead of displaying it: 1 2 3 PROC FREQ DATA = SAS - data - set NOPRINT ; TABLE variable / OUT = SAS - new - data - set ; RUN ;","title":"PROC FREQ"},{"location":"essentials/summary/#proc-means","text":"NMISS option displays the number of observations with missing values The MIN / MAX values can be useful to check if the data is within a range To get the MIN / MAX values per group and the number of observations per group follow the next example: 1 2 3 4 5 PROC MEANS DATA = SAS - data - set MIN MAX ; CLASS grouping - variable ; VAR evaluation - variable ; ODS OUTPUT SUMMARY = SAS - output - data - set ; RUN ;","title":"PROC MEANS"},{"location":"essentials/summary/#proc-univariate","text":"PROC UNIVARIATE is a procedure that is useful for detecting data outliers that also produces summary reports of descriptive statistics . 1 2 3 4 5 6 7 PROC UNIVARIATE DATA=SAS-data-set; VAR variable(s); ID variable_to_relate; HISTOGRAM variables </options> ; PROBPLOT variables </options> ; INSET keywords </options> ; RUN; If you omit the VAR statement, all numeric variables in the data set are analyzed The Extreme Observations table contains useful information to locate outliers: it displays the 5 lowest/highest values by default along with the corresponding observation number. The ID statement specifies that SAS will use this variable as a label in the table of extreme observations and as an identifier for any extreme. To specify the number of listed observations you can use NEXTROBS= HISTOGRAM / PROBPLOT options: normal(mu=est sigma=est) creates a normal curve overlay to the histogram using the estimates of the population mean and standard deviation INSET writes a legend for the graph. / position=ne moves the INSET to the north-east corner of the graph. To include in the report only one of the automatically produced tables: Check the specific table name in the LOG information using ODS TRACE ON : 1 2 3 4 5 ODS TRACE ON ; PROC UNIVARIATE DATA = SAS - data - set ; VAR variable ( s ); RUN ; ODS TRACE OFF ; Select the wanted table with ODS SELECT : 1 2 3 4 ODS SELECT ExtremeObs ; PROC UNIVARIATE DATA = SAS - data - set ; VAR variable ( s ); RUN ;","title":"PROC UNIVARIATE"},{"location":"essentials/summary/#summary-of-validation-procedures","text":"","title":"Summary of Validation Procedures"},{"location":"essentials/summary/#using-the-sas-output-delivery-system-ods","text":"1 2 3 ODS destination FILE = \"filename\" < options > ; < SAS code to generate the report > ODS destination CLOSE ; You can have multiple destinations open and execute multiple procedures All generated output will be sent to every open destination You might not be able to view the file, or the most updated file, outside of SAS until you close the destination E.g.: 1 2 3 4 5 6 7 8 9 ODS pdf FILE = \"C:/output/test.pdf\" ; (...) ODS pdf CLOSE ; ODS csvall FILE = \"C:/output/test.cvs\" ; ODS rtf FILE = \"C:/output/test.rtf\" ; (...) ODS csvall CLOSE ; ODS rtf CLOSE ;","title":"Using the SAS Output Delivery System (ODS)"},{"location":"essentials/summary/#allowed-file-formats-and-their-corresponding-destinations","text":"You can also export a database to a different format: Export to *.csv : 1 2 3 4 5 PROC EXPORT DATA = sashelp . class OUTFILE = 'c:\\temp\\sashelp class.csv' DBMS = CSV REPLACE ; RUN ; Export to *.dat : 1 2 3 4 5 data _null_ ; set library . SAS-data-set ; file 'C:\\your-custom-path\\your-file-name.dat' ; put variable1 variable2 variable3 ; run ;","title":"Allowed File Formats and Their Corresponding Destinations"},{"location":"macros/debugging-options/","text":"Minimizing Errors in Your Macros Link You should use a five-step approach to developing macro programs that generate SAS code. This approach will streamline your development and debugging process: Write and debug the SAS program without macro coding Generalize by replacing hardcoded values with macro variable references Create a macro definition with macro parameters Add macro-level programming for conditional and iterative processing Add data-driven customization There are several system options that are useful for macro debugging: Option Description MCOMPILENOTE Issues a note to the SAS log after a macro completes compilation MLOGIC Writes messages that trace macro execution to the SAS log MPRINT Specifies that the text that is sent to the compiler when a macro executes is printed in the SAS log SYMBOLGEN Displays the values of macro variables as they resolve Debugging During Macro Compilation Link MCOMPILENOTE= Option Link By default, SAS does not display any indication that a macro has completed compilation. You can use the MCOMPILENOTE= option with the ALL argument to issue a note to the SAS log after a macro compiles. 1 OPTIONS MCOMPILENOTE = NONE | ALL ; Tracking Errors During Macro Execution Link MPRINT Option Link The MPRINT option displays the SAS statements generated by macro execution. 1 OPTIONS MPRINT | NOMPRINT ; MLOGIC Option Link The MLOGIC option prints messages that indicate macro actions that were taken during macro execution. 1 OPTIONS MLOGIC | NOMLOGIC ; When the MLOGIC system option is in effect, the messages that SAS displays in the log include information about the following: * The beginning of macro execution * The values of any parameters * The results of arithmetic and logical macro operations * The end of macro execution When you're working with a program that uses SAS macro language, you should typically turn the MLOGIC option, along with the MPRINT option and the SYMBOLGEN option * on for development and debugging purposes * off when the program is in production mode Comment your Macros Link Your macros might benefit from comments. Comments can be especially helpful if you plan to save your macros permanently or share them with other users. 1 %* comment; To use the macro comment statement, specify the percent sign, followed by an asterisk and then your comment. The comment can be any text. Like other SAS statements, each macro comment statement ends with a semicolon. You can also use the comment symbols / * and * / inside a macro. When these symbols appear, the macro processor ignores the text within the comment.","title":"Debugging Options"},{"location":"macros/debugging-options/#minimizing-errors-in-your-macros","text":"You should use a five-step approach to developing macro programs that generate SAS code. This approach will streamline your development and debugging process: Write and debug the SAS program without macro coding Generalize by replacing hardcoded values with macro variable references Create a macro definition with macro parameters Add macro-level programming for conditional and iterative processing Add data-driven customization There are several system options that are useful for macro debugging: Option Description MCOMPILENOTE Issues a note to the SAS log after a macro completes compilation MLOGIC Writes messages that trace macro execution to the SAS log MPRINT Specifies that the text that is sent to the compiler when a macro executes is printed in the SAS log SYMBOLGEN Displays the values of macro variables as they resolve","title":"Minimizing Errors in Your Macros"},{"location":"macros/debugging-options/#debugging-during-macro-compilation","text":"","title":"Debugging During Macro Compilation"},{"location":"macros/debugging-options/#mcompilenote-option","text":"By default, SAS does not display any indication that a macro has completed compilation. You can use the MCOMPILENOTE= option with the ALL argument to issue a note to the SAS log after a macro compiles. 1 OPTIONS MCOMPILENOTE = NONE | ALL ;","title":"MCOMPILENOTE= Option"},{"location":"macros/debugging-options/#tracking-errors-during-macro-execution","text":"","title":"Tracking Errors During Macro Execution"},{"location":"macros/debugging-options/#mprint-option","text":"The MPRINT option displays the SAS statements generated by macro execution. 1 OPTIONS MPRINT | NOMPRINT ;","title":"MPRINT Option"},{"location":"macros/debugging-options/#mlogic-option","text":"The MLOGIC option prints messages that indicate macro actions that were taken during macro execution. 1 OPTIONS MLOGIC | NOMLOGIC ; When the MLOGIC system option is in effect, the messages that SAS displays in the log include information about the following: * The beginning of macro execution * The values of any parameters * The results of arithmetic and logical macro operations * The end of macro execution When you're working with a program that uses SAS macro language, you should typically turn the MLOGIC option, along with the MPRINT option and the SYMBOLGEN option * on for development and debugging purposes * off when the program is in production mode","title":"MLOGIC Option"},{"location":"macros/debugging-options/#comment-your-macros","text":"Your macros might benefit from comments. Comments can be especially helpful if you plan to save your macros permanently or share them with other users. 1 %* comment; To use the macro comment statement, specify the percent sign, followed by an asterisk and then your comment. The comment can be any text. Like other SAS statements, each macro comment statement ends with a semicolon. You can also use the comment symbols / * and * / inside a macro. When these symbols appear, the macro processor ignores the text within the comment.","title":"Comment your Macros"},{"location":"macros/introducing-macrovar/","text":"Basic Concepts Link Macro variables substitute text into your SAS programs. The macro facility enables you to create and resolve macro variables anywhere within a SAS program. There are two types of macro variables: automatic macro variables , which SAS provides, and user-defined macro variables , which you create and define. Automatic Macro Variables Link Automatic macro variables contain system information such as the date and time that the current SAS session began. Some automatic macro variables have fixed values that SAS defines when the session starts. This table shows several common automatic macro variables: Name Description Example SYSDATE Date when the current SAS session began (DATE7.) 16JAN13 SYSDATE9 Date when the current SAS session began (DATE9.) 16JAN2013 SYSDAY Day of the week when the current SAS session began Friday SYSSCP Abbreviation for the operating system being used OS, WIN, HP 64 SYSTIME Time that the current SAS session began 13:39 SYSUSERID The user ID or login of the current SAS process MyUserid SYSVER Release of SAS software being used 9.4 Other automatic macro variables have values that change automatically, based on the SAS statements that you submit. Name Description SYSLAST Name of the most recently created data set in the form libref.name (the value is _NULL_ when none has been created) SYSPARM Value specified at SAS invocation SYSERR SAS DATA or PROC step return code (0=success) SYSLIBRC LIBNAME statement return code (0=success) Creating User-defined Macro Variables Link You use the %LET statement to create a macro variable and assign a value to it. Macro variable names start with a letter or an underscore and can be followed by letters, digits, or underscores. The prefixes AF , DMS , SQL , and SYS are not recommended because they are frequently used in SAS software when creating macro variables. If you assign a macro variable name that isn't valid, SAS writes an error message to the log. When assigning values to macro variables in the %LET statement, SAS does the following: Stores all macro variable values as text strings, even if they contain only numbers Doesn't evaluate mathematical expressions in macro variable values Stores the value in the case that is specified in the %LET statement Stores quotation marks as part of the macro variable value Removes leading and trailing blanks from the macro variable value before storing it SAS doesn't remove blanks within the macro variable value To reference a user-defined macro variable , you precede the name of the macro variable with an ampersand ( &macrovariable ). When you submit the program, the macro processor resolves the reference and substitutes the macro variable's value before the program compiles and executes. Tip If you need to reference a macro variable within quotation marks, such as in a title, you must use double quotation marks. Macro variables remain in the global symbol table until they are deleted or the session ends. To delete macro variables , you use the %SYMDEL statement followed by the name or names of the macro variables that you want to delete. Displaying Macro Variables in the SAS Log Link There are several methods that you can use to display the values of macro variables in the SAS log. Using the %PUT Statement Link You can use the %PUT statement to write your own messages, including macro variable values, to the SAS log: %PUT The value of the macro variable is: &macrovar; or %PUT &=macrovar; . You can add one of the following optional arguments to the %PUT statement: %PUT <text | _ALL_ / _AUTOMATIC_ / _USER_>; Argument Result in the SAS Log _ALL_ Lists the values of all macro variables _AUTOMATIC_ Lists the value of all automatic macro variables _USER_ Lists the values of all user-defined macro variables Using the SYMBOLGEN system option Link You can also use the SYMBOLGEN system option to display the values of macro variables. 1 OPTIONS SYMBOLGEN | NOSYMBOLGEN ; The default option is NOSYMBOLGEN . When you turn the SYMBOLGEN system option on, SAS writes macro variable values to the SAS log as they are resolved. The message states the macro variable name and the resolved value. Because SYMBOLGEN is a system option, its setting remains in effect until you modify it or until you end your SAS session. Processing Macro Variables Link When you submit a SAS program, it's copied to an area of memory called the input stack. The word scanner reads the text and breaks the text into fundamental units, called tokens, and passes the tokens, one at time, to the appropriate compiler upon demand. The compiler requests tokens until it receives a semicolon and then performs a syntax check on the statement. The compiler repeats this process for each additional statement. SAS suspends compilation when a step boundary ( RUN statements or the beginning of a new DATA / PROC step) is encountered. If there are no compilation errors, SAS executes the compiled code. This process is repeated for each program step. A token is the fundamental unit that the word scanner passes to the compiler. The word scanner recognizes the four classes of tokens shown in the table below. Class Description Example name A character string that begins with a letter or underscore and continues with underscores, letters, or numerals infile _n_ dollar10.2 special Any character, or combination of characters, other than a letter, numeral, or underscore * / + ** ; $ ( ) . & % literal A string of characters enclosed in single or double quotation marks 'Report for May' \"Sydney Office\" number Integer numbers, including SAS date constants or floating point numbers, that contain a decimal point and/or an exponent 23 109 5e8 42.7 '01jan2012'd Certain token sequences, known as macro triggers , alert the word scanner that the subsequent code should be sent to the macro processor. The word scanner recognizes the following token sequences as macro triggers and passes the code to the macro processor for evaluation: A percent sign followed immediately by a name token (such as %LET ) An ampersand followed immediately by a name token (such as &macrovar ) A macro variable reference triggers the macro processor to search the symbol table for the reference. The macro processor resolves the macro variable reference by replacing it with the value in the symbol table. Using Double Quoting to Reference Macro Variables Link You need to use double quotes, not single, to reference a macro variable in the code, otherwise the macro variable won't be resolved by the Macro Processor. 1 2 3 4 5 6 7 8 9 10 11 %LET firstletter = a; PROC PRINT DATA = SAS - dat - set ; WHERE letter = '&firstletter' ; RUN ; /* Wrong */ PROC PRINT DATA = SAS - dat - set ; WHERE letter = \" & firstletter \" ; RUN ; /* Correct */ Referencing Macro Variables Using . Link A period . is used as delimiter that defines the end of a macro variable. It is usually not necessary but there are cases on which it can be really useful. 1 2 3 4 5 6 7 8 9 %LET firstletter = a; %LET thirdletter = c; %LET abc = &firstletterb&thirdletter; /* Wrong */ /* WARNING : Apparent symbolic reference FIRSTLETTERB not resolved . */ %LET abc = &firstletter.b&thirdletter; /* Correct */ You can even need to use two . in certain cases: 1 2 3 4 5 %LET library = library-name; %LET dataset = dataset-name; PROC PRINT DATA = & library .. & dataset ; RUN ;","title":"Introducing Macro Variables"},{"location":"macros/introducing-macrovar/#basic-concepts","text":"Macro variables substitute text into your SAS programs. The macro facility enables you to create and resolve macro variables anywhere within a SAS program. There are two types of macro variables: automatic macro variables , which SAS provides, and user-defined macro variables , which you create and define.","title":"Basic Concepts"},{"location":"macros/introducing-macrovar/#automatic-macro-variables","text":"Automatic macro variables contain system information such as the date and time that the current SAS session began. Some automatic macro variables have fixed values that SAS defines when the session starts. This table shows several common automatic macro variables: Name Description Example SYSDATE Date when the current SAS session began (DATE7.) 16JAN13 SYSDATE9 Date when the current SAS session began (DATE9.) 16JAN2013 SYSDAY Day of the week when the current SAS session began Friday SYSSCP Abbreviation for the operating system being used OS, WIN, HP 64 SYSTIME Time that the current SAS session began 13:39 SYSUSERID The user ID or login of the current SAS process MyUserid SYSVER Release of SAS software being used 9.4 Other automatic macro variables have values that change automatically, based on the SAS statements that you submit. Name Description SYSLAST Name of the most recently created data set in the form libref.name (the value is _NULL_ when none has been created) SYSPARM Value specified at SAS invocation SYSERR SAS DATA or PROC step return code (0=success) SYSLIBRC LIBNAME statement return code (0=success)","title":"Automatic Macro Variables"},{"location":"macros/introducing-macrovar/#creating-user-defined-macro-variables","text":"You use the %LET statement to create a macro variable and assign a value to it. Macro variable names start with a letter or an underscore and can be followed by letters, digits, or underscores. The prefixes AF , DMS , SQL , and SYS are not recommended because they are frequently used in SAS software when creating macro variables. If you assign a macro variable name that isn't valid, SAS writes an error message to the log. When assigning values to macro variables in the %LET statement, SAS does the following: Stores all macro variable values as text strings, even if they contain only numbers Doesn't evaluate mathematical expressions in macro variable values Stores the value in the case that is specified in the %LET statement Stores quotation marks as part of the macro variable value Removes leading and trailing blanks from the macro variable value before storing it SAS doesn't remove blanks within the macro variable value To reference a user-defined macro variable , you precede the name of the macro variable with an ampersand ( &macrovariable ). When you submit the program, the macro processor resolves the reference and substitutes the macro variable's value before the program compiles and executes. Tip If you need to reference a macro variable within quotation marks, such as in a title, you must use double quotation marks. Macro variables remain in the global symbol table until they are deleted or the session ends. To delete macro variables , you use the %SYMDEL statement followed by the name or names of the macro variables that you want to delete.","title":"Creating User-defined Macro Variables"},{"location":"macros/introducing-macrovar/#displaying-macro-variables-in-the-sas-log","text":"There are several methods that you can use to display the values of macro variables in the SAS log.","title":"Displaying Macro Variables in the SAS Log"},{"location":"macros/introducing-macrovar/#using-the-put-statement","text":"You can use the %PUT statement to write your own messages, including macro variable values, to the SAS log: %PUT The value of the macro variable is: &macrovar; or %PUT &=macrovar; . You can add one of the following optional arguments to the %PUT statement: %PUT <text | _ALL_ / _AUTOMATIC_ / _USER_>; Argument Result in the SAS Log _ALL_ Lists the values of all macro variables _AUTOMATIC_ Lists the value of all automatic macro variables _USER_ Lists the values of all user-defined macro variables","title":"Using the %PUT Statement"},{"location":"macros/introducing-macrovar/#using-the-symbolgen-system-option","text":"You can also use the SYMBOLGEN system option to display the values of macro variables. 1 OPTIONS SYMBOLGEN | NOSYMBOLGEN ; The default option is NOSYMBOLGEN . When you turn the SYMBOLGEN system option on, SAS writes macro variable values to the SAS log as they are resolved. The message states the macro variable name and the resolved value. Because SYMBOLGEN is a system option, its setting remains in effect until you modify it or until you end your SAS session.","title":"Using the SYMBOLGEN system option"},{"location":"macros/introducing-macrovar/#processing-macro-variables","text":"When you submit a SAS program, it's copied to an area of memory called the input stack. The word scanner reads the text and breaks the text into fundamental units, called tokens, and passes the tokens, one at time, to the appropriate compiler upon demand. The compiler requests tokens until it receives a semicolon and then performs a syntax check on the statement. The compiler repeats this process for each additional statement. SAS suspends compilation when a step boundary ( RUN statements or the beginning of a new DATA / PROC step) is encountered. If there are no compilation errors, SAS executes the compiled code. This process is repeated for each program step. A token is the fundamental unit that the word scanner passes to the compiler. The word scanner recognizes the four classes of tokens shown in the table below. Class Description Example name A character string that begins with a letter or underscore and continues with underscores, letters, or numerals infile _n_ dollar10.2 special Any character, or combination of characters, other than a letter, numeral, or underscore * / + ** ; $ ( ) . & % literal A string of characters enclosed in single or double quotation marks 'Report for May' \"Sydney Office\" number Integer numbers, including SAS date constants or floating point numbers, that contain a decimal point and/or an exponent 23 109 5e8 42.7 '01jan2012'd Certain token sequences, known as macro triggers , alert the word scanner that the subsequent code should be sent to the macro processor. The word scanner recognizes the following token sequences as macro triggers and passes the code to the macro processor for evaluation: A percent sign followed immediately by a name token (such as %LET ) An ampersand followed immediately by a name token (such as &macrovar ) A macro variable reference triggers the macro processor to search the symbol table for the reference. The macro processor resolves the macro variable reference by replacing it with the value in the symbol table.","title":"Processing Macro Variables"},{"location":"macros/introducing-macrovar/#using-double-quoting-to-reference-macro-variables","text":"You need to use double quotes, not single, to reference a macro variable in the code, otherwise the macro variable won't be resolved by the Macro Processor. 1 2 3 4 5 6 7 8 9 10 11 %LET firstletter = a; PROC PRINT DATA = SAS - dat - set ; WHERE letter = '&firstletter' ; RUN ; /* Wrong */ PROC PRINT DATA = SAS - dat - set ; WHERE letter = \" & firstletter \" ; RUN ; /* Correct */","title":"Using Double Quoting to Reference Macro Variables"},{"location":"macros/introducing-macrovar/#referencing-macro-variables-using","text":"A period . is used as delimiter that defines the end of a macro variable. It is usually not necessary but there are cases on which it can be really useful. 1 2 3 4 5 6 7 8 9 %LET firstletter = a; %LET thirdletter = c; %LET abc = &firstletterb&thirdletter; /* Wrong */ /* WARNING : Apparent symbolic reference FIRSTLETTERB not resolved . */ %LET abc = &firstletter.b&thirdletter; /* Correct */ You can even need to use two . in certain cases: 1 2 3 4 5 %LET library = library-name; %LET dataset = dataset-name; PROC PRINT DATA = & library .. & dataset ; RUN ;","title":"Referencing Macro Variables Using ."},{"location":"macros/macroprog-processing/","text":"Processing Statements Conditionally Link You can use %IF-%THEN and %ELSE statements to perform conditional processing in a macro program. 1 2 % IF expression % THEN text ; <% ELSE text ;> You specify the keyword %IF followed by an expression. The expression can be any valid macro expression that resolves to an integer. Then, you specify the keyword %THEN followed by some text. The text can be a SAS constant, a text expression, a macro variable reference, a macro call, or a macro program statement. The %ELSE statement is optional. The text in a %ELSE statement can also be a SAS constant, a text expression, a macro variable reference, a macro call, or a macro program statement. If the expression following %IF resolves to zero , the expression is false and the %THEN text isn't processed. If you include an optional %ELSE statement, that text is processed instead of the %THEN text. If the expression resolves to any integer other than zero , then the expression is true and the %THEN text is processed. If the expression resolves to null or to any noninteger value , SAS issues an error message. There are several important differences between the macro %IF-%THEN statement and the DATA step IF-THEN statement. %IF-%THEN IF-THEN Is used only in a macro program Is used only in a DATA step program Executes during macro execution Executes during DATA step execution Uses macro variables in logical expressions and cannot refer to DATA step variables in logical expressions Uses DATA step variables in logical expressions Determines what text should be copied to the input stack Determines what DATA step statement(s) should be executed Macro expressions are similar to SAS expressions in the following ways: They use the same arithmetic, logical, and comparison operators as SAS expressions They are case sensitive Special WHERE operators are not valid Macro expressions are dissimilar to SAS expressions in the following ways: Character operands are not quoted Expressions in which comparison operators surround a macro expression might resolve with unexpected results You can use %DO and %END statements along with a %IF-%THEN statement to generate code that contains semicolons. 1 2 3 4 5 6 % IF expression % THEN % DO ; text and / or macro language elements ; % END ; % ELSE % DO ; text and / or macro language elements ; % END ; The syntax for using %DO and %END statements with a %IF-%THEN statement is shown here. The keyword %DO follows the keyword %THEN . You must pair each %DO statement with a %END statement. Between the %DO and the %END keywords, you insert one or more statements that contain constant text, text expressions, or macro statements. Using Conditional Processing to Validate Parameters: the IN operator Link You can use the comparison operator IN to search for character and numeric values that are equal to one from a list of values. You can also use the IN operator when you evaluate arithmetic or logical expressions during macro execution. 1 %MACRO macro-name <(parameter-list) ></MINOPERATOR | NOMINOPERATOR; To use the macro IN operator, you use the MINOPERATOR option with the %MACRO statement, preceded by a slash. Then you can use the macro IN operator to modify the %IF statement. The macro IN operator is similar to the SAS IN operator, but it doesn't require parentheses. If you use NOT with the IN operator, NOT must precede the IN expression and parentheses are required around the expression. You can use the PROC SQL INTO clause with the SEPARATED BY argument to create a macro variable that contains a list of unique values. You can combine PROC SQL with conditional processing to validate the parameters in a macro. Processing Statements Iteratively: the %DO statement Link With the iterative %DO statement, you can repeatedly execute macro programming code and generate SAS code. 1 2 3 % DO index - variable = start % TO stop <% BY increment > ; text and / or macro language elements ; % END ; The values for start , stop , and increment must be integers or macro expressions that resolve to integer values. The %BY clause is optional. Increment specifies either an integer (other than 0) or a macro expression that generates an integer to be added to the value of the index variable in each iteration of the loop. By default, the increment is 1. A %END statement ends the iterative loop. Warning %DO and %END statements are valid only inside a macro definition . Other Flavors of %DO Loops Link You can execute a %DO loop conditionally with %DO %WHILE and %DO %UNTIL statements. %DO %WHILE : executes a section of a macro repetitively while a condition is true 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 % DO % WHILE ( expression ); text and macro program statements % END ; /* Example: Removing Markup Tags from a Title */ %macro untag ( title ); %let stbk = %str ( < ); %let etbk = %str ( > ); /* Do loop while tags exist */ %do %while ( %index ( & title , & stbk ) > 0 ) ; %let pretag = ; %let posttag = ; %let pos_et = %index ( & title , & etbk ); %let len_ti = %length ( & title ); /* Is < first character? */ %if ( %qsubstr ( & title , 1 , 1 ) =& stbk ) %then %do ; %if ( & pos_et ne & len_ti ) %then %let posttag = %qsubstr ( & title , & pos_et + 1 ); %end ; %else %do ; %let pretag = %qsubstr ( & title , 1 ,( %index ( & title , & stbk ) - 1 )); /* More characters beyond end of tag (>) ? */ %if ( & pos_et ne & len_ti ) %then %let posttag = %qsubstr ( & title , & pos_et + 1 ); %end ; /* Build title with text before and after tag */ %let title =& pretag & posttag ; %end ; title \"&title\" ; %mend untag ; %untag ( < title > Total < emph > Overdue </ emph > Accounts </ title > ) /* TITLE \"Total Overdue Accounts\"; */ %DO %UNTIL : executes a section of a macro repetitively until a condition is true 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 % DO % UNTIL ( expression ); text and macro language statements % END ; /* Example: Validating a Parameter */ %macro grph ( type ); %let type = %upcase ( & type ); %let options = BLOCK HBAR VBAR ; %let i = 0 ; %do %until ( & type = %scan ( & options , & i ) or ( & i > 3 )) ; %let i = %eval ( & i + 1 ); %end ; %if & i > 3 %then %do ; %put ERROR : & type type not supported ; %end ; %else %do ; proc chart ; & type sex / group = dept ; run ; %end ; %mend grph ; %macro grph ( HBAR ); /* PROC CHART; HBAR SEX / GROUP=DEPT; RUN;*/ %macro grph ( PIE ); /* ERROR: PIE type not supported */ Generating Repetitive Pieces of Text Using %DO Loops Link To generate repetitive pieces of text, use an iterative %DO loop. For example, the following macro, NAMES , uses an iterative %DO loop to create a series of names to be used in a DATA statement: 1 2 3 4 5 %macro names ( name = , number = ); %do n = 1 %to & number ; & name & n %end ; %mend names ; The macro NAMES creates a series of names by concatenating the value of the parameter NAME and the value of the macro variable N . You supply the stopping value for N as the value of the parameter NUMBER , as in the following DATA statement: 1 data % names ( name = dsn , number = 5 ); Submitting this statement produces the following complete DATA statement: 1 data dsn1 dsn2 dsn3 dsn4 dsn5 ;","title":"Using Conditional and Iterative Processing in Macro Programs"},{"location":"macros/macroprog-processing/#processing-statements-conditionally","text":"You can use %IF-%THEN and %ELSE statements to perform conditional processing in a macro program. 1 2 % IF expression % THEN text ; <% ELSE text ;> You specify the keyword %IF followed by an expression. The expression can be any valid macro expression that resolves to an integer. Then, you specify the keyword %THEN followed by some text. The text can be a SAS constant, a text expression, a macro variable reference, a macro call, or a macro program statement. The %ELSE statement is optional. The text in a %ELSE statement can also be a SAS constant, a text expression, a macro variable reference, a macro call, or a macro program statement. If the expression following %IF resolves to zero , the expression is false and the %THEN text isn't processed. If you include an optional %ELSE statement, that text is processed instead of the %THEN text. If the expression resolves to any integer other than zero , then the expression is true and the %THEN text is processed. If the expression resolves to null or to any noninteger value , SAS issues an error message. There are several important differences between the macro %IF-%THEN statement and the DATA step IF-THEN statement. %IF-%THEN IF-THEN Is used only in a macro program Is used only in a DATA step program Executes during macro execution Executes during DATA step execution Uses macro variables in logical expressions and cannot refer to DATA step variables in logical expressions Uses DATA step variables in logical expressions Determines what text should be copied to the input stack Determines what DATA step statement(s) should be executed Macro expressions are similar to SAS expressions in the following ways: They use the same arithmetic, logical, and comparison operators as SAS expressions They are case sensitive Special WHERE operators are not valid Macro expressions are dissimilar to SAS expressions in the following ways: Character operands are not quoted Expressions in which comparison operators surround a macro expression might resolve with unexpected results You can use %DO and %END statements along with a %IF-%THEN statement to generate code that contains semicolons. 1 2 3 4 5 6 % IF expression % THEN % DO ; text and / or macro language elements ; % END ; % ELSE % DO ; text and / or macro language elements ; % END ; The syntax for using %DO and %END statements with a %IF-%THEN statement is shown here. The keyword %DO follows the keyword %THEN . You must pair each %DO statement with a %END statement. Between the %DO and the %END keywords, you insert one or more statements that contain constant text, text expressions, or macro statements.","title":"Processing Statements Conditionally"},{"location":"macros/macroprog-processing/#using-conditional-processing-to-validate-parameters-the-in-operator","text":"You can use the comparison operator IN to search for character and numeric values that are equal to one from a list of values. You can also use the IN operator when you evaluate arithmetic or logical expressions during macro execution. 1 %MACRO macro-name <(parameter-list) ></MINOPERATOR | NOMINOPERATOR; To use the macro IN operator, you use the MINOPERATOR option with the %MACRO statement, preceded by a slash. Then you can use the macro IN operator to modify the %IF statement. The macro IN operator is similar to the SAS IN operator, but it doesn't require parentheses. If you use NOT with the IN operator, NOT must precede the IN expression and parentheses are required around the expression. You can use the PROC SQL INTO clause with the SEPARATED BY argument to create a macro variable that contains a list of unique values. You can combine PROC SQL with conditional processing to validate the parameters in a macro.","title":"Using Conditional Processing to Validate Parameters: the IN operator"},{"location":"macros/macroprog-processing/#processing-statements-iteratively-the-do-statement","text":"With the iterative %DO statement, you can repeatedly execute macro programming code and generate SAS code. 1 2 3 % DO index - variable = start % TO stop <% BY increment > ; text and / or macro language elements ; % END ; The values for start , stop , and increment must be integers or macro expressions that resolve to integer values. The %BY clause is optional. Increment specifies either an integer (other than 0) or a macro expression that generates an integer to be added to the value of the index variable in each iteration of the loop. By default, the increment is 1. A %END statement ends the iterative loop. Warning %DO and %END statements are valid only inside a macro definition .","title":"Processing Statements Iteratively: the %DO statement"},{"location":"macros/macroprog-processing/#other-flavors-of-do-loops","text":"You can execute a %DO loop conditionally with %DO %WHILE and %DO %UNTIL statements. %DO %WHILE : executes a section of a macro repetitively while a condition is true 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 % DO % WHILE ( expression ); text and macro program statements % END ; /* Example: Removing Markup Tags from a Title */ %macro untag ( title ); %let stbk = %str ( < ); %let etbk = %str ( > ); /* Do loop while tags exist */ %do %while ( %index ( & title , & stbk ) > 0 ) ; %let pretag = ; %let posttag = ; %let pos_et = %index ( & title , & etbk ); %let len_ti = %length ( & title ); /* Is < first character? */ %if ( %qsubstr ( & title , 1 , 1 ) =& stbk ) %then %do ; %if ( & pos_et ne & len_ti ) %then %let posttag = %qsubstr ( & title , & pos_et + 1 ); %end ; %else %do ; %let pretag = %qsubstr ( & title , 1 ,( %index ( & title , & stbk ) - 1 )); /* More characters beyond end of tag (>) ? */ %if ( & pos_et ne & len_ti ) %then %let posttag = %qsubstr ( & title , & pos_et + 1 ); %end ; /* Build title with text before and after tag */ %let title =& pretag & posttag ; %end ; title \"&title\" ; %mend untag ; %untag ( < title > Total < emph > Overdue </ emph > Accounts </ title > ) /* TITLE \"Total Overdue Accounts\"; */ %DO %UNTIL : executes a section of a macro repetitively until a condition is true 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 % DO % UNTIL ( expression ); text and macro language statements % END ; /* Example: Validating a Parameter */ %macro grph ( type ); %let type = %upcase ( & type ); %let options = BLOCK HBAR VBAR ; %let i = 0 ; %do %until ( & type = %scan ( & options , & i ) or ( & i > 3 )) ; %let i = %eval ( & i + 1 ); %end ; %if & i > 3 %then %do ; %put ERROR : & type type not supported ; %end ; %else %do ; proc chart ; & type sex / group = dept ; run ; %end ; %mend grph ; %macro grph ( HBAR ); /* PROC CHART; HBAR SEX / GROUP=DEPT; RUN;*/ %macro grph ( PIE ); /* ERROR: PIE type not supported */","title":"Other Flavors of %DO Loops"},{"location":"macros/macroprog-processing/#generating-repetitive-pieces-of-text-using-do-loops","text":"To generate repetitive pieces of text, use an iterative %DO loop. For example, the following macro, NAMES , uses an iterative %DO loop to create a series of names to be used in a DATA statement: 1 2 3 4 5 %macro names ( name = , number = ); %do n = 1 %to & number ; & name & n %end ; %mend names ; The macro NAMES creates a series of names by concatenating the value of the parameter NAME and the value of the macro variable N . You supply the stopping value for N as the value of the parameter NUMBER , as in the following DATA statement: 1 data % names ( name = dsn , number = 5 ); Submitting this statement produces the following complete DATA statement: 1 data dsn1 dsn2 dsn3 dsn4 dsn5 ;","title":"Generating Repetitive Pieces of Text Using %DO Loops"},{"location":"macros/macroprog/","text":"Basic Concepts Link Macros are compiled programs that you can execute independently or call as part of a SAS program. Like macro variables, you generally use macros to generate text. But, macros provide additional capabilities. Macros can contain programming statements that enable you to control how and when the text is generated. Also, macros can accept parameters that specify variables. Using parameters, you can write generic macros that can serve a number of uses. A SAS program can reference any number of macros, and you can invoke a macro any number of times within a single SAS program. There are three steps to create and use a macro: define the macro, compile the macro, and call the macro. Define a Macro Link A macro definition is the container that holds a macro program. You begin a macro definition with a %MACRO statement and end it with a %MEND statement. 1 2 3 %MACRO macro-name; text %MEND <macro-name>; Each macro that you define has a distinct name. Choose a name for your macro that indicates what the macro does and follow standard SAS naming conventions. Avoid SAS language keywords or call routine names, as well as words that are reserved by the SAS macro facility. You are not required to include the macro name in the %MEND statement. But if you do, be sure that the two macro names match. The text can include constant text, SAS data set names, SAS variable names, and SAS statements. It can also include macro variables ( &country ), macro functions ( %sysfunc() ), macro program statements ( %put sysdate=&sysdate; ) and any combination of the above. Compile a Macro Link To compile a macro, submit the code. If the macro processor finds syntax errors in the macro language statements, it writes any error messages to the SAS log and creates a dummy, non-executable macro. If the macro processor does not find any macro-level syntax errors, it compiles the macro and stores the compiled definition in a temporary SAS catalog named Work.Sasmacr . Macros in this catalog are known as session-compiled macros and are available for execution during the SAS session in which they're compiled. SAS deletes the temporary catalog that stores the macros at the end of the session. Call a Macro Link To call a macro, precede the name of the macro with a percent sign anywhere in a program, except within the data lines of a DATALINES statement. The macro call requires no semicolon because it is not a SAS statement. 1 %macro - name When you call a macro, the word scanner passes the macro call to the macro processor. The macro processor searches Work.Sasmacr for that macro. Then it executes compiled macro language statements within the macro and sends any remaining text to the input stack. When the SAS compiler receives a global SAS statement or encounters a SAS step boundary, the macro processor suspends macro execution. After the SAS code executes, the macro processor resumes execution of macro language statements. The macro processor ends execution when it reaches the %MEND statement. Using Macro Parameters Link Using Parameters in the Macro Definition Link You can use parameter lists in order to update the macro variables within your macros. Each time that you call the macro, the parameter list passes values to the macro variables as part of the call, rather than by using separate %LET statements. A parameter list is an optional part of the %MACRO statement. The list names one or more macro variables whose values you specify when you call the macro. When you call a macro, SAS automatically creates a macro variable for each parameter. There are two types of parameters that you can use to update macro variables in your macro programs: positional parameters and keyword parameters . Using Positional Parameters Link To define macros that include positional parameters, you list the names of the macro variables in parentheses and separate the names using commas in the %MACRO statement. 1 2 3 %MACRO macro-name(parameter-1 <, ...parameter-n>); text %MEND <macro-name>; To call a macro that includes positional parameters, you precede the name of the macro with a percent sign and enclose the parameter values in parentheses. You must specify the values in the same order as in the macro definition and separate them with commas. 1 %macro - name ( value - 1 < , ... value - n > ) The values listed in a macro call can be text, macro variable references, macro calls, or null values. SAS assigns these values to the parameters using a one-to-one correspondence. Using Keyword Parameters Link When you use keyword parameters, you list both the name and the value of each macro variable in the macro definition. First specify the keyword, or macro variable name, then the equal sign, and then the value. 1 2 3 %MACRO macro-name(keyword=value, ..., keyword=value); text %MEND <macro-name>; You can list keyword parameters in any order. Whatever value you assign to each parameter, or macro variable, in the %MACRO statement becomes its default value. Null values are allowed. To call a macro that includes keyword parameters, you specify both the keyword and the value for the desired parameters, in any order. If you omit a keyword parameter from the macro call, SAS assigns the keyword parameter its default value. 1 %macro - name ( keyword = value , ..., keyword = value ) Mixed Parameter Lists Link You can use a parameter list that includes both positional and keyword parameters. You must list all positional parameters in the %MACRO statement before any keyword parameters . 1 2 3 4 %MACRO macro-name(parameter-1<, ...parameter-n> keyword = value , ..., keyword=value); text %MEND <macro-name>; Similarly, when you call a macro that includes a mixed parameter list, you must list the positional values before any keyword values. 1 2 %macro - name ( value - 1 < , ... value - n > , keyword = value , ..., keyword = value ); Global and Local Symbol Tables Link Global Symbol Table Link The global symbol table is created during SAS initialization, initialized with automatic macro variables and their values, and deleted at the end of the session. The values of user-defined macro variables are often stored in the global symbol table as well. Macro variables stored in the global symbol table are called global macro variables. To create a global macro variable, you can use a %LET statement anywhere in a program, except within a macro definition. You can also call the SYMPUTX routine in a DATA step or use a SELECT statement that contains an INTO clause in a PROC SQL step. The %GLOBAL statement enables you to create one or more global macro variables. You specify the keyword %GLOBAL , followed by a list of the macro variables that you want to create, separated by spaces. 1 %GLOBAL macro-variable-1 ...macro-variable-n; You can use the %GLOBAL statement anywhere in a SAS program, either inside or outside a macro definition. If the named variables do not exist in the global symbol table, SAS creates them and assigns a null value. If the macro variables already exist, then this statement does not change their values. To delete a macro variable from the global symbol table, you use the %SYMDEL statement. 1 %SYMDEL macro-variables; Working with Local Macro Variables Link SAS creates a local symbol table when you call a macro that includes a parameter list or when a local macro variable is created during macro execution. You should use local macro variables instead of global macro variables whenever possible. Local macro variables exist only while the macro is executing. When the macro ends, SAS deletes the local symbol table and the memory used by that table can be reused. To create local macro variables, you can use parameters in a macro definition or you can use one of the following methods inside a macro definition: a %LET statement, the SYMPUTX routine in a DATA step, or a SELECT statement that contains an INTO clause in a PROC SQL step. The SYMPUTX routine only creates a local macro variable if a local symbol table already exists. If no local symbol table exists, the SYMPUTX routine creates a global variable. Another way to create a local macro variable is to use the %LOCAL statement inside a macro definition. 1 %LOCAL macro-variable-1 ...macro-variable-n; You specify the keyword, %LOCAL , followed by a list of the macro variables that you want to create, separated by spaces. The %LOCAL statement creates one or more macro variables in the local symbol table and assigns null values to them. It has no effect on variables that are already in the local table. Because local symbol tables exist separately from the global symbol table, it is possible to have a local macro variable and a global macro variable that have the same name and different values. You should use a local variable as the index variable for macro loops. This prevents accidental contamination of a like-named macro variable in the global symbol table or in another local table. If you define a macro program that calls another macro program, and if both macros create local symbol tables, then two local symbol tables will exist while the second macro executes. You can use the SYMPUTX routine with an optional third argument, scope, in order to specify where a macro variable should be stored. 1 CALL SYMPUTX ( macro - variable , value < , scope > ); You specify either a scope of G to indicate that the macro variable is to be created in the global symbol table, or a scope of L to indicate that the macro variable is to be created in the local symbol table. If no local symbol table exists for the current macro, then one will be created. Writing Utility Macros Link Sometimes there are routine tasks that you need to do repeatedly. It can be useful to define a macro so that the program code for these tasks can be easily reused. To save these utility macros so that you can reuse them in the future, you can store the compiled macro definitions in a permanent SAS catalog . Setting the two system options, MSTORED and SASMSTORE= , enables you to store macros in a permanent library and specifies that the macro facility will search for compiled macros in the SAS data library that is referenced by the SASMSTORE= option. This libref cannot be work. 1 2 OPTIONS MSTORED | NOMSTORED ; OPTIONS SASMSTORE = libref ; To create a permanently stored compiled macro, you use the STORE option in the %MACRO statement when you submit the macro definition. You can assign a descriptive title for the macro entry in the SAS catalog, by specifying the DES= option. 1 2 3 %MACRO macro-name <(parameter-list)> / STORE <DES='description'>; text %MEND <macro-name>; To access a stored compiled macro, you must set the MSTORED and SASMSTORE= system options, if they are not already set, and then simply call the macro. 1 2 3 OPTIONS MSTORED ; OPTIONS SAMSTORE = libref ; %macro - name","title":"Creating and Using Macro Programs"},{"location":"macros/macroprog/#basic-concepts","text":"Macros are compiled programs that you can execute independently or call as part of a SAS program. Like macro variables, you generally use macros to generate text. But, macros provide additional capabilities. Macros can contain programming statements that enable you to control how and when the text is generated. Also, macros can accept parameters that specify variables. Using parameters, you can write generic macros that can serve a number of uses. A SAS program can reference any number of macros, and you can invoke a macro any number of times within a single SAS program. There are three steps to create and use a macro: define the macro, compile the macro, and call the macro.","title":"Basic Concepts"},{"location":"macros/macroprog/#define-a-macro","text":"A macro definition is the container that holds a macro program. You begin a macro definition with a %MACRO statement and end it with a %MEND statement. 1 2 3 %MACRO macro-name; text %MEND <macro-name>; Each macro that you define has a distinct name. Choose a name for your macro that indicates what the macro does and follow standard SAS naming conventions. Avoid SAS language keywords or call routine names, as well as words that are reserved by the SAS macro facility. You are not required to include the macro name in the %MEND statement. But if you do, be sure that the two macro names match. The text can include constant text, SAS data set names, SAS variable names, and SAS statements. It can also include macro variables ( &country ), macro functions ( %sysfunc() ), macro program statements ( %put sysdate=&sysdate; ) and any combination of the above.","title":"Define a Macro"},{"location":"macros/macroprog/#compile-a-macro","text":"To compile a macro, submit the code. If the macro processor finds syntax errors in the macro language statements, it writes any error messages to the SAS log and creates a dummy, non-executable macro. If the macro processor does not find any macro-level syntax errors, it compiles the macro and stores the compiled definition in a temporary SAS catalog named Work.Sasmacr . Macros in this catalog are known as session-compiled macros and are available for execution during the SAS session in which they're compiled. SAS deletes the temporary catalog that stores the macros at the end of the session.","title":"Compile a Macro"},{"location":"macros/macroprog/#call-a-macro","text":"To call a macro, precede the name of the macro with a percent sign anywhere in a program, except within the data lines of a DATALINES statement. The macro call requires no semicolon because it is not a SAS statement. 1 %macro - name When you call a macro, the word scanner passes the macro call to the macro processor. The macro processor searches Work.Sasmacr for that macro. Then it executes compiled macro language statements within the macro and sends any remaining text to the input stack. When the SAS compiler receives a global SAS statement or encounters a SAS step boundary, the macro processor suspends macro execution. After the SAS code executes, the macro processor resumes execution of macro language statements. The macro processor ends execution when it reaches the %MEND statement.","title":"Call a Macro"},{"location":"macros/macroprog/#using-macro-parameters","text":"","title":"Using Macro Parameters"},{"location":"macros/macroprog/#using-parameters-in-the-macro-definition","text":"You can use parameter lists in order to update the macro variables within your macros. Each time that you call the macro, the parameter list passes values to the macro variables as part of the call, rather than by using separate %LET statements. A parameter list is an optional part of the %MACRO statement. The list names one or more macro variables whose values you specify when you call the macro. When you call a macro, SAS automatically creates a macro variable for each parameter. There are two types of parameters that you can use to update macro variables in your macro programs: positional parameters and keyword parameters .","title":"Using Parameters in the Macro Definition"},{"location":"macros/macroprog/#using-positional-parameters","text":"To define macros that include positional parameters, you list the names of the macro variables in parentheses and separate the names using commas in the %MACRO statement. 1 2 3 %MACRO macro-name(parameter-1 <, ...parameter-n>); text %MEND <macro-name>; To call a macro that includes positional parameters, you precede the name of the macro with a percent sign and enclose the parameter values in parentheses. You must specify the values in the same order as in the macro definition and separate them with commas. 1 %macro - name ( value - 1 < , ... value - n > ) The values listed in a macro call can be text, macro variable references, macro calls, or null values. SAS assigns these values to the parameters using a one-to-one correspondence.","title":"Using Positional Parameters"},{"location":"macros/macroprog/#using-keyword-parameters","text":"When you use keyword parameters, you list both the name and the value of each macro variable in the macro definition. First specify the keyword, or macro variable name, then the equal sign, and then the value. 1 2 3 %MACRO macro-name(keyword=value, ..., keyword=value); text %MEND <macro-name>; You can list keyword parameters in any order. Whatever value you assign to each parameter, or macro variable, in the %MACRO statement becomes its default value. Null values are allowed. To call a macro that includes keyword parameters, you specify both the keyword and the value for the desired parameters, in any order. If you omit a keyword parameter from the macro call, SAS assigns the keyword parameter its default value. 1 %macro - name ( keyword = value , ..., keyword = value )","title":"Using Keyword Parameters"},{"location":"macros/macroprog/#mixed-parameter-lists","text":"You can use a parameter list that includes both positional and keyword parameters. You must list all positional parameters in the %MACRO statement before any keyword parameters . 1 2 3 4 %MACRO macro-name(parameter-1<, ...parameter-n> keyword = value , ..., keyword=value); text %MEND <macro-name>; Similarly, when you call a macro that includes a mixed parameter list, you must list the positional values before any keyword values. 1 2 %macro - name ( value - 1 < , ... value - n > , keyword = value , ..., keyword = value );","title":"Mixed Parameter Lists"},{"location":"macros/macroprog/#global-and-local-symbol-tables","text":"","title":"Global and Local Symbol Tables"},{"location":"macros/macroprog/#global-symbol-table","text":"The global symbol table is created during SAS initialization, initialized with automatic macro variables and their values, and deleted at the end of the session. The values of user-defined macro variables are often stored in the global symbol table as well. Macro variables stored in the global symbol table are called global macro variables. To create a global macro variable, you can use a %LET statement anywhere in a program, except within a macro definition. You can also call the SYMPUTX routine in a DATA step or use a SELECT statement that contains an INTO clause in a PROC SQL step. The %GLOBAL statement enables you to create one or more global macro variables. You specify the keyword %GLOBAL , followed by a list of the macro variables that you want to create, separated by spaces. 1 %GLOBAL macro-variable-1 ...macro-variable-n; You can use the %GLOBAL statement anywhere in a SAS program, either inside or outside a macro definition. If the named variables do not exist in the global symbol table, SAS creates them and assigns a null value. If the macro variables already exist, then this statement does not change their values. To delete a macro variable from the global symbol table, you use the %SYMDEL statement. 1 %SYMDEL macro-variables;","title":"Global Symbol Table"},{"location":"macros/macroprog/#working-with-local-macro-variables","text":"SAS creates a local symbol table when you call a macro that includes a parameter list or when a local macro variable is created during macro execution. You should use local macro variables instead of global macro variables whenever possible. Local macro variables exist only while the macro is executing. When the macro ends, SAS deletes the local symbol table and the memory used by that table can be reused. To create local macro variables, you can use parameters in a macro definition or you can use one of the following methods inside a macro definition: a %LET statement, the SYMPUTX routine in a DATA step, or a SELECT statement that contains an INTO clause in a PROC SQL step. The SYMPUTX routine only creates a local macro variable if a local symbol table already exists. If no local symbol table exists, the SYMPUTX routine creates a global variable. Another way to create a local macro variable is to use the %LOCAL statement inside a macro definition. 1 %LOCAL macro-variable-1 ...macro-variable-n; You specify the keyword, %LOCAL , followed by a list of the macro variables that you want to create, separated by spaces. The %LOCAL statement creates one or more macro variables in the local symbol table and assigns null values to them. It has no effect on variables that are already in the local table. Because local symbol tables exist separately from the global symbol table, it is possible to have a local macro variable and a global macro variable that have the same name and different values. You should use a local variable as the index variable for macro loops. This prevents accidental contamination of a like-named macro variable in the global symbol table or in another local table. If you define a macro program that calls another macro program, and if both macros create local symbol tables, then two local symbol tables will exist while the second macro executes. You can use the SYMPUTX routine with an optional third argument, scope, in order to specify where a macro variable should be stored. 1 CALL SYMPUTX ( macro - variable , value < , scope > ); You specify either a scope of G to indicate that the macro variable is to be created in the global symbol table, or a scope of L to indicate that the macro variable is to be created in the local symbol table. If no local symbol table exists for the current macro, then one will be created.","title":"Working with Local Macro Variables"},{"location":"macros/macroprog/#writing-utility-macros","text":"Sometimes there are routine tasks that you need to do repeatedly. It can be useful to define a macro so that the program code for these tasks can be easily reused. To save these utility macros so that you can reuse them in the future, you can store the compiled macro definitions in a permanent SAS catalog . Setting the two system options, MSTORED and SASMSTORE= , enables you to store macros in a permanent library and specifies that the macro facility will search for compiled macros in the SAS data library that is referenced by the SASMSTORE= option. This libref cannot be work. 1 2 OPTIONS MSTORED | NOMSTORED ; OPTIONS SASMSTORE = libref ; To create a permanently stored compiled macro, you use the STORE option in the %MACRO statement when you submit the macro definition. You can assign a descriptive title for the macro entry in the SAS catalog, by specifying the DES= option. 1 2 3 %MACRO macro-name <(parameter-list)> / STORE <DES='description'>; text %MEND <macro-name>; To access a stored compiled macro, you must set the MSTORED and SASMSTORE= system options, if they are not already set, and then simply call the macro. 1 2 3 OPTIONS MSTORED ; OPTIONS SAMSTORE = libref ; %macro - name","title":"Writing Utility Macros"},{"location":"macros/macrovar-at-execution-time/","text":"Creating a Macro Variable during DATA Step Execution Link Using the CALL SYMPUTX Routine Link You can use the SAS CALL routine SYMPUTX to create and assign a value to a macro variable during execution. 1 CALL SYMPUTX ( macro - variable , value ); CALL routines are similar to functions in that you can pass arguments to them and they perform a calculation or manipulation of the arguments. CALL routines differ from functions in that you can't use CALL routines in assignment statements or expressions. CALL routines don't return a value. Instead, they might alter the value of one or more arguments. To invoke a SAS CALL routine, you specify the name of the routine after the keyword CALL in the CALL statement. The first argument to SYMPUTX is the name of the macro variable to be created or modified. It can be a character literal, a variable, or an expression that resolves to a valid macro variable name. The second argument is the value to be assigned to the macro variable. It can be a character or numeric constant, a variable, or an expression. If character literals are used as arguments, they must be quoted. Example: What is the value of foot after execution of this DATA step? 1 2 3 4 data _null_ ; call symputx ( ' Foot ',' No Internet orders ' ); %let foot = Some Internet orders ; run ; The value of foot is 'No Internet Orders'. Word scanning begins. The %LET is executed by the macro processor. The step boundary is reached and the DATA step executes. SYMPUTX changes the value of foot. Using SYMPUTX with a DATA Step Variable Link You can use the SYMPUTX routine to assign the value of a DATA step variable to a macro variable. This time, the second argument will be a DATA step variable. 1 CALL SYMPUTX ( macro - variable , DATA - step - variable ); The value of the DATA step variable is assigned to the macro variable during execution. Numeric values are automatically converted to character using the BEST32. format. Leading and trailing blanks are removed from both arguments. You can assign a maximum of 32,767 characters to the receiving macro variable. Using SYMPUTX with a DATA Step Expression Link The second argument to the SYMPUTX routine can also be an expression, and it can include arithmetic operations or function calls to manipulate data. 1 CALL SYMPUTX ( macro - variable , expression ); You can use the PUT function as part of the call to the SYMPUTX routine in a DATA step to explicitly convert a numeric value to a character value. 1 2 3 4 PUT ( source , format .); /* Example: */ CALL SYMPUTX ( 'date' , PUT ( Begin_Date , mmddyy10 .)); This CALL statement assigns the value of the DATA step variable Begin_Date to the macro variable date. The PUT function explicitly converts the value of Begin_Date to a character value using the MMDDYY10. format. The conversion occurs before the value is assigned to the macro variable. Passing Data between Steps Link Tip You can use a DATA _NULL_ step with the SYMPUTX routine to create macro variables and pass data between program steps. Creating Indirect References to Macro Variables Link You can use the SYMPUTX routine with DATA step expressions for both arguments to create a series of macro variables, each with a unique name. To create an indirect reference , you precede a name token with multiple ampersands . When the macro processor encounters two ampersands, it resolves them to one ampersand and continues to rescan from left to right, from the point where the multiple ampersands begin. This action is known as the Forward Rescan Rule . Example Given the macro variables and values shown in the following global symbol table, the PROC PRINT step will print all classes that are taught in a particular city. The statement is written in such a way that you would need to change only the value of crsloc in order for the PROC PRINT step to print classes that are taught in a different city. Global Symbol Table Name Value city1 Dallas city2 Boston city3 Seattle %let crsloc=2; proc print data=schedule; where location=\"&&city&crsloc\"; run; You precede the macro variable name city with two ampersands . Then you add a reference to the macro variable crsloc immediately after the first reference in order to build a new token. You need to use three ampersands in front of a macro variable name when its value exactly matches the name of a second macro variable . Example Given the macro variables and values shown in this global symbol table, the correspondance between each macro variable reference and its resolved value. Global Symbol Table Name Value cat100 Outerwear cat120 Accessories sale cat100 Macro variable Resolved value &sale cat100 &&sale cat100 &&&sale Outerwear Creating Macro Variables Using PROC SQL Link Creating Multiple Macro Variables at a Time Link You can also create or update macro variables during the execution of a PROC SQL step. 1 2 3 4 5 6 7 8 PROC SQL ; SELECT column - 1 < , column - 2 , \u2026 > INTO : macro - variable - 1 < , : macro - variable - 2 , \u2026 > FROM table - 1 | view - 1 < WHERE expression > < ORDER BY order - by - item < ,... order - by - item >> < other clauses > ; QUIT ; The SELECT statement generates a report by selecting one or more columns from a table The INTO clause in a SELECT statement enables you to create or update macro variables. The values of the selected columns are assigned to the new macro variables. You specify the keyword INTO , followed by a colon and then the name of the macro variable(s) to be created. Separate multiple macro variables with commas; each must start with a colon. The colon doesn't become part of the name . Unlike the %LET statement and the SYMPUTX routine, the PROC SQL INTO clause doesn't remove leading and trailing blanks from the values. You can use a %LET statement to remove any leading or trailing blanks that are stored in the value. Tip You can use the PROC SQL NOPRINT option to suppress the report if you don't want output to be displayed. Example The following SELECT statement creates a series of macro variables named place1 , place2 , place3 , and so on (as many new macro variables as are needed so that each new macro variable will be assigned a value of a distinct city and state where a student lives as provided in the data set variable City_State ). The first SELECT statement uses the N function to count the number of distinct city_state values and assigns this number to the macro variable numlocs . proc sql; select N(distinct city_state) into :numlocs from students; %let numlocs=&numlocs; select distinct city_state into :place1-:place&numlocs from students; quit; Storing a List of Values in a Macro Variable Link You can use the INTO clause in a PROC SQL step to create one new macro variable for each row in the result of the SELECT statement. You can use an alternate form of the INTO clause in order to take all values of a column (variable) and concatenate them into the value of one macro variable. 1 2 3 4 5 6 7 8 9 10 PROC SQL NOPRINT ; SELECT < DISTINCT > column - 1 INTO : macro - variable - 1 SEPARATED BY 'delimiter-1' FROM table - 1 | view - 1 < WHERE expression > < other clauses > ; QUIT ; % PUT &= macro - variable - 1 ; The INTO clause names the macro variable to be created The SEPARATED BY clause specifies the character that will be used as a delimiter in the value of the macro variable (notice that the character is enclosed in quotation marks) The DISTINCT keyword eliminates duplicates by selecting unique values of the selected column After you execute the PROC SQL step, you can use the %PUT statement to write the values to the log","title":"Creating Macro Variables at Execution Time"},{"location":"macros/macrovar-at-execution-time/#creating-a-macro-variable-during-data-step-execution","text":"","title":"Creating a Macro Variable during DATA Step Execution"},{"location":"macros/macrovar-at-execution-time/#using-the-call-symputx-routine","text":"You can use the SAS CALL routine SYMPUTX to create and assign a value to a macro variable during execution. 1 CALL SYMPUTX ( macro - variable , value ); CALL routines are similar to functions in that you can pass arguments to them and they perform a calculation or manipulation of the arguments. CALL routines differ from functions in that you can't use CALL routines in assignment statements or expressions. CALL routines don't return a value. Instead, they might alter the value of one or more arguments. To invoke a SAS CALL routine, you specify the name of the routine after the keyword CALL in the CALL statement. The first argument to SYMPUTX is the name of the macro variable to be created or modified. It can be a character literal, a variable, or an expression that resolves to a valid macro variable name. The second argument is the value to be assigned to the macro variable. It can be a character or numeric constant, a variable, or an expression. If character literals are used as arguments, they must be quoted. Example: What is the value of foot after execution of this DATA step? 1 2 3 4 data _null_ ; call symputx ( ' Foot ',' No Internet orders ' ); %let foot = Some Internet orders ; run ; The value of foot is 'No Internet Orders'. Word scanning begins. The %LET is executed by the macro processor. The step boundary is reached and the DATA step executes. SYMPUTX changes the value of foot.","title":"Using the CALL SYMPUTX Routine"},{"location":"macros/macrovar-at-execution-time/#using-symputx-with-a-data-step-variable","text":"You can use the SYMPUTX routine to assign the value of a DATA step variable to a macro variable. This time, the second argument will be a DATA step variable. 1 CALL SYMPUTX ( macro - variable , DATA - step - variable ); The value of the DATA step variable is assigned to the macro variable during execution. Numeric values are automatically converted to character using the BEST32. format. Leading and trailing blanks are removed from both arguments. You can assign a maximum of 32,767 characters to the receiving macro variable.","title":"Using SYMPUTX with a DATA Step Variable"},{"location":"macros/macrovar-at-execution-time/#using-symputx-with-a-data-step-expression","text":"The second argument to the SYMPUTX routine can also be an expression, and it can include arithmetic operations or function calls to manipulate data. 1 CALL SYMPUTX ( macro - variable , expression ); You can use the PUT function as part of the call to the SYMPUTX routine in a DATA step to explicitly convert a numeric value to a character value. 1 2 3 4 PUT ( source , format .); /* Example: */ CALL SYMPUTX ( 'date' , PUT ( Begin_Date , mmddyy10 .)); This CALL statement assigns the value of the DATA step variable Begin_Date to the macro variable date. The PUT function explicitly converts the value of Begin_Date to a character value using the MMDDYY10. format. The conversion occurs before the value is assigned to the macro variable.","title":"Using SYMPUTX with a DATA Step Expression"},{"location":"macros/macrovar-at-execution-time/#passing-data-between-steps","text":"Tip You can use a DATA _NULL_ step with the SYMPUTX routine to create macro variables and pass data between program steps.","title":"Passing Data between Steps"},{"location":"macros/macrovar-at-execution-time/#creating-indirect-references-to-macro-variables","text":"You can use the SYMPUTX routine with DATA step expressions for both arguments to create a series of macro variables, each with a unique name. To create an indirect reference , you precede a name token with multiple ampersands . When the macro processor encounters two ampersands, it resolves them to one ampersand and continues to rescan from left to right, from the point where the multiple ampersands begin. This action is known as the Forward Rescan Rule . Example Given the macro variables and values shown in the following global symbol table, the PROC PRINT step will print all classes that are taught in a particular city. The statement is written in such a way that you would need to change only the value of crsloc in order for the PROC PRINT step to print classes that are taught in a different city. Global Symbol Table Name Value city1 Dallas city2 Boston city3 Seattle %let crsloc=2; proc print data=schedule; where location=\"&&city&crsloc\"; run; You precede the macro variable name city with two ampersands . Then you add a reference to the macro variable crsloc immediately after the first reference in order to build a new token. You need to use three ampersands in front of a macro variable name when its value exactly matches the name of a second macro variable . Example Given the macro variables and values shown in this global symbol table, the correspondance between each macro variable reference and its resolved value. Global Symbol Table Name Value cat100 Outerwear cat120 Accessories sale cat100 Macro variable Resolved value &sale cat100 &&sale cat100 &&&sale Outerwear","title":"Creating Indirect References to Macro Variables"},{"location":"macros/macrovar-at-execution-time/#creating-macro-variables-using-proc-sql","text":"","title":"Creating Macro Variables Using PROC SQL"},{"location":"macros/macrovar-at-execution-time/#creating-multiple-macro-variables-at-a-time","text":"You can also create or update macro variables during the execution of a PROC SQL step. 1 2 3 4 5 6 7 8 PROC SQL ; SELECT column - 1 < , column - 2 , \u2026 > INTO : macro - variable - 1 < , : macro - variable - 2 , \u2026 > FROM table - 1 | view - 1 < WHERE expression > < ORDER BY order - by - item < ,... order - by - item >> < other clauses > ; QUIT ; The SELECT statement generates a report by selecting one or more columns from a table The INTO clause in a SELECT statement enables you to create or update macro variables. The values of the selected columns are assigned to the new macro variables. You specify the keyword INTO , followed by a colon and then the name of the macro variable(s) to be created. Separate multiple macro variables with commas; each must start with a colon. The colon doesn't become part of the name . Unlike the %LET statement and the SYMPUTX routine, the PROC SQL INTO clause doesn't remove leading and trailing blanks from the values. You can use a %LET statement to remove any leading or trailing blanks that are stored in the value. Tip You can use the PROC SQL NOPRINT option to suppress the report if you don't want output to be displayed. Example The following SELECT statement creates a series of macro variables named place1 , place2 , place3 , and so on (as many new macro variables as are needed so that each new macro variable will be assigned a value of a distinct city and state where a student lives as provided in the data set variable City_State ). The first SELECT statement uses the N function to count the number of distinct city_state values and assigns this number to the macro variable numlocs . proc sql; select N(distinct city_state) into :numlocs from students; %let numlocs=&numlocs; select distinct city_state into :place1-:place&numlocs from students; quit;","title":"Creating Multiple Macro Variables at a Time"},{"location":"macros/macrovar-at-execution-time/#storing-a-list-of-values-in-a-macro-variable","text":"You can use the INTO clause in a PROC SQL step to create one new macro variable for each row in the result of the SELECT statement. You can use an alternate form of the INTO clause in order to take all values of a column (variable) and concatenate them into the value of one macro variable. 1 2 3 4 5 6 7 8 9 10 PROC SQL NOPRINT ; SELECT < DISTINCT > column - 1 INTO : macro - variable - 1 SEPARATED BY 'delimiter-1' FROM table - 1 | view - 1 < WHERE expression > < other clauses > ; QUIT ; % PUT &= macro - variable - 1 ; The INTO clause names the macro variable to be created The SEPARATED BY clause specifies the character that will be used as a delimiter in the value of the macro variable (notice that the character is enclosed in quotation marks) The DISTINCT keyword eliminates duplicates by selecting unique values of the selected column After you execute the PROC SQL step, you can use the %PUT statement to write the values to the log","title":"Storing a List of Values in a Macro Variable"},{"location":"macros/more-macro/","text":"Check these Websites Developing Large SAS Macro Applications The Fundamentals of Macro Quoting Functions How to define optional macro arguments . Remove element/string from macro variable Link 1 2 3 4 5 6 %put &= list ; /* Check list contents before */ %let removefromlist = string_to_remove ; %let list = %sysfunc ( tranwrd ( & list ., & removefromlist ., %str ()));; %put &= list ; /* Check list contents after */ Call a Macro for a List of Variable Names Link 1 2 3 4 5 6 7 8 9 10 %macro runall ( paramlist ); %let num = %sysfunc ( countw ( & paramlist )); %local i ; %do i = 1 %to & num ; %let parameter & i = %scan ( & paramlist , & i ); %macro_analysis ( variablename =&& parameter & i ); %end ; %mend ; %runall ( item1 item2 item3 item4 item5 ); Create Macrovariable from Data Set Values Link 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 DATA _NULL_ ; SET OddsRatios ; CALL SYMPUT ( 'var1' , OddsRatioEst ); CALL SYMPUT ( 'var2' , LowerCl ); CALL SYMPUT ( 'var3' , UpperCL ); * The variables have a lot of extra spaces ; % LET OR2report = OR : & var1 . (& var2 .,& var3 .); % PUT & OR2report ; RUN ; DATA _NULL_ ; SET OddsRatios ; length OddsRatioEst LowerCL UpperCL 7 ; Estaux = int ( 1000 * OddsRatioEst )/ 1000 ; Loweraux = int ( 1000 * LowerCl )/ 1000 ; Upperaux = int ( 1000 * UpperCL )/ 1000 ; * The extra blancks has been reduced with the CATX function ; fullOR = CATX ( ' ' , 'OR:' , Estaux , '(' , Loweraux , ',' , Upperaux , ')' ); CALL SYMPUT ( 'OR2report' , fullOR ); % PUT & OR2report ; RUN ; Useful Functions for Macro Programming Link VVALUEX Function Link Returns the formatted value that is associated with the argument that you specify. The argument specifies a character constant, variable, or expression that evaluates to a variable name. Warning The value of the specified expression cannot denote an array reference. 1 2 3 4 5 date1 = '31mar02' d ; date2 = 'date1' ; format date1 date7 .; datevalue = vvaluex ( date2 ); put datevalue ; /* 31MAR02 */ Macros Available in SAS Link Check this powerpoint presentation for more tips. Color Utility Macros Link To initiate these macros in your current session you call the %COLORMAC macro. If you submit the following line: 1 %HELPCLR(HELP); You will get a guide of the color utility macros available: 1 2 3 4 5 6 7 8 9 Color Utility Macros Help HELP is currently available for the following macros CMY CMYK CNS HLS HVS RGB HLS2RGB RGB2HLS Enter % HELPCLR ( macroname ) for details on each macro , or % HELPCLR ( ALL ) for details on all macros . SG Annotation Macros Link They can be used within a DATA to simplify the process of creating annotation observations. 1 2 3 4 5 6 7 8 9 10 %SGARROW %SGPOLYGON %SGIMAGE %SGPOLYLINE %SGLINE %SGRECTANGLE %SGOVAL %SGTEXT %SGPOLYCONT %SGTEXTCONT Template Modification Macros Link %MODSTYLE macro allows you to easily make changes to style templates without accessing the code %MODTMPLT macro allows you to easily make limited changes to graph templates without accessing the code Graphical Macros Link %CompactMatrixMacro (Author: Sanjay Matange): it help you modify graphs based on panels %NEWSURV macro (Author: Jeff Meyers): it helps you tune the properties of survival plots %FORESTPLOT macro (Author: Jeff Meyers): it allows another way of presenting results %EULER_MACRO : useful to present proportion Euler diagrams %VENN macro: useful to plot intersection between different events %GTLPieChartMacro : useful for pie charts Export Macros Link %DS2CSV : exports a dataset to *.csv format. Where to Find these Macros? Link Color utility macros, SGAnnotation macros, %MODSTYLE and %MODTMPLT are SAS autocall macros %AXISBREAK %COMPACTMATRIXMACRO %ORTHO3D_MACRO %NEWSURV %FORESTPLOT %EULER_MACRO %VENN %GTLPIECHARTMACRO Macro examples Link Macro Program for Creating Box Plots for All of Predictor Variables Link 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 %let categorical = House_Style2 Overall_Qual2 Overall_Cond2 Fireplaces Season_Sold Garage_Type_2 Foundation_2 Heating_QC Masonry_Veneer Lot_Shape_2 Central_Air ; /* Macro Usage: %box(DSN = , Response = , CharVar = ) */ %macro box ( dsn = , response = , Charvar = ); %let i = 1 ; %do %while ( %scan ( & charvar , & i , %str ( )) ^= %str ()) ; %let var = %scan ( & charvar , & i , %str ( )); proc sgplot data =& dsn ; vbox & response / category =& var grouporder = ascending connect = mean ; title \"&response across Levels of &var\" ; run ; %let i = %eval ( & i + 1 ) ; %end ; %mend box ; %box ( dsn = statdata . ameshousing3 , response = SalePrice , charvar = & categorical ); title ; options label ; Checking if a data set is exists Link 1 2 3 4 5 6 DATA test - SAS - data - set ; SET %if %sysfunc ( exist ( SAS - data - set - part1 )) %then %do ; SAS - data - set - part1 %end ; SAS - data - set - part2 ; RUN ; Macro's Sources Link Les macros SAS de Dominique Ladiray Mayo Clinic: Locally written SAS macros Here you have some macro repositories. Kaplan-Meier Survival Plotting Macro %NEWSURV","title":"More on Macro Programming"},{"location":"macros/more-macro/#remove-elementstring-from-macro-variable","text":"1 2 3 4 5 6 %put &= list ; /* Check list contents before */ %let removefromlist = string_to_remove ; %let list = %sysfunc ( tranwrd ( & list ., & removefromlist ., %str ()));; %put &= list ; /* Check list contents after */","title":"Remove element/string from macro variable"},{"location":"macros/more-macro/#call-a-macro-for-a-list-of-variable-names","text":"1 2 3 4 5 6 7 8 9 10 %macro runall ( paramlist ); %let num = %sysfunc ( countw ( & paramlist )); %local i ; %do i = 1 %to & num ; %let parameter & i = %scan ( & paramlist , & i ); %macro_analysis ( variablename =&& parameter & i ); %end ; %mend ; %runall ( item1 item2 item3 item4 item5 );","title":"Call a Macro for a List of Variable Names"},{"location":"macros/more-macro/#create-macrovariable-from-data-set-values","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 DATA _NULL_ ; SET OddsRatios ; CALL SYMPUT ( 'var1' , OddsRatioEst ); CALL SYMPUT ( 'var2' , LowerCl ); CALL SYMPUT ( 'var3' , UpperCL ); * The variables have a lot of extra spaces ; % LET OR2report = OR : & var1 . (& var2 .,& var3 .); % PUT & OR2report ; RUN ; DATA _NULL_ ; SET OddsRatios ; length OddsRatioEst LowerCL UpperCL 7 ; Estaux = int ( 1000 * OddsRatioEst )/ 1000 ; Loweraux = int ( 1000 * LowerCl )/ 1000 ; Upperaux = int ( 1000 * UpperCL )/ 1000 ; * The extra blancks has been reduced with the CATX function ; fullOR = CATX ( ' ' , 'OR:' , Estaux , '(' , Loweraux , ',' , Upperaux , ')' ); CALL SYMPUT ( 'OR2report' , fullOR ); % PUT & OR2report ; RUN ;","title":"Create Macrovariable from Data Set Values"},{"location":"macros/more-macro/#useful-functions-for-macro-programming","text":"","title":"Useful Functions for Macro Programming"},{"location":"macros/more-macro/#vvaluex-function","text":"Returns the formatted value that is associated with the argument that you specify. The argument specifies a character constant, variable, or expression that evaluates to a variable name. Warning The value of the specified expression cannot denote an array reference. 1 2 3 4 5 date1 = '31mar02' d ; date2 = 'date1' ; format date1 date7 .; datevalue = vvaluex ( date2 ); put datevalue ; /* 31MAR02 */","title":"VVALUEX Function"},{"location":"macros/more-macro/#macros-available-in-sas","text":"Check this powerpoint presentation for more tips.","title":"Macros Available in SAS"},{"location":"macros/more-macro/#color-utility-macros","text":"To initiate these macros in your current session you call the %COLORMAC macro. If you submit the following line: 1 %HELPCLR(HELP); You will get a guide of the color utility macros available: 1 2 3 4 5 6 7 8 9 Color Utility Macros Help HELP is currently available for the following macros CMY CMYK CNS HLS HVS RGB HLS2RGB RGB2HLS Enter % HELPCLR ( macroname ) for details on each macro , or % HELPCLR ( ALL ) for details on all macros .","title":"Color Utility Macros"},{"location":"macros/more-macro/#sg-annotation-macros","text":"They can be used within a DATA to simplify the process of creating annotation observations. 1 2 3 4 5 6 7 8 9 10 %SGARROW %SGPOLYGON %SGIMAGE %SGPOLYLINE %SGLINE %SGRECTANGLE %SGOVAL %SGTEXT %SGPOLYCONT %SGTEXTCONT","title":"SG Annotation Macros"},{"location":"macros/more-macro/#template-modification-macros","text":"%MODSTYLE macro allows you to easily make changes to style templates without accessing the code %MODTMPLT macro allows you to easily make limited changes to graph templates without accessing the code","title":"Template Modification Macros"},{"location":"macros/more-macro/#graphical-macros","text":"%CompactMatrixMacro (Author: Sanjay Matange): it help you modify graphs based on panels %NEWSURV macro (Author: Jeff Meyers): it helps you tune the properties of survival plots %FORESTPLOT macro (Author: Jeff Meyers): it allows another way of presenting results %EULER_MACRO : useful to present proportion Euler diagrams %VENN macro: useful to plot intersection between different events %GTLPieChartMacro : useful for pie charts","title":"Graphical Macros"},{"location":"macros/more-macro/#export-macros","text":"%DS2CSV : exports a dataset to *.csv format.","title":"Export Macros"},{"location":"macros/more-macro/#where-to-find-these-macros","text":"Color utility macros, SGAnnotation macros, %MODSTYLE and %MODTMPLT are SAS autocall macros %AXISBREAK %COMPACTMATRIXMACRO %ORTHO3D_MACRO %NEWSURV %FORESTPLOT %EULER_MACRO %VENN %GTLPIECHARTMACRO","title":"Where to Find these Macros?"},{"location":"macros/more-macro/#macro-examples","text":"","title":"Macro examples"},{"location":"macros/more-macro/#macro-program-for-creating-box-plots-for-all-of-predictor-variables","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 %let categorical = House_Style2 Overall_Qual2 Overall_Cond2 Fireplaces Season_Sold Garage_Type_2 Foundation_2 Heating_QC Masonry_Veneer Lot_Shape_2 Central_Air ; /* Macro Usage: %box(DSN = , Response = , CharVar = ) */ %macro box ( dsn = , response = , Charvar = ); %let i = 1 ; %do %while ( %scan ( & charvar , & i , %str ( )) ^= %str ()) ; %let var = %scan ( & charvar , & i , %str ( )); proc sgplot data =& dsn ; vbox & response / category =& var grouporder = ascending connect = mean ; title \"&response across Levels of &var\" ; run ; %let i = %eval ( & i + 1 ) ; %end ; %mend box ; %box ( dsn = statdata . ameshousing3 , response = SalePrice , charvar = & categorical ); title ; options label ;","title":"Macro Program for Creating Box Plots for All of Predictor Variables"},{"location":"macros/more-macro/#checking-if-a-data-set-is-exists","text":"1 2 3 4 5 6 DATA test - SAS - data - set ; SET %if %sysfunc ( exist ( SAS - data - set - part1 )) %then %do ; SAS - data - set - part1 %end ; SAS - data - set - part2 ; RUN ;","title":"Checking if a data set is exists"},{"location":"macros/more-macro/#macros-sources","text":"Les macros SAS de Dominique Ladiray Mayo Clinic: Locally written SAS macros Here you have some macro repositories. Kaplan-Meier Survival Plotting Macro %NEWSURV","title":"Macro's Sources"},{"location":"macros/using-macrofunc/","text":"Overview of Macro Functions Link Macro Functions Link Macro functions enable you to manipulate text strings that SAS inserts in your code. When you submit a program, SAS executes the macro functions before the program compiles. To use a macro function, specify the function name, which starts with a percent sign. Enclose the function arguments in parentheses, separated by commas. 1 %function - name ( argument - 1 < , argument - n > ) The arguments can include: Constant text ( S&P 500 ) Macro variable references ( &sysdate9 ) Macro functions ( %lenght(&var) ) Macro calls ( %time ) Tip When you use constant text, do not enclose the text in quotation marks. If you do include them, they'll become part of the argument. You can use all macro functions in both open code and macro definitions. Macro functions are categorized in four types: Macro character functions Macro evaluation functions Macro quoting functions Other macro functions Using Macro Character Functions Link Macro character functions enable you to manipulate character strings or obtain information about them. %SUBSTR Function Link %SUBSTR extracts a substring of characters from an argument consisting of a character string or text expression. Position specifies where the extraction should begin. n specifies the number of characters to extract. If you don't specify n, the remainder of the string is extracted. 1 %SUBSTR (argument, position <, n>) %SCAN Function Link The %SCAN function enables you to extract words from a macro variable or text expression. %SCAN returns the nth word in an argument, where the words in the argument are separated by delimiters. If n is greater than the number of words in the argument, the function returns a null string. Delimiters refers to the characters that separate words or text expressions. 1 %SCAN (argument, n <, delimiters>) If you omit the optional delimiter information, %SCAN uses a default set of delimiters shown below. Encoding Type Default Delimiters ASCII blank . < ( + & ! $ * ) ; ^ - / , % | EBCDIC blank . < ( + | & ! $ * ) ; \u00ac - / , % \u00a6 \u00a2 %UPCASE Function Link The %UPCASE function enables you to convert characters to uppercase before substituting that value in a SAS program. 1 %UPCASE (argument) %INDEX Function Link The %INDEX function enables you to search for a string within a source. %INDEX searches source for the first occurrence of string and returns the position of its first character. If an exact match of string is not found, the function returns 0. 1 %INDEX (source, string) Using Arithmetic and Logical Expressions Link %EVAL Function Link The %EVAL function evaluates arithmetic and logical expressions. 1 %EVAL (arithmetic or logical expression) Arithmetic Expressions Logical Expressions 1 + 2 &DAY = FRIDAY 4 * 3 A < a 4 / 2 1 < &INDEX 00FFx - 003Ax &START NE &END When %EVAL evaluates an arithmetic expression , it temporarily converts operands to numeric values and performs an integer arithmetic operation. If the result of the expression is noninteger, %EVAL truncates the value to an integer. The result is expressed as text. The %EVAL function generates an error message in the log when it encounters an expression that contains noninteger values. 1 2 3 4 /* Example */ %let x = %eval ( 5 , 3 ); %put x =& x /*result 1*/ When %EVAL evaluates a logical expression , it returns a value of 0 to indicate that the expression is false, or a value of 1 to indicate that the expression is true. 1 2 3 4 5 /* Example */ %let x = %eval ( 10 lt 2 ); /*10 less than 2*/ /*The expression is false*/ %put x =& x /*result 0*/ %SYSEVALF Function Link The %SYSEVALF function evaluates arithmetic and logical expressions using floating-point arithmetic and returns a value that is formatted using the BEST32. format (meaning that decimal contributions are not accounted in the operation). The result of the evaluation is always text. 1 2 3 4 5 6 % SYSEVALF ( expression < , conversion - type > ) /* Example */ %let value = %sysevalf ( 10.5 + 20.8 ); %put 10.5 + 20.8 = & value ; /* result 10.5+20.8 = 30 */ You can use %SYSEVALF with an optional conversion type ( BOOLEAN , CEIL , FLOOR , or INTEGER ) that tailors the value returned. Using SAS Functions with Macro Variables Link %SYSFUNC Function Link You can use the %SYSFUNC macro function to execute SAS functions within the macro facility. 1 2 3 4 % SYSFUNC ( SAS - function ( argument ( s )) < , format > ) /* Example */ %let current = %sysfunc ( time (), time .); Because %SYSFUNC is a macro function, you don't enclose character values in quotation marks, as you do in SAS functions. You can specify an optional format for the value returned by the function. If you do not specify a format, numeric results are converted to a character string using the BEST12. format. SAS returns character results as they are, without formatting or translation. You can use almost any SAS function with the %SYSFUNC macro function. The exceptions are shown in this table. Function Type Function Name Array processing DIM , HBOUND , LBOUND Variable information VNAME , VLABEL , MISSING Macro interface RESOLVE , SYMGET Data conversion INPUT , PUT Other functions ORCMSG , LAG , DIF Tip Use INPUTC and INPUTN in place of INPUT , and PUTC and PUTN in place of PUT . Using Macro Functions to Mask Special Characters Link Macro quoting functions enable you to clearly indicate to the macro processor how it is to interpret special characters and mnemonics. %STR Macro Quoting Function Link The macro quoting function %STR masks (or quotes) special characters during compilation so that the macro processor does not interpret them as macro-level syntax. 1 %STR (argument) %STR can also be used to quote tokens that typically occur in pairs, such as the apostrophe, quotation marks, and open and closed parentheses. The unmatched token must be preceded by a % . Here is a list of all of the special characters and mnemonics masked by %STR . 1 2 3 + - * / < > = \u00ac ^ ~ ; , # blank AND OR NOT EQ NE LE LT GE GT IN ' \" ) ( Note that %STR does not mask the characters & or % . %NRSTR Macro Quoting Function Link %NRSTR masks the same characters as %STR and also masks the special characters & and % . Using %NRSTR instead of %STR prevents macro and macro variable resolution. 1 %NSTR (argument) %INCLUDE Is Not a Macro Language Statement Link The %INCLUDE statement retrieves SAS source code from an external file and places it on the input stack. It is a global SAS statement, not a macro language statement, and it can be used only on a statement boundary. 1 % INCLUDE file - specification </ SOURCE2 > ; To use the %INCLUDE statement, you specify the keyword %INCLUDE , followed by a file specification. The file-specification value is the physical name or fileref of the file to be retrieved. You can optionally specify SOURCE2 following the file specification. SOURCE2 causes the SAS statements that are inserted into the input stack to be displayed in the SAS log. If you don't specify SOURCE2 in the %INCLUDE statement, the setting of the SAS system option SOURCE2 controls whether the inserted code is displayed.","title":"Using Macro Functions"},{"location":"macros/using-macrofunc/#overview-of-macro-functions","text":"","title":"Overview of Macro Functions"},{"location":"macros/using-macrofunc/#macro-functions","text":"Macro functions enable you to manipulate text strings that SAS inserts in your code. When you submit a program, SAS executes the macro functions before the program compiles. To use a macro function, specify the function name, which starts with a percent sign. Enclose the function arguments in parentheses, separated by commas. 1 %function - name ( argument - 1 < , argument - n > ) The arguments can include: Constant text ( S&P 500 ) Macro variable references ( &sysdate9 ) Macro functions ( %lenght(&var) ) Macro calls ( %time ) Tip When you use constant text, do not enclose the text in quotation marks. If you do include them, they'll become part of the argument. You can use all macro functions in both open code and macro definitions. Macro functions are categorized in four types: Macro character functions Macro evaluation functions Macro quoting functions Other macro functions","title":"Macro Functions"},{"location":"macros/using-macrofunc/#using-macro-character-functions","text":"Macro character functions enable you to manipulate character strings or obtain information about them.","title":"Using Macro Character Functions"},{"location":"macros/using-macrofunc/#substr-function","text":"%SUBSTR extracts a substring of characters from an argument consisting of a character string or text expression. Position specifies where the extraction should begin. n specifies the number of characters to extract. If you don't specify n, the remainder of the string is extracted. 1 %SUBSTR (argument, position <, n>)","title":"%SUBSTR Function"},{"location":"macros/using-macrofunc/#scan-function","text":"The %SCAN function enables you to extract words from a macro variable or text expression. %SCAN returns the nth word in an argument, where the words in the argument are separated by delimiters. If n is greater than the number of words in the argument, the function returns a null string. Delimiters refers to the characters that separate words or text expressions. 1 %SCAN (argument, n <, delimiters>) If you omit the optional delimiter information, %SCAN uses a default set of delimiters shown below. Encoding Type Default Delimiters ASCII blank . < ( + & ! $ * ) ; ^ - / , % | EBCDIC blank . < ( + | & ! $ * ) ; \u00ac - / , % \u00a6 \u00a2","title":"%SCAN Function"},{"location":"macros/using-macrofunc/#upcase-function","text":"The %UPCASE function enables you to convert characters to uppercase before substituting that value in a SAS program. 1 %UPCASE (argument)","title":"%UPCASE Function"},{"location":"macros/using-macrofunc/#index-function","text":"The %INDEX function enables you to search for a string within a source. %INDEX searches source for the first occurrence of string and returns the position of its first character. If an exact match of string is not found, the function returns 0. 1 %INDEX (source, string)","title":"%INDEX Function"},{"location":"macros/using-macrofunc/#using-arithmetic-and-logical-expressions","text":"","title":"Using Arithmetic and Logical Expressions"},{"location":"macros/using-macrofunc/#eval-function","text":"The %EVAL function evaluates arithmetic and logical expressions. 1 %EVAL (arithmetic or logical expression) Arithmetic Expressions Logical Expressions 1 + 2 &DAY = FRIDAY 4 * 3 A < a 4 / 2 1 < &INDEX 00FFx - 003Ax &START NE &END When %EVAL evaluates an arithmetic expression , it temporarily converts operands to numeric values and performs an integer arithmetic operation. If the result of the expression is noninteger, %EVAL truncates the value to an integer. The result is expressed as text. The %EVAL function generates an error message in the log when it encounters an expression that contains noninteger values. 1 2 3 4 /* Example */ %let x = %eval ( 5 , 3 ); %put x =& x /*result 1*/ When %EVAL evaluates a logical expression , it returns a value of 0 to indicate that the expression is false, or a value of 1 to indicate that the expression is true. 1 2 3 4 5 /* Example */ %let x = %eval ( 10 lt 2 ); /*10 less than 2*/ /*The expression is false*/ %put x =& x /*result 0*/","title":"%EVAL Function"},{"location":"macros/using-macrofunc/#sysevalf-function","text":"The %SYSEVALF function evaluates arithmetic and logical expressions using floating-point arithmetic and returns a value that is formatted using the BEST32. format (meaning that decimal contributions are not accounted in the operation). The result of the evaluation is always text. 1 2 3 4 5 6 % SYSEVALF ( expression < , conversion - type > ) /* Example */ %let value = %sysevalf ( 10.5 + 20.8 ); %put 10.5 + 20.8 = & value ; /* result 10.5+20.8 = 30 */ You can use %SYSEVALF with an optional conversion type ( BOOLEAN , CEIL , FLOOR , or INTEGER ) that tailors the value returned.","title":"%SYSEVALF Function"},{"location":"macros/using-macrofunc/#using-sas-functions-with-macro-variables","text":"","title":"Using SAS Functions with Macro Variables"},{"location":"macros/using-macrofunc/#sysfunc-function","text":"You can use the %SYSFUNC macro function to execute SAS functions within the macro facility. 1 2 3 4 % SYSFUNC ( SAS - function ( argument ( s )) < , format > ) /* Example */ %let current = %sysfunc ( time (), time .); Because %SYSFUNC is a macro function, you don't enclose character values in quotation marks, as you do in SAS functions. You can specify an optional format for the value returned by the function. If you do not specify a format, numeric results are converted to a character string using the BEST12. format. SAS returns character results as they are, without formatting or translation. You can use almost any SAS function with the %SYSFUNC macro function. The exceptions are shown in this table. Function Type Function Name Array processing DIM , HBOUND , LBOUND Variable information VNAME , VLABEL , MISSING Macro interface RESOLVE , SYMGET Data conversion INPUT , PUT Other functions ORCMSG , LAG , DIF Tip Use INPUTC and INPUTN in place of INPUT , and PUTC and PUTN in place of PUT .","title":"%SYSFUNC Function"},{"location":"macros/using-macrofunc/#using-macro-functions-to-mask-special-characters","text":"Macro quoting functions enable you to clearly indicate to the macro processor how it is to interpret special characters and mnemonics.","title":"Using Macro Functions to Mask Special Characters"},{"location":"macros/using-macrofunc/#str-macro-quoting-function","text":"The macro quoting function %STR masks (or quotes) special characters during compilation so that the macro processor does not interpret them as macro-level syntax. 1 %STR (argument) %STR can also be used to quote tokens that typically occur in pairs, such as the apostrophe, quotation marks, and open and closed parentheses. The unmatched token must be preceded by a % . Here is a list of all of the special characters and mnemonics masked by %STR . 1 2 3 + - * / < > = \u00ac ^ ~ ; , # blank AND OR NOT EQ NE LE LT GE GT IN ' \" ) ( Note that %STR does not mask the characters & or % .","title":"%STR Macro Quoting Function"},{"location":"macros/using-macrofunc/#nrstr-macro-quoting-function","text":"%NRSTR masks the same characters as %STR and also masks the special characters & and % . Using %NRSTR instead of %STR prevents macro and macro variable resolution. 1 %NSTR (argument)","title":"%NRSTR Macro Quoting Function"},{"location":"macros/using-macrofunc/#include-is-not-a-macro-language-statement","text":"The %INCLUDE statement retrieves SAS source code from an external file and places it on the input stack. It is a global SAS statement, not a macro language statement, and it can be used only on a statement boundary. 1 % INCLUDE file - specification </ SOURCE2 > ; To use the %INCLUDE statement, you specify the keyword %INCLUDE , followed by a file specification. The file-specification value is the physical name or fileref of the file to be retrieved. You can optionally specify SOURCE2 following the file specification. SOURCE2 causes the SAS statements that are inserted into the input stack to be displayed in the SAS log. If you don't specify SOURCE2 in the %INCLUDE statement, the setting of the SAS system option SOURCE2 controls whether the inserted code is displayed.","title":"%INCLUDE Is Not a Macro Language Statement"},{"location":"medical/exposure/","text":"Dose Intensity Link In comparing various protocols you will find that not only do doses vary from one protocol to the next, but also how many doses and when they are given. This can make comparisons difficult or impossible. Absolute Dose Intensity Link The concept of Dose Intensity (DI) may allow a rough comparison. Dose Intensity is the total amount of drug given in a fixed unit of time (usually 1 week), thus is a function of dose and frequency of administration. $DI = \\frac{sum \\ of \\ all \\ administered \\ doses}{(last \\ dose \\ date - 1^{st} \\ dose \\ date + planned \\ cycle \\ duration)/ 7} $ Relative Dose Intensity Link The Relative Dose Intensity (RDI) is the ratio of delivered respect to the planned dose intensity and can be expressed as a percentage. An RDI of 100% indicates that the drug was administered at the dose planned per protocol, without delay and without cancellations. $RDI = \\frac{sum \\ of \\ all \\ administered \\ doses}{(last \\ dose \\ date - 1^{st} \\ dose \\ date + planned \\ cycle \\ duration)/ 7} \\cdot \\left (\\frac{planned \\ dose \\ per \\ cycle}{planned \\ cycle \\ duration / 7} \\right )^{-1} \\cdot 100 =$ $= \\frac{sum \\ of \\ all \\ administered \\ doses}{planned \\ dose \\ per \\ cycle} \\cdot \\frac{planned \\ cycle \\ duration}{last \\ dose \\ date - 1^{st} \\ dose \\ date + planned \\ cycle \\ duration} \\cdot 100$ Note For RDI calculation, the full theoretical dose and cycle duration stablished by protocol are used (not the intended dose registered by cycle which may account for reductions). Regarding the last cycle, the protocol or principal investigator should specify the criteria to accound for interrupted treatments. Example: treatment consisting on administrations on day 1, 7 and 15 of a 21-day cycle. What should we do with a patient who received the 100% of the treatment until the last cycle on which he only received the doses of day 1 and 7? Should we consider the administration of day 15 as zero dose (RDI will be less than 100%)? Should we consider this last cycle as reduced in duration (finished with the last dose administered and, thus, RDI maintained in 100%)? When there is a special interest in safety (Phase I trials) we need to be careful when defining these variables, otherwise the variations introduced by these details are minimal and it's not worth the trouble. If there is a delay during the cycle (between doses), we may need to use the last dose date plus the theoretical rest period (until the end of the cycle) to account for the cycle duration (moreover if it's the last one). A different definition is sometimes used for the Relative Dose Intensity (RDI) on which the time factor is not considered: $RDI = \\frac{sum \\ of \\ all \\ administered \\ doses}{sum \\ of \\ all \\ planned \\ doses} \\cdot 100$ This definition is equivalent to the previous one if there are not delays in the treatment respect to the planned cycle duration, however, if that is the case (there are delays) using this definition one would be overestimating the RDI . Derived Variables Link Key derived variables for exposure derived datasets could be: The Cumulative dose , usually given in $mg$, is the sum of all administered Dose/Cycles The Planned Treatment Duration ($days$) is the planned time between two consecutive administrations, i.e. the planned cycle duration The Treatment Duration ($weeks$) is calculated as: (date of last administration of trial drug - date of first administration of trial drug + Planned Duration)/ 7 The Dose Intensity (DI) ($mg/week$) is calculated as: Cumulative dose ($mg$) / Treatment duration ($weeks$) The Planned Dose Intensity (PDI) ($mg/week$) is calculated as: Cumulative planned dose per cycle ($mg$) / (Planned Treatment duration/7) ($weeks$) The Relative Dose Intensity (RDI) ($%$) is calculated as: 100 * DI ($mg/week$) / PDI ($mg/week$) Related considerations: An RDI of 100% indicates that the drug was administered at the right dose within the planned timeframe Dose may also be measured as $mg/m^2$ when treatment are infused, in this case the total dose expressed in $mg$ is divided by the subject body surface area (BSA) ($m^2$) measured at the time of the drug administration For some study drugs the doses may also be measured as $mg/kg$ Both DI and RDI together with Cumulative Dose and Treatment Duration, are described by means of descriptive statistics for continuos variables. Additionally frequency distribution together with % of number of Administered Cycle is also provided In combination studies, when applicable, the above information is derived and presented for each drug administered taking into account that the treatment duration may be different for each one of them Treatment Reductions, Delays, Interruptions and Drug Withdrawal Link If it's not explicitely defined in the protocol, in oncology clinical trials a delay of more than 3 days will be considered as a relevant delay. Delays of less than 3 days will be considered as if the treatment was given at the right date. This standard definition is stablished to account for weekends as non-working days. Other point that must be defined in the protocol is if delays are permitted or not, so you wouldn't/would need to include 0-dose cycles when computing the DI or RDI. A temporary interruption occurs when leaving one or more cycles with dose = 0 and then keep going with the treatment in a cycle that does not exceed the limits of 0-dose cycles allowed by the protocol. A delay is a cycle that simply begins outside the window in which it was planned, but on which the dose is administered (not a 0-dose cycle). The curious circumstance (and this has to be explicitly in the protocol), is that a cycle can be delayed beyond the full time period of the corresponding cycle and i then enters the next window, so, except that the protocol consider that all cycles must be administered regardless of whether one is delayed, then the delayed cycle is recorded where it belonged without leaving gaps of blank cycles. If this is not expressed in the protocol (which all cycles must be administered) then you may consider that when one cycle overlaps with the next window, the cycle in question has to be considered interruption with dosage = 0 . If the delay was too long and the study recommended the withdrawal, then the delays would become treatment withdrawals . In summary, a delay of more than one cycle becomes interruption of the treatment during that cycle and if such interruption is prolonged beyond the number of cycles allowed by the protocol then it becomes a withdrawal .","title":"Treatment Exposure"},{"location":"medical/exposure/#dose-intensity","text":"In comparing various protocols you will find that not only do doses vary from one protocol to the next, but also how many doses and when they are given. This can make comparisons difficult or impossible.","title":"Dose Intensity"},{"location":"medical/exposure/#absolute-dose-intensity","text":"The concept of Dose Intensity (DI) may allow a rough comparison. Dose Intensity is the total amount of drug given in a fixed unit of time (usually 1 week), thus is a function of dose and frequency of administration. $DI = \\frac{sum \\ of \\ all \\ administered \\ doses}{(last \\ dose \\ date - 1^{st} \\ dose \\ date + planned \\ cycle \\ duration)/ 7} $","title":"Absolute Dose Intensity"},{"location":"medical/exposure/#relative-dose-intensity","text":"The Relative Dose Intensity (RDI) is the ratio of delivered respect to the planned dose intensity and can be expressed as a percentage. An RDI of 100% indicates that the drug was administered at the dose planned per protocol, without delay and without cancellations. $RDI = \\frac{sum \\ of \\ all \\ administered \\ doses}{(last \\ dose \\ date - 1^{st} \\ dose \\ date + planned \\ cycle \\ duration)/ 7} \\cdot \\left (\\frac{planned \\ dose \\ per \\ cycle}{planned \\ cycle \\ duration / 7} \\right )^{-1} \\cdot 100 =$ $= \\frac{sum \\ of \\ all \\ administered \\ doses}{planned \\ dose \\ per \\ cycle} \\cdot \\frac{planned \\ cycle \\ duration}{last \\ dose \\ date - 1^{st} \\ dose \\ date + planned \\ cycle \\ duration} \\cdot 100$ Note For RDI calculation, the full theoretical dose and cycle duration stablished by protocol are used (not the intended dose registered by cycle which may account for reductions). Regarding the last cycle, the protocol or principal investigator should specify the criteria to accound for interrupted treatments. Example: treatment consisting on administrations on day 1, 7 and 15 of a 21-day cycle. What should we do with a patient who received the 100% of the treatment until the last cycle on which he only received the doses of day 1 and 7? Should we consider the administration of day 15 as zero dose (RDI will be less than 100%)? Should we consider this last cycle as reduced in duration (finished with the last dose administered and, thus, RDI maintained in 100%)? When there is a special interest in safety (Phase I trials) we need to be careful when defining these variables, otherwise the variations introduced by these details are minimal and it's not worth the trouble. If there is a delay during the cycle (between doses), we may need to use the last dose date plus the theoretical rest period (until the end of the cycle) to account for the cycle duration (moreover if it's the last one). A different definition is sometimes used for the Relative Dose Intensity (RDI) on which the time factor is not considered: $RDI = \\frac{sum \\ of \\ all \\ administered \\ doses}{sum \\ of \\ all \\ planned \\ doses} \\cdot 100$ This definition is equivalent to the previous one if there are not delays in the treatment respect to the planned cycle duration, however, if that is the case (there are delays) using this definition one would be overestimating the RDI .","title":"Relative Dose Intensity"},{"location":"medical/exposure/#derived-variables","text":"Key derived variables for exposure derived datasets could be: The Cumulative dose , usually given in $mg$, is the sum of all administered Dose/Cycles The Planned Treatment Duration ($days$) is the planned time between two consecutive administrations, i.e. the planned cycle duration The Treatment Duration ($weeks$) is calculated as: (date of last administration of trial drug - date of first administration of trial drug + Planned Duration)/ 7 The Dose Intensity (DI) ($mg/week$) is calculated as: Cumulative dose ($mg$) / Treatment duration ($weeks$) The Planned Dose Intensity (PDI) ($mg/week$) is calculated as: Cumulative planned dose per cycle ($mg$) / (Planned Treatment duration/7) ($weeks$) The Relative Dose Intensity (RDI) ($%$) is calculated as: 100 * DI ($mg/week$) / PDI ($mg/week$) Related considerations: An RDI of 100% indicates that the drug was administered at the right dose within the planned timeframe Dose may also be measured as $mg/m^2$ when treatment are infused, in this case the total dose expressed in $mg$ is divided by the subject body surface area (BSA) ($m^2$) measured at the time of the drug administration For some study drugs the doses may also be measured as $mg/kg$ Both DI and RDI together with Cumulative Dose and Treatment Duration, are described by means of descriptive statistics for continuos variables. Additionally frequency distribution together with % of number of Administered Cycle is also provided In combination studies, when applicable, the above information is derived and presented for each drug administered taking into account that the treatment duration may be different for each one of them","title":"Derived Variables"},{"location":"medical/exposure/#treatment-reductions-delays-interruptions-and-drug-withdrawal","text":"If it's not explicitely defined in the protocol, in oncology clinical trials a delay of more than 3 days will be considered as a relevant delay. Delays of less than 3 days will be considered as if the treatment was given at the right date. This standard definition is stablished to account for weekends as non-working days. Other point that must be defined in the protocol is if delays are permitted or not, so you wouldn't/would need to include 0-dose cycles when computing the DI or RDI. A temporary interruption occurs when leaving one or more cycles with dose = 0 and then keep going with the treatment in a cycle that does not exceed the limits of 0-dose cycles allowed by the protocol. A delay is a cycle that simply begins outside the window in which it was planned, but on which the dose is administered (not a 0-dose cycle). The curious circumstance (and this has to be explicitly in the protocol), is that a cycle can be delayed beyond the full time period of the corresponding cycle and i then enters the next window, so, except that the protocol consider that all cycles must be administered regardless of whether one is delayed, then the delayed cycle is recorded where it belonged without leaving gaps of blank cycles. If this is not expressed in the protocol (which all cycles must be administered) then you may consider that when one cycle overlaps with the next window, the cycle in question has to be considered interruption with dosage = 0 . If the delay was too long and the study recommended the withdrawal, then the delays would become treatment withdrawals . In summary, a delay of more than one cycle becomes interruption of the treatment during that cycle and if such interruption is prolonged beyond the number of cycles allowed by the protocol then it becomes a withdrawal .","title":"Treatment Reductions, Delays, Interruptions and Drug Withdrawal"},{"location":"medical/general/","text":"Check these websites STAT 509: Clinical Trials Symptons vs Signs Link A symptom is any subjective evidence of disease, while a sign is any objective evidence of disease. Therefore, a symptom is a phenomenon that is experienced by the individual affected by the disease, while a sign is a phenomenon that can be detected by someone other than the individual affected by the disease. Analysing Comorbidities Link Check these websites Concept: Charlson Comorbidity Index NCI Comorbidity Index Overview The Charlson Comorbidity Index was first developed by Mary Charlson and colleagues (1987) as a weighted index to predict risk of death within 1 year of hospitalization for patients with specific comorbid conditions. Nineteen conditions were included in the index. Each condition was assigned a weight from 1 to 6 , based on the estimated 1-year mortality hazard ratio from a Cox proportional hazards model. These weights were summed to produce the Charlson comorbidity score . Some years later, Richard Deyo (1992) and Patrick Romano (1993) separately adapted the Charlson index to ICD-9-CM diagnosis and procedure codes and CPT-4 codes so that the index could be calculated using administrative data. In 2000 Carrie Klabunde and colleagues developed a cancer-specific NCI Comorbidity Index using breast and prostate cancer cases included in the SEER-Medicare data. Klabunde et al. used codes on the Medicare data to identify the conditions identified by Charlson et al. The NCI Comorbidity Index excludes solid tumors, leukemias, and lymphomas as comorbid conditions, given that the NCI Comorbidity Index was developed from a cohort of cancer patients. The remaining 16 Charlson index conditions were included in the NCI Comorbidity Index, with further consolidation to 14 conditions . The approach for creating the NCI Comorbidity Index included review of Medicare claims for the year prior to the date of diagnosis, excluding the month of diagnosis. Separate comorbidity indexes were created for hospitalizations (Medicare Part A) and physician claims (Medicare Part B). Conditions ascertained by physician claims were required to occur more than once in a period greater than 30 days to reduce the risk of false positives. These indexes were later combined into a single NCI Comorbidity Index , which allowed for greater efficiency in analysis.","title":"General"},{"location":"medical/general/#symptons-vs-signs","text":"A symptom is any subjective evidence of disease, while a sign is any objective evidence of disease. Therefore, a symptom is a phenomenon that is experienced by the individual affected by the disease, while a sign is a phenomenon that can be detected by someone other than the individual affected by the disease.","title":"Symptons vs Signs"},{"location":"medical/general/#analysing-comorbidities","text":"Check these websites Concept: Charlson Comorbidity Index NCI Comorbidity Index Overview The Charlson Comorbidity Index was first developed by Mary Charlson and colleagues (1987) as a weighted index to predict risk of death within 1 year of hospitalization for patients with specific comorbid conditions. Nineteen conditions were included in the index. Each condition was assigned a weight from 1 to 6 , based on the estimated 1-year mortality hazard ratio from a Cox proportional hazards model. These weights were summed to produce the Charlson comorbidity score . Some years later, Richard Deyo (1992) and Patrick Romano (1993) separately adapted the Charlson index to ICD-9-CM diagnosis and procedure codes and CPT-4 codes so that the index could be calculated using administrative data. In 2000 Carrie Klabunde and colleagues developed a cancer-specific NCI Comorbidity Index using breast and prostate cancer cases included in the SEER-Medicare data. Klabunde et al. used codes on the Medicare data to identify the conditions identified by Charlson et al. The NCI Comorbidity Index excludes solid tumors, leukemias, and lymphomas as comorbid conditions, given that the NCI Comorbidity Index was developed from a cohort of cancer patients. The remaining 16 Charlson index conditions were included in the NCI Comorbidity Index, with further consolidation to 14 conditions . The approach for creating the NCI Comorbidity Index included review of Medicare claims for the year prior to the date of diagnosis, excluding the month of diagnosis. Separate comorbidity indexes were created for hospitalizations (Medicare Part A) and physician claims (Medicare Part B). Conditions ascertained by physician claims were required to occur more than once in a period greater than 30 days to reduce the risk of false positives. These indexes were later combined into a single NCI Comorbidity Index , which allowed for greater efficiency in analysis.","title":"Analysing Comorbidities"},{"location":"medical/irecist/","text":"Check these websites iRECIST at the RECIST Working Group Official Website Merck Imaging Tip Sheet for RECIST 1.1 and iRECIST iRECIST vs RECIST 1.1 Link Unchanged Link Definitions of measurable, non-measurable disease Definitions of target (T) and non target (NT) lesions Measurement and management of nodal disease Calculation of the sum of measurement (SOM) Definitions of complete (CR) and partial response (PR), stable disease (SD) and their duration Confirmation of CR and PR and when applicable Definition of progression in T and NT (iRECIST terms i-unconfirmed progression (iUPD)) Changed Link Management of new lesions Time point response after RECIST 1.1 progression Confirmation of progression required Collection of reason why progression cannot be confirmed Inclusion and recording of clinical status New Lesions Link New lesions (NL) are assessed using RECIST 1.1 principles: Classified as measurable or non-measurable Up to 5 (2 per site) measured (but not included in the sum of measurements of target lesions identified at baseline) and recorded as new lesions target (NL-T) with an i-sum of measurements (iSOM) Other new lesions (measurable/non-measurable) are recorded as new lesions non-target (NL-NT) New lesions do not have to resolve for subsequent iSD or iPR providing that the next assessment did not confirm progression Time Point Response Link In iRECIST there can be iSD , iPR or iCR after RECIST 1.1 PD. Once a PD always a PD is no longer the case First RECIST 1.1 PD is \"unconfirmed\" for iRECIST \u2013 termed iUPD iUPD must be confirmed at the next assessment (4-8 weeks) If confirmed, termed iCPD Time point response is dynamic and based on: * Change from baseline (for iCR, iPR, iSD) or change from nadir (for PD) * The last i-response Progression Link Treatment past RECIST 1.1 PD should only be considered if patient clinically stable (recommendation, may be protocol specific): No worsening of performance status No clinically relevant worsening in disease related symptoms No requirement for intensified management of disease related symptoms (analgesics, radiation, palliative care) Record the reason iUPD not confirmed: Not stable Treatment stopped but patient not reassessed/imaging not performed iCPD never occurs Patient has died iRECIST: Confirming Progression (iCPD) Link Must be the NEXT assessment \u2013 if iSD, iPR or iCR intervenes then bar is reset and iUPD must occur again and be confirmed Two ways to confirm Existing iUPD gets worse \u2013 \"low bar\" Lesion category without prior iUPD now meet RECIST 1.1 criteria for PD \u2013 \"RECIST PD\" If confirmatory scans not done must document reason why Statistical and Data Considerations Link Primary and Exploratory Response Criteria Link RECIST 1.1 should remain primary criteria iRECIST exploratory Date of i-Progression Link Will be the same as RECIST 1.1 date (i.e. first iUPD date) UNLESS iSD, iPR or iCR intervenes Will be the UPD date which has been subsequently confirmed The date used is the first UPD date If iUPD never confirmed If a subsequent iSD, iPR or iCR is seen with no later iUPD or iCPD then the initial iUPD is ignored Otherwise the iUPD date is used Patient not considered to be clinically stable, stops protocol treatment and no further response assessments are done The next TPRs are all iUPD, and iCPD never occurs. The patient dies of cancer Data Collection Link Investigator/site assessment is the primary method of evaluation for RECIST and iRECIST in keeping with RWG principles. Record time-point and best overall response for both. RECIST 1.1 iRECIST Record reasons Treatment discontinued when iUPD iCPD not confirmed Independent imaging review can occur in parallel if indicated. We recommend CT images be collected if feasible. Summary: RECIST 1.1 vs. iRECIST Link RECIST 1.1 iRECIST Definitions of measurable and non-measurable disease; numbers and site of target disease Measurable lesions are >=10mm in long diameter (15mm for nodal lesions); maximum of 5 lesions (2 per organ); all other disease considered not-target (must be 10mm of longer in short axis for nodal disease) No change; however, NEW lesions assessed per RECIST 1.1 Recorded separately on the CRF NOT included in the SOM for target lesions identified at baseline CR, PR or SD Cannot have met criteria for PD prior to CR, PR or SD May have had iUPD (1 or more instances), but not iCPD, prior to iCR, iPR or iSD Confirmation of CR, PR Only required for non-randomized trials As per RECIST 1.1 Confirmation of SD Not required As per RECIST 1.1 New lesions Results in PD. Recorded but not measured Results in iUPD but iCPD is only assigned based on this category if at next assessment Additional NL appear or Increase in size of NLs (\u22655mm for SOM of NLT or any increase in NLNT) Remember NLs can also confirm iCPD if iUPD was only in T or NT disease Independent blinded review and central collection of scans Recommended in some circumstances Collection of scans (but not independent review) recommended for all trials Confirmation of PD Not required (unless equivocal) Always required Consideration of clinical status Not included in assessment Clinical stability is always considered and collected on case record form iRECIST in a Nutshell Link RECIST 1.1 \u2013 primary criteria iRECIST exploratory and applicable only after RECIST1.1 progression occurs Most patients will not have \u2018pseudoprogression\u2019 Principles of iRECIST follow RECIST 1.1 very closely RECIST 1.1 principles are generally the default except: Management of new lesions What constitutes confirmation of progression Assess RECIST 1.1 and iRECIST separately but in parallel at each time point Progression must be confirmed Consider treatment past progression only in carefully defined scenarios Confirmation requires some worsening of disease bulk Must be next evaluable assessment after iUPD Lesion category with existing iUPD just needs to get a little bit worse OR Lesion category without prior iUPD has to meet RECIST 1.1 criteria for progression New lesions Managed using RECIST 1.1 principles NOT added to SOM (but included in separate iSOM) Unconfirmed progression does not preclude a later i-response Response after iUPD is driven by TARGET disease (as long as iCPD not confirmed) This means that can have subsequent iSD or iPR in target lesions (compared to baseline) EVEN IF The new lesion seen at the time of iUPD is still there The unequivocal increase in non-target lesions at the time of iUPD hasn\u2019t improved \"Bar reset\" does mean that: a previously observed iUPD can be ignored if there is an intervening response (i.e. if criteria for iPR, iCR, or iSD are met ) \"Bar reset\" does not mean that: the baseline or the nadir are re-set iCR/iPR/iSD still calculated from BASELINE i progression date still calculated from NADIR (which may or may not be the same as baseline \u2013 and could be before or after any iUPD)","title":"iRECIST"},{"location":"medical/irecist/#irecist-vs-recist-11","text":"","title":"iRECIST vs RECIST 1.1"},{"location":"medical/irecist/#unchanged","text":"Definitions of measurable, non-measurable disease Definitions of target (T) and non target (NT) lesions Measurement and management of nodal disease Calculation of the sum of measurement (SOM) Definitions of complete (CR) and partial response (PR), stable disease (SD) and their duration Confirmation of CR and PR and when applicable Definition of progression in T and NT (iRECIST terms i-unconfirmed progression (iUPD))","title":"Unchanged"},{"location":"medical/irecist/#changed","text":"Management of new lesions Time point response after RECIST 1.1 progression Confirmation of progression required Collection of reason why progression cannot be confirmed Inclusion and recording of clinical status","title":"Changed"},{"location":"medical/irecist/#new-lesions","text":"New lesions (NL) are assessed using RECIST 1.1 principles: Classified as measurable or non-measurable Up to 5 (2 per site) measured (but not included in the sum of measurements of target lesions identified at baseline) and recorded as new lesions target (NL-T) with an i-sum of measurements (iSOM) Other new lesions (measurable/non-measurable) are recorded as new lesions non-target (NL-NT) New lesions do not have to resolve for subsequent iSD or iPR providing that the next assessment did not confirm progression","title":"New Lesions"},{"location":"medical/irecist/#time-point-response","text":"In iRECIST there can be iSD , iPR or iCR after RECIST 1.1 PD. Once a PD always a PD is no longer the case First RECIST 1.1 PD is \"unconfirmed\" for iRECIST \u2013 termed iUPD iUPD must be confirmed at the next assessment (4-8 weeks) If confirmed, termed iCPD Time point response is dynamic and based on: * Change from baseline (for iCR, iPR, iSD) or change from nadir (for PD) * The last i-response","title":"Time Point Response"},{"location":"medical/irecist/#progression","text":"Treatment past RECIST 1.1 PD should only be considered if patient clinically stable (recommendation, may be protocol specific): No worsening of performance status No clinically relevant worsening in disease related symptoms No requirement for intensified management of disease related symptoms (analgesics, radiation, palliative care) Record the reason iUPD not confirmed: Not stable Treatment stopped but patient not reassessed/imaging not performed iCPD never occurs Patient has died","title":"Progression"},{"location":"medical/irecist/#irecist-confirming-progression-icpd","text":"Must be the NEXT assessment \u2013 if iSD, iPR or iCR intervenes then bar is reset and iUPD must occur again and be confirmed Two ways to confirm Existing iUPD gets worse \u2013 \"low bar\" Lesion category without prior iUPD now meet RECIST 1.1 criteria for PD \u2013 \"RECIST PD\" If confirmatory scans not done must document reason why","title":"iRECIST: Confirming Progression (iCPD)"},{"location":"medical/irecist/#statistical-and-data-considerations","text":"","title":"Statistical and Data Considerations"},{"location":"medical/irecist/#primary-and-exploratory-response-criteria","text":"RECIST 1.1 should remain primary criteria iRECIST exploratory","title":"Primary and Exploratory Response Criteria"},{"location":"medical/irecist/#date-of-i-progression","text":"Will be the same as RECIST 1.1 date (i.e. first iUPD date) UNLESS iSD, iPR or iCR intervenes Will be the UPD date which has been subsequently confirmed The date used is the first UPD date If iUPD never confirmed If a subsequent iSD, iPR or iCR is seen with no later iUPD or iCPD then the initial iUPD is ignored Otherwise the iUPD date is used Patient not considered to be clinically stable, stops protocol treatment and no further response assessments are done The next TPRs are all iUPD, and iCPD never occurs. The patient dies of cancer","title":"Date of i-Progression"},{"location":"medical/irecist/#data-collection","text":"Investigator/site assessment is the primary method of evaluation for RECIST and iRECIST in keeping with RWG principles. Record time-point and best overall response for both. RECIST 1.1 iRECIST Record reasons Treatment discontinued when iUPD iCPD not confirmed Independent imaging review can occur in parallel if indicated. We recommend CT images be collected if feasible.","title":"Data Collection"},{"location":"medical/irecist/#summary-recist-11-vs-irecist","text":"RECIST 1.1 iRECIST Definitions of measurable and non-measurable disease; numbers and site of target disease Measurable lesions are >=10mm in long diameter (15mm for nodal lesions); maximum of 5 lesions (2 per organ); all other disease considered not-target (must be 10mm of longer in short axis for nodal disease) No change; however, NEW lesions assessed per RECIST 1.1 Recorded separately on the CRF NOT included in the SOM for target lesions identified at baseline CR, PR or SD Cannot have met criteria for PD prior to CR, PR or SD May have had iUPD (1 or more instances), but not iCPD, prior to iCR, iPR or iSD Confirmation of CR, PR Only required for non-randomized trials As per RECIST 1.1 Confirmation of SD Not required As per RECIST 1.1 New lesions Results in PD. Recorded but not measured Results in iUPD but iCPD is only assigned based on this category if at next assessment Additional NL appear or Increase in size of NLs (\u22655mm for SOM of NLT or any increase in NLNT) Remember NLs can also confirm iCPD if iUPD was only in T or NT disease Independent blinded review and central collection of scans Recommended in some circumstances Collection of scans (but not independent review) recommended for all trials Confirmation of PD Not required (unless equivocal) Always required Consideration of clinical status Not included in assessment Clinical stability is always considered and collected on case record form","title":"Summary: RECIST 1.1 vs. iRECIST"},{"location":"medical/irecist/#irecist-in-a-nutshell","text":"RECIST 1.1 \u2013 primary criteria iRECIST exploratory and applicable only after RECIST1.1 progression occurs Most patients will not have \u2018pseudoprogression\u2019 Principles of iRECIST follow RECIST 1.1 very closely RECIST 1.1 principles are generally the default except: Management of new lesions What constitutes confirmation of progression Assess RECIST 1.1 and iRECIST separately but in parallel at each time point Progression must be confirmed Consider treatment past progression only in carefully defined scenarios Confirmation requires some worsening of disease bulk Must be next evaluable assessment after iUPD Lesion category with existing iUPD just needs to get a little bit worse OR Lesion category without prior iUPD has to meet RECIST 1.1 criteria for progression New lesions Managed using RECIST 1.1 principles NOT added to SOM (but included in separate iSOM) Unconfirmed progression does not preclude a later i-response Response after iUPD is driven by TARGET disease (as long as iCPD not confirmed) This means that can have subsequent iSD or iPR in target lesions (compared to baseline) EVEN IF The new lesion seen at the time of iUPD is still there The unequivocal increase in non-target lesions at the time of iUPD hasn\u2019t improved \"Bar reset\" does mean that: a previously observed iUPD can be ignored if there is an intervening response (i.e. if criteria for iPR, iCR, or iSD are met ) \"Bar reset\" does not mean that: the baseline or the nadir are re-set iCR/iPR/iSD still calculated from BASELINE i progression date still calculated from NADIR (which may or may not be the same as baseline \u2013 and could be before or after any iUPD)","title":"iRECIST in a Nutshell"},{"location":"medical/labs/","text":"Check these websites Lab Tests Online: Your Trusted Guide Hematology Link Hematology is the study of blood and blood disorders to help in the diagnosis, treatment, and prevention of diseases of the blood and bone marrow as well as of the immunologic, hemostatic (blood clotting) and vascular systems. Because of the nature of blood, the science of hematology profoundly affects the understanding of many diseases. All our blood cells develop from stem cells in the bone marrow . Stem cells are blood cells at the earliest stage of development and they stay inside the bone marrow until they are fully developed which is when they go into the bloodstream. Blood cells do not live long. The bone marrow normally makes millions of new blood cells every day to replace blood cells as they are needed. The three main types of blood cells are: Red blood cells (RBC) or erythrocytes are the most common type of blood cells. They lack a cell nucleus and most organelles, in order to accommodate maximum space for hemoglobin; they can be viewed as sacks of hemoglobin, with a plasma membrane as the sack. Hemoglobin (HGB) is an iron-containing biomolecule that can bind oxygen and is responsible for the red color of the cells and the blood. The hematocrit (HCT) , is the volume percentage of red blood cells in blood. Platelets (PLAT) or thrombocytes help the blood to clot and prevent bleeding and bruising. White blood cells (WBC) or leucocytes fight and prevent infection. Basophils (BASO) Eosinophils (EOS) Neutrophils (NEUT) Monocytes (MONO) Lymphocytes (LYM) Normal Ranges' Rule of Thumb Link Type of blood cell Unit Levels Red blood cells (RBC) g/l Men: 130-180, Women: 115-165 Hematocrit (HCT) % Men: 41-50, Women: 36-44 Hemoglobin (HGB) g/l Men: 13.5-16.5, Women: 12.0-15.0 Platelets 10^9/L 150-400 White blood cells 10^9/L 4.0-11.0 Neutrophils 10^9/L 2.0-7.5 Lymphocytes 10^9/L 1.5-4.5 Check these websites Laboratory values Blood-Related Anomalies Link Neutrophil-to-Lymphocyte Ratio (NLR) , defined as absolute neutrophils count divided by absolute lymphocytes count, has been reported as poor prognostic factor in several neoplastic diseases. In the case of colorectal cancer, elevated pre-operative ($NLR>5$) is associated with poorer long-term survival. NLR is a useful biomarker in delineating those patients with poorer prognosis and whom may benefit from adjuvant therapies. Leucocytosis , often defined as an elevated white blood cell (WBC) count greater than $11000/mm^3$ ($11.0 \\cdot 10^9/L$) in nonpregnant adults, is a relatively common finding with a wide differential. It is important for clinicians to be able to distinguish malignant from nonmalignant etiologies, and to differentiate between the most common nonmalignant causes of leukocytosis. Leukocytosis in the range of approximately $50-100 \\cdot 10^9/L$ is sometimes referred to as a leukemoid reaction . This level of elevation can occur in some severe infections, such as Clostridium difficile infection, sepsis, organ rejection, or in patients with solid tumors. Leukocytosis greater than $100 \\cdot 10^9/L$ is almost always caused by leukemias or myeloproliferative disorders. An abnormally low hematocrit (as well as a decrease in the total amount of red blood cells or hemoglobin in the blood) may suggest anemia , a decrease in the total amount of red blood cells, while an abnormally high hematocrit is called polycythemia . Both are potentially life-threatening disorders. Thrombocythemia or thrombocytosis is the presence of high platelet counts in the blood. Although often symptomless, it can predispose to thrombosis in some patients. Thrombocytosis can be contrasted with thrombocytopenia , a loss of platelets in the blood. Chemistry Link Basic Metabolic Panel (BMP) Link It usually contains 8 tests, all of which are found in the CMP (below); provides information about the current status of a person's kidneys and respiratory system as well as electrolyte and acid/base balance and level of blood glucose. Glucose : energy source for the body; a steady supply must be available for use, and a relatively constant level of glucose must be maintained in the blood. High levels of glucose can indicate the presence of diabetes or another endocrine disorder. Calcium ($Ca$) : one of the most important minerals in the body; it is essential for the proper functioning of muscles, nerves, and the heart and is required in blood clotting and in the formation of bones. Electrolytes Link Sodium ($Na^+$) concentration in blood serum : Sodium is vital to normal body processes, including nerve and muscle function . The kidneys work to excrete any excess sodium that is ingested in food and beverages. Sodium levels fluctuate with dehydration or over-hydration, the food and beverages consumed, diarrhea, endocrine disorders, water retention (various causes), trauma and bleeding. Hyponatraemia : $Na_{serum}< 135 \\: mmol/l$ Hypernatraemia : $Na_{serum}> 150 \\: mmol/l$ Causes of sodium deficiency : renal/skin/intestinal losses Causes of sodium excess : decreased sodium excretion or excessive intake Potassium ($K^+$) concentration in blood serum : plays an important role in muscle contractions and cell function . Both high and low levels of potassium can cause problems with the rhythm of the heart so it is important to monitor the level of potassium after surgery. Patients who are taking diuretics regularly may require regular blood tests to monitor potassium levels, as some diuretics cause the kidneys to excrete too much potassium. Hypokalaemia : $K_{serum} < 3.5 \\: mmol/l$ Hyperkalaemia : $K_{serum} < 5.2 \\: mmol/l$ Carbon Dioxide ($CO_2$) : Most carbon dioxide is present in the form of bicarbonate , which is regulated by the lungs and kidneys and helps to maintain the body's acid-base balance. The test result is an indication of how well the kidneys, and sometimes the lungs, are managing the bicarbonate level in the blood. Chloride ($Cl^-$) concentration in blood serum : Chloride binds with electrolytes including potassium and sodium in the blood and plays a role in maintaining the proper pH of the blood. Chloride levels can vary widely if the patient is dehydrated or overly hydrated, if the kidneys are not functioning properly. Heart failure and endocrine problems can also contribute to abnormal chloride results. Kidney Tests Link Blood Urea Nitrogen (BUN) : waste product filtered out of the blood by the kidneys; conditions that affect the kidney have the potential to affect the amount of urea in the blood. A high level may indicate that the kidneys are functioning less than normal. Creatinine : waste product generated by the body during the process of normal muscle breakdown; it is filtered out of the blood by the kidneys so blood levels are a good indication of how well the kidneys are working. High levels may indicate kidney impairment, low blood pressure, high blood pressure or another condition. Creatinine Clearance : The amount of blood filtered per minute by the kidneys is known as the glomerular filtration rate (GFR) . If the kidneys are damaged or diseased, or if blood circulation is slowed, then less creatinine will be removed from the blood and released into the urine and the GFR will be decreased. GFR is difficult to measure directly. A way to estimate GFR is to calculate creatinine clearance. There are several versions of the creatinine clearance calculation. All of them include the measurement of the amount of creatinine in a blood sample collected just before or after the urine collection, the amount of creatinine in a 24-hour urine sample, and the 24-hour urine volume. Since the amount of creatinine produced depends on muscle mass, some calculations also use a correction factor that takes into account a person's body surface area. Comprehensive Metabolic Panel (CMP) Link It usually includes 14 tests; provides the same information as the BMP with the addition of the status of a person's liver and important blood proteins. Proteins Link Albumin : a small protein produced in the liver; the major protein in serum. Total protein (TP) : the concentration of all fractions of plasma proteins (including albumin and several types of globulin). Hypoproteinaemia : total blood protein decrease below 65 g/l (life threatening when total protein concentration $< 40 g/l$ and albumin $< 20 g/l$ Hyperproteinaemia : is an abnormal increase in serum and plasma proteins. Liver Tests Link Aminotransferases : they catalyze the process of transamination and are present in every organ and tissue. \u0410L\u0422/SGPT (ALanine amino Transferase/Serum Glutamic Pyruvic Transaminase) : Highest concentration of ALT is noted in the liver cells. Increased ALT activity is most frequently revealed in acute liver and biliary ducts diseases. \u0410S\u0422/SGOT (ASpartate amino Transferase/Serum Glutamic Oxaloacetic Transaminase) : High concentration of AST is noted in heart and skeletal muscles, liver, kidneys, pancreas and erythrocytes. Damage of any of them leads to significant increase of AST in the blood serum. Increase in AST activity reveals hepatic pathology.Low AST activity usually reveals vitamin \u04126 deficiency, renal failure, pregnancy. Ritis ratio : AST/ALT ratio. Its normal value is around 1\u20131.3. It decreases in liver diseases and increases in heart diseases. Alkaline phosphatase (ALP) : is produced by various tissues: intestinal mucous membrane, osteoblasts, biliary ducts, placenta and mammary gland during lactation. Bone (ALP increases with bone formation), liver (main indicator of biliary tract pathology), intestinal (small part of total ALP activity, may be increased by intestinal diseases accompanied by diarrhoea) and placental (appears in pregnancy during the 3rd trimester, increase in women with placenta damage) ones are the most significant for clinical and diagnostic purposes. Total Bilirubin : is an orange-yellow pigment, a waste product primarily produced by the normal breakdown of heme (component of hemoglobin found in RBCs) when aged red blood cells (RBCs) are recycled. Bilirubin is ultimately processed by the liver to allow its elimination from the body. This test measures the amount of bilirubin in the blood to evaluate a person's liver function or to help diagnose anemias caused by RBC destruction (hemolytic anemia). Electrolyte Panel Link It is helpful for detecting a problem with the body's fluid and electrolyte balance. Electrolytes are minerals that are found in body tissues and blood in the form of dissolved salts. As electrically charged particles, electrolytes help move nutrients into and wastes out of the body's cells, maintain a healthy water balance, and help stabilize the body's acid/base (pH) level. In the first instance pH is maintained by physiological buffers. Buffers may be intracellular and extracellular. Different buffer systems work in correlation with one another. It means that changes in one buffer system lead to changes in another. The main buffer systems are the following: Bicarbonate buffer (53% buffering capacity): the most important extracellular buffer, produced by kidneys. Haemoglobin buffer (35% buffering capacity): main intracellular buffer of the blood. Protein buffer (7% buffering capacity): is an extracellular buffer represented by plasma proteins. Phosphate buffer (5% buffering capacity): takes part in hydrogen ions excretion in renal tubules, is not of great importance in blood. If blood pH increases , $H^+$ ions move from cells to extracellular fluid in exchange of $K$ ions that enter the cells. That's why alkalosis is usually accompanied by hypokalaemia . If blood pH decreases , $H^+$ ions enter the cells in exchange of $K$ ions that leaves the cells. That's why acidosis may cause hyperkalaemia . The electrolyte panel measures the blood levels of the main electrolytes in the body: sodium (Na+), potassium (K+), chloride (Cl-), and bicarbonate (HCO3-; sometimes reported as total CO2), which are included in the BMP . Lipid Panel Link Also known as coronary risk panel , measures the level of specific lipids in blood to help assess someone's risk of cardiovascular disease (CVD). Total Cholesterol : The cholesterol forms the membranes for cells in all organs and tissues in the body. It is used to make hormones that are essential for development, growth, and reproduction. It forms bile acids that are needed to absorb nutrients from food. The test for cholesterol measures total cholesterol that is carried in the blood by lipoproteins. Triglycerides : They are a form of fat and a major source of energy for the body. Most triglycerides are found in fat (adipose) tissue, but some triglycerides circulate in the blood to provide fuel for muscles to work. Most triglycerides are carried in the blood by lipoproteins called very low-density lipoproteins (VLDL) . High levels of triglycerides in the blood are associated with an increased risk of developing cardiovascular disease (CVD). Certain factors can contribute to high triglyceride levels and to risk of CVD, including lack of exercise, being overweight, smoking cigarettes, consuming excess alcohol, and having medical conditions such as diabetes and kidney disease. A small amount of cholesterol circulates in the blood in complex particles called lipoproteins . Each particle contains a combination of protein, cholesterol, triglyceride, and phospholipid molecules and the particles are classified by their density into: High-density lipoprotein cholesterol (HDL-C) : often called good cholesterol because it removes excess cholesterol and carries it to the liver for removal. Low-density lipoprotein cholesterol (LDL-C) : often called bad cholesterol because it deposits excess cholesterol in walls of blood vessels, which can contribute to atherosclerosis. Liver Panel Link Also called Hepatic Function Panel , it is used to screen for, detect, evaluate, and monitor actue and chronic liver inflammation (hepatitis), liver disease and/or damage. Along with the Liver tests from the CMP , others are contained in this panel: The CMP includes most of the liver panel tests (ALT, ALP, AST, bilirubin, albumin, total protein). Depending on the healthcare provider and the laboratory, other tests that may be included in a liver panel are: Gamma-Glutamyl Transpeptidase (GGT) : an increased activity has a great significance in diagnosis of any pathology of liver and bile ducts . The liver is considered as the main source of normal serum activity, despite the fact that the kidney has the highest level of the enzyme. Pancreas also contains GGT (100% of patients with acute pancreatitis show GGT activity 10-20 times higher than normal). If GGT activity is normal, liver disease probability is very low. Thus, GGT is a good marker for differential diagnosis of liver pathology. The most significant increase is observed in cholestasis . Determination of GGT in urine allows to diagnose the early stages of kidney disease, which is accompanied by proximal renal tubular damage. Lactate Dehydrogenase (LDH) : is an enzyme involved in energy production that is found in almost all of the body's cells, with the highest levels found in the cells of the heart, liver, muscles, kidneys, lungs, and in blood cells; bacteria also produce LDH. Only a small amount of LDH is usually detectable in serum or plasma. LDH is released from the cells into the serum when cells are damaged or destroyed. Thus, an LDH blood level is a non-specific marker for the presence of tissue damage somewhere in the body . By itself, it cannot be used to identify the underlying cause or location of the cellular damage. However, it may be used, in conjunction with other blood tests, to help evaluate for and/or monitor conditions that lead to tissue damage, such as liver or blood diseases or cancer. Prothrombin Time (PT) : the liver produces proteins involved in the clotting (coagulation) of blood; the PT measures clotting function and, if abnormal, may indicate liver damage. A PT measures the number of seconds it takes for a clot to form in a person's sample of blood after substances are added. International Normalized Ratio (INR) : is a calculation based on results of a PT that is used to monitor individuals who are being treated with the blood-thinning medication (anticoagulant) warfarin. The INR is a calculation that adjusts for changes in the PT reagents and allows for results from different laboratories to be compared. Renal Panel Link Also called Kidney Function Panel , it contains tests such as albumin, creatinine, BUN, eGFR to evaluate kidney function. The individual tests included in a renal panel can vary by laboratory, but the tests typically performed include: electrolytes , minerals (phosporus and calcium), protein (albumin), waste products (BUN, creatinine) and sugar (glucose). Phosphorous ($P$) : is a mineral that combines with other substances to form organic and inorganic phosphate compounds. Phosphates are vital for energy production, muscle and nerve function, and bone growth. They also play an important role as a buffer, helping to maintain the body's acid-base balance. Most of the body's phosphates combine with calcium to help form bones and teeth. Smaller amounts are found in muscle and nerve tissue. The rest is found within cells throughout the body, where they are mainly used to store energy. Hypophosphatemia may be seen with malnutrition, malabsorption, acid-base imbalances, increased blood calcium, and with disorders that affect kidney function. Hypophosphatemia may be seen with increased intake of the mineral, low blood calcium, and with kidney dysfunction. Thyroid Function Panel Link This panel helps in evaluating thyroid gland function and diagnosing thyroid disorders. Other tests (???) Link Magnesium ($Mg$) : is a mineral that is vital for energy production, muscle contraction, nerve function, and the maintenance of strong bones. This test is used to evaluate the level of magnesium in your blood and to help determine the cause of abnormal levels of magnesium, calcium and/or potassium. Hypomagnesemia : may be seen with malnutrition, conditions that cause malabsorption, and with excess loss of magnesium by the kidneys. Hypermagnesemia : may be seen with the ingestion of antacids that contain magnesium and with decreased ability of the kidneys to excrete magnesium. HbA1c : Hemoglobin A1c, also called A1c or glycated hemoglobin, is hemoglobin with glucose attached. The A1c test evaluates the average amount of glucose in the blood over the last 2 to 3 months by measuring the percentage of glycated (glycosylated) hemoglobin. There are several types of normal hemoglobin, but the predominant form \u2013 about 95-98% \u2013 is hemoglobin A. As glucose circulates in the blood, some of it spontaneously binds to hemoglobin A. Once the glucose binds to the hemoglobin, it remains there for the life of the red blood cell \u2013 normally about 120 days. The predominant form of glycated hemoglobin is referred to as A1c. This test may be used to screen for and diagnose diabetes or risk of developing diabetes. Uric acid : is produced by the breakdown of purines. Purines are nitrogen-containing compounds found in the cells of the body, including our DNA. As cells get old and die, they break down, releasing purines into the blood. If too much uric acid is produced or not enough is removed, it can accumulate in the body, causing increased levels in the blood (hyperuricemia). The presence of excess uric acid can cause gout, a condition characterized by inflammation of the joints due to the formation of uric acid crystals in the joint (synovial) fluid. Excess uric acid can also be deposited in tissues such as the kidney, leading to kidney stones or kidney failure. Normal Ranges' Rule of Thumb Link https://www.verywellhealth.com/blood-chemistry-tests-and-results-3156998 Type of test Unit Levels Glucose $mmol/l$ 3.9-5.6 Calcium Serum Sodium $mmol/l$ 3.5-5 Serum Potassium $mmol/l$ 3.5-5 Carbon Dioxide (bicarbonate) $mmol/l$ 24-30 Serum Chloride $mmol/l$ 100-106 Blood Urea Nitrogen (BUN) $mmol/l$ 2.9-8.9 Creatinine $\\mu mol/l$ Men: 15-40; Women: 25-70 Creatinine Clearance Albumin Total Protein (TP) ALT/SGPT AST/SGOT ALP Total Bilirubin GGT LDH Prothrombin Time INR Phosphorous Total Cholesterol $mmol/l$ <5.18 (high >6.22) Triglycerides $mmol/l$ <1.7 (high >2.3) HDL-C $mmol/l$ Men: 1.0-1.3; Women: 1.3-1.5 LDL-C $mmol/l$ <2.59 (high >160) Magnesium HbA1c Uric Acid Urinalysis Link Coagulation Link Biomarkers Link CEA (carcinoembryonic antigen) is a protein found in many types of cells but associated with tumors and the developing fetus. A common cutoff is $5 \\mu g/L$ (values bigger than this could be a sign of disease). Mutations in KRAS exon 2 , BRAF and PIK3CA are commonly present in colorectal cancer (CRC).","title":"Laboratory Test"},{"location":"medical/labs/#hematology","text":"Hematology is the study of blood and blood disorders to help in the diagnosis, treatment, and prevention of diseases of the blood and bone marrow as well as of the immunologic, hemostatic (blood clotting) and vascular systems. Because of the nature of blood, the science of hematology profoundly affects the understanding of many diseases. All our blood cells develop from stem cells in the bone marrow . Stem cells are blood cells at the earliest stage of development and they stay inside the bone marrow until they are fully developed which is when they go into the bloodstream. Blood cells do not live long. The bone marrow normally makes millions of new blood cells every day to replace blood cells as they are needed. The three main types of blood cells are: Red blood cells (RBC) or erythrocytes are the most common type of blood cells. They lack a cell nucleus and most organelles, in order to accommodate maximum space for hemoglobin; they can be viewed as sacks of hemoglobin, with a plasma membrane as the sack. Hemoglobin (HGB) is an iron-containing biomolecule that can bind oxygen and is responsible for the red color of the cells and the blood. The hematocrit (HCT) , is the volume percentage of red blood cells in blood. Platelets (PLAT) or thrombocytes help the blood to clot and prevent bleeding and bruising. White blood cells (WBC) or leucocytes fight and prevent infection. Basophils (BASO) Eosinophils (EOS) Neutrophils (NEUT) Monocytes (MONO) Lymphocytes (LYM)","title":"Hematology"},{"location":"medical/labs/#normal-ranges-rule-of-thumb","text":"Type of blood cell Unit Levels Red blood cells (RBC) g/l Men: 130-180, Women: 115-165 Hematocrit (HCT) % Men: 41-50, Women: 36-44 Hemoglobin (HGB) g/l Men: 13.5-16.5, Women: 12.0-15.0 Platelets 10^9/L 150-400 White blood cells 10^9/L 4.0-11.0 Neutrophils 10^9/L 2.0-7.5 Lymphocytes 10^9/L 1.5-4.5 Check these websites Laboratory values","title":"Normal Ranges' Rule of Thumb"},{"location":"medical/labs/#blood-related-anomalies","text":"Neutrophil-to-Lymphocyte Ratio (NLR) , defined as absolute neutrophils count divided by absolute lymphocytes count, has been reported as poor prognostic factor in several neoplastic diseases. In the case of colorectal cancer, elevated pre-operative ($NLR>5$) is associated with poorer long-term survival. NLR is a useful biomarker in delineating those patients with poorer prognosis and whom may benefit from adjuvant therapies. Leucocytosis , often defined as an elevated white blood cell (WBC) count greater than $11000/mm^3$ ($11.0 \\cdot 10^9/L$) in nonpregnant adults, is a relatively common finding with a wide differential. It is important for clinicians to be able to distinguish malignant from nonmalignant etiologies, and to differentiate between the most common nonmalignant causes of leukocytosis. Leukocytosis in the range of approximately $50-100 \\cdot 10^9/L$ is sometimes referred to as a leukemoid reaction . This level of elevation can occur in some severe infections, such as Clostridium difficile infection, sepsis, organ rejection, or in patients with solid tumors. Leukocytosis greater than $100 \\cdot 10^9/L$ is almost always caused by leukemias or myeloproliferative disorders. An abnormally low hematocrit (as well as a decrease in the total amount of red blood cells or hemoglobin in the blood) may suggest anemia , a decrease in the total amount of red blood cells, while an abnormally high hematocrit is called polycythemia . Both are potentially life-threatening disorders. Thrombocythemia or thrombocytosis is the presence of high platelet counts in the blood. Although often symptomless, it can predispose to thrombosis in some patients. Thrombocytosis can be contrasted with thrombocytopenia , a loss of platelets in the blood.","title":"Blood-Related Anomalies"},{"location":"medical/labs/#chemistry","text":"","title":"Chemistry"},{"location":"medical/labs/#basic-metabolic-panel-bmp","text":"It usually contains 8 tests, all of which are found in the CMP (below); provides information about the current status of a person's kidneys and respiratory system as well as electrolyte and acid/base balance and level of blood glucose. Glucose : energy source for the body; a steady supply must be available for use, and a relatively constant level of glucose must be maintained in the blood. High levels of glucose can indicate the presence of diabetes or another endocrine disorder. Calcium ($Ca$) : one of the most important minerals in the body; it is essential for the proper functioning of muscles, nerves, and the heart and is required in blood clotting and in the formation of bones.","title":"Basic Metabolic Panel (BMP)"},{"location":"medical/labs/#electrolytes","text":"Sodium ($Na^+$) concentration in blood serum : Sodium is vital to normal body processes, including nerve and muscle function . The kidneys work to excrete any excess sodium that is ingested in food and beverages. Sodium levels fluctuate with dehydration or over-hydration, the food and beverages consumed, diarrhea, endocrine disorders, water retention (various causes), trauma and bleeding. Hyponatraemia : $Na_{serum}< 135 \\: mmol/l$ Hypernatraemia : $Na_{serum}> 150 \\: mmol/l$ Causes of sodium deficiency : renal/skin/intestinal losses Causes of sodium excess : decreased sodium excretion or excessive intake Potassium ($K^+$) concentration in blood serum : plays an important role in muscle contractions and cell function . Both high and low levels of potassium can cause problems with the rhythm of the heart so it is important to monitor the level of potassium after surgery. Patients who are taking diuretics regularly may require regular blood tests to monitor potassium levels, as some diuretics cause the kidneys to excrete too much potassium. Hypokalaemia : $K_{serum} < 3.5 \\: mmol/l$ Hyperkalaemia : $K_{serum} < 5.2 \\: mmol/l$ Carbon Dioxide ($CO_2$) : Most carbon dioxide is present in the form of bicarbonate , which is regulated by the lungs and kidneys and helps to maintain the body's acid-base balance. The test result is an indication of how well the kidneys, and sometimes the lungs, are managing the bicarbonate level in the blood. Chloride ($Cl^-$) concentration in blood serum : Chloride binds with electrolytes including potassium and sodium in the blood and plays a role in maintaining the proper pH of the blood. Chloride levels can vary widely if the patient is dehydrated or overly hydrated, if the kidneys are not functioning properly. Heart failure and endocrine problems can also contribute to abnormal chloride results.","title":"Electrolytes"},{"location":"medical/labs/#kidney-tests","text":"Blood Urea Nitrogen (BUN) : waste product filtered out of the blood by the kidneys; conditions that affect the kidney have the potential to affect the amount of urea in the blood. A high level may indicate that the kidneys are functioning less than normal. Creatinine : waste product generated by the body during the process of normal muscle breakdown; it is filtered out of the blood by the kidneys so blood levels are a good indication of how well the kidneys are working. High levels may indicate kidney impairment, low blood pressure, high blood pressure or another condition. Creatinine Clearance : The amount of blood filtered per minute by the kidneys is known as the glomerular filtration rate (GFR) . If the kidneys are damaged or diseased, or if blood circulation is slowed, then less creatinine will be removed from the blood and released into the urine and the GFR will be decreased. GFR is difficult to measure directly. A way to estimate GFR is to calculate creatinine clearance. There are several versions of the creatinine clearance calculation. All of them include the measurement of the amount of creatinine in a blood sample collected just before or after the urine collection, the amount of creatinine in a 24-hour urine sample, and the 24-hour urine volume. Since the amount of creatinine produced depends on muscle mass, some calculations also use a correction factor that takes into account a person's body surface area.","title":"Kidney Tests"},{"location":"medical/labs/#comprehensive-metabolic-panel-cmp","text":"It usually includes 14 tests; provides the same information as the BMP with the addition of the status of a person's liver and important blood proteins.","title":"Comprehensive Metabolic Panel (CMP)"},{"location":"medical/labs/#proteins","text":"Albumin : a small protein produced in the liver; the major protein in serum. Total protein (TP) : the concentration of all fractions of plasma proteins (including albumin and several types of globulin). Hypoproteinaemia : total blood protein decrease below 65 g/l (life threatening when total protein concentration $< 40 g/l$ and albumin $< 20 g/l$ Hyperproteinaemia : is an abnormal increase in serum and plasma proteins.","title":"Proteins"},{"location":"medical/labs/#liver-tests","text":"Aminotransferases : they catalyze the process of transamination and are present in every organ and tissue. \u0410L\u0422/SGPT (ALanine amino Transferase/Serum Glutamic Pyruvic Transaminase) : Highest concentration of ALT is noted in the liver cells. Increased ALT activity is most frequently revealed in acute liver and biliary ducts diseases. \u0410S\u0422/SGOT (ASpartate amino Transferase/Serum Glutamic Oxaloacetic Transaminase) : High concentration of AST is noted in heart and skeletal muscles, liver, kidneys, pancreas and erythrocytes. Damage of any of them leads to significant increase of AST in the blood serum. Increase in AST activity reveals hepatic pathology.Low AST activity usually reveals vitamin \u04126 deficiency, renal failure, pregnancy. Ritis ratio : AST/ALT ratio. Its normal value is around 1\u20131.3. It decreases in liver diseases and increases in heart diseases. Alkaline phosphatase (ALP) : is produced by various tissues: intestinal mucous membrane, osteoblasts, biliary ducts, placenta and mammary gland during lactation. Bone (ALP increases with bone formation), liver (main indicator of biliary tract pathology), intestinal (small part of total ALP activity, may be increased by intestinal diseases accompanied by diarrhoea) and placental (appears in pregnancy during the 3rd trimester, increase in women with placenta damage) ones are the most significant for clinical and diagnostic purposes. Total Bilirubin : is an orange-yellow pigment, a waste product primarily produced by the normal breakdown of heme (component of hemoglobin found in RBCs) when aged red blood cells (RBCs) are recycled. Bilirubin is ultimately processed by the liver to allow its elimination from the body. This test measures the amount of bilirubin in the blood to evaluate a person's liver function or to help diagnose anemias caused by RBC destruction (hemolytic anemia).","title":"Liver Tests"},{"location":"medical/labs/#electrolyte-panel","text":"It is helpful for detecting a problem with the body's fluid and electrolyte balance. Electrolytes are minerals that are found in body tissues and blood in the form of dissolved salts. As electrically charged particles, electrolytes help move nutrients into and wastes out of the body's cells, maintain a healthy water balance, and help stabilize the body's acid/base (pH) level. In the first instance pH is maintained by physiological buffers. Buffers may be intracellular and extracellular. Different buffer systems work in correlation with one another. It means that changes in one buffer system lead to changes in another. The main buffer systems are the following: Bicarbonate buffer (53% buffering capacity): the most important extracellular buffer, produced by kidneys. Haemoglobin buffer (35% buffering capacity): main intracellular buffer of the blood. Protein buffer (7% buffering capacity): is an extracellular buffer represented by plasma proteins. Phosphate buffer (5% buffering capacity): takes part in hydrogen ions excretion in renal tubules, is not of great importance in blood. If blood pH increases , $H^+$ ions move from cells to extracellular fluid in exchange of $K$ ions that enter the cells. That's why alkalosis is usually accompanied by hypokalaemia . If blood pH decreases , $H^+$ ions enter the cells in exchange of $K$ ions that leaves the cells. That's why acidosis may cause hyperkalaemia . The electrolyte panel measures the blood levels of the main electrolytes in the body: sodium (Na+), potassium (K+), chloride (Cl-), and bicarbonate (HCO3-; sometimes reported as total CO2), which are included in the BMP .","title":"Electrolyte Panel"},{"location":"medical/labs/#lipid-panel","text":"Also known as coronary risk panel , measures the level of specific lipids in blood to help assess someone's risk of cardiovascular disease (CVD). Total Cholesterol : The cholesterol forms the membranes for cells in all organs and tissues in the body. It is used to make hormones that are essential for development, growth, and reproduction. It forms bile acids that are needed to absorb nutrients from food. The test for cholesterol measures total cholesterol that is carried in the blood by lipoproteins. Triglycerides : They are a form of fat and a major source of energy for the body. Most triglycerides are found in fat (adipose) tissue, but some triglycerides circulate in the blood to provide fuel for muscles to work. Most triglycerides are carried in the blood by lipoproteins called very low-density lipoproteins (VLDL) . High levels of triglycerides in the blood are associated with an increased risk of developing cardiovascular disease (CVD). Certain factors can contribute to high triglyceride levels and to risk of CVD, including lack of exercise, being overweight, smoking cigarettes, consuming excess alcohol, and having medical conditions such as diabetes and kidney disease. A small amount of cholesterol circulates in the blood in complex particles called lipoproteins . Each particle contains a combination of protein, cholesterol, triglyceride, and phospholipid molecules and the particles are classified by their density into: High-density lipoprotein cholesterol (HDL-C) : often called good cholesterol because it removes excess cholesterol and carries it to the liver for removal. Low-density lipoprotein cholesterol (LDL-C) : often called bad cholesterol because it deposits excess cholesterol in walls of blood vessels, which can contribute to atherosclerosis.","title":"Lipid Panel"},{"location":"medical/labs/#liver-panel","text":"Also called Hepatic Function Panel , it is used to screen for, detect, evaluate, and monitor actue and chronic liver inflammation (hepatitis), liver disease and/or damage. Along with the Liver tests from the CMP , others are contained in this panel: The CMP includes most of the liver panel tests (ALT, ALP, AST, bilirubin, albumin, total protein). Depending on the healthcare provider and the laboratory, other tests that may be included in a liver panel are: Gamma-Glutamyl Transpeptidase (GGT) : an increased activity has a great significance in diagnosis of any pathology of liver and bile ducts . The liver is considered as the main source of normal serum activity, despite the fact that the kidney has the highest level of the enzyme. Pancreas also contains GGT (100% of patients with acute pancreatitis show GGT activity 10-20 times higher than normal). If GGT activity is normal, liver disease probability is very low. Thus, GGT is a good marker for differential diagnosis of liver pathology. The most significant increase is observed in cholestasis . Determination of GGT in urine allows to diagnose the early stages of kidney disease, which is accompanied by proximal renal tubular damage. Lactate Dehydrogenase (LDH) : is an enzyme involved in energy production that is found in almost all of the body's cells, with the highest levels found in the cells of the heart, liver, muscles, kidneys, lungs, and in blood cells; bacteria also produce LDH. Only a small amount of LDH is usually detectable in serum or plasma. LDH is released from the cells into the serum when cells are damaged or destroyed. Thus, an LDH blood level is a non-specific marker for the presence of tissue damage somewhere in the body . By itself, it cannot be used to identify the underlying cause or location of the cellular damage. However, it may be used, in conjunction with other blood tests, to help evaluate for and/or monitor conditions that lead to tissue damage, such as liver or blood diseases or cancer. Prothrombin Time (PT) : the liver produces proteins involved in the clotting (coagulation) of blood; the PT measures clotting function and, if abnormal, may indicate liver damage. A PT measures the number of seconds it takes for a clot to form in a person's sample of blood after substances are added. International Normalized Ratio (INR) : is a calculation based on results of a PT that is used to monitor individuals who are being treated with the blood-thinning medication (anticoagulant) warfarin. The INR is a calculation that adjusts for changes in the PT reagents and allows for results from different laboratories to be compared.","title":"Liver Panel"},{"location":"medical/labs/#renal-panel","text":"Also called Kidney Function Panel , it contains tests such as albumin, creatinine, BUN, eGFR to evaluate kidney function. The individual tests included in a renal panel can vary by laboratory, but the tests typically performed include: electrolytes , minerals (phosporus and calcium), protein (albumin), waste products (BUN, creatinine) and sugar (glucose). Phosphorous ($P$) : is a mineral that combines with other substances to form organic and inorganic phosphate compounds. Phosphates are vital for energy production, muscle and nerve function, and bone growth. They also play an important role as a buffer, helping to maintain the body's acid-base balance. Most of the body's phosphates combine with calcium to help form bones and teeth. Smaller amounts are found in muscle and nerve tissue. The rest is found within cells throughout the body, where they are mainly used to store energy. Hypophosphatemia may be seen with malnutrition, malabsorption, acid-base imbalances, increased blood calcium, and with disorders that affect kidney function. Hypophosphatemia may be seen with increased intake of the mineral, low blood calcium, and with kidney dysfunction.","title":"Renal Panel"},{"location":"medical/labs/#thyroid-function-panel","text":"This panel helps in evaluating thyroid gland function and diagnosing thyroid disorders.","title":"Thyroid Function Panel"},{"location":"medical/labs/#other-tests","text":"Magnesium ($Mg$) : is a mineral that is vital for energy production, muscle contraction, nerve function, and the maintenance of strong bones. This test is used to evaluate the level of magnesium in your blood and to help determine the cause of abnormal levels of magnesium, calcium and/or potassium. Hypomagnesemia : may be seen with malnutrition, conditions that cause malabsorption, and with excess loss of magnesium by the kidneys. Hypermagnesemia : may be seen with the ingestion of antacids that contain magnesium and with decreased ability of the kidneys to excrete magnesium. HbA1c : Hemoglobin A1c, also called A1c or glycated hemoglobin, is hemoglobin with glucose attached. The A1c test evaluates the average amount of glucose in the blood over the last 2 to 3 months by measuring the percentage of glycated (glycosylated) hemoglobin. There are several types of normal hemoglobin, but the predominant form \u2013 about 95-98% \u2013 is hemoglobin A. As glucose circulates in the blood, some of it spontaneously binds to hemoglobin A. Once the glucose binds to the hemoglobin, it remains there for the life of the red blood cell \u2013 normally about 120 days. The predominant form of glycated hemoglobin is referred to as A1c. This test may be used to screen for and diagnose diabetes or risk of developing diabetes. Uric acid : is produced by the breakdown of purines. Purines are nitrogen-containing compounds found in the cells of the body, including our DNA. As cells get old and die, they break down, releasing purines into the blood. If too much uric acid is produced or not enough is removed, it can accumulate in the body, causing increased levels in the blood (hyperuricemia). The presence of excess uric acid can cause gout, a condition characterized by inflammation of the joints due to the formation of uric acid crystals in the joint (synovial) fluid. Excess uric acid can also be deposited in tissues such as the kidney, leading to kidney stones or kidney failure.","title":"Other tests (???)"},{"location":"medical/labs/#normal-ranges-rule-of-thumb_1","text":"https://www.verywellhealth.com/blood-chemistry-tests-and-results-3156998 Type of test Unit Levels Glucose $mmol/l$ 3.9-5.6 Calcium Serum Sodium $mmol/l$ 3.5-5 Serum Potassium $mmol/l$ 3.5-5 Carbon Dioxide (bicarbonate) $mmol/l$ 24-30 Serum Chloride $mmol/l$ 100-106 Blood Urea Nitrogen (BUN) $mmol/l$ 2.9-8.9 Creatinine $\\mu mol/l$ Men: 15-40; Women: 25-70 Creatinine Clearance Albumin Total Protein (TP) ALT/SGPT AST/SGOT ALP Total Bilirubin GGT LDH Prothrombin Time INR Phosphorous Total Cholesterol $mmol/l$ <5.18 (high >6.22) Triglycerides $mmol/l$ <1.7 (high >2.3) HDL-C $mmol/l$ Men: 1.0-1.3; Women: 1.3-1.5 LDL-C $mmol/l$ <2.59 (high >160) Magnesium HbA1c Uric Acid","title":"Normal Ranges' Rule of Thumb"},{"location":"medical/labs/#urinalysis","text":"","title":"Urinalysis"},{"location":"medical/labs/#coagulation","text":"","title":"Coagulation"},{"location":"medical/labs/#biomarkers","text":"CEA (carcinoembryonic antigen) is a protein found in many types of cells but associated with tumors and the developing fetus. A common cutoff is $5 \\mu g/L$ (values bigger than this could be a sign of disease). Mutations in KRAS exon 2 , BRAF and PIK3CA are commonly present in colorectal cancer (CRC).","title":"Biomarkers"},{"location":"medical/mutations/","text":"RAS mutations Link Rat sarcoma virus (RAS) proto-oncogenes encode 21,000-dalton membrane bound small GTPases that include Harvey-Ras (HRAS), Kirsten-Ras (KRAS), and neuroblastoma-Ras (NRAS), collectively called RAS. RAS gene mutations are among the most frequently mutated genes in human cancers, found in approximately 30% of all tumor types and in approximately 50% of colorectal cancer (CRC). This group of genes are involved in a pathway called the epidermal growth factor receptor (EGFR) pathway and acts as an on/off switch in cell signalling. When it functions normally, it controls cell proliferation. When it is mutated, negative signalling is disrupted. Thus, cells can continuously proliferate, and often develop into cancer. Despite the efforts to find a clinically effective inhibitor, targeting RAS mutations has remained elusive, so much so that some have termed oncogenic RAS mutations as \"undruggable\". In addition, oncogenic RAS mutations cause acquired resistance to anti-epidermal growth factor receptor (EGFR) therapies such as cetuximab and panitumumab . HRAS : KRAS or K-ras or Ki-ras is a short name for the gene v-Ki-ras2 Kirsten rat sarcoma viral oncogene homolog. NRAS : Cancer Type % KRAS % NRAS % HRAS % All RAS Pancreatic adenocarcinoma 90-98 0-0.5 0 91-98 Colorectal adenocarcinoma 40-45 4-8 0 44-53 Multiple myeloma 22 19 0 42 Lung adenocarcinoma 16-33 0.6-0.9 0.3-0.5 17-33 Skin cutaneous melanoma 0.8 28 1 29 Biliary carcinoma 25 3 0 27 Uterine endometroid carcinoma 14-21 2-3 0.4-0.5 16-25 Small intestine adenocarcinoma 23 0.7 0 23 Chronic myelomonocytic leukemia 9 13 0 22 Thyroid carcinoma 1-2 6-9 4 13-14 Acute myeloid leukemia 3-4 7-11 2 11-15 Cervical adenocarcinoma 7-8 0.8 0-6 7-15 Urothelial carcinoma 3-4 1-2 6-9 11-15 Stomach adenocarcinoma 6-11 1 0-1 9-12 Head and neck squamous cell carcinoma 0.5-2 0.3-2 5-6 5-10 Gastric carcinoma 4.0-6 1 0-1 5-9 Esophageal adenocarcinoma 2-4 0 0.6-0.7 3-5 Source: Therapeutic Approaches to RAS Mutation (May 2017) BRAF Link BRAF is a human gene that encodes a protein called B-Raf. The gene is also referred to as proto-oncogene B-Raf and v-Raf murine sarcoma viral oncogene homolog B, while the protein is more formally known as serine/threonine-protein kinase B-Raf. The BRAF gene provides instructions for making a protein that helps transmit chemical signals from outside the cell to the cell's nucleus. This protein is part of a signaling pathway known as the RAS/MAPK pathway, which controls several important cell functions. Specifically, the RAS/MAPK pathway regulates the growth and division (proliferation) of cells, the process by which cells mature to carry out specific functions (differentiation), cell movement (migration), and the self-destruction of cells (apoptosis). Chemical signaling through this pathway is essential for normal development before birth. In 2002, Davis et al. shown to be faulty (mutated) in some human cancers.","title":"Mutations"},{"location":"medical/mutations/#ras-mutations","text":"Rat sarcoma virus (RAS) proto-oncogenes encode 21,000-dalton membrane bound small GTPases that include Harvey-Ras (HRAS), Kirsten-Ras (KRAS), and neuroblastoma-Ras (NRAS), collectively called RAS. RAS gene mutations are among the most frequently mutated genes in human cancers, found in approximately 30% of all tumor types and in approximately 50% of colorectal cancer (CRC). This group of genes are involved in a pathway called the epidermal growth factor receptor (EGFR) pathway and acts as an on/off switch in cell signalling. When it functions normally, it controls cell proliferation. When it is mutated, negative signalling is disrupted. Thus, cells can continuously proliferate, and often develop into cancer. Despite the efforts to find a clinically effective inhibitor, targeting RAS mutations has remained elusive, so much so that some have termed oncogenic RAS mutations as \"undruggable\". In addition, oncogenic RAS mutations cause acquired resistance to anti-epidermal growth factor receptor (EGFR) therapies such as cetuximab and panitumumab . HRAS : KRAS or K-ras or Ki-ras is a short name for the gene v-Ki-ras2 Kirsten rat sarcoma viral oncogene homolog. NRAS : Cancer Type % KRAS % NRAS % HRAS % All RAS Pancreatic adenocarcinoma 90-98 0-0.5 0 91-98 Colorectal adenocarcinoma 40-45 4-8 0 44-53 Multiple myeloma 22 19 0 42 Lung adenocarcinoma 16-33 0.6-0.9 0.3-0.5 17-33 Skin cutaneous melanoma 0.8 28 1 29 Biliary carcinoma 25 3 0 27 Uterine endometroid carcinoma 14-21 2-3 0.4-0.5 16-25 Small intestine adenocarcinoma 23 0.7 0 23 Chronic myelomonocytic leukemia 9 13 0 22 Thyroid carcinoma 1-2 6-9 4 13-14 Acute myeloid leukemia 3-4 7-11 2 11-15 Cervical adenocarcinoma 7-8 0.8 0-6 7-15 Urothelial carcinoma 3-4 1-2 6-9 11-15 Stomach adenocarcinoma 6-11 1 0-1 9-12 Head and neck squamous cell carcinoma 0.5-2 0.3-2 5-6 5-10 Gastric carcinoma 4.0-6 1 0-1 5-9 Esophageal adenocarcinoma 2-4 0 0.6-0.7 3-5 Source: Therapeutic Approaches to RAS Mutation (May 2017)","title":"RAS mutations"},{"location":"medical/mutations/#braf","text":"BRAF is a human gene that encodes a protein called B-Raf. The gene is also referred to as proto-oncogene B-Raf and v-Raf murine sarcoma viral oncogene homolog B, while the protein is more formally known as serine/threonine-protein kinase B-Raf. The BRAF gene provides instructions for making a protein that helps transmit chemical signals from outside the cell to the cell's nucleus. This protein is part of a signaling pathway known as the RAS/MAPK pathway, which controls several important cell functions. Specifically, the RAS/MAPK pathway regulates the growth and division (proliferation) of cells, the process by which cells mature to carry out specific functions (differentiation), cell movement (migration), and the self-destruction of cells (apoptosis). Chemical signaling through this pathway is essential for normal development before birth. In 2002, Davis et al. shown to be faulty (mutated) in some human cancers.","title":"BRAF"},{"location":"medical/orr-dcr/","text":"Objective Response Rate (ORR) Link In medicine, a response rate is the percentage of patients whose cancer shrinks or disappears after treatment. When used as a clinical endpoint for clinical trials of cancer treatments, this often called the objective response rate (ORR) . The FDA definition of ORR is \"the proportion of patients with tumor size reduction of a predefined amount and for a minimum time period\" . When defined in this manner, ORR is a direct measure of drug antitumor activity, which can be evaluated in a single-arm study. Stable disease should not be a component of ORR. Stable disease can reflect the natural history of disease, whereas tumor reduction is a direct therapeutic effect. Disease Control Rate (DCR) Link The disease control rate (DCR) or clinical benefit rate (CBR) is defined as the percentage of patients with advanced or metastatic cancer who have achieved complete response, partial response or stable disease to a therapeutic intervention in clinical trials of anticancer agents. It is then a composite of ORR and stable disease and is useful to measure the efficacy of therapies that have tumoristatic effects rather than tumoricidal effects. Calculation Link Remember ORR and DCR are usually calculated both taking into account not evaluable (NE) best responses and without them included in the total number of patients. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 data orr_dcr ; input pt $ bestresp $ orr dcr c_orr $ c_dcr $ ; datalines ; 001 PD 0 0 N N 002 SD 0 1 N Y 003 SD 0 1 N Y 004 PD 0 0 N N 005 PD 0 0 N N 006 PD 0 0 N N 007 SD 0 1 N Y 008 SD 0 1 N Y 009 PD 0 0 N N 010 SD 0 1 N Y 011 PR 1 1 Y Y 012 PR 1 1 Y Y ; run ; ods exclude all ; proc freq data = orr_dcr ; table orr / nocum binomial ( exact level = 2 ) alpha = 0 . 05 ; ods output BinomialCLs = orr_prop_ci ; run ; proc freq data = orr_dcr ; table dcr / nocum binomial ( exact level = 2 ) alpha = 0 . 05 ; ods output BinomialCLs = dcr_prop_ci ; run ; ods exclude none ; data orr_dcr_prop_ci ; set orr_prop_ci dcr_prop_ci ; propci = trim ( left ( put ( Proportion * 100 , 5 . 2 ))) || ' ( ' || trim ( left ( put ( LowerCL * 100 , 5 . 2 ))) || ' , ' || trim ( left ( put ( UpperCL * 100 , 5 . 2 ))) || ' ) ' ; prop = trim ( left ( put ( Proportion * 100 , 5 . 2 ))) ; ci = trim ( left ( put ( LowerCL * 100 , 5 . 2 ))) || ' , ' || trim ( left ( put ( UpperCL * 100 , 5 . 2 ))) ; if table = ' Table orr ' then texto = ' ORR ' ; if table = ' Table dcr ' then texto = ' DCR ' ; drop type ; run ; proc report data = orr_dcr_prop_ci nowd headline style ( header ) = { background = very light grey fontsize = 8 pt } missing style ( column ) = { fontsize = 8 pt } style ( report ) = { width = 100 % } split = ' * ' ; title \" Response Rates \" ; column ( \" Response Rates \" texto propci prop ci ) ; define texto / display ' ' flow ; define propci / display ' Proportion (CI 95%) ' flow ; define prop / display ' Proportion ' flow ; define ci / display ' CI 95% ' flow ; run ;","title":"ORR and DCR"},{"location":"medical/orr-dcr/#objective-response-rate-orr","text":"In medicine, a response rate is the percentage of patients whose cancer shrinks or disappears after treatment. When used as a clinical endpoint for clinical trials of cancer treatments, this often called the objective response rate (ORR) . The FDA definition of ORR is \"the proportion of patients with tumor size reduction of a predefined amount and for a minimum time period\" . When defined in this manner, ORR is a direct measure of drug antitumor activity, which can be evaluated in a single-arm study. Stable disease should not be a component of ORR. Stable disease can reflect the natural history of disease, whereas tumor reduction is a direct therapeutic effect.","title":"Objective Response Rate (ORR)"},{"location":"medical/orr-dcr/#disease-control-rate-dcr","text":"The disease control rate (DCR) or clinical benefit rate (CBR) is defined as the percentage of patients with advanced or metastatic cancer who have achieved complete response, partial response or stable disease to a therapeutic intervention in clinical trials of anticancer agents. It is then a composite of ORR and stable disease and is useful to measure the efficacy of therapies that have tumoristatic effects rather than tumoricidal effects.","title":"Disease Control Rate (DCR)"},{"location":"medical/orr-dcr/#calculation","text":"Remember ORR and DCR are usually calculated both taking into account not evaluable (NE) best responses and without them included in the total number of patients. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 data orr_dcr ; input pt $ bestresp $ orr dcr c_orr $ c_dcr $ ; datalines ; 001 PD 0 0 N N 002 SD 0 1 N Y 003 SD 0 1 N Y 004 PD 0 0 N N 005 PD 0 0 N N 006 PD 0 0 N N 007 SD 0 1 N Y 008 SD 0 1 N Y 009 PD 0 0 N N 010 SD 0 1 N Y 011 PR 1 1 Y Y 012 PR 1 1 Y Y ; run ; ods exclude all ; proc freq data = orr_dcr ; table orr / nocum binomial ( exact level = 2 ) alpha = 0 . 05 ; ods output BinomialCLs = orr_prop_ci ; run ; proc freq data = orr_dcr ; table dcr / nocum binomial ( exact level = 2 ) alpha = 0 . 05 ; ods output BinomialCLs = dcr_prop_ci ; run ; ods exclude none ; data orr_dcr_prop_ci ; set orr_prop_ci dcr_prop_ci ; propci = trim ( left ( put ( Proportion * 100 , 5 . 2 ))) || ' ( ' || trim ( left ( put ( LowerCL * 100 , 5 . 2 ))) || ' , ' || trim ( left ( put ( UpperCL * 100 , 5 . 2 ))) || ' ) ' ; prop = trim ( left ( put ( Proportion * 100 , 5 . 2 ))) ; ci = trim ( left ( put ( LowerCL * 100 , 5 . 2 ))) || ' , ' || trim ( left ( put ( UpperCL * 100 , 5 . 2 ))) ; if table = ' Table orr ' then texto = ' ORR ' ; if table = ' Table dcr ' then texto = ' DCR ' ; drop type ; run ; proc report data = orr_dcr_prop_ci nowd headline style ( header ) = { background = very light grey fontsize = 8 pt } missing style ( column ) = { fontsize = 8 pt } style ( report ) = { width = 100 % } split = ' * ' ; title \" Response Rates \" ; column ( \" Response Rates \" texto propci prop ci ) ; define texto / display ' ' flow ; define propci / display ' Proportion (CI 95%) ' flow ; define prop / display ' Proportion ' flow ; define ci / display ' CI 95% ' flow ; run ;","title":"Calculation"},{"location":"medical/recist/","text":"Response Evaluation Criteria in Solid Tumors (RECIST) were developed and published in 2000, based on the original World Health Organization (WHO) guidelines first published in 1981. In 2009, revisions were made (RECIST 1.1) incorporating major changes, including a reduction in the number of lesions to be assessed, a new measurement method to classify lymph nodes as pathologic or normal, the clarification of the requirement to confirm a complete response (CR) or partial response (PR) and new methodologies for more appropriate measurement of disease progression . Subject Eligibility Link Only patients with measurable disease at baseline (presence of at least one measurable lesion) should be included in protocols where objective tumor response is the primary endpoint. In studies where the primary endpoint is tumor progression (either time to progression or proportion with progression at a fixed date), the protocol must specify if entry is restricted to those with measurable disease or whether patients having non-measurable disease only are also eligible . Methods of Assessment Link The same method of assessment and the same technique should be used to characterize each identified and reported lesion at baseline and during follow-up. CT is the best currently available and reproducible method to measure lesions selected for response assessment. MRI is also acceptable in certain situations (e.g., for body scans but not for lung). Lesions on a chest X-ray may be considered measurable lesions if they are clearly defined and surrounded by aerated lung. However, CT is preferable. Clinical lesions will only be considered measurable when they are superficial and \u226510 mm in diameter as assessed using calipers. For the case of skin lesions, documentation by color photography, including a ruler to estimate the size of the lesion, is recommended. Ultrasound (US) should not be used to measure tumor lesions. Tumor markers alone cannot be used to assess response. If markers are initially above the upper normal limit, they must normalize for a patient to be considered in complete response. Cytology and histology can be used in rare cases (e.g., for evaluation of residual masses to differentiate between Partial Response and Complete Response or evaluation of new or enlarging effusions to differentiate between Progressive Disease and Response/Stable Disease). Use of endoscopy and laparoscopy is not advised. However, they can be used to confirm complete pathological response. Baseline Disease Assessment Link All baseline evaluations should be performed as closely as possible to the beginning of treatment and never more than 4 weeks before the beginning of the treatment. Measurable lesions Link Must be accurately measured in at least one dimension ( longest diameter in the plane of measurement is to be recorded ) with a minimum size of: 10 mm by CT scan (CT scan slice thickness no greater than 5 mm; when CT scans have slice thickness >5 mm, the minimum size should be twice the slice thickness). 10 mm caliper measurement by clinical exam (lesions which cannot be accurately measured with calipers should be recorded as nonmeasurable). 20 mm by chest X-ray. Malignant lymph nodes . To be considered pathologically enlarged and measurable, a lymph node must be \u226515 mm in short axis when assessed by CT scan (CT scan slice thickness is recommended to be no greater than 5 mm). At baseline and in follow-up, only the short axis will be measured and followed. Lytic bone lesions or mixed lytic-blastic lesions with identifiable soft tissue components that can be evaluated by cross-sectional imaging techniques such as CT or MRI can be considered measurable if the soft tissue component meets the definition of measurability described above. \u2018Cystic lesions\u2019 thought to represent cystic metastases can be considered measurable if they meet the definition of measurability described above. However, if non-cystic lesions are present in the same patient, these are preferred for selection as target lesions. Non-measurable lesions Link Non-measurable lesions are all other lesions, including small lesions (longest diameter <10 mm or pathological lymph nodes with 10 to <15 mm short axis), as well as truly non-measurable lesions. Lesions considered truly non-measurable include: leptomeningeal disease, ascites, pleural or pericardial effusion, inflammatory breast disease, lymphangitic involvement of skin or lung, abdominal masses/abdominal organomegaly identified by physical exam that is not measurable by reproducible imaging techniques. Blastic bone lesions are non-measurable. Lesions with prior local treatment , such as those situated in a previously irradiated area or in an area subjected to other loco-regional therapy, are usually not considered measurable unless there has been demonstrated progression in the lesion. Study protocols should detail the conditions under which such lesions would be considered measurable. Target Lesions Link All measurable lesions up to a maximum of two lesions per organ and five lesions in total, representative of all involved organs, should be identified as target lesions and recorded and measured at baseline. Target lesions should be selected on the basis of their size (lesions with the longest diameter) and be representative of all involved organs, as well as their suitability for reproducible repeated measurements. All measurements should be recorded in metric notation using calipers if clinically assessed. A sum of the diameters (longest for non-nodal lesions, short axis for nodal lesions) for all target lesions will be calculated and reported as the baseline sum diameters, which will be used as reference to further characterize any objective tumor regression in the measurable dimension of the disease. If lymph nodes are to be included in the sum, only the short axis will contribute. Non-target Lesions Link All lesions (or sites of disease) not identified as target lesions, including pathological lymph nodes and all non-measurable lesions, should be identified as non-target lesions and be recorded at baseline. Measurements of these lesions are not required and they should be followed as \u2018present\u2019, \u2018absent\u2019 or in rare cases, \u2018unequivocal progression\u2019 Response Criteria Link Evaluation of target lesions Link Complete Response (CR) : Disappearance of all target lesions. Any pathological lymph nodes (whether target or non-target) must have reduction in short axis to <10 mm. Partial Response (PR) : At least a 30% decrease in the sum of diameters of target lesions, taking as reference the baseline sum of diameters. Progressive Disease (PD) : At least a 20% increase in the sum of diameters of target lesions, taking as reference the smallest sum on study (this may include the baseline sum). The sum must also demonstrate an absolute increase of at least 5 mm. Stable Disease (SD) : Neither sufficient shrinkage to qualify for PR nor sufficient increase to qualify for PD. Special notes on the assessment of target lesions Link Lymph nodes identified as target lesions should always have the actual short axis measurement recorded even if the nodes regress to below 10 mm on study. When lymph nodes are included as target lesions, the \u2018sum\u2019 of lesions may not be zero even if complete response criteria are met since a normal lymph node is defined as having a short axis of <10 mm. Target lesions that become \u2018too small to measure\u2019 . While on study, all lesions (nodal and non-nodal) recorded at baseline should have their actual measurements recorded at each subsequent evaluation, even when very small. However, sometimes lesions or lymph nodes become so faint on a CT scan that the radiologist may not feel comfortable assigning an exact measure and may report them as being \u2018too small to measure\u2019, in which case a default value of 5 mm should be assigned. Lesions that split or coalesce on treatment . When non-nodal lesions \u2018fragment\u2019, the longest diameters of the fragmented portions should be added together to calculate the target lesion sum. Similarly, as lesions coalesce, a plane between them may be maintained that would aid in obtaining maximal diameter measurements of each individual lesion. If the lesions have truly coalesced such that they are no longer separable, the vector of the longest diameter in this instance should be the maximal longest diameter for the \u2018coalesced lesion\u2019. Evaluation of non-target lesions Link Complete Response (CR) : Disappearance of all non-target lesions and normalization of tumor marker levels. All lymph nodes must be non-pathological in size (<10 mm short axis). Non-CR / Non-PD : Persistence of one or more non-target lesion(s) and/or maintenance of tumor marker levels above normal limits. Progressive Disease (PD) : Unequivocal progression of existing non-target lesions. When patient has measurable disease . To achieve \u2018unequivocal progression\u2019 on the basis of the non-target disease, there must be an overall level of substantial worsening in non-target disease such that, even in presence of SD or PR in target disease, the overall tumor burden has increased sufficiently to merit discontinuation of therapy. A modest \u2018increase\u2019 in the size of one or more non-target lesions is usually not sufficient to qualify for unequivocal progression status. When patient has only non-measurable disease . There is no measurable disease assessment to factor into the interpretation of an increase in non-measurable disease burden. Because worsening in non-target disease cannot be easily quantified, a useful test that can be applied is to consider if the increase in overall disease burden based on change in nonmeasurable disease is comparable in magnitude to the increase that would be required to declare PD for measurable disease. Examples include an increase in a pleural effusion from \u2018trace\u2019 to \u2018large\u2019 or an increase in lymphangitic disease from localized to widespread. New lesions Link The appearance of new malignant lesions denotes disease progression: The finding of a new lesion should be unequivocal (i.e., not attributable to differences in scanning technique, change in imaging modality or findings thought to represent something other than tumor, especially when the patient\u2019s baseline lesions show partial or complete response). If a new lesion is equivocal, for example because of its small size, continued therapy and follow-up evaluation will clarify if it represents truly new disease. If repeat scans confirm there is definitely a new lesion, then progression should be declared using the date of the initial scan. A lesion identified on a follow-up study in an anatomical location that was not scanned at baseline is considered a new lesion and disease progression. It is sometimes reasonable to incorporate the use of FDG-PET scanning to complement CT in assessment of progression (particularly possible \u2018new\u2019 disease). New lesions on the basis of FDG-PET imaging can be identified according to the following algorithm: Negative FDG-PET at baseline, with a positive FDG-PET at follow-up is PD based on a new lesion. No FDG-PET at baseline and a positive FDG-PET at follow-up : If the positive FDG-PET at follow-up corresponds to a new site of disease confirmed by CT, this is PD. If the positive FDG-PET at follow-up is not confirmed as a new site of disease on CT, additional follow-up CT scans are needed to determine if there is truly progression occurring at that site (if so, the date of PD will be the date of the initial abnormal FDG-PET scan). If the positive FDG-PET at follow-up corresponds to a pre-existing site of disease on CT that is not progressing on the basis of the anatomic images, this is not PD. The following table provides a summary of the overall response status calculation at each time point for patients who have measurable disease at baseline. Time point response: Patients with target (+/\u2013 non-target) disease Target lesions Non-target lesions New lesions Overall response CR CR No CR CR Non-CR /non-PD No PR CR NE No PR PR Non-PD /or not all evaluated No PR SD Non-PD /or not all evaluated No SD Not all evaluated Non-PD No NE PD Any Yes or No PD Any PD Yes or No PD Any Any Yes PD When patients have non-measurable (therefore non-target) disease only, the next table is to be used. Time point response: Patients with non-target disease Non-target lesions New lesions Overall response CR No CR Non-CR/non-PD No Non-CR/non-PD 1 Not all evaluated No NE Unequivocal PD Yes or No PD Any Yes PD 1 Non-CR / non-PD is preferred over \u2018Stable Disease\u2019 for non-target disease since SD is increasingly used as an endpoint for assessment of efficacy in some trials. To assign this category when no lesions can be measured is not advised. Confirmation Link In non-randomized trials where response is the primary endpoint, confirmation of PR and CR is required to ensure responses identified are not the result of measurement error. This will also permit appropriate interpretation of results in the context of historical data where response has traditionally required confirmation in such trials. However, in all other circumstances, (i.e., in randomized phase II or III trials or studies where stable disease or progression are the primary endpoints), confirmation of response is not required since it will not add value to the interpretation of trial results. However, elimination of the requirement for response confirmation may increase the importance of central review to protect against bias, in particular in studies which are not blinded. In the case of SD, measurements must have met the SD criteria at least once after study entry at a minimum interval (in general not less than 6\u20138 weeks) that is defined in the study protocol. The Best Overall Response (BOR) is the best response recorded from the start of the study treatment until the disease progression/recurrence: Complete Response (CR) upon confirmation at least 4 weeks after first CR Partial Response (PR) upon confirmation at least 4 weeks after first PR Stable Disease (SD) should be met at least once no less than 6-8 weeks after the first dose of trial treatment/baseline assessment, otherwise the best response will be Not Evaluable (NE) Progressive Disease (PD) does not need confirmation The criteria for confirmation of the response is summarized in the following table for the best overall response when confirmation of CR and PR are required: Overall response 1st time point Overall response subsequent time point Best Overall Response CR CR CR CR PR SD, PD or PR 2 CR SD SD provided minimum criteria for2 SD duration met, otherwise, PD CR PD SD provided minimum criteria for SD duration met, otherwise, PD CR NE SD provided minimum criteria for SD duration met, otherwise, NE PR CR PR PR PR PR PR SD SD PR PD SD provided minimum criteria for SD duration met, otherwise, PD PR NE SD provided minimum criteria for SD duration met, otherwise, NE NE NE NE 2 If a CR is truly met at first time point, then any disease seen at a subsequent time point, even disease meeting PR criteria relative to baseline, makes the disease PD at that point (since disease must have reappeared after CR). Best response would depend on whether minimum duration for SD was met. However, sometimes CR may be claimed when subsequent scans suggest small lesions were likely still present and in fact the patient had PR, not CR at the first time point. Under these circumstances, the original CR should be changed to PR and the best response is PR. Missing Assessments and Inevaluable Designation Link When no imaging/measurement is done at all at a particular time point, the patient is not evaluable (NE) at that time point. If only a subset of lesion measurements are made at an assessment, usually the case is also considered NE at that time point, unless a convincing argument can be made that the contribution of the individual missing lesion(s) would not change the assigned time point response. This would most likely happen in the case of PD. RECIST FAQs Link When measuring the longest diameter of target lesions in response to treatment, is the same axis that was used initially used subsequently, even if there is a shape change to the lesion that may have produced a new longest diameter? The longest diameter of the lesion should always be measured even if the actual axis is different from the one used to measure the lesion initially (or at a different time point during followup). The only exception to this is lymph nodes \u2014per RECIST 1.1 the short axis should always be followed and as in the case of target lesions, the vector of the short axis may change on follow-up. What if a single non-target lesion cannot be reviewed (for whatever reason)? Does this negate the overall assessment? Sometimes the major contribution of a single non-target lesion may be in the setting of CR having otherwise been achieved; failure to examine one non-target in that setting will leave you unable to claim CR. It is also possible that the non-target lesion has undergone such substantial progression that it would override the target disease and render the patient PD. However, this is very unlikely, especially if the rest of the measurable disease is stable or responding.","title":"RECIST v1.1"},{"location":"medical/recist/#subject-eligibility","text":"Only patients with measurable disease at baseline (presence of at least one measurable lesion) should be included in protocols where objective tumor response is the primary endpoint. In studies where the primary endpoint is tumor progression (either time to progression or proportion with progression at a fixed date), the protocol must specify if entry is restricted to those with measurable disease or whether patients having non-measurable disease only are also eligible .","title":"Subject Eligibility"},{"location":"medical/recist/#methods-of-assessment","text":"The same method of assessment and the same technique should be used to characterize each identified and reported lesion at baseline and during follow-up. CT is the best currently available and reproducible method to measure lesions selected for response assessment. MRI is also acceptable in certain situations (e.g., for body scans but not for lung). Lesions on a chest X-ray may be considered measurable lesions if they are clearly defined and surrounded by aerated lung. However, CT is preferable. Clinical lesions will only be considered measurable when they are superficial and \u226510 mm in diameter as assessed using calipers. For the case of skin lesions, documentation by color photography, including a ruler to estimate the size of the lesion, is recommended. Ultrasound (US) should not be used to measure tumor lesions. Tumor markers alone cannot be used to assess response. If markers are initially above the upper normal limit, they must normalize for a patient to be considered in complete response. Cytology and histology can be used in rare cases (e.g., for evaluation of residual masses to differentiate between Partial Response and Complete Response or evaluation of new or enlarging effusions to differentiate between Progressive Disease and Response/Stable Disease). Use of endoscopy and laparoscopy is not advised. However, they can be used to confirm complete pathological response.","title":"Methods of Assessment"},{"location":"medical/recist/#baseline-disease-assessment","text":"All baseline evaluations should be performed as closely as possible to the beginning of treatment and never more than 4 weeks before the beginning of the treatment.","title":"Baseline Disease Assessment"},{"location":"medical/recist/#measurable-lesions","text":"Must be accurately measured in at least one dimension ( longest diameter in the plane of measurement is to be recorded ) with a minimum size of: 10 mm by CT scan (CT scan slice thickness no greater than 5 mm; when CT scans have slice thickness >5 mm, the minimum size should be twice the slice thickness). 10 mm caliper measurement by clinical exam (lesions which cannot be accurately measured with calipers should be recorded as nonmeasurable). 20 mm by chest X-ray. Malignant lymph nodes . To be considered pathologically enlarged and measurable, a lymph node must be \u226515 mm in short axis when assessed by CT scan (CT scan slice thickness is recommended to be no greater than 5 mm). At baseline and in follow-up, only the short axis will be measured and followed. Lytic bone lesions or mixed lytic-blastic lesions with identifiable soft tissue components that can be evaluated by cross-sectional imaging techniques such as CT or MRI can be considered measurable if the soft tissue component meets the definition of measurability described above. \u2018Cystic lesions\u2019 thought to represent cystic metastases can be considered measurable if they meet the definition of measurability described above. However, if non-cystic lesions are present in the same patient, these are preferred for selection as target lesions.","title":"Measurable lesions"},{"location":"medical/recist/#non-measurable-lesions","text":"Non-measurable lesions are all other lesions, including small lesions (longest diameter <10 mm or pathological lymph nodes with 10 to <15 mm short axis), as well as truly non-measurable lesions. Lesions considered truly non-measurable include: leptomeningeal disease, ascites, pleural or pericardial effusion, inflammatory breast disease, lymphangitic involvement of skin or lung, abdominal masses/abdominal organomegaly identified by physical exam that is not measurable by reproducible imaging techniques. Blastic bone lesions are non-measurable. Lesions with prior local treatment , such as those situated in a previously irradiated area or in an area subjected to other loco-regional therapy, are usually not considered measurable unless there has been demonstrated progression in the lesion. Study protocols should detail the conditions under which such lesions would be considered measurable.","title":"Non-measurable lesions"},{"location":"medical/recist/#target-lesions","text":"All measurable lesions up to a maximum of two lesions per organ and five lesions in total, representative of all involved organs, should be identified as target lesions and recorded and measured at baseline. Target lesions should be selected on the basis of their size (lesions with the longest diameter) and be representative of all involved organs, as well as their suitability for reproducible repeated measurements. All measurements should be recorded in metric notation using calipers if clinically assessed. A sum of the diameters (longest for non-nodal lesions, short axis for nodal lesions) for all target lesions will be calculated and reported as the baseline sum diameters, which will be used as reference to further characterize any objective tumor regression in the measurable dimension of the disease. If lymph nodes are to be included in the sum, only the short axis will contribute.","title":"Target Lesions"},{"location":"medical/recist/#non-target-lesions","text":"All lesions (or sites of disease) not identified as target lesions, including pathological lymph nodes and all non-measurable lesions, should be identified as non-target lesions and be recorded at baseline. Measurements of these lesions are not required and they should be followed as \u2018present\u2019, \u2018absent\u2019 or in rare cases, \u2018unequivocal progression\u2019","title":"Non-target Lesions"},{"location":"medical/recist/#response-criteria","text":"","title":"Response Criteria"},{"location":"medical/recist/#evaluation-of-target-lesions","text":"Complete Response (CR) : Disappearance of all target lesions. Any pathological lymph nodes (whether target or non-target) must have reduction in short axis to <10 mm. Partial Response (PR) : At least a 30% decrease in the sum of diameters of target lesions, taking as reference the baseline sum of diameters. Progressive Disease (PD) : At least a 20% increase in the sum of diameters of target lesions, taking as reference the smallest sum on study (this may include the baseline sum). The sum must also demonstrate an absolute increase of at least 5 mm. Stable Disease (SD) : Neither sufficient shrinkage to qualify for PR nor sufficient increase to qualify for PD.","title":"Evaluation of target lesions"},{"location":"medical/recist/#special-notes-on-the-assessment-of-target-lesions","text":"Lymph nodes identified as target lesions should always have the actual short axis measurement recorded even if the nodes regress to below 10 mm on study. When lymph nodes are included as target lesions, the \u2018sum\u2019 of lesions may not be zero even if complete response criteria are met since a normal lymph node is defined as having a short axis of <10 mm. Target lesions that become \u2018too small to measure\u2019 . While on study, all lesions (nodal and non-nodal) recorded at baseline should have their actual measurements recorded at each subsequent evaluation, even when very small. However, sometimes lesions or lymph nodes become so faint on a CT scan that the radiologist may not feel comfortable assigning an exact measure and may report them as being \u2018too small to measure\u2019, in which case a default value of 5 mm should be assigned. Lesions that split or coalesce on treatment . When non-nodal lesions \u2018fragment\u2019, the longest diameters of the fragmented portions should be added together to calculate the target lesion sum. Similarly, as lesions coalesce, a plane between them may be maintained that would aid in obtaining maximal diameter measurements of each individual lesion. If the lesions have truly coalesced such that they are no longer separable, the vector of the longest diameter in this instance should be the maximal longest diameter for the \u2018coalesced lesion\u2019.","title":"Special notes on the assessment of target lesions"},{"location":"medical/recist/#evaluation-of-non-target-lesions","text":"Complete Response (CR) : Disappearance of all non-target lesions and normalization of tumor marker levels. All lymph nodes must be non-pathological in size (<10 mm short axis). Non-CR / Non-PD : Persistence of one or more non-target lesion(s) and/or maintenance of tumor marker levels above normal limits. Progressive Disease (PD) : Unequivocal progression of existing non-target lesions. When patient has measurable disease . To achieve \u2018unequivocal progression\u2019 on the basis of the non-target disease, there must be an overall level of substantial worsening in non-target disease such that, even in presence of SD or PR in target disease, the overall tumor burden has increased sufficiently to merit discontinuation of therapy. A modest \u2018increase\u2019 in the size of one or more non-target lesions is usually not sufficient to qualify for unequivocal progression status. When patient has only non-measurable disease . There is no measurable disease assessment to factor into the interpretation of an increase in non-measurable disease burden. Because worsening in non-target disease cannot be easily quantified, a useful test that can be applied is to consider if the increase in overall disease burden based on change in nonmeasurable disease is comparable in magnitude to the increase that would be required to declare PD for measurable disease. Examples include an increase in a pleural effusion from \u2018trace\u2019 to \u2018large\u2019 or an increase in lymphangitic disease from localized to widespread.","title":"Evaluation of non-target lesions"},{"location":"medical/recist/#new-lesions","text":"The appearance of new malignant lesions denotes disease progression: The finding of a new lesion should be unequivocal (i.e., not attributable to differences in scanning technique, change in imaging modality or findings thought to represent something other than tumor, especially when the patient\u2019s baseline lesions show partial or complete response). If a new lesion is equivocal, for example because of its small size, continued therapy and follow-up evaluation will clarify if it represents truly new disease. If repeat scans confirm there is definitely a new lesion, then progression should be declared using the date of the initial scan. A lesion identified on a follow-up study in an anatomical location that was not scanned at baseline is considered a new lesion and disease progression. It is sometimes reasonable to incorporate the use of FDG-PET scanning to complement CT in assessment of progression (particularly possible \u2018new\u2019 disease). New lesions on the basis of FDG-PET imaging can be identified according to the following algorithm: Negative FDG-PET at baseline, with a positive FDG-PET at follow-up is PD based on a new lesion. No FDG-PET at baseline and a positive FDG-PET at follow-up : If the positive FDG-PET at follow-up corresponds to a new site of disease confirmed by CT, this is PD. If the positive FDG-PET at follow-up is not confirmed as a new site of disease on CT, additional follow-up CT scans are needed to determine if there is truly progression occurring at that site (if so, the date of PD will be the date of the initial abnormal FDG-PET scan). If the positive FDG-PET at follow-up corresponds to a pre-existing site of disease on CT that is not progressing on the basis of the anatomic images, this is not PD. The following table provides a summary of the overall response status calculation at each time point for patients who have measurable disease at baseline. Time point response: Patients with target (+/\u2013 non-target) disease Target lesions Non-target lesions New lesions Overall response CR CR No CR CR Non-CR /non-PD No PR CR NE No PR PR Non-PD /or not all evaluated No PR SD Non-PD /or not all evaluated No SD Not all evaluated Non-PD No NE PD Any Yes or No PD Any PD Yes or No PD Any Any Yes PD When patients have non-measurable (therefore non-target) disease only, the next table is to be used. Time point response: Patients with non-target disease Non-target lesions New lesions Overall response CR No CR Non-CR/non-PD No Non-CR/non-PD 1 Not all evaluated No NE Unequivocal PD Yes or No PD Any Yes PD 1 Non-CR / non-PD is preferred over \u2018Stable Disease\u2019 for non-target disease since SD is increasingly used as an endpoint for assessment of efficacy in some trials. To assign this category when no lesions can be measured is not advised.","title":"New lesions"},{"location":"medical/recist/#confirmation","text":"In non-randomized trials where response is the primary endpoint, confirmation of PR and CR is required to ensure responses identified are not the result of measurement error. This will also permit appropriate interpretation of results in the context of historical data where response has traditionally required confirmation in such trials. However, in all other circumstances, (i.e., in randomized phase II or III trials or studies where stable disease or progression are the primary endpoints), confirmation of response is not required since it will not add value to the interpretation of trial results. However, elimination of the requirement for response confirmation may increase the importance of central review to protect against bias, in particular in studies which are not blinded. In the case of SD, measurements must have met the SD criteria at least once after study entry at a minimum interval (in general not less than 6\u20138 weeks) that is defined in the study protocol. The Best Overall Response (BOR) is the best response recorded from the start of the study treatment until the disease progression/recurrence: Complete Response (CR) upon confirmation at least 4 weeks after first CR Partial Response (PR) upon confirmation at least 4 weeks after first PR Stable Disease (SD) should be met at least once no less than 6-8 weeks after the first dose of trial treatment/baseline assessment, otherwise the best response will be Not Evaluable (NE) Progressive Disease (PD) does not need confirmation The criteria for confirmation of the response is summarized in the following table for the best overall response when confirmation of CR and PR are required: Overall response 1st time point Overall response subsequent time point Best Overall Response CR CR CR CR PR SD, PD or PR 2 CR SD SD provided minimum criteria for2 SD duration met, otherwise, PD CR PD SD provided minimum criteria for SD duration met, otherwise, PD CR NE SD provided minimum criteria for SD duration met, otherwise, NE PR CR PR PR PR PR PR SD SD PR PD SD provided minimum criteria for SD duration met, otherwise, PD PR NE SD provided minimum criteria for SD duration met, otherwise, NE NE NE NE 2 If a CR is truly met at first time point, then any disease seen at a subsequent time point, even disease meeting PR criteria relative to baseline, makes the disease PD at that point (since disease must have reappeared after CR). Best response would depend on whether minimum duration for SD was met. However, sometimes CR may be claimed when subsequent scans suggest small lesions were likely still present and in fact the patient had PR, not CR at the first time point. Under these circumstances, the original CR should be changed to PR and the best response is PR.","title":"Confirmation"},{"location":"medical/recist/#missing-assessments-and-inevaluable-designation","text":"When no imaging/measurement is done at all at a particular time point, the patient is not evaluable (NE) at that time point. If only a subset of lesion measurements are made at an assessment, usually the case is also considered NE at that time point, unless a convincing argument can be made that the contribution of the individual missing lesion(s) would not change the assigned time point response. This would most likely happen in the case of PD.","title":"Missing Assessments and Inevaluable Designation"},{"location":"medical/recist/#recist-faqs","text":"When measuring the longest diameter of target lesions in response to treatment, is the same axis that was used initially used subsequently, even if there is a shape change to the lesion that may have produced a new longest diameter? The longest diameter of the lesion should always be measured even if the actual axis is different from the one used to measure the lesion initially (or at a different time point during followup). The only exception to this is lymph nodes \u2014per RECIST 1.1 the short axis should always be followed and as in the case of target lesions, the vector of the short axis may change on follow-up. What if a single non-target lesion cannot be reviewed (for whatever reason)? Does this negate the overall assessment? Sometimes the major contribution of a single non-target lesion may be in the setting of CR having otherwise been achieved; failure to examine one non-target in that setting will leave you unable to claim CR. It is also possible that the non-target lesion has undergone such substantial progression that it would override the target disease and render the patient PD. However, this is very unlikely, especially if the rest of the measurable disease is stable or responding.","title":"RECIST FAQs"},{"location":"medical/sensitivity-specificity/","text":"Check these websites Estimating sensitivity, specificity, positive and negative predictive values, and other statistics in SAS Sensitivity vs Specificity Sensitivity and specificity are statistical measures of the performance of a binary classification test, also known in statistics as a classification function, that are widely used in medicine: Sensitivity (also called the true positive rate) measures the proportion of actual positives that are correctly identified as such. It is often mistakenly confused with the detection limit, while the detection limit is calculated from the analytical sensitivity , not from the epidemiological sensitivity . Specificity (also called the true negative rate) measures the proportion of actual negatives that are correctly identified as such. Positive predictive value (PPV) is the probability that subjects with a positive screening test truly have the disease. Negative predictive value (NPV) is the probability that subjects with a negative screening test truly don't have the disease.","title":"Sensitivity and Specificity"},{"location":"miscellanea/arrays/","text":"Declaring arrays Link The dimension has to be known in advance (???) There's no way to write an implicit loop through all the elements of the array (???) 1 2 3 4 5 6 7 8 9 10 11 data _null_ ; ARRAY arrayname [ 2 , 3 ] < $ > value11 - value13 ( 0 0 0 ) < $ > value21 - value23 ( 0 0 0 ) ; DO i = 1 TO DIM ( arrayname ) ; arrayname [ i ] = arrayname [ i ] + 1 ; END ; result = CATX ( ' , ' , OF value11 - value13 ) ; PUT result = ; RUN ; Assigning Initial Values to Array Variables or Elements Link The following ARRAY statements illustrate the initialization of numeric and character values: 1 2 ARRAY sizes [ * ] petite small medium large extra_large ( 2 , 4 , 6 , 8 , 10 ); ARRAY cities [ 4 ] $ 10 ( 'New York' 'Los Angeles' 'Dallas' 'Chicago' ); You can also initialize the elements of an array with the same value using an iteration factor, as shown in the following example that initializes 10 elements with a value of 0: 1 ARRAY values [ 10 ] 10 * 0 ; When elements are initialized within an ARRAY statement, the values are automatically retained from one iteration of the DATA step to another; a RETAIN statement is not necessary. Date Arrays Link You can only assign lengths, not formats in an array definition. Use a separate format statement to specify the date format. 1 2 3 4 5 DATA new - SAS - data - set ; SET existing - SAS - data - set ; ARRAY arrayname [ 8 ] element1 - element8 ; FORMAT element1 - element8 ddmmyy10 .; RUN ;","title":"Working with Arrays"},{"location":"miscellanea/arrays/#declaring-arrays","text":"The dimension has to be known in advance (???) There's no way to write an implicit loop through all the elements of the array (???) 1 2 3 4 5 6 7 8 9 10 11 data _null_ ; ARRAY arrayname [ 2 , 3 ] < $ > value11 - value13 ( 0 0 0 ) < $ > value21 - value23 ( 0 0 0 ) ; DO i = 1 TO DIM ( arrayname ) ; arrayname [ i ] = arrayname [ i ] + 1 ; END ; result = CATX ( ' , ' , OF value11 - value13 ) ; PUT result = ; RUN ;","title":"Declaring arrays"},{"location":"miscellanea/arrays/#assigning-initial-values-to-array-variables-or-elements","text":"The following ARRAY statements illustrate the initialization of numeric and character values: 1 2 ARRAY sizes [ * ] petite small medium large extra_large ( 2 , 4 , 6 , 8 , 10 ); ARRAY cities [ 4 ] $ 10 ( 'New York' 'Los Angeles' 'Dallas' 'Chicago' ); You can also initialize the elements of an array with the same value using an iteration factor, as shown in the following example that initializes 10 elements with a value of 0: 1 ARRAY values [ 10 ] 10 * 0 ; When elements are initialized within an ARRAY statement, the values are automatically retained from one iteration of the DATA step to another; a RETAIN statement is not necessary.","title":"Assigning Initial Values to Array Variables or Elements"},{"location":"miscellanea/arrays/#date-arrays","text":"You can only assign lengths, not formats in an array definition. Use a separate format statement to specify the date format. 1 2 3 4 5 DATA new - SAS - data - set ; SET existing - SAS - data - set ; ARRAY arrayname [ 8 ] element1 - element8 ; FORMAT element1 - element8 ddmmyy10 .; RUN ;","title":"Date Arrays"},{"location":"miscellanea/execution-time/","text":"Measure your code execution time Link 1 2 3 4 5 6 7 %let datetime_start = %sysfunc ( TIME ()) ; %put START TIME : %sysfunc ( datetime (), datetime14 .); [ YOUR CODE HERE ] %put END TIME : %sysfunc ( datetime (), datetime14 .); %put TOTAL TIME : %sysfunc ( putn ( %sysevalf ( %sysfunc ( TIME ()) -& datetime_start .), mmss .)) ( mm : ss ) ;","title":"Execution time"},{"location":"miscellanea/execution-time/#measure-your-code-execution-time","text":"1 2 3 4 5 6 7 %let datetime_start = %sysfunc ( TIME ()) ; %put START TIME : %sysfunc ( datetime (), datetime14 .); [ YOUR CODE HERE ] %put END TIME : %sysfunc ( datetime (), datetime14 .); %put TOTAL TIME : %sysfunc ( putn ( %sysevalf ( %sysfunc ( TIME ()) -& datetime_start .), mmss .)) ( mm : ss ) ;","title":"Measure your code execution time"},{"location":"miscellanea/patient-identification/","text":"Dealing with Study Identification Numbers Link Site calculation from the two first numbers of the patient number: Link 1 site = SUBSTR ( PUT ( patient , z4 .), 1 , 2 ); PUT : turns the numeric variable patient into a string ( z4. adds leading zeroes if needed) SUBSTR : takes the first 2 characters starting from position 1 Warning The patient variable has to be numeric , otherwise an format note will be generated. Build a numeric version of your patient variable if it originally it is a character value. Subtract the patient number (e.g. last 4 characters) from a string: Link 1 patient = substr ( patient_code , max ( 1 , length ( patient_code ) - 3 )); Join the site number and the patient number to get a more general ID number for each patient: Link 1 patient = PUT ( nsite , z2 .) || PUT ( npatient , z2 .); Number of Patients into Macrovariable Link 1 2 3 4 5 6 7 8 proc sql noprint ; select count ( distinct pt ) into : npat from library - name . index ( where = ( ITT eq '1' )); quit ; %let npatients = %left ( %trim ( & npat .)); %put npatients =& npatients ;","title":"Patient identification"},{"location":"miscellanea/patient-identification/#dealing-with-study-identification-numbers","text":"","title":"Dealing with Study Identification Numbers"},{"location":"miscellanea/patient-identification/#site-calculation-from-the-two-first-numbers-of-the-patient-number","text":"1 site = SUBSTR ( PUT ( patient , z4 .), 1 , 2 ); PUT : turns the numeric variable patient into a string ( z4. adds leading zeroes if needed) SUBSTR : takes the first 2 characters starting from position 1 Warning The patient variable has to be numeric , otherwise an format note will be generated. Build a numeric version of your patient variable if it originally it is a character value.","title":"Site calculation from the two first numbers of the patient number:"},{"location":"miscellanea/patient-identification/#subtract-the-patient-number-eg-last-4-characters-from-a-string","text":"1 patient = substr ( patient_code , max ( 1 , length ( patient_code ) - 3 ));","title":"Subtract the patient number (e.g. last 4 characters) from a string:"},{"location":"miscellanea/patient-identification/#join-the-site-number-and-the-patient-number-to-get-a-more-general-id-number-for-each-patient","text":"1 patient = PUT ( nsite , z2 .) || PUT ( npatient , z2 .);","title":"Join the site number and the patient number to get a more general ID number for each patient:"},{"location":"miscellanea/patient-identification/#number-of-patients-into-macrovariable","text":"1 2 3 4 5 6 7 8 proc sql noprint ; select count ( distinct pt ) into : npat from library - name . index ( where = ( ITT eq '1' )); quit ; %let npatients = %left ( %trim ( & npat .)); %put npatients =& npatients ;","title":"Number of Patients into Macrovariable"},{"location":"miscellanea/random-list/","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 /* Load library containing patients data set*/ %let path = C : \\ your - custom - path ; libname data \"&path.\\path-to-library\" ; /* Load demographics data set: - keep only patient ID variable - build a new counter variable */ data work . patientslist ; set data . demographics ; counter = _n_ ; keep pt counter ; run ; /* Extract total number of patients from the table */ %macro get_table_size ( inset , macvar ); data _null_ ; set & inset NOBS = size ; call symput ( \"&macvar\" , size ); stop ; run ; %mend ; %let npatients = ; %get_table_size ( work . patientslist , npatients ); %put &= npatients ; /* Transform into a numeric value */ %let totalpatients = % SYSEVALF ( & npatients .); %put &= totalpatients ; /* Calculate number of cases needed for random selection */ %let neededpatients = % SYSFUNC ( sqrt ( & totalpatients .)); /* sqrt(# patients) */ %let neededpatients = % SYSEVALF ( & neededpatients . + 1 ); /* +1 */ %let neededpatients = % SYSFUNC ( ceil ( & neededpatients .)); /* round up */ %put &= neededpatients ; /* Generate the list of random counter index numbers */ proc plan seed = 12345 ; factors selected = & neededpatients . of & totalpatients . random ; output out = work . list ; run ; /* Put all selected random numbers into a macro variable named ParamList */ proc sql noprint ; select selected into : ParamList separated by ' ' from work . list ; quit ; %put ParamList = & ParamList ; /* display list in SAS log to check it */ /* Select the patients from the list that correspond to those indexes */ data work . selectedpatients ; set work . patientslist ; where counter in ( & ParamList .); run ; /* Order selected patient list */ proc sort data = selectedpatients ; by pt ; run ; /* Print out the list of selected patients and print out to a word doc */ ods rtf bodytitle path = \"&path.\" file = \"random_list.doc\" ; proc print data = work . selectedpatients ; var pt ; run ; ods rtf close ;","title":"Generating a Random List"},{"location":"miscellanea/shell/","text":"Examples of Shell Scripting in SAS Link 1 2 3 4 5 6 7 data _null_ ; X % unquote ( % str ( % 'copy \"original path/file to move.txt\" \"new path\\\" /y%' )); X \"cd path-to-enter/\" ; X \"mkdir new_folder\" ; run ;","title":"Shell Scripting in SAS"},{"location":"miscellanea/shell/#examples-of-shell-scripting-in-sas","text":"1 2 3 4 5 6 7 data _null_ ; X % unquote ( % str ( % 'copy \"original path/file to move.txt\" \"new path\\\" /y%' )); X \"cd path-to-enter/\" ; X \"mkdir new_folder\" ; run ;","title":"Examples of Shell Scripting in SAS"},{"location":"miscellanea/time-intervals/","text":"Time periods in months or years Link 1 2 3 4 5 * In years ; interval = YRDIF ( inidat , enddat , 'age' ); * In months ; interval = YRDIF ( inidat , enddat , 'age' ) * 12 ; Time periods in days Link In general, the difference between two SAS dates in days can most easily be calculated as 1 duration = end_date - start_date ; The INTCK function in SAS can calculate the difference between any two dates or datetime values, and return whatever interval you're looking for (days, minutes, hours, weeks, months). 1 2 3 4 5 6 data test ; set test ; by subj ; days = intck ( 'dtday' , date1 , date2 ); put days = ; run ; For hospital stays, you might have special rules about calculating day intervals. For example, a patient who is admitted and charged on the same day might count as a 1-day stay, and a patient who is discharged on the second day might still count as a 1-day stay -- so you might need to modify the formula to add 1 in the case of a same-day discharge--. If your data are such that any record with a hospital admission represents at least a 1-day stay (for reporting purposes), then your formula might be: 1 2 /* return the higher of two values: calculated interval or 1 */ dur = max ( intck ( ' day ' , date_1 , date_2 ) , 1 ) ; The INTICK Function Link Everyone knows that the INTCK function returns the integer count of the number of interval boundaries between two dates, two times, or two datetime values. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 * Date examples ; *--------------- ; years = intck ( ' year ' , ' 01jan2009 ' d , ' 01jan2010 ' d ) ; SEMIYEAR = intck ( ' SEMIYEAR ' , ' 01jan2009 ' d , ' 01jan2010 ' d ) ; quarters = intck ( ' qtr ' , ' 01jan2009 ' d , ' 01jan2010 ' d ) ; months = intck ( ' month ' , ' 01jan2009 ' d , ' 01jan2010 ' d ) ; weeks = intck ( ' week ' , ' 01jan2009 ' d , ' 01jan2010 ' d ) ; days = intck ( ' day ' , ' 01jan2009 ' d , ' 01jan2010 ' d ) ; * Date + time examples ; *---------------------- ; hours = intck ( ' hour ' , ' 01jan2009:00:00:00 ' dt , ' 01jan2010:00:00:00 ' dt ) ; minutes = intck ( ' minute ' , ' 01jan2009:00:00:00 ' dt , ' 01jan2010:00:00:00 ' dt ) ; seconds = intck ( ' second ' , ' 01jan2009:00:00:00 ' dt , ' 01jan2010:00:00:00 ' dt ) ; * Time examples ; *--------------- ; hours = intck ( ' hour ' , ' 00:00:00 ' t , ' 12:00:00 ' t ) ; minutes = intck ( ' minute ' , ' 00:00:00 ' t , ' 12:00:00 ' t ) ; seconds = intck ( ' second ' , ' 00:00:00 ' t , ' 12:00:00 ' t ) ; * Use ' days365 ' to calculate number of years instead of number of interval boundaries ( would be 1 for this case ) ; days365 = intck ( ' day365 ' , ' 31dec2009 ' d , ' 01jan2010 ' d ) ; Using Timepart() and Datepart() Link To extract from a date+time variable only the date or only the time you may use this functions: 1 2 3 4 5 6 7 8 9 10 11 12 13 format a1 b1 date9 .; a0 = '01jan2009:00:00:00' dt ; b0 = '01jan2010:00:00:00' dt ; a1 = datepart ( a0 ); b1 = datepart ( b0 ); days = intck ( 'day' , a1 , b1 ); format a1 b1 date9 .; a0 = '01jan2009:00:00:00' dt ; b0 = '01jan2010:12:00:00' dt ; a1 = timepart ( a0 ); b1 = timepart ( b0 ); hour = intck ( 'hour' , a1 , b1 ); The INTNX Function Link This function increments a date, time, or datetime value by a given interval or intervals, and returns a date, time, or datetime value. 1 2 3 4 5 6 format day week month_ year date9 .; day = intnx ( 'day' , '01FEB2010' d , 7 ); /* +7 days */ week = intnx ( 'week' , '01FEB2010' d , 1 ); /* 01 of Feb 2010 is Monday*/ month_ = intnx ( 'month' , '01FEB2010' d , 2 ); /* +2 month */ year = intnx ( 'year' , '01FEB2010' d , 1 ); /* +1 year */","title":"Time intervals"},{"location":"miscellanea/time-intervals/#time-periods-in-months-or-years","text":"1 2 3 4 5 * In years ; interval = YRDIF ( inidat , enddat , 'age' ); * In months ; interval = YRDIF ( inidat , enddat , 'age' ) * 12 ;","title":"Time periods in months or years"},{"location":"miscellanea/time-intervals/#time-periods-in-days","text":"In general, the difference between two SAS dates in days can most easily be calculated as 1 duration = end_date - start_date ; The INTCK function in SAS can calculate the difference between any two dates or datetime values, and return whatever interval you're looking for (days, minutes, hours, weeks, months). 1 2 3 4 5 6 data test ; set test ; by subj ; days = intck ( 'dtday' , date1 , date2 ); put days = ; run ; For hospital stays, you might have special rules about calculating day intervals. For example, a patient who is admitted and charged on the same day might count as a 1-day stay, and a patient who is discharged on the second day might still count as a 1-day stay -- so you might need to modify the formula to add 1 in the case of a same-day discharge--. If your data are such that any record with a hospital admission represents at least a 1-day stay (for reporting purposes), then your formula might be: 1 2 /* return the higher of two values: calculated interval or 1 */ dur = max ( intck ( ' day ' , date_1 , date_2 ) , 1 ) ;","title":"Time periods in days"},{"location":"miscellanea/time-intervals/#the-intick-function","text":"Everyone knows that the INTCK function returns the integer count of the number of interval boundaries between two dates, two times, or two datetime values. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 * Date examples ; *--------------- ; years = intck ( ' year ' , ' 01jan2009 ' d , ' 01jan2010 ' d ) ; SEMIYEAR = intck ( ' SEMIYEAR ' , ' 01jan2009 ' d , ' 01jan2010 ' d ) ; quarters = intck ( ' qtr ' , ' 01jan2009 ' d , ' 01jan2010 ' d ) ; months = intck ( ' month ' , ' 01jan2009 ' d , ' 01jan2010 ' d ) ; weeks = intck ( ' week ' , ' 01jan2009 ' d , ' 01jan2010 ' d ) ; days = intck ( ' day ' , ' 01jan2009 ' d , ' 01jan2010 ' d ) ; * Date + time examples ; *---------------------- ; hours = intck ( ' hour ' , ' 01jan2009:00:00:00 ' dt , ' 01jan2010:00:00:00 ' dt ) ; minutes = intck ( ' minute ' , ' 01jan2009:00:00:00 ' dt , ' 01jan2010:00:00:00 ' dt ) ; seconds = intck ( ' second ' , ' 01jan2009:00:00:00 ' dt , ' 01jan2010:00:00:00 ' dt ) ; * Time examples ; *--------------- ; hours = intck ( ' hour ' , ' 00:00:00 ' t , ' 12:00:00 ' t ) ; minutes = intck ( ' minute ' , ' 00:00:00 ' t , ' 12:00:00 ' t ) ; seconds = intck ( ' second ' , ' 00:00:00 ' t , ' 12:00:00 ' t ) ; * Use ' days365 ' to calculate number of years instead of number of interval boundaries ( would be 1 for this case ) ; days365 = intck ( ' day365 ' , ' 31dec2009 ' d , ' 01jan2010 ' d ) ;","title":"The INTICK Function"},{"location":"miscellanea/time-intervals/#using-timepart-and-datepart","text":"To extract from a date+time variable only the date or only the time you may use this functions: 1 2 3 4 5 6 7 8 9 10 11 12 13 format a1 b1 date9 .; a0 = '01jan2009:00:00:00' dt ; b0 = '01jan2010:00:00:00' dt ; a1 = datepart ( a0 ); b1 = datepart ( b0 ); days = intck ( 'day' , a1 , b1 ); format a1 b1 date9 .; a0 = '01jan2009:00:00:00' dt ; b0 = '01jan2010:12:00:00' dt ; a1 = timepart ( a0 ); b1 = timepart ( b0 ); hour = intck ( 'hour' , a1 , b1 );","title":"Using Timepart() and Datepart()"},{"location":"miscellanea/time-intervals/#the-intnx-function","text":"This function increments a date, time, or datetime value by a given interval or intervals, and returns a date, time, or datetime value. 1 2 3 4 5 6 format day week month_ year date9 .; day = intnx ( 'day' , '01FEB2010' d , 7 ); /* +7 days */ week = intnx ( 'week' , '01FEB2010' d , 1 ); /* 01 of Feb 2010 is Monday*/ month_ = intnx ( 'month' , '01FEB2010' d , 2 ); /* +2 month */ year = intnx ( 'year' , '01FEB2010' d , 1 ); /* +1 year */","title":"The INTNX Function"},{"location":"other-analysis/HR_vs_OR_vs_RR/","text":"Odds Ratio, Hazard Ratio and Relative Risk Relative Risk Link In medical studies, probability of seeing a certain event in some group is usually called risk, while epidemiologists might prefer the term incidence (Savitz, 1992). For comparison of risks between groups, the ratio of risks, or the relative risk , is a statistic of choice. Formally, if $\\pi_1$ is the probability of the event in group 1, and $\\pi_2$ is the probability of the event in group 2, then the relative risk is $RR=\\frac{\\pi_1}{\\pi_2}$ The reason of preferring relative risk over the difference of risks lies in the fact that the population risks of most diseases are rather small and so differences less dramatic (Walter, 2000). Odds Ratio Link The other statistics, commonly encountered in medical literature, is the odds ratio (Bland and Altman, 2000). Odds are the ratio of the probability of an event occurring in a group, divided by the probability of that event not occurring $odds= \\frac{\\pi}{1-\\pi}$ For example, if probability of death in a group is 0.75, the odds are equal to 3, since the probability of death is three times higher than the probability of surviving. If risk was the same in both groups, the odds would be equal. A comparison of odds, the odds ratio, might then make sense. $OR= \\frac{\\frac{\\pi_1}{1-\\pi_1}}{\\frac{\\pi_2}{1-\\pi_2}}$ This is very different from the relative risk calculated on the same data and may come as a surprise to some readers who are accustomed of thinking of odds ratio as of relative risk (Greenland, 1987). Hazard Ratio Link Comparison Link Relative risk and odds ratio Link The literature dealing with the relation between relative risk and odds ratio is quite extensive. It can be deduced that $OR=RR \\frac{1-\\pi_2}{1-\\pi_1}$ From this we see that OR is always further away from 1 than RR. But, more importantly, we see that the odds ratio is close to the relative risk if probabilities of the outcome are small (Davies et al., 1998). And it is this fact that enables us, most of the time, to approximate the relative risk with the odds ratio. Relative risk and hazard ratio Link https://www.students4bestevidence.net/tutorial-hazard-ratios/ One of the main differences between risk ratio (relative risk) and hazard ratio is that risk ratio does not care about the timing of the event but only about the occurrence of the event by the end of the study (i.e. whether they occurred or not: the total number of events by the end of the study period). In contrast, hazard ratio takes account not only of the total number of events, but also of the timing of each event. So most of the confusion, or wrong perception, probably comes from this 'natural' line of thought: if hazard ratio is $k$ at all times, then the relative risk must be $k$ at all times. And this is of course wrong. Relative risk (RR) is a ratio of two probabilities: probability of an event in one group divided by the probability of the same event in the other group. When studying survival, we have to explicitly state in which time interval we are calculating this probability . Maybe the easiest way to understand that a hazard ratio cannot be equal to the relative risk for any time t is to realize that eventually everybody dies, so the relative risk will approach 1 with time, even though the hazard ratio is constant.","title":"Odds Ratio, Hazard Ratio and Relative Risk"},{"location":"other-analysis/HR_vs_OR_vs_RR/#relative-risk","text":"In medical studies, probability of seeing a certain event in some group is usually called risk, while epidemiologists might prefer the term incidence (Savitz, 1992). For comparison of risks between groups, the ratio of risks, or the relative risk , is a statistic of choice. Formally, if $\\pi_1$ is the probability of the event in group 1, and $\\pi_2$ is the probability of the event in group 2, then the relative risk is $RR=\\frac{\\pi_1}{\\pi_2}$ The reason of preferring relative risk over the difference of risks lies in the fact that the population risks of most diseases are rather small and so differences less dramatic (Walter, 2000).","title":"Relative Risk"},{"location":"other-analysis/HR_vs_OR_vs_RR/#odds-ratio","text":"The other statistics, commonly encountered in medical literature, is the odds ratio (Bland and Altman, 2000). Odds are the ratio of the probability of an event occurring in a group, divided by the probability of that event not occurring $odds= \\frac{\\pi}{1-\\pi}$ For example, if probability of death in a group is 0.75, the odds are equal to 3, since the probability of death is three times higher than the probability of surviving. If risk was the same in both groups, the odds would be equal. A comparison of odds, the odds ratio, might then make sense. $OR= \\frac{\\frac{\\pi_1}{1-\\pi_1}}{\\frac{\\pi_2}{1-\\pi_2}}$ This is very different from the relative risk calculated on the same data and may come as a surprise to some readers who are accustomed of thinking of odds ratio as of relative risk (Greenland, 1987).","title":"Odds Ratio"},{"location":"other-analysis/HR_vs_OR_vs_RR/#hazard-ratio","text":"","title":"Hazard Ratio"},{"location":"other-analysis/HR_vs_OR_vs_RR/#comparison","text":"","title":"Comparison"},{"location":"other-analysis/HR_vs_OR_vs_RR/#relative-risk-and-odds-ratio","text":"The literature dealing with the relation between relative risk and odds ratio is quite extensive. It can be deduced that $OR=RR \\frac{1-\\pi_2}{1-\\pi_1}$ From this we see that OR is always further away from 1 than RR. But, more importantly, we see that the odds ratio is close to the relative risk if probabilities of the outcome are small (Davies et al., 1998). And it is this fact that enables us, most of the time, to approximate the relative risk with the odds ratio.","title":"Relative risk and odds ratio"},{"location":"other-analysis/HR_vs_OR_vs_RR/#relative-risk-and-hazard-ratio","text":"https://www.students4bestevidence.net/tutorial-hazard-ratios/ One of the main differences between risk ratio (relative risk) and hazard ratio is that risk ratio does not care about the timing of the event but only about the occurrence of the event by the end of the study (i.e. whether they occurred or not: the total number of events by the end of the study period). In contrast, hazard ratio takes account not only of the total number of events, but also of the timing of each event. So most of the confusion, or wrong perception, probably comes from this 'natural' line of thought: if hazard ratio is $k$ at all times, then the relative risk must be $k$ at all times. And this is of course wrong. Relative risk (RR) is a ratio of two probabilities: probability of an event in one group divided by the probability of the same event in the other group. When studying survival, we have to explicitly state in which time interval we are calculating this probability . Maybe the easiest way to understand that a hazard ratio cannot be equal to the relative risk for any time t is to realize that eventually everybody dies, so the relative risk will approach 1 with time, even though the hazard ratio is constant.","title":"Relative risk and hazard ratio"},{"location":"other-analysis/adjusted-pvalue/","text":"Adjusted p-values as Part of Multiple Comparisons Link We were commonly asked why multiple comparisons tests following one-way (or two-way) ANOVA don't report individual P values for each comparison, rather than simply reporting which comparisons are statistically significant. It sounds like a simple question, but the answer is not so simple. You have two options: Don't correct for multiple comparisons at all. Making individual comparisons after ANOVA, without taking into account the number of comparisons. It is called the unprotected Fisher's LSD test. Report multiplicity adjusted p-values. What are adjusted p-values? Link The definition of the adjusted p-value is the answer to this question: What is the smallest significance level, when applied to an entire family of comparisons, at which a particular comparison will be deemed statistically significant? In other words, the adjusted p-value is the smallest familywise significance level at which a particular comparison will be declared statistically significant as part of the multiple comparison testing. A separate adjusted p-value is computed for each comparison in a family of comparisons, but its value depends on the entire family. The adjusted p-value for one particular comparison would have a different value if there were a different number of comparisons or if the data in the other comparisons were changed. Each comparison will have a unique adjusted P value. But these P values are computed from all the comparisons, and really can't be interpreted for just one comparison. If you added another group to the ANOVA, all of the adjusted P values would change. Why, When and How to Adjust Your p-values? Link","title":"Adjusted P-value"},{"location":"other-analysis/adjusted-pvalue/#adjusted-p-values-as-part-of-multiple-comparisons","text":"We were commonly asked why multiple comparisons tests following one-way (or two-way) ANOVA don't report individual P values for each comparison, rather than simply reporting which comparisons are statistically significant. It sounds like a simple question, but the answer is not so simple. You have two options: Don't correct for multiple comparisons at all. Making individual comparisons after ANOVA, without taking into account the number of comparisons. It is called the unprotected Fisher's LSD test. Report multiplicity adjusted p-values.","title":"Adjusted p-values as Part of Multiple Comparisons"},{"location":"other-analysis/adjusted-pvalue/#what-are-adjusted-p-values","text":"The definition of the adjusted p-value is the answer to this question: What is the smallest significance level, when applied to an entire family of comparisons, at which a particular comparison will be deemed statistically significant? In other words, the adjusted p-value is the smallest familywise significance level at which a particular comparison will be declared statistically significant as part of the multiple comparison testing. A separate adjusted p-value is computed for each comparison in a family of comparisons, but its value depends on the entire family. The adjusted p-value for one particular comparison would have a different value if there were a different number of comparisons or if the data in the other comparisons were changed. Each comparison will have a unique adjusted P value. But these P values are computed from all the comparisons, and really can't be interpreted for just one comparison. If you added another group to the ANOVA, all of the adjusted P values would change.","title":"What are adjusted p-values?"},{"location":"other-analysis/adjusted-pvalue/#why-when-and-how-to-adjust-your-p-values","text":"","title":"Why, When and How to Adjust Your p-values?"},{"location":"other-analysis/clustering/","text":"Introduction to Cluster Analysis Link When subjects are sampled, randomized or allocated by clusters, several statistical problems arise. If observations within a cluster are correlated, one of the assumptions of estimation and hypothesis testing is violated. Because of this correlation, the analyses must be modified to take into account the cluster design effect. When cluster designs are used, there are two sources of variations in the observations. The first is the one between subjects within a cluster , and the second is the variability among clusters . These two sources of variation cause the variance to inflate and must be taken into account in the analysis. Gettin' Ready for a Cluster Analysis Link Check for Missing Data Link Variables with missing data should be excluded from the calculation unless they can be imputed. 1 2 3 4 DATA SAS - data - set - without - missing ; SET SAS - data - set - with - missing ; IF CMISS ( OF _ALL_ ) THEN DELETE ; RUN ; Dealing with Categorical Variables Link Composite variables Link Other questionnaire data like binary (yes/no questions) or a spectrum of responses can be transformed into composite variables to capture multiple questions into a ranked ordinal scale . A composite variable is a variable created by combining two or more individual variables, called indicators, into a single variable. Each indicator alone doesn't provide sufficient information, but altogether they can represent the more complex concept. A lot of work goes into creating composite variables. The indicators of the multidimensional concept must be specified. It's important that each indicator contribute unique information to the final score. The formula for combining the indicators into a single score, called aggregating data, must be established. The computation involved will depend on the type of data that is being aggregated. To aggregate the data, raw scores might be summed, averaged, transformed, and/or weighted. Hot encoding Link Check this website for a macro to generate dummy variables. Ordinal Categorical Variables Link If your categorical variables have an ordinal meaning you can create an auxiliary numeric variable with indexes representing the ordinal scale and use this new variable, which can be standardized, for the analysis. Methods for data reduction Link You may need to reduce the number of variables to include in the analysis. There are several methods for this: Principal Component Analysis with PROC FACTOR Variable Reduction for Modeling using PROC VARCLUS Factor Analysis Link Factor analysis is a method of data reduction. It does this by seeking underlying unobservable variables ( latent variables ) that are reflected in the observed variables ( manifest variables ). There are many different methods that can be used to conduct a factor analysis (such as principal axis factor, maximum likelihood, generalized least squares, unweighted least squares). Each of them generates uncorrelated factors. There are also many different types of rotations that can be done after the initial extraction of factors, including orthogonal rotations, such as varimax and equimax, which impose the restriction that the factors cannot be correlated, and oblique rotations, such as promax, which allow the factors to be correlated with one another. It is generally considered that using a rotation in factor analysis will produce more interpretable results. If the factor analysis is being performed specifically to gain an explanation of what factors or groups exist in the data or to confirm hypothesized assumptions about the data, rotation can be especially helpful. Factor patterns can be rotated through two different ways: Orthogonal rotations which retain uncorrelated factors Oblique rotations which create correlated factors While arguments exist supporting both types of rotation methods, factor analysis which uses an orthogonal rotation often creates a solution that is easier to grasp and interpret than a solution obtained from an oblique rotation. You also need to determine the number of factors that you want to extract. Normally, the objetive during the initial factor analysis is to determine the minimum number of factors that will adequately account for the covariation among the larger number of analysis variables. This objective can be achieved by using any of the initial factoring methods. Given the number of factor analytic techniques and options, it is not surprising that different analysts could reach very different results analyzing the same data set. However, all analysts are looking for simple structure. Simple structure is pattern of results such that each variable loads highly onto one and only one factor . Factor analysis is a technique that requires a large sample size because it is based on the correlation matrix of the variables involved, and correlations usually need a large sample size before they stabilize. Advisable sample size 50 cases is very poor, 100 is poor, 200 is fair, 300 is good, 500 is very good, and 1000 or more is excellent As a rule of thumb, a bare minimum of 10 observations per variable is necessary to avoid computational difficulties 1 2 3 PROC FACTOR DATA = SAS - data - set NFACTORS = 3 CORR SCREE EV REORDER ROTATE = VARIMAX METHOD = PRINIT PRIORS = SMC ; VAR var1 var2 var3 ... varn ; RUN ; METHOD= specifies what type of method is to be used to extract the initial factors. The Principal Components Analysis (PCA) method ( PRINCIPAL ) simply transforms the set of variables into another set of variables; that is, the data is summarized by means of a linear combination of the observed data. This transformation is performed when the objective is to account for as much variation as possible in the data. With PCA, the first component/factor is defined in such a way that the largest amount of variance in the data is explained by the first component. The second component/factor explains the second most about the variance in the data and is perpendicular (thus, uncorrelated) to the first component. The remaining components/factors are found in a similar manner. MINEIGEN = specifies the smallest eigenvalue for which a factor is retained. NFACTOR = specifies the number of factors to be extracted from the data. The default value, if this option is not specified, is the number of variables in the data. The optimum number can be selected by first running PROC FACTOR without the NFACTOR option and analyzing the eigenvalues and SCREE plot. REORDER : This option reorders the output, so the variables that explain the largest amount of the variance for each factor are printed in descending order down to those that explain the smallest amount of the variance for each factor. CORR generates the Correlations table containing the correlations between the original variables (the ones specified on the VAR statement). Before conducting a principal components analysis, you want to check the correlations between the variables. If any of the correlations are too high (say above .9), you may need to remove one of the variables from the analysis, as the two variables seem to be measuring the same thing. Another alternative would be to combine the variables in some way (perhaps by taking the average). If the correlations are too low, say below .1, then one or more of the variables might load only onto one factor (in other words, make its own factor). EV displays the eigenvectors of the reduced correlation matrix, of which the diagonal elements are replaced with the communality estimates. SCREE or PLOTS=SCREE graph the eigenvalue against the factor number. The ploted values are contained in the Initial Factor Method: Iterated Principal Factor Analysis table: Iteration : This column lists the number of the iteration. Change : When the change becomes smaller than the criterion, the iterating process stops. The numbers in this column are the largest absolute difference between iterations. The difference given for the first iteration is the difference between the values at the first iteration and the squared multiple correlations (sometimes called iteration 0). Communalities : These are the communality estimates at each iteration. For each iteration, the communality for each variable is listed. PRIORS=SMC enables the squared multiple correlation to be used on the diagonal of the correlation matrix. If this option is not used, 1\u2019s are on the diagonal, and you will do a PCA instead of a principal axis factor analysis. The Prior Communality Estimates: SMC table gives the communality estimates prior to the rotation. The communalities (also known as h2) are the estimates of the variance of the factors, as opposed to the variance of the variable which includes measurement error. This table contains different values: Eigenvalue : This is the initial eigenvalue. An eigenvalue is the variance of the factor. Because this is an unrotated solution, the first factor will account for the most variance, the second will account for the second highest amount of variance, and so on. Some of the eigenvalues are negative because the matrix is not of full rank. This means that there are probably only four dimensions (corresponding to the four factors whose eigenvalues are greater than zero). Although it is strange to have a negative variance, this happens because the factor analysis is only analyzing the common variance, which is less than the total variance. If we were doing a principal components analysis, we would have had 1\u2019s on the diagonal, which means that all of the variance is being analyzed (which is another way of saying that we are assuming that we have no measurement error), and we would not have negative eigenvalues. In general, it is not uncommon to have negative eigenvalues. Difference : This column gives the difference between the eigenvalues and allows you to see how quickly the eigenvalues are decreasing. Proportion : This is the proportion of the total variance that each factor accounts for. Cumulative : This is the sum of the proportion column. Eigenvalues of the Reduced Correlation Matrix table contains: Eigenvalue : This is the eigenvalue obtained after the principal axis factoring but before the varimax rotation . Difference , Proportion and Cumulative show the same information described for the Prior Communality Estimates: SMC table The Eigenvectors table shows the linear combinations of the original variables. They tell you about the strength of the relationship between the original (manifest) variables and the (latent) factors. The Factor Pattern table contains the unrotated factor loadings, which are the correlations between the variable and the factor. Because these are correlations, possible values range from -1 to +1 . The Final Communality Estimates table shows the proportion of each variable\u2019s variance that can be explained by the retained factors prior to the rotation. Variables with high values are well represented in the common factor space, while variables with low values are not well represented. They are the reproduced variances from the factors that you have extracted. You can find these values on the diagonal of the reproduced correlation matrix. The Orthogonal Transformation Matrix table shows the matrix by which you multiply the unrotated factor matrix to get the rotated factor matrix. The Rotated Factor Pattern table contains the rotated factor loadings, which are the correlations between the variable and the factor. Because these are correlations, possible values range from -1 to +1 . The Factor columns in this table are the rotated factors that have been extracted. These are the factors that analysts are most interested in and try to name looking at the items that load highly on it. The second factor might be called \"relating to students\" because items like \"instructor is sensitive to students\" and \"instructor allows me to ask questions\" load highly on it. The third factor has to do with comparisons to other instructors and courses. ROTATE = PROMAX | VARIMAX invokes one of the rotation methods available. If no method is specified, the SAS default is to use no rotation. With an orthogonal rotation, such as the VARIMAX , the factors are not permitted to be correlated. Orthogonal transformations simplify the interpretation of the factors by maximizing the variances of the squared loadings for each factor, which are the columns of the factor pattern. Please note that with orthogonal rotations the factor pattern and the factor structure matrices are the equal. With an oblique rotation, such as a PROMAX rotation, the factors are permitted to be correlated with one another and both factor pattern and factor structure matrices are produced. The factor pattern matrix gives the linear combination of the variables that make up the factors. The factor structure matrix presents the correlations between the variables and the factors. To completely interpret an oblique rotation one needs to take into account both the factor pattern and the factor structure matrices and the correlations among the factors. Check these websites Factor Analysis: SAS Annotated Output PROC FACTOR : How to Interpret the Output of a Real-World Example Standardize your Data Link When performing multivariate analysis, having variables that are measured at different scales can influence the numerical stability and precision of the estimators. Standardizing the data prior to performing statistical analysis can often prevent this problem. 1 2 3 4 5 6 7 8 DATA SAS - data - set ; SET SAS - data - set - unformatted ; format variable1 variable2 variable3 10 . 4 ; RUN ; PROC STANDARD DATA = SAS - data - set out = SAS - output - data - set MEAN = 0 STD = 1 ; VAR variable1 variable2 variable3 ; RUN ; Warning Do not forget to change the format of your numerical data and increase the number of decimal places before performing the standardization. Otherwise you may lose a lot of details on this process that can be crucial for the data analysis. Check these websites Standardization Procedures Standardization of Variables in Cluster Analysis SAS Procedures to Perform Cluster Analysis Link Ward's minimum-variance hierarchical clustering method using agglomerative (bottom-up) approach and Ward's linkage. Check these websites Introduction to Clustering Procedures PROC CLUSTER : Hierarchical Cluster Analysis Link The CLUSTER procedure hierarchically clusters the observations in a SAS data set by using one of 11 methods. The data can be coordinates or distances. All methods are based on the usual agglomerative hierarchical clustering procedure. Each observation begins in a cluster by itself. The two closest clusters are merged to form a new cluster that replaces the two old clusters. Merging of the two closest clusters is repeated until only one cluster is left. The various clustering methods differ in how the distance between two clusters is computed. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 data t ; input cid $ 1 - 2 income educ ; cards ; c1 5 5 c2 6 6 c3 15 14 c4 16 15 c5 25 20 c6 30 19 run ; proc cluster simple noeigen method = centroid rmsstd rsquare nonorm out = tree ; id cid ; var income educ ; run ; The SIMPLE option displays simple, descriptive statistics. The NOEIGEN option suppresses computation of eigenvalues. Specifying the NOEIGEN option saves time if the number of variables is large, but it should be used only if the variables are nearly uncorrelated or if you are not interested in the cubic clustering criterion. The METHOD= specification determines the clustering method used by the procedure. Here, we are using CENTROID method. The Centroid Distance that appears in the output is simply the Euclidian distance between the centroid of the two clusters that are to be joined or merged. It is a measure of the homogeneity of merged clusters and the value should be small. The RMSSTD option displays the root-mean-square standard deviation of each cluster. RMSSTD is the pooled standard deviation of all the variables forming the cluster. Since the objective of cluster analysis is to form homogeneous groups, the RMSSTD of a cluster should be as small as possible. The RSQUARE option displays the $R^2$ ( RSQ ) and semipartial $R^2$ ( SPRSQ ) to evaluate cluster solution. RSQ measures the extent to which groups or clusters are different from each other (so, when you have just one cluster RSQ value is, intuitively, zero). Thus, the RSQ value should be high. SPRSQ is a measure of the homogeneity of merged clusters, i.e. the loss of homogeneity due to combining two groups or clusters to form a new group or cluster. Thus, its value should be small to imply that we are merging two homogeneous groups. The NONORM option prevents the distances from being normalized to unit mean or unit root mean square with most methods. The values of the ID variable identify observations in the displayed cluster history and in the OUTTREE= data set. If the ID statement is omitted, each observation is denoted by OBn , where n is the observation number. The VAR statement lists numeric variables to be used in the cluster analysis. If you omit the VAR statement, all numeric variables not listed in other statements are used. PROC FASTCLUS : Disjoint Cluster Analysis Link The FASTCLUS procedure performs a disjoint cluster analysis on the basis of distances computed from one or more quantitative variables. The observations are divided into clusters such that every observation belongs to one and only one cluster ; the clusters do not form a tree structure as they do in the CLUSTER procedure. If you want separate analyses for different numbers of clusters, you can run PROC FASTCLUS once for each analysis. The FASTCLUS procedure requires time proportional to the number of observations and thus can be used with much larger data sets than PROC CLUSTER . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 data t2 ; input cid $ 1 - 2 income educ ; cards ; c1 5 5 c2 6 6 c3 15 14 c4 16 15 c5 25 20 c6 30 19 run ; proc fastclus radius = 0 replace = full maxclusters = 3 maxiter = 20 list distance ; id cid ; var income educ ; run ; You must specify either the MAXCLUSTERS= or the RADIUS= argument in the PROC FASTCLUS statement. The RADIUS= option establishes the minimum distance criterion for selecting new seeds. No observation is considered as a new seed unless its minimum distance to previous seeds exceeds the value given by the RADIUS= option. The default value is 0. The MAXCLUSTERS= option specifies the maximum number of clusters allowed. If you omit the MAXCLUSTERS= option, a value of 100 is assumed. The REPLACE= option specifies how seed replacement is performed. FULL requests default seed replacement. PART requests seed replacement only when the distance between the observation and the closest seed is greater than the minimum distance between seeds. NONE suppresses seed replacement. RANDOM selects a simple pseudo-random sample of complete observations as initial cluster seeds. The MAXITER= option specifies the maximum number of iterations for recomputing cluster seeds. When the value of the MAXITER= option is greater than 0, each observation is assigned to the nearest seed, and the seeds are recomputed as the means of the clusters. The LIST option lists all observations, giving the value of the ID variable (if any), the number of the cluster to which the observation is assigned, and the distance between the observation and the final cluster seed. The DISTANCE option computes distances between the cluster means. The ID variable, which can be character or numeric, identifies observations on the output when you specify the LIST option. The VAR statement lists the numeric variables to be used in the cluster analysis. If you omit the VAR statement, all numeric variables not listed in other statements are used. The cluster analysis may converge to a solution at the $n^{th}$ iteration because the change in cluster seeds at this iteration is less than the convergence criterion. Note that a zero change in the centroid of the cluster seeds for the $n^{th}$ iteration implies that the reallocation did not result in any reassignment of observations. The statistics used for the evaluation of the cluster solution are the same as in the hierarchical cluster analysis. The cluster solution can also be evaluated with respect to each clustering variable . If the measurement scales are not the same, then for each variable one should obtain the ratio of the respective Within STD to the Total STD , and compare this ratio across the variables. Interesting Examples Multivariate Statistical Analysis in SAS: Segmentation and Classification of Behavioral Data Mixed Clustering Link On large data sets a useful methodology consists first in summarizing the observations in a large enough number of clusters (100 may be a standard value) and then applying a hierarchical clustering technique for aggregating these groups (Example here ). This procedure has the advantages of the hierarchical method for showing an optimal number of clusters and solves the difficulty of the too high initial number of observations by first clustering them, using a non hierarchical method, in a smaller number of clusters. This number is a parameter of the procedure; it must be high enough in order not to impose a prior partitionning of the data. PROC VARCLUS : Variable Clustering Link The VARCLUS procedure divides a set of numeric variables into disjoint or hierarchical clusters. PROC VARCLUS tries to maximize the variance that is explained by the cluster components, summed over all the clusters. In an ordinary principal component analysis, all components are computed from the same variables, and the first principal component is orthogonal to the second principal component and to every other principal component. In PROC VARCLUS , each cluster component is computed from a set of variables that is different from all the other cluster components. The first principal component of one cluster might be correlated with the first principal component of another cluster. Hence, the PROC VARCLUS algorithm is a type of oblique component analysis. PROC VARCLUS can be used as a variable-reduction method . A large set of variables can often be replaced by the set of cluster components with little loss of information. A given number of cluster components does not generally explain as much variance as the same number of principal components on the full set of variables, but the cluster components are usually easier to interpret than the principal components, even if the latter are rotated. 1 2 3 PROC VARCLUS DATA = SAS - data - set MAXEIGEN = 0 . 7 OUTTREE = fortree short noprint ; VAR variable1 variable2 variable3 ; RUN ; PROC TREE Link The TREE procedure produces a tree diagram from a data set created by the CLUSTER or VARCLUS procedure that contains the results of hierarchical clustering as a tree structure. 1 2 3 4 PROC TREE DATA = tree OUT = clus3 NCLUSTERS = 3 ; ID id - variable ; COPY variable1 variable2 variable3 ; RUN ; The TREE procedure produces a tree diagram, also known as a dendrogram or phenogram, using a data set created by the CLUSTER procedure. The CLUSTER procedure creates output data sets that contain the results of hierarchical clustering as a tree structure . The TREE procedure uses the output data set to produce a diagram of the tree structure. The NCLUSTERS= option specifies the number of clusters desired in the OUT= data set. The ID variable is used to identify the objects (leaves) in the tree on the output. The ID variable can be a character or numeric variable of any length. The COPY statement specifies one or more character or numeric variables to be copied to the OUT= data set. Choosing the Optimal Number of Clusters for the Analysis Link In most cases, before using a clustering technique you have no prior idea of the number of clusters which will give the better differenciation of the data. The main objective is to summarize the data in the best way possible, i.e. getting a compromise between a good degree of differentiation and a not too high number of clusters. For hierarchical clustering try the Sarle's Cubic Clustering Criterion in PROC CLUSTER: plot CCC versus the number of clusters and look for peaks where ccc > 3 or look for local peaks of pseudo-F statistic ( PSF ) combined with a small value of the pseudo-t^2 statistic ( PST2 ) and a larger pseudo t^2 for the next cluster fusion. For K-Means clustering use this approach on a sample of your data to determine the max limit for k and assign it to the maxc= option in PROC FASTCLUS on the complete data. For K-means cluster analysis, one can use PROC FASTCLUS like PROC FASTCLUS DATA=SAS-data-set OUT=out MAXC=4 MAXITER=20; and change the number defined by MAXC= , and run a number of times, then compare the Pseduo F and CCC values, to see which number of clusters gives peaks. You can also use PROC CLUSTER PROC CLUSTER data=mydata METHOD=WARD out=out ccc pseudo print=15; to find the number of clusters with pseudo F , pseudo-$t^2$ and CCC , and also look at junp in Semipartial R-Square . Sometimes these indications do not agree to each other. which indicator is more reliable? If you are doubting between 2 k-values, you can use Beale's F-type statistic to determine the final number of clusters. It will tell you whether the larger solution is significantly better or not (in the latter case the solution with fewer clusters is preferable). This technique is discussed in the \"Applied Clustering Techniques\" course notes. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 * Define the variables for clustering ; *------------------------------------ ; %let varlist = var1 var2 var3 var4 var5 ; * Macro with the cluster procedure , to call it with different number of clusters ; *------------------------------------------------------------------------------- ; % MACRO CLUSTERSIZE ( datain = , dataout = , maxclusters = , maxiter = ); proc fastclus data =& datain . out =& dataout . & maxclusters . outstat = statdata & maxclusters . maxclusters =& maxclusters . maxiter =& maxiter . noprint ; id patient ; var & varlist .; run ; title ; % MEND ; * Calcultion of the cluster analysis statistics for 1 - 20 clusters and data set creation for elbow plot with RSQ values ; *--------------------------------------------------------------------------------------------------------------------- ; %macro statCLUSTER ; %do k = 1 %to 20 ; % CLUSTERSIZE ( datain = SAS - data - set , dataout = clusterdata , maxclusters =& k ., maxiter = 1000 ); data clusrsq & k .; set statdata & k .; nclust =& k .; if _type_ = ' RSQ ' ; keep nclust over_all ; run ; %end ; %mend ; %stat CLUSTER ; data clus_rsq ; set clusrsq1 clusrsq2 clusrsq3 clusrsq4 clusrsq5 clusrsq6 clusrsq7 clusrsq8 clusrsq9 clusrsq10 clusrsq11 clusrsq12 clusrsq13 clusrsq14 clusrsq15 clusrsq16 clusrsq17 clusrsq18 clusrsq19 clusrsq20 ; run ; * Remove useless data sets ; *------------------------- ; proc datasets lib = work nowarn nolist nodetails ; delete clusrsq : statdata : clusterdata : ; run ; quit ; * Plot elbow curve using r - square values highlighting the best candidates to optimum number of clusters ; *------------------------------------------------------------------------------------------------------ ; symbol1 color = blue interpol = join ; axis1 label = ( ' Number of clusters in the analysis ' ) order = ( 0 to 15 by 1 ) reflabel = ( j = c h = 9 pt ' Candidate 1 ' ' Candidate 2 ' ); axis2 label = ( ' R ^ 2 values ' j = c ); proc gplot data = clus_rsq ; plot over_all * nclust / haxis = axis1 vaxis = axis2 href = 3 5 ; run ; Check these websites The Number of Clusters Further Examination of the Cluster Analysis Solution Link Visualizing the Results Link To interpret a cluster analysis solution, the first thing you want to try is to graph the cluster in a scatter plot to see whether or not they overlap with each other in terms of their location in the $p-$dimensional space. If the vectors have a high dimensionality, we use Canonical Discriminant Analysis (CDA) . It is a dimension-reduction technique related to principal component analysis and canonical correlation . It creates a smaller number of variables that are linear combinations of the original $p$ clustering variables. The new variables, called canonical variables, are ordered in terms of the proportion of variance in the clustering variables that is accounted for by each of the canonical variables. Usually, the majority of the variants in the clustering variable will be accounted for by the first couple of canonical varaibles and those are the variables we can plot. In SAS we can use the CANDISC procedure to create the canonical variables for our cluster analysis output data set that has the cluster assignment variable that we created when we ran the cluster analysis. 1 2 3 4 PROC CANDISC DATA = clusterdata OUT = candata ; CLASS cluster - assignment - variable ; VAR clustering1 clustering2 ; RUN ; The OUT= is the output data set that includes the canonical variables that are estimated by the canonical discriminate analysis. The CLASS variable (mandatory) is the cluster assignment variable which is a categorical variable. In the VAR statement we list the clustering variables You can then plot the first two canonical variables using the SGPLOT procedure: 1 2 3 PROC SGPLOT DATA = candata ; SCATTER Y = can2 X = can1 / GROUP = cluster - assignment - variable ; RUN ; Let's analyze the following example result for a 4-cluster analysis. What this shows is that the observations in clusters 1 and 4 are densely packed, meaning they are pretty highly correlated with each other, and within cluster variance is relatively low . In addition, they do not overlap very much with the other clusters. The observations from cluster 2 are a little more spread out, indicating less correlation among the observations and higher within cluster variance. But generally, the cluster is relatively distinct with the exception of some observations which are closer to clusters 1 and 4 indicating some overlap with these clusters. However, cluster 3 is all over the place. There is come indication of a cluster but the observations are spread out more than the other clusters. This means that the within cluster variance is high as there is less correlation between the observations in this cluster, so we do not really know what is going to happen with that cluster. What this suggests is that the best cluster solution may have fewer than 4 clusters , meaning that it would be especially important to further evaluate the cluster solutions with fewer than four clusters. Check these websites Discriminant Function Analysis in SAS (UCLA) Introduction to Discriminant Procedures Cluster Means and Standard Deviations Link You can also check these values per cluster to detect possible similarities between groups and detect the most different ones.","title":"Clustering"},{"location":"other-analysis/clustering/#introduction-to-cluster-analysis","text":"When subjects are sampled, randomized or allocated by clusters, several statistical problems arise. If observations within a cluster are correlated, one of the assumptions of estimation and hypothesis testing is violated. Because of this correlation, the analyses must be modified to take into account the cluster design effect. When cluster designs are used, there are two sources of variations in the observations. The first is the one between subjects within a cluster , and the second is the variability among clusters . These two sources of variation cause the variance to inflate and must be taken into account in the analysis.","title":"Introduction to Cluster Analysis"},{"location":"other-analysis/clustering/#gettin-ready-for-a-cluster-analysis","text":"","title":"Gettin' Ready for a Cluster Analysis"},{"location":"other-analysis/clustering/#check-for-missing-data","text":"Variables with missing data should be excluded from the calculation unless they can be imputed. 1 2 3 4 DATA SAS - data - set - without - missing ; SET SAS - data - set - with - missing ; IF CMISS ( OF _ALL_ ) THEN DELETE ; RUN ;","title":"Check for Missing Data"},{"location":"other-analysis/clustering/#dealing-with-categorical-variables","text":"","title":"Dealing with Categorical Variables"},{"location":"other-analysis/clustering/#composite-variables","text":"Other questionnaire data like binary (yes/no questions) or a spectrum of responses can be transformed into composite variables to capture multiple questions into a ranked ordinal scale . A composite variable is a variable created by combining two or more individual variables, called indicators, into a single variable. Each indicator alone doesn't provide sufficient information, but altogether they can represent the more complex concept. A lot of work goes into creating composite variables. The indicators of the multidimensional concept must be specified. It's important that each indicator contribute unique information to the final score. The formula for combining the indicators into a single score, called aggregating data, must be established. The computation involved will depend on the type of data that is being aggregated. To aggregate the data, raw scores might be summed, averaged, transformed, and/or weighted.","title":"Composite variables"},{"location":"other-analysis/clustering/#hot-encoding","text":"Check this website for a macro to generate dummy variables.","title":"Hot encoding"},{"location":"other-analysis/clustering/#ordinal-categorical-variables","text":"If your categorical variables have an ordinal meaning you can create an auxiliary numeric variable with indexes representing the ordinal scale and use this new variable, which can be standardized, for the analysis.","title":"Ordinal Categorical Variables"},{"location":"other-analysis/clustering/#methods-for-data-reduction","text":"You may need to reduce the number of variables to include in the analysis. There are several methods for this: Principal Component Analysis with PROC FACTOR Variable Reduction for Modeling using PROC VARCLUS","title":"Methods for data reduction"},{"location":"other-analysis/clustering/#factor-analysis","text":"Factor analysis is a method of data reduction. It does this by seeking underlying unobservable variables ( latent variables ) that are reflected in the observed variables ( manifest variables ). There are many different methods that can be used to conduct a factor analysis (such as principal axis factor, maximum likelihood, generalized least squares, unweighted least squares). Each of them generates uncorrelated factors. There are also many different types of rotations that can be done after the initial extraction of factors, including orthogonal rotations, such as varimax and equimax, which impose the restriction that the factors cannot be correlated, and oblique rotations, such as promax, which allow the factors to be correlated with one another. It is generally considered that using a rotation in factor analysis will produce more interpretable results. If the factor analysis is being performed specifically to gain an explanation of what factors or groups exist in the data or to confirm hypothesized assumptions about the data, rotation can be especially helpful. Factor patterns can be rotated through two different ways: Orthogonal rotations which retain uncorrelated factors Oblique rotations which create correlated factors While arguments exist supporting both types of rotation methods, factor analysis which uses an orthogonal rotation often creates a solution that is easier to grasp and interpret than a solution obtained from an oblique rotation. You also need to determine the number of factors that you want to extract. Normally, the objetive during the initial factor analysis is to determine the minimum number of factors that will adequately account for the covariation among the larger number of analysis variables. This objective can be achieved by using any of the initial factoring methods. Given the number of factor analytic techniques and options, it is not surprising that different analysts could reach very different results analyzing the same data set. However, all analysts are looking for simple structure. Simple structure is pattern of results such that each variable loads highly onto one and only one factor . Factor analysis is a technique that requires a large sample size because it is based on the correlation matrix of the variables involved, and correlations usually need a large sample size before they stabilize. Advisable sample size 50 cases is very poor, 100 is poor, 200 is fair, 300 is good, 500 is very good, and 1000 or more is excellent As a rule of thumb, a bare minimum of 10 observations per variable is necessary to avoid computational difficulties 1 2 3 PROC FACTOR DATA = SAS - data - set NFACTORS = 3 CORR SCREE EV REORDER ROTATE = VARIMAX METHOD = PRINIT PRIORS = SMC ; VAR var1 var2 var3 ... varn ; RUN ; METHOD= specifies what type of method is to be used to extract the initial factors. The Principal Components Analysis (PCA) method ( PRINCIPAL ) simply transforms the set of variables into another set of variables; that is, the data is summarized by means of a linear combination of the observed data. This transformation is performed when the objective is to account for as much variation as possible in the data. With PCA, the first component/factor is defined in such a way that the largest amount of variance in the data is explained by the first component. The second component/factor explains the second most about the variance in the data and is perpendicular (thus, uncorrelated) to the first component. The remaining components/factors are found in a similar manner. MINEIGEN = specifies the smallest eigenvalue for which a factor is retained. NFACTOR = specifies the number of factors to be extracted from the data. The default value, if this option is not specified, is the number of variables in the data. The optimum number can be selected by first running PROC FACTOR without the NFACTOR option and analyzing the eigenvalues and SCREE plot. REORDER : This option reorders the output, so the variables that explain the largest amount of the variance for each factor are printed in descending order down to those that explain the smallest amount of the variance for each factor. CORR generates the Correlations table containing the correlations between the original variables (the ones specified on the VAR statement). Before conducting a principal components analysis, you want to check the correlations between the variables. If any of the correlations are too high (say above .9), you may need to remove one of the variables from the analysis, as the two variables seem to be measuring the same thing. Another alternative would be to combine the variables in some way (perhaps by taking the average). If the correlations are too low, say below .1, then one or more of the variables might load only onto one factor (in other words, make its own factor). EV displays the eigenvectors of the reduced correlation matrix, of which the diagonal elements are replaced with the communality estimates. SCREE or PLOTS=SCREE graph the eigenvalue against the factor number. The ploted values are contained in the Initial Factor Method: Iterated Principal Factor Analysis table: Iteration : This column lists the number of the iteration. Change : When the change becomes smaller than the criterion, the iterating process stops. The numbers in this column are the largest absolute difference between iterations. The difference given for the first iteration is the difference between the values at the first iteration and the squared multiple correlations (sometimes called iteration 0). Communalities : These are the communality estimates at each iteration. For each iteration, the communality for each variable is listed. PRIORS=SMC enables the squared multiple correlation to be used on the diagonal of the correlation matrix. If this option is not used, 1\u2019s are on the diagonal, and you will do a PCA instead of a principal axis factor analysis. The Prior Communality Estimates: SMC table gives the communality estimates prior to the rotation. The communalities (also known as h2) are the estimates of the variance of the factors, as opposed to the variance of the variable which includes measurement error. This table contains different values: Eigenvalue : This is the initial eigenvalue. An eigenvalue is the variance of the factor. Because this is an unrotated solution, the first factor will account for the most variance, the second will account for the second highest amount of variance, and so on. Some of the eigenvalues are negative because the matrix is not of full rank. This means that there are probably only four dimensions (corresponding to the four factors whose eigenvalues are greater than zero). Although it is strange to have a negative variance, this happens because the factor analysis is only analyzing the common variance, which is less than the total variance. If we were doing a principal components analysis, we would have had 1\u2019s on the diagonal, which means that all of the variance is being analyzed (which is another way of saying that we are assuming that we have no measurement error), and we would not have negative eigenvalues. In general, it is not uncommon to have negative eigenvalues. Difference : This column gives the difference between the eigenvalues and allows you to see how quickly the eigenvalues are decreasing. Proportion : This is the proportion of the total variance that each factor accounts for. Cumulative : This is the sum of the proportion column. Eigenvalues of the Reduced Correlation Matrix table contains: Eigenvalue : This is the eigenvalue obtained after the principal axis factoring but before the varimax rotation . Difference , Proportion and Cumulative show the same information described for the Prior Communality Estimates: SMC table The Eigenvectors table shows the linear combinations of the original variables. They tell you about the strength of the relationship between the original (manifest) variables and the (latent) factors. The Factor Pattern table contains the unrotated factor loadings, which are the correlations between the variable and the factor. Because these are correlations, possible values range from -1 to +1 . The Final Communality Estimates table shows the proportion of each variable\u2019s variance that can be explained by the retained factors prior to the rotation. Variables with high values are well represented in the common factor space, while variables with low values are not well represented. They are the reproduced variances from the factors that you have extracted. You can find these values on the diagonal of the reproduced correlation matrix. The Orthogonal Transformation Matrix table shows the matrix by which you multiply the unrotated factor matrix to get the rotated factor matrix. The Rotated Factor Pattern table contains the rotated factor loadings, which are the correlations between the variable and the factor. Because these are correlations, possible values range from -1 to +1 . The Factor columns in this table are the rotated factors that have been extracted. These are the factors that analysts are most interested in and try to name looking at the items that load highly on it. The second factor might be called \"relating to students\" because items like \"instructor is sensitive to students\" and \"instructor allows me to ask questions\" load highly on it. The third factor has to do with comparisons to other instructors and courses. ROTATE = PROMAX | VARIMAX invokes one of the rotation methods available. If no method is specified, the SAS default is to use no rotation. With an orthogonal rotation, such as the VARIMAX , the factors are not permitted to be correlated. Orthogonal transformations simplify the interpretation of the factors by maximizing the variances of the squared loadings for each factor, which are the columns of the factor pattern. Please note that with orthogonal rotations the factor pattern and the factor structure matrices are the equal. With an oblique rotation, such as a PROMAX rotation, the factors are permitted to be correlated with one another and both factor pattern and factor structure matrices are produced. The factor pattern matrix gives the linear combination of the variables that make up the factors. The factor structure matrix presents the correlations between the variables and the factors. To completely interpret an oblique rotation one needs to take into account both the factor pattern and the factor structure matrices and the correlations among the factors. Check these websites Factor Analysis: SAS Annotated Output PROC FACTOR : How to Interpret the Output of a Real-World Example","title":"Factor Analysis"},{"location":"other-analysis/clustering/#standardize-your-data","text":"When performing multivariate analysis, having variables that are measured at different scales can influence the numerical stability and precision of the estimators. Standardizing the data prior to performing statistical analysis can often prevent this problem. 1 2 3 4 5 6 7 8 DATA SAS - data - set ; SET SAS - data - set - unformatted ; format variable1 variable2 variable3 10 . 4 ; RUN ; PROC STANDARD DATA = SAS - data - set out = SAS - output - data - set MEAN = 0 STD = 1 ; VAR variable1 variable2 variable3 ; RUN ; Warning Do not forget to change the format of your numerical data and increase the number of decimal places before performing the standardization. Otherwise you may lose a lot of details on this process that can be crucial for the data analysis. Check these websites Standardization Procedures Standardization of Variables in Cluster Analysis","title":"Standardize your Data"},{"location":"other-analysis/clustering/#sas-procedures-to-perform-cluster-analysis","text":"Ward's minimum-variance hierarchical clustering method using agglomerative (bottom-up) approach and Ward's linkage. Check these websites Introduction to Clustering Procedures","title":"SAS Procedures to Perform Cluster Analysis"},{"location":"other-analysis/clustering/#proc-cluster-hierarchical-cluster-analysis","text":"The CLUSTER procedure hierarchically clusters the observations in a SAS data set by using one of 11 methods. The data can be coordinates or distances. All methods are based on the usual agglomerative hierarchical clustering procedure. Each observation begins in a cluster by itself. The two closest clusters are merged to form a new cluster that replaces the two old clusters. Merging of the two closest clusters is repeated until only one cluster is left. The various clustering methods differ in how the distance between two clusters is computed. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 data t ; input cid $ 1 - 2 income educ ; cards ; c1 5 5 c2 6 6 c3 15 14 c4 16 15 c5 25 20 c6 30 19 run ; proc cluster simple noeigen method = centroid rmsstd rsquare nonorm out = tree ; id cid ; var income educ ; run ; The SIMPLE option displays simple, descriptive statistics. The NOEIGEN option suppresses computation of eigenvalues. Specifying the NOEIGEN option saves time if the number of variables is large, but it should be used only if the variables are nearly uncorrelated or if you are not interested in the cubic clustering criterion. The METHOD= specification determines the clustering method used by the procedure. Here, we are using CENTROID method. The Centroid Distance that appears in the output is simply the Euclidian distance between the centroid of the two clusters that are to be joined or merged. It is a measure of the homogeneity of merged clusters and the value should be small. The RMSSTD option displays the root-mean-square standard deviation of each cluster. RMSSTD is the pooled standard deviation of all the variables forming the cluster. Since the objective of cluster analysis is to form homogeneous groups, the RMSSTD of a cluster should be as small as possible. The RSQUARE option displays the $R^2$ ( RSQ ) and semipartial $R^2$ ( SPRSQ ) to evaluate cluster solution. RSQ measures the extent to which groups or clusters are different from each other (so, when you have just one cluster RSQ value is, intuitively, zero). Thus, the RSQ value should be high. SPRSQ is a measure of the homogeneity of merged clusters, i.e. the loss of homogeneity due to combining two groups or clusters to form a new group or cluster. Thus, its value should be small to imply that we are merging two homogeneous groups. The NONORM option prevents the distances from being normalized to unit mean or unit root mean square with most methods. The values of the ID variable identify observations in the displayed cluster history and in the OUTTREE= data set. If the ID statement is omitted, each observation is denoted by OBn , where n is the observation number. The VAR statement lists numeric variables to be used in the cluster analysis. If you omit the VAR statement, all numeric variables not listed in other statements are used.","title":"PROC CLUSTER: Hierarchical Cluster Analysis"},{"location":"other-analysis/clustering/#proc-fastclus-disjoint-cluster-analysis","text":"The FASTCLUS procedure performs a disjoint cluster analysis on the basis of distances computed from one or more quantitative variables. The observations are divided into clusters such that every observation belongs to one and only one cluster ; the clusters do not form a tree structure as they do in the CLUSTER procedure. If you want separate analyses for different numbers of clusters, you can run PROC FASTCLUS once for each analysis. The FASTCLUS procedure requires time proportional to the number of observations and thus can be used with much larger data sets than PROC CLUSTER . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 data t2 ; input cid $ 1 - 2 income educ ; cards ; c1 5 5 c2 6 6 c3 15 14 c4 16 15 c5 25 20 c6 30 19 run ; proc fastclus radius = 0 replace = full maxclusters = 3 maxiter = 20 list distance ; id cid ; var income educ ; run ; You must specify either the MAXCLUSTERS= or the RADIUS= argument in the PROC FASTCLUS statement. The RADIUS= option establishes the minimum distance criterion for selecting new seeds. No observation is considered as a new seed unless its minimum distance to previous seeds exceeds the value given by the RADIUS= option. The default value is 0. The MAXCLUSTERS= option specifies the maximum number of clusters allowed. If you omit the MAXCLUSTERS= option, a value of 100 is assumed. The REPLACE= option specifies how seed replacement is performed. FULL requests default seed replacement. PART requests seed replacement only when the distance between the observation and the closest seed is greater than the minimum distance between seeds. NONE suppresses seed replacement. RANDOM selects a simple pseudo-random sample of complete observations as initial cluster seeds. The MAXITER= option specifies the maximum number of iterations for recomputing cluster seeds. When the value of the MAXITER= option is greater than 0, each observation is assigned to the nearest seed, and the seeds are recomputed as the means of the clusters. The LIST option lists all observations, giving the value of the ID variable (if any), the number of the cluster to which the observation is assigned, and the distance between the observation and the final cluster seed. The DISTANCE option computes distances between the cluster means. The ID variable, which can be character or numeric, identifies observations on the output when you specify the LIST option. The VAR statement lists the numeric variables to be used in the cluster analysis. If you omit the VAR statement, all numeric variables not listed in other statements are used. The cluster analysis may converge to a solution at the $n^{th}$ iteration because the change in cluster seeds at this iteration is less than the convergence criterion. Note that a zero change in the centroid of the cluster seeds for the $n^{th}$ iteration implies that the reallocation did not result in any reassignment of observations. The statistics used for the evaluation of the cluster solution are the same as in the hierarchical cluster analysis. The cluster solution can also be evaluated with respect to each clustering variable . If the measurement scales are not the same, then for each variable one should obtain the ratio of the respective Within STD to the Total STD , and compare this ratio across the variables. Interesting Examples Multivariate Statistical Analysis in SAS: Segmentation and Classification of Behavioral Data","title":"PROC FASTCLUS: Disjoint Cluster Analysis"},{"location":"other-analysis/clustering/#mixed-clustering","text":"On large data sets a useful methodology consists first in summarizing the observations in a large enough number of clusters (100 may be a standard value) and then applying a hierarchical clustering technique for aggregating these groups (Example here ). This procedure has the advantages of the hierarchical method for showing an optimal number of clusters and solves the difficulty of the too high initial number of observations by first clustering them, using a non hierarchical method, in a smaller number of clusters. This number is a parameter of the procedure; it must be high enough in order not to impose a prior partitionning of the data.","title":"Mixed Clustering"},{"location":"other-analysis/clustering/#proc-varclus-variable-clustering","text":"The VARCLUS procedure divides a set of numeric variables into disjoint or hierarchical clusters. PROC VARCLUS tries to maximize the variance that is explained by the cluster components, summed over all the clusters. In an ordinary principal component analysis, all components are computed from the same variables, and the first principal component is orthogonal to the second principal component and to every other principal component. In PROC VARCLUS , each cluster component is computed from a set of variables that is different from all the other cluster components. The first principal component of one cluster might be correlated with the first principal component of another cluster. Hence, the PROC VARCLUS algorithm is a type of oblique component analysis. PROC VARCLUS can be used as a variable-reduction method . A large set of variables can often be replaced by the set of cluster components with little loss of information. A given number of cluster components does not generally explain as much variance as the same number of principal components on the full set of variables, but the cluster components are usually easier to interpret than the principal components, even if the latter are rotated. 1 2 3 PROC VARCLUS DATA = SAS - data - set MAXEIGEN = 0 . 7 OUTTREE = fortree short noprint ; VAR variable1 variable2 variable3 ; RUN ;","title":"PROC VARCLUS: Variable Clustering"},{"location":"other-analysis/clustering/#proc-tree","text":"The TREE procedure produces a tree diagram from a data set created by the CLUSTER or VARCLUS procedure that contains the results of hierarchical clustering as a tree structure. 1 2 3 4 PROC TREE DATA = tree OUT = clus3 NCLUSTERS = 3 ; ID id - variable ; COPY variable1 variable2 variable3 ; RUN ; The TREE procedure produces a tree diagram, also known as a dendrogram or phenogram, using a data set created by the CLUSTER procedure. The CLUSTER procedure creates output data sets that contain the results of hierarchical clustering as a tree structure . The TREE procedure uses the output data set to produce a diagram of the tree structure. The NCLUSTERS= option specifies the number of clusters desired in the OUT= data set. The ID variable is used to identify the objects (leaves) in the tree on the output. The ID variable can be a character or numeric variable of any length. The COPY statement specifies one or more character or numeric variables to be copied to the OUT= data set.","title":"PROC TREE"},{"location":"other-analysis/clustering/#choosing-the-optimal-number-of-clusters-for-the-analysis","text":"In most cases, before using a clustering technique you have no prior idea of the number of clusters which will give the better differenciation of the data. The main objective is to summarize the data in the best way possible, i.e. getting a compromise between a good degree of differentiation and a not too high number of clusters. For hierarchical clustering try the Sarle's Cubic Clustering Criterion in PROC CLUSTER: plot CCC versus the number of clusters and look for peaks where ccc > 3 or look for local peaks of pseudo-F statistic ( PSF ) combined with a small value of the pseudo-t^2 statistic ( PST2 ) and a larger pseudo t^2 for the next cluster fusion. For K-Means clustering use this approach on a sample of your data to determine the max limit for k and assign it to the maxc= option in PROC FASTCLUS on the complete data. For K-means cluster analysis, one can use PROC FASTCLUS like PROC FASTCLUS DATA=SAS-data-set OUT=out MAXC=4 MAXITER=20; and change the number defined by MAXC= , and run a number of times, then compare the Pseduo F and CCC values, to see which number of clusters gives peaks. You can also use PROC CLUSTER PROC CLUSTER data=mydata METHOD=WARD out=out ccc pseudo print=15; to find the number of clusters with pseudo F , pseudo-$t^2$ and CCC , and also look at junp in Semipartial R-Square . Sometimes these indications do not agree to each other. which indicator is more reliable? If you are doubting between 2 k-values, you can use Beale's F-type statistic to determine the final number of clusters. It will tell you whether the larger solution is significantly better or not (in the latter case the solution with fewer clusters is preferable). This technique is discussed in the \"Applied Clustering Techniques\" course notes. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 * Define the variables for clustering ; *------------------------------------ ; %let varlist = var1 var2 var3 var4 var5 ; * Macro with the cluster procedure , to call it with different number of clusters ; *------------------------------------------------------------------------------- ; % MACRO CLUSTERSIZE ( datain = , dataout = , maxclusters = , maxiter = ); proc fastclus data =& datain . out =& dataout . & maxclusters . outstat = statdata & maxclusters . maxclusters =& maxclusters . maxiter =& maxiter . noprint ; id patient ; var & varlist .; run ; title ; % MEND ; * Calcultion of the cluster analysis statistics for 1 - 20 clusters and data set creation for elbow plot with RSQ values ; *--------------------------------------------------------------------------------------------------------------------- ; %macro statCLUSTER ; %do k = 1 %to 20 ; % CLUSTERSIZE ( datain = SAS - data - set , dataout = clusterdata , maxclusters =& k ., maxiter = 1000 ); data clusrsq & k .; set statdata & k .; nclust =& k .; if _type_ = ' RSQ ' ; keep nclust over_all ; run ; %end ; %mend ; %stat CLUSTER ; data clus_rsq ; set clusrsq1 clusrsq2 clusrsq3 clusrsq4 clusrsq5 clusrsq6 clusrsq7 clusrsq8 clusrsq9 clusrsq10 clusrsq11 clusrsq12 clusrsq13 clusrsq14 clusrsq15 clusrsq16 clusrsq17 clusrsq18 clusrsq19 clusrsq20 ; run ; * Remove useless data sets ; *------------------------- ; proc datasets lib = work nowarn nolist nodetails ; delete clusrsq : statdata : clusterdata : ; run ; quit ; * Plot elbow curve using r - square values highlighting the best candidates to optimum number of clusters ; *------------------------------------------------------------------------------------------------------ ; symbol1 color = blue interpol = join ; axis1 label = ( ' Number of clusters in the analysis ' ) order = ( 0 to 15 by 1 ) reflabel = ( j = c h = 9 pt ' Candidate 1 ' ' Candidate 2 ' ); axis2 label = ( ' R ^ 2 values ' j = c ); proc gplot data = clus_rsq ; plot over_all * nclust / haxis = axis1 vaxis = axis2 href = 3 5 ; run ; Check these websites The Number of Clusters","title":"Choosing the Optimal Number of Clusters for the Analysis"},{"location":"other-analysis/clustering/#further-examination-of-the-cluster-analysis-solution","text":"","title":"Further Examination of the Cluster Analysis Solution"},{"location":"other-analysis/clustering/#visualizing-the-results","text":"To interpret a cluster analysis solution, the first thing you want to try is to graph the cluster in a scatter plot to see whether or not they overlap with each other in terms of their location in the $p-$dimensional space. If the vectors have a high dimensionality, we use Canonical Discriminant Analysis (CDA) . It is a dimension-reduction technique related to principal component analysis and canonical correlation . It creates a smaller number of variables that are linear combinations of the original $p$ clustering variables. The new variables, called canonical variables, are ordered in terms of the proportion of variance in the clustering variables that is accounted for by each of the canonical variables. Usually, the majority of the variants in the clustering variable will be accounted for by the first couple of canonical varaibles and those are the variables we can plot. In SAS we can use the CANDISC procedure to create the canonical variables for our cluster analysis output data set that has the cluster assignment variable that we created when we ran the cluster analysis. 1 2 3 4 PROC CANDISC DATA = clusterdata OUT = candata ; CLASS cluster - assignment - variable ; VAR clustering1 clustering2 ; RUN ; The OUT= is the output data set that includes the canonical variables that are estimated by the canonical discriminate analysis. The CLASS variable (mandatory) is the cluster assignment variable which is a categorical variable. In the VAR statement we list the clustering variables You can then plot the first two canonical variables using the SGPLOT procedure: 1 2 3 PROC SGPLOT DATA = candata ; SCATTER Y = can2 X = can1 / GROUP = cluster - assignment - variable ; RUN ; Let's analyze the following example result for a 4-cluster analysis. What this shows is that the observations in clusters 1 and 4 are densely packed, meaning they are pretty highly correlated with each other, and within cluster variance is relatively low . In addition, they do not overlap very much with the other clusters. The observations from cluster 2 are a little more spread out, indicating less correlation among the observations and higher within cluster variance. But generally, the cluster is relatively distinct with the exception of some observations which are closer to clusters 1 and 4 indicating some overlap with these clusters. However, cluster 3 is all over the place. There is come indication of a cluster but the observations are spread out more than the other clusters. This means that the within cluster variance is high as there is less correlation between the observations in this cluster, so we do not really know what is going to happen with that cluster. What this suggests is that the best cluster solution may have fewer than 4 clusters , meaning that it would be especially important to further evaluate the cluster solutions with fewer than four clusters. Check these websites Discriminant Function Analysis in SAS (UCLA) Introduction to Discriminant Procedures","title":"Visualizing the Results"},{"location":"other-analysis/clustering/#cluster-means-and-standard-deviations","text":"You can also check these values per cluster to detect possible similarities between groups and detect the most different ones.","title":"Cluster Means and Standard Deviations"},{"location":"other-analysis/effect-size/","text":"The effect size will be larger if The absolute difference between the averages is higher, or the responses are consistently close to the average values and not widely spread out (the standard deviation is low). T-Test Link A T-Test's effect size indicates whether or not the difference between two groups' averages is large enough to have practical meaning, whether or not it is statistically significant. Cohen's d Link Cohen's d is defined as the difference between two means divided by a standard deviation for the data, i.e. $d = \\frac{\\bar{x}_1-\\bar{x}_2}{s}=\\frac{\\mu_1-\\mu_2}{s}$. By default SPSS and SAS compute the SD as an inferential statistic (i.e., S) rather than as the population parameter (i.e., $\\sigma$) by using N-1 in the denominator of the SD equation rather than N. In order to obtain Cohen's d rather than Hedge's g, the inferential statistic S will be transformed to $\\sigma$. Effect size d Reference Very small 0.01 Sawilowsky, 2009 Small 0.20 Cohen, 1988 Medium 0.50 Cohen, 1988 Large 0.80 Cohen, 1988 Very large 1.20 Sawilowsky, 2009 Huge 2.00 Sawilowsky, 2009 Macro Example Link 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 %macro effectsize ( scorelist = , scorelablist = ); %let nscore = %eval ( %sysfunc ( count ( & scorelist , $ )) + 1 ); %do iscore = 1 %to & nscore ; %let score & iscore = %scan ( & scorelist ., & iscore ., $ ); %let scorelab & iscore = %scan ( & scorelablist ., & iscore ., $ ); ods exclude all ; proc ttest data = objetivos_cambio ; paired && score & iscore .. _fin *&& score & iscore .. _bs ; ods output Statistics = stat_ & iscore . TTests = ttest_ & iscore .; run ; proc means data = objetivos_cambio mean stddev n skewness kurtosis t prt CLM ; var cambio && score & iscore ..; ods output Summary = means_ & iscore .; run ; data cohensd_ & iscore .; set means_ & iscore .; t = cambio && score & iscore .. _t ; n = cambio && score & iscore .. _n ; df = n - 1 ; d = t / sqrt ( n ); ncp_lower = TNONCT ( t , df , .975 ); ncp_upper = TNONCT ( t , df , .025 ); d_lower = ncp_lower / sqrt ( n ); d_upper = ncp_upper / sqrt ( n ); d_ic = '(' || trim ( left ( put ( d_lower , 6.2 ))) || ',' || trim ( left ( put ( d_upper , 6.2 ))) || ')' ; Variable1 = \"&&score&iscore.._fin\" ; Variable2 = \"&&score&iscore.._bs\" ; output ; run ; ods exclude none ; data tab_ & iscore .; length label Variable1 Variable2 Difference $50 ; merge stat_ & iscore . ttest_ & iscore . cohensd_ & iscore .( keep = Variable1 Variable2 d d_ic ); by Variable1 Variable2 ; label = \"&&scorelab&iscore\" ; run ; %end ; data final_tab ; set %do iscore = 1 %to & nscore ; tab_ & iscore . %end ; ; meansd = trim ( left ( put ( mean , 6.2 ))) || ' ( ' || trim ( left ( put ( stddev , 6.2 ))) || ')' ; cimean = '(' || trim ( left ( put ( LowerCLMean , 6.2 ))) || ',' || trim ( left ( put ( UpperCLMean , 6.2 ))) || ')' ; minmax = '(' || trim ( left ( put ( Minimum , 6.2 ))) || ',' || trim ( left ( put ( maximum , 6.2 ))) || ')' ; run ; proc report data = final_tab nowd headline style ( header ) = { background = very light grey fontsize = 8 pt } missing style ( column ) = { fontsize = 8 pt } split = '*' ; column ( \"Tama\u00f1o del efecto para muestras pareadas de objetivos primario y secundarios\" ( label n meansd cimean minmax probt d d_ic )); define label / display ' Cuestionario ' flow ; define n / display 'N' flow ; define meansd / display ' Diff . Media ( DE ) ' flow ; define cimean / display ' IC Diff . Media ( 95 % ) ' flow ; define minmax / display ' ( M\u00ednimo , M\u00e1ximo ) ' flow ; define probt / display ' p - valor ' flow ; define d / display f = 5.2 ' d de Cohen ' flow ; define d_ic / display ' IC d de Cohen ( 95 % ) ' flow ; run ; proc datasets lib = work nowarn nolist nodetails ; delete tab_ : stat_ : ttest_ : means_ : cohensd_ :; run ; quit ; %mend ; * %effectsize ( scorelist = varname1$varname2$varname3 , scorelablist = Variable label 1 $Variable label 2 $Variable label 3 );","title":"Effect Size"},{"location":"other-analysis/effect-size/#t-test","text":"A T-Test's effect size indicates whether or not the difference between two groups' averages is large enough to have practical meaning, whether or not it is statistically significant.","title":"T-Test"},{"location":"other-analysis/effect-size/#cohens-d","text":"Cohen's d is defined as the difference between two means divided by a standard deviation for the data, i.e. $d = \\frac{\\bar{x}_1-\\bar{x}_2}{s}=\\frac{\\mu_1-\\mu_2}{s}$. By default SPSS and SAS compute the SD as an inferential statistic (i.e., S) rather than as the population parameter (i.e., $\\sigma$) by using N-1 in the denominator of the SD equation rather than N. In order to obtain Cohen's d rather than Hedge's g, the inferential statistic S will be transformed to $\\sigma$. Effect size d Reference Very small 0.01 Sawilowsky, 2009 Small 0.20 Cohen, 1988 Medium 0.50 Cohen, 1988 Large 0.80 Cohen, 1988 Very large 1.20 Sawilowsky, 2009 Huge 2.00 Sawilowsky, 2009","title":"Cohen's d"},{"location":"other-analysis/effect-size/#macro-example","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 %macro effectsize ( scorelist = , scorelablist = ); %let nscore = %eval ( %sysfunc ( count ( & scorelist , $ )) + 1 ); %do iscore = 1 %to & nscore ; %let score & iscore = %scan ( & scorelist ., & iscore ., $ ); %let scorelab & iscore = %scan ( & scorelablist ., & iscore ., $ ); ods exclude all ; proc ttest data = objetivos_cambio ; paired && score & iscore .. _fin *&& score & iscore .. _bs ; ods output Statistics = stat_ & iscore . TTests = ttest_ & iscore .; run ; proc means data = objetivos_cambio mean stddev n skewness kurtosis t prt CLM ; var cambio && score & iscore ..; ods output Summary = means_ & iscore .; run ; data cohensd_ & iscore .; set means_ & iscore .; t = cambio && score & iscore .. _t ; n = cambio && score & iscore .. _n ; df = n - 1 ; d = t / sqrt ( n ); ncp_lower = TNONCT ( t , df , .975 ); ncp_upper = TNONCT ( t , df , .025 ); d_lower = ncp_lower / sqrt ( n ); d_upper = ncp_upper / sqrt ( n ); d_ic = '(' || trim ( left ( put ( d_lower , 6.2 ))) || ',' || trim ( left ( put ( d_upper , 6.2 ))) || ')' ; Variable1 = \"&&score&iscore.._fin\" ; Variable2 = \"&&score&iscore.._bs\" ; output ; run ; ods exclude none ; data tab_ & iscore .; length label Variable1 Variable2 Difference $50 ; merge stat_ & iscore . ttest_ & iscore . cohensd_ & iscore .( keep = Variable1 Variable2 d d_ic ); by Variable1 Variable2 ; label = \"&&scorelab&iscore\" ; run ; %end ; data final_tab ; set %do iscore = 1 %to & nscore ; tab_ & iscore . %end ; ; meansd = trim ( left ( put ( mean , 6.2 ))) || ' ( ' || trim ( left ( put ( stddev , 6.2 ))) || ')' ; cimean = '(' || trim ( left ( put ( LowerCLMean , 6.2 ))) || ',' || trim ( left ( put ( UpperCLMean , 6.2 ))) || ')' ; minmax = '(' || trim ( left ( put ( Minimum , 6.2 ))) || ',' || trim ( left ( put ( maximum , 6.2 ))) || ')' ; run ; proc report data = final_tab nowd headline style ( header ) = { background = very light grey fontsize = 8 pt } missing style ( column ) = { fontsize = 8 pt } split = '*' ; column ( \"Tama\u00f1o del efecto para muestras pareadas de objetivos primario y secundarios\" ( label n meansd cimean minmax probt d d_ic )); define label / display ' Cuestionario ' flow ; define n / display 'N' flow ; define meansd / display ' Diff . Media ( DE ) ' flow ; define cimean / display ' IC Diff . Media ( 95 % ) ' flow ; define minmax / display ' ( M\u00ednimo , M\u00e1ximo ) ' flow ; define probt / display ' p - valor ' flow ; define d / display f = 5.2 ' d de Cohen ' flow ; define d_ic / display ' IC d de Cohen ( 95 % ) ' flow ; run ; proc datasets lib = work nowarn nolist nodetails ; delete tab_ : stat_ : ttest_ : means_ : cohensd_ :; run ; quit ; %mend ; * %effectsize ( scorelist = varname1$varname2$varname3 , scorelablist = Variable label 1 $Variable label 2 $Variable label 3 );","title":"Macro Example"},{"location":"other-analysis/interim-analysis/","text":"The purpose of the SEQDESIGN procedure is to design interim analyses for clinical trials. In a fixed-sample trial , data about all individuals are first collected and then examined at the end of the study. In contrast, a group sequential trial provides for interim analyses before the completion of the trial while maintaining the specified overall Type I and Type II error probabilities. A group sequential trial is most useful in situations where it is important to monitor the trial to prevent unnecessary exposure of patients to an unsafe new drug, or alternatively to a placebo treatment if the new drug shows significant improvement. In most cases, if a group sequential trial stops early for safety concerns , fewer patients are exposed to the new treatment than in the fixed-sample trial. If a trial stops early for efficacy reasons , the new treatment is available sooner than it would be in a fixed-sample trial. Early stopping can also save time and resources. A group sequential design provides detailed specifications for a group sequential trial. In addition to the usual specification for a fixed-sample design, it provides the total number of stages (the number of interim stages plus a final stage) and a stopping criterion to reject, to accept, or to either reject or accept the null hypothesis at each interim stage. It also provides critical values and the sample size at each stage for the trial. PROC SEQDESIGN Link You use the SEQDESIGN procedure compute the initial boundary values and required sample sizes for the trial. PROC SEQTEST Link You use the SEQTEST procedure to compare the test statistic with its boundary values. http://support.sas.com/documentation/cdl/en/statug/68162/HTML/default/viewer.htm#statug_seqdesign_overview01.htm https://support.sas.com/resources/papers/proceedings09/311-2009.pdf https://support.sas.com/documentation/cdl/en/statug/63033/HTML/default/viewer.htm#statug_seqdesign_sect035.htm","title":"Interim Analysis Design"},{"location":"other-analysis/interim-analysis/#proc-seqdesign","text":"You use the SEQDESIGN procedure compute the initial boundary values and required sample sizes for the trial.","title":"PROC SEQDESIGN"},{"location":"other-analysis/interim-analysis/#proc-seqtest","text":"You use the SEQTEST procedure to compare the test statistic with its boundary values. http://support.sas.com/documentation/cdl/en/statug/68162/HTML/default/viewer.htm#statug_seqdesign_overview01.htm https://support.sas.com/resources/papers/proceedings09/311-2009.pdf https://support.sas.com/documentation/cdl/en/statug/63033/HTML/default/viewer.htm#statug_seqdesign_sect035.htm","title":"PROC SEQTEST"},{"location":"other-analysis/missing-data/","text":"Check these websites Check out this review on techniques for treating missing data. A Conceptual Strategy and Macro Approach for Partial Date Handling in Data De-Identification Did you know? Missing values can be \"formatted\". .A thru .Z are all valid missing values just as . . Missing Data Mechanisms and Patterns Link To use the more appropriate method to deal with your missing data, you should consider the missing data mechanism of your data which describes the process that is believed to have generated the missing values. According to Rubin (1976) , there are three mechanisms under which missing data can occur: Missing completely at random (MCAR) : neither the variables in the dataset nor the unobserved value of the variable itself predict whether a value will be missing Missing at random (MAR) : other variables (but not the variable itself) in the dataset can be used to predict missingness on a given variable Missing not at random (MNAR) : value of the unobserved variable itself predicts missingness Check these websites Check out the formal description of each missing mechanism in the \"Missing data mechanisms\" section of this paper. Objetives of Imputation Link Depending on the type of data and model you will be using, techniques such as multiple imputation or direct maximum likelihood may better serve your needs. The main goals of statistical analysis with missing data are: Minimize bias Maximize use of available information Obtain appropriate estimates of uncertainty Imputed values are not equivalent to observed values and serve only to help estimate the covariances between variables needed for inference. Automated imputations generally fall into one of six categories: Deterministic imputation Model based imputation Deck imputation Mixed imputation Expert Systems Neural networks Deletion procedures Link Complete case analysis (listwise deletion) : deleting cases in a particular dataset that are missing data on any variable of interest (for MCAR cases the power is reduced but it does not add any bias). It is a common technique because it is easy to implement and works with any type of analysis. Available case analysis (pairwise deletion) : deleting cases where a variable required for a particular analysis is missing, but including those cases in analyses for which all required variables are present. One of the main drawbacks of this method is no consistent sample size because depending on the pairwise comparison examined, the sample size will change based on the amount of missing present in one or both variables. This method became popular because the loss of power due to missing information is not as substantial as with complete case analysis. Unless the mechanism of missing data is MCAR, this method will introduce bias into the parameter estimates. Therefore, this method is not recommended. Replacement procedures Link Data replacement does not compensate for a badly designed instrument or for poor data collection. Overall, replacement procedures can be used in certain cases, as long as the researcher has a good reason for replacing. The most important advantages of these procedures are the retention of the sample size (statistical power). To a greater or lesser extent, all replacement procedures are biased if there is a non-random distribution of missing values. In assessing the effectiveness of these procedures, both the accuracy of estimating the value of missing data and the accuracy of estimating the statistical effects have to be considered. Many different missing data replacement procedures have been developed over the years. In general, the differences between the various methods decrease with: (a) larger sample size, (b) a smaller percentage of missing values, (c) fewer missing variables and (d) a decrease in the level of the correlations between the variables. Single Imputation Methods Link Single imputation denotes that the missing value is replaced by a value. However, the imputed values are assumed to be the real values that would have been observed when the data would have been complete. When we have missing data, this is never the case. We can never be completely certain about imputed values . Unconditional Mean Imputation / Mean Substitution : replacing the missing values for an individual variable wih it's overall estimated mean from the available cases. Its more important problem is that it will result in an artificial reduction in variability due to the fact you are imputing values at the center of the variable's distribution. This also has the unintended consequence of changing the magnitude of correlations between the imputed variable and other variables. Regression Imputation : This is a two-step approach: first, the relationships among variables are estimated, and then the regression coefficients are used to estimate the missing value. The underlying assumption of regression imputation is the existence of a linear relationship between the predictors and the missing variable. The technique also assumes that values are missing at random (i.e., a missing value is not related to the value of the predictors). Stochastic Regression Imputation : In recognition of the problems with regression imputation and the reduced variability associated with this approach, researchers developed a technique to incorporate or \u201cadd back\u201d lost variability. A residual term, that is randomly drawn from a normal distribution with mean zero and variance equal to the residual variance from the regression model, is added to the predicted scores from the regression imputation thus restoring some of the lost variability. This method is superior to the previous methods as it will produce unbiased coefficient estimates under MAR. However, the standard errors produced during regression estimation while less biased then the single imputation approach, will still be attenuated. The deterministic imputations are exactly at the regression predictions and ignore predictive uncertainty. In contrast, the random imputations are more variable and better capture the range of the data. Hot-deck Imputation : According to this technique, the researcher should replace a missing value with the actual score from a similar case in the dataset. One form of hot-deck imputation is called \"last observation carried forward\" (LOCF), which involves sorting a dataset according to any of a number of variables, thus creating an ordered dataset. The technique then finds the first missing value and uses the cell value immediately prior to the data that are missing to impute the missing value. This method is known to increase risk of increasing bias and potentially false conclusions. For this reason LOCF is not recommended for use. Cold-deck Imputation : This method replaces a missing value of an item with a constant value from an external source such as a value from a previous survey. Single Imputation : Multiple imputation Link Check these websites Visit this website for more information. Bayesian Methods for Completing Data are simply methods based on conditional probability. Multiple Imputation is always superior to any of the single imputation methods because: A single imputed value is never used The variance estimates reflect the appropriate amount of uncertainty surrounding parameter estimates There are several decisions to be made before performing a multiple imputation including distribution , auxiliary variables and number of imputations that can affect the quality of the imputation. Imputation phase ( PROC MI ) : the user specifies the imputation model to be used and the number of imputed datasets to be created Analysis phase ( PROG GLM / PROC GENMOD ) : runs the analytic model of interest within each of the imputed datasets Pooling phase ( PROC MIANALYZE ) : combines all the estimates across all the imputed datasets and outputs one set of parameter estimates for the model of interest MVN vs FCS Link Auxiliary variables Link They can can help improve the likelihood of meeting the MAR assumption They help yield more accurate and stable estimates and thus reduce the estimated standard errors in analytic models Including them can also help to increase power Number of imputations (m) Link Estimates of coefficients stabilize at much lower values of m than estimates of variances and covariances of error terms A larger number of imputations may also allow hypothesis tests with less restrictive assumptions (i.e., that do not assume equal fractions of missing information for all coefficients) Multiple runs of m imputations are recommended to assess the stability of the parameter estimates Recommendations: For low fractions of missing information (and relatively simple analysis techniques) 5-20 imputations and 50 or more when the proportion of missing data is relatively high The number of imputations should equal the percentage of incomplete cases ( m =max(FMI%)), this way the error associated with estimating the regression coefficients, standard errors and the resulting p-values is considerably reduced and results in an adequate level of reproducibility More comments Link You should include the dependent variable (DV) in the imputation model unless you would like to impute independent variables (IVs) assuming they are uncorrelated with your DV Although MI can perform well up to 50% missing observations, the larger the amount the higher the chance of finding estimation problems during the imputation process and the lower the chance of meeting the MAR assumption Model-based Procedures Link Direct Maximum Likelihood Link This approach to analyzing missing data has many different forms. In its simplest form, it assumes that the observed data are a sample drawn from a multivariate normal distribution. The parameters are estimated by available data, and then missing scores are estimated based on the parameters just estimated. Contrary to the techniques discussed above, maximum likelihood procedures allow explicit modeling of missing data that is open to scientific analysis and critique. Expectation Maximization Link This algorithm is an iterative process. The first iteration estimates missing data and then parameters using maximum likelihood. The second iteration re-estimates the missing data based on the new parameter estimates and then recalculates the new parameters estimates based on actual and re-estimated missing data. The approach continues until there is convergence in the parameter estimates. Summary Link The best technique to deal with your missing data depends on: The amount of missing data (what percentage of data is missing) Type of missing data (MAR, MCAR, NMAR) According to this nice review , if more than 10% data is missing, the best solution is: Maximum likelihood imputation if data are NMAR (non-missing at random) Maximum likelihood and hot-deck if data are MAR (missing at random) Pairwise deletion, hot-deck or regression if data are MCAR (missing completely at random) Moreover, multiple imputation by chained equations is regarded the best imputation method by many researchers.","title":"Dealing with Missing Data"},{"location":"other-analysis/missing-data/#missing-data-mechanisms-and-patterns","text":"To use the more appropriate method to deal with your missing data, you should consider the missing data mechanism of your data which describes the process that is believed to have generated the missing values. According to Rubin (1976) , there are three mechanisms under which missing data can occur: Missing completely at random (MCAR) : neither the variables in the dataset nor the unobserved value of the variable itself predict whether a value will be missing Missing at random (MAR) : other variables (but not the variable itself) in the dataset can be used to predict missingness on a given variable Missing not at random (MNAR) : value of the unobserved variable itself predicts missingness Check these websites Check out the formal description of each missing mechanism in the \"Missing data mechanisms\" section of this paper.","title":"Missing Data Mechanisms and Patterns"},{"location":"other-analysis/missing-data/#objetives-of-imputation","text":"Depending on the type of data and model you will be using, techniques such as multiple imputation or direct maximum likelihood may better serve your needs. The main goals of statistical analysis with missing data are: Minimize bias Maximize use of available information Obtain appropriate estimates of uncertainty Imputed values are not equivalent to observed values and serve only to help estimate the covariances between variables needed for inference. Automated imputations generally fall into one of six categories: Deterministic imputation Model based imputation Deck imputation Mixed imputation Expert Systems Neural networks","title":"Objetives of Imputation"},{"location":"other-analysis/missing-data/#deletion-procedures","text":"Complete case analysis (listwise deletion) : deleting cases in a particular dataset that are missing data on any variable of interest (for MCAR cases the power is reduced but it does not add any bias). It is a common technique because it is easy to implement and works with any type of analysis. Available case analysis (pairwise deletion) : deleting cases where a variable required for a particular analysis is missing, but including those cases in analyses for which all required variables are present. One of the main drawbacks of this method is no consistent sample size because depending on the pairwise comparison examined, the sample size will change based on the amount of missing present in one or both variables. This method became popular because the loss of power due to missing information is not as substantial as with complete case analysis. Unless the mechanism of missing data is MCAR, this method will introduce bias into the parameter estimates. Therefore, this method is not recommended.","title":"Deletion procedures"},{"location":"other-analysis/missing-data/#replacement-procedures","text":"Data replacement does not compensate for a badly designed instrument or for poor data collection. Overall, replacement procedures can be used in certain cases, as long as the researcher has a good reason for replacing. The most important advantages of these procedures are the retention of the sample size (statistical power). To a greater or lesser extent, all replacement procedures are biased if there is a non-random distribution of missing values. In assessing the effectiveness of these procedures, both the accuracy of estimating the value of missing data and the accuracy of estimating the statistical effects have to be considered. Many different missing data replacement procedures have been developed over the years. In general, the differences between the various methods decrease with: (a) larger sample size, (b) a smaller percentage of missing values, (c) fewer missing variables and (d) a decrease in the level of the correlations between the variables.","title":"Replacement procedures"},{"location":"other-analysis/missing-data/#single-imputation-methods","text":"Single imputation denotes that the missing value is replaced by a value. However, the imputed values are assumed to be the real values that would have been observed when the data would have been complete. When we have missing data, this is never the case. We can never be completely certain about imputed values . Unconditional Mean Imputation / Mean Substitution : replacing the missing values for an individual variable wih it's overall estimated mean from the available cases. Its more important problem is that it will result in an artificial reduction in variability due to the fact you are imputing values at the center of the variable's distribution. This also has the unintended consequence of changing the magnitude of correlations between the imputed variable and other variables. Regression Imputation : This is a two-step approach: first, the relationships among variables are estimated, and then the regression coefficients are used to estimate the missing value. The underlying assumption of regression imputation is the existence of a linear relationship between the predictors and the missing variable. The technique also assumes that values are missing at random (i.e., a missing value is not related to the value of the predictors). Stochastic Regression Imputation : In recognition of the problems with regression imputation and the reduced variability associated with this approach, researchers developed a technique to incorporate or \u201cadd back\u201d lost variability. A residual term, that is randomly drawn from a normal distribution with mean zero and variance equal to the residual variance from the regression model, is added to the predicted scores from the regression imputation thus restoring some of the lost variability. This method is superior to the previous methods as it will produce unbiased coefficient estimates under MAR. However, the standard errors produced during regression estimation while less biased then the single imputation approach, will still be attenuated. The deterministic imputations are exactly at the regression predictions and ignore predictive uncertainty. In contrast, the random imputations are more variable and better capture the range of the data. Hot-deck Imputation : According to this technique, the researcher should replace a missing value with the actual score from a similar case in the dataset. One form of hot-deck imputation is called \"last observation carried forward\" (LOCF), which involves sorting a dataset according to any of a number of variables, thus creating an ordered dataset. The technique then finds the first missing value and uses the cell value immediately prior to the data that are missing to impute the missing value. This method is known to increase risk of increasing bias and potentially false conclusions. For this reason LOCF is not recommended for use. Cold-deck Imputation : This method replaces a missing value of an item with a constant value from an external source such as a value from a previous survey. Single Imputation :","title":"Single Imputation Methods"},{"location":"other-analysis/missing-data/#multiple-imputation","text":"Check these websites Visit this website for more information. Bayesian Methods for Completing Data are simply methods based on conditional probability. Multiple Imputation is always superior to any of the single imputation methods because: A single imputed value is never used The variance estimates reflect the appropriate amount of uncertainty surrounding parameter estimates There are several decisions to be made before performing a multiple imputation including distribution , auxiliary variables and number of imputations that can affect the quality of the imputation. Imputation phase ( PROC MI ) : the user specifies the imputation model to be used and the number of imputed datasets to be created Analysis phase ( PROG GLM / PROC GENMOD ) : runs the analytic model of interest within each of the imputed datasets Pooling phase ( PROC MIANALYZE ) : combines all the estimates across all the imputed datasets and outputs one set of parameter estimates for the model of interest","title":"Multiple imputation"},{"location":"other-analysis/missing-data/#mvn-vs-fcs","text":"","title":"MVN vs FCS"},{"location":"other-analysis/missing-data/#auxiliary-variables","text":"They can can help improve the likelihood of meeting the MAR assumption They help yield more accurate and stable estimates and thus reduce the estimated standard errors in analytic models Including them can also help to increase power","title":"Auxiliary variables"},{"location":"other-analysis/missing-data/#number-of-imputations-m","text":"Estimates of coefficients stabilize at much lower values of m than estimates of variances and covariances of error terms A larger number of imputations may also allow hypothesis tests with less restrictive assumptions (i.e., that do not assume equal fractions of missing information for all coefficients) Multiple runs of m imputations are recommended to assess the stability of the parameter estimates Recommendations: For low fractions of missing information (and relatively simple analysis techniques) 5-20 imputations and 50 or more when the proportion of missing data is relatively high The number of imputations should equal the percentage of incomplete cases ( m =max(FMI%)), this way the error associated with estimating the regression coefficients, standard errors and the resulting p-values is considerably reduced and results in an adequate level of reproducibility","title":"Number of imputations (m)"},{"location":"other-analysis/missing-data/#more-comments","text":"You should include the dependent variable (DV) in the imputation model unless you would like to impute independent variables (IVs) assuming they are uncorrelated with your DV Although MI can perform well up to 50% missing observations, the larger the amount the higher the chance of finding estimation problems during the imputation process and the lower the chance of meeting the MAR assumption","title":"More comments"},{"location":"other-analysis/missing-data/#model-based-procedures","text":"","title":"Model-based Procedures"},{"location":"other-analysis/missing-data/#direct-maximum-likelihood","text":"This approach to analyzing missing data has many different forms. In its simplest form, it assumes that the observed data are a sample drawn from a multivariate normal distribution. The parameters are estimated by available data, and then missing scores are estimated based on the parameters just estimated. Contrary to the techniques discussed above, maximum likelihood procedures allow explicit modeling of missing data that is open to scientific analysis and critique.","title":"Direct Maximum Likelihood"},{"location":"other-analysis/missing-data/#expectation-maximization","text":"This algorithm is an iterative process. The first iteration estimates missing data and then parameters using maximum likelihood. The second iteration re-estimates the missing data based on the new parameter estimates and then recalculates the new parameters estimates based on actual and re-estimated missing data. The approach continues until there is convergence in the parameter estimates.","title":"Expectation Maximization"},{"location":"other-analysis/missing-data/#summary","text":"The best technique to deal with your missing data depends on: The amount of missing data (what percentage of data is missing) Type of missing data (MAR, MCAR, NMAR) According to this nice review , if more than 10% data is missing, the best solution is: Maximum likelihood imputation if data are NMAR (non-missing at random) Maximum likelihood and hot-deck if data are MAR (missing at random) Pairwise deletion, hot-deck or regression if data are MCAR (missing completely at random) Moreover, multiple imputation by chained equations is regarded the best imputation method by many researchers.","title":"Summary"},{"location":"other-analysis/model-selection/","text":"Check these websites The GLMSELECT Procedure Introducing the GLMSELECT Procedure for Model Selection Penalized Regression Methods for Linear Models in SAS Traditional Model Selection Algoritms Link The GLMSELECT procedure extends the familiar forward, backward, and stepwise methods as implemented in the REG procedure to GLM-type models. Quite simply, FORWARD selection adds parameters one at a time, BACKWARD elimination deletes them, and STEPWISE selection switches between adding and deleting them. Forward Method ( FORWARD ) Link It is important to keep in mind that forward selection bases the decision about what effect to add at any step by considering models that differ by one effect from the current model . This search paradigm cannot guarantee reaching a \"best\" subset model. Furthermore, the add decision is greedy in the sense that the effect deemed most significant is the effect that is added. However, if your goal is to find a model that is best in terms of some selection criterion other than the significance level of the entering effect, then even this one step choice might not be optimal. For example, the effect you would add to get a model with the smallest value of the PRESS statistic at the next step is not necessarily the same effect that has the most significant entry F statistic. Note that in the case where all effects are variables (that is, effects with one degree of freedom and no hierarchy), using ADJRSQ , AIC , AICC , BIC , CP , RSQUARE , or SBC as the selection criterion for forward selection produces the same sequence of additions. However, if the degrees of freedom contributed by different effects are not constant, or if an out-of-sample prediction-based criterion is used, then different sequences of additions might be obtained. Backward Method ( BACKWARD ) Link The backward elimination technique starts from the full model including all independent effects. Then effects are deleted one by one until a stopping condition is satisfied. At each step, the effect showing the smallest contribution to the model is deleted. In traditional implementations of backward elimination, the contribution of an effect to the model is assessed by using an F statistic. At any step, the predictor producing the least significant F statistic is dropped and the process continues until all effects remaining in the model have F statistics significant at a stay significance level ( SLS ). Stepwise Method ( STEPWISE ) Link In the traditional implementation of stepwise selection method, the same entry and removal F statistics for the forward selection and backward elimination methods are used to assess contributions of effects as they are added to or removed from a model. If at a step of the stepwise method, any effect in the model is not significant at the SLSTAY= level, then the least significant of these effects is removed from the model and the algorithm proceeds to the next step. This ensures that no effect can be added to a model while some effect currently in the model is not deemed significant. Only after all necessary deletions have been accomplished can another effect be added to the model. For selection criteria other than significance level, PROC GLMSELECT optionally supports a further modification in the stepwise method. In the standard stepwise method, no effect can enter the model if removing any effect currently in the model would yield an improved value of the selection criterion. In the modification, you can use the DROP=COMPETITIVE option to specify that addition and deletion of effects should be treated competitively. The selection criterion is evaluated for all models obtained by deleting an effect from the current model or by adding an effect to this model. The action that most improves the selection criterion is the action taken. Code Examples Link 1 selection = forward adds effects that at each step give the lowest value of the SBC statistic and stops at the step where adding any effect would increase the SBC statistic. 1 selection = forward ( select = SL ) adds effects based on significance level and stops when all candidate effects for entry at a step have a significance level greater than the default entry significance level of 0.50. 1 selection = forward ( select = ADJRSQ stop = SL SLE = 0 . 2 ) adds effects that at each step give the largest value of the adjusted R-square statistic and stops at the step where the significance level corresponding to the addition of this effect is greater than 0.2. 1 selection = forward ( select = SL stop = AIC ) terminates at the step where the effect to be added at the next step would produce a model with an AIC statistic larger than the AIC statistic of the current model. Provided that the entry significance level is large enough that the local extremum of the named criterion occurs before the final step, specifying any of these options the same model is selected, but more steps are done in the second case: 1 2 selection = forward ( select = SL choose = CRITERION ) selection = forward ( select = SL stop = CRITERION ) In some cases there might be a better local extremum that cannot be reached if you specify the STOP= option but can be found if you use the CHOOSE= option. Also, you can use the CHOOSE= option in preference to the STOP= option if you want examine how the named criterion behaves as you move beyond the step where the first local minimum of this criterion occurs. Note that you can specify both the CHOOSE= and STOP= options. You might want to consider models generated by forward selection that have at most some fixed number of effects but select from within this set based on a criterion you specify. The following example requests that forward selection continue until there are 20 effects in the final model and chooses among the sequence of models the one that has the largest value of the adjusted R-square statistic. 1 selection = forward ( stop = 20 choose = ADJRSQ ) You can also combine these options to select a model where one of two conditions is met. The following example chooses whatever occurs first between a local minimum of the predicted residual sum of squares ( PRESS ) and a local minimum of corrected Akaike\u2019s information criterion ( AICC ). 1 selection = forward ( stop = AICC choose = PRESS ) PROC GLMSELECT enables you to specify the criterion to optimize at each step by using the SELECT= option. For example, the following example requests that at each step the effect that is added be the one that gives a model with the smallest value of the Mallows\u2019 $C_p$ statistic. 1 selection = forward ( select = CP ) You can use SELECT= together with CHOOSE= and STOP= . If you specify only the SELECT= criterion, then this criterion is also used as the stopping criterion. In the previous example where only the selection criterion is specified, not only do effects enter based on the Mallows\u2019 $C_p$ statistic, but the selection terminates when the $C_p$ statistic first increases. 1 selection = backward removes effects that at each step produce the largest value of the Schwarz Bayesian information criterion ( SBC ) statistic and stops at the step where removing any effect increases the SBC statistic. 1 selection = backward ( stop = press ) removes effects based on the SBC statistic and stops at the step where removing any effect increases the predicted residual sum of squares ( PRESS ). 1 selection = backward ( select = SL ) removes effects based on significance level and stops when all candidate effects for removal at a step have a significance level less than the default stay significance level of 0.10. 1 selection = backward ( select = SL choose = validate SLS = 0 . 1 ) removes effects based on significance level and stops when all effects in the model are significant at the 0.1 level. Finally, from the sequence of models generated, choose the one that gives the smallest average square error when scored on the validation data. Penalized Regression Methods for Linear Models Link Least Absolute Selection Shrinkage Operator ( LASSO ) Link Lasso regression is what is called the Penalized regression method, often used in machine learning to select the subset of variables. It is a supervised machine learning method. Specifically, LASSO is a Shrinkage and Variable Selection method for linear regression models . The LASSO algorithm imposes a constraint on the sum of the absolute values of the model parameters , where the sum has a specified constant as an upper bound. This constraint causes regression coefficients for some variables to shrink towards zero allowing for a better interpretation of the model and to identifiying the variables most strongly associated with the response variable by obtaining the subset of predictors that minimizes prediction error. So why use Lasso instead of just using ordinary least squares (OLS) multiple regression? It can provide greater prediction accuracy . If the true relationship between the response variable and the predictors is approximately linear and you have a large number of observations, then OLS regression parameter estimates will have low bias and low variance. However, if you have a relatively small number of observations and a large number of predictors, then the variance of the OLS parameter estimates will be higher . In this case, LASSO regression is useful because shrinking the regression coefficients can reduce variance without a substantial increase in bias. LASSO regression can increase model interpretability . Often times, at least some of the explanatory variables in an OLS multiple regression analysis are not really associated with the response variable resulting in overfitted models which are more difficult to interpret. With Lasso Regression, the regression coefficients for unimportant variables are reduced to zero which effectively removes them from the model and produces a simpler one. In Lasso Regression, a tuning parameter, $\\lambda$, is included in the model to control the strength of the penalty. As $\\lambda$ increases, more coefficients are reduced to zero, that is fewer predictors are selected and there is more shrinkage of the non-zero coefficient. With Lasso Regression when $\\lambda=0$ we have an OLS regression analysis. Bias increases and variance decreases as $\\lambda$ increases. Although LASSO regression models can handle categorical variables with more than two levels, you can also create a serie of auxiliary binary categorical varaibles in order to improve the interpretability of the selected model. Binary substitutes variables for measure with individual questions. Least Angle Regression ( LAR ) Link The LAR algorithm starts with no predictors in the model and adds a predictor at each step. It first adds a predictor that is most correlated with the response variable and moves it towards least square estimate, until there is another predictor that is equally correlated with the model residual. It adds this predictor to the model and starts the least square estimation process over again, with both variables. The LAR algorithm continues with this process until it has tested all the predictors. Parameter estimates at any step are shrunk and predictors with coefficients that are shrunk to zero are removed from the model so the process starts all over again. Code Examples Link 1 2 3 4 5 * LASSO multiple regression with LARS algorithm k = 10 fold validation ; PROG GLMSELECT DATA = SAS - data - set PLOTS = ALL SEED = 12345 ; PARTITION ROLE = selected ( train = ' 1 ' test = ' 0 ' ) ; MODEL response = predictor1 predictor2 ... predictorN / SELECTION = LAR ( CHOOSE = CV STOP = NONE ) CVMETHOD = RANDOM ( 10 ) ; RUN ; The PARTITION statement assigns each observation a role, based on the variable called selected, to indicate whether the observation is a training or test observation. After the slash of the MODEL statement, we specify the options we want to use to test the model. The SELECTION option tells us which method to use to compute the parameters for variable selection ( LAR algorithm in this example). The CHOOSE=CV option, asks SAS to use cross validation to choose the final statistical model. STOP=NONE ensures that the model doesn't stop running until each of the candidate predictor variables is tested. CVMETHOD=RANDOM(10) , specifies that a K-fold cross-validation method with ten randomly selected folds will be used. The model with the lowest average means square error is selected by SAS as the best model. Tip In LASSO regression, the penalty term is not fair if the predictive variables are not on the same scale. Meaning that not all the predictors will get the same penalty. The SAS GLMSELECT procedure handles this by automatically standardizing the predictor variables, so that they all have a mean equal to zero and a standard deviation equal to one, which places them all on the same scale. LASSO Regression Limitations Link As with any statistical methods, the LASSO regression has some limitations. Selection of variables is 100% statistically driven. The LASSO selection process does not think like a human being, who take into account theory and other factors in deciding which predictors to include. There might be a good rational for including a predictor, even if it appears to have no association with response variable. If predictors are strongly correlated with each other, the Lasso will arbitrarily select one of them . You may have different ideas about which of the predictors you would choose include or whether it's important to keep more than one. Estimating p-values for LASSO regression is not so straightforward , although methods to calculate p-values have been developed. Different selection methods and even different software packages can produce different results. There's no guarantee that the model selected by the LASSO regression will not be overfitted or the best model. If you find yourself in a position which you have a large number of potential predictors of a response variable, what do you do? The best solution may be a combination of machine learning, human intervention, and independent application . Model Selection Algorithms Pros and Cons Link Note that while the model selection question seems reasonable, trying to answer it for real data can lead to problematic pitfalls, including The selected model is not guaranteed to be the \"best\"; there may be other, more parsimonious or more intuitively reasonable models that may provide nearly as good or even better models, but which the particular heuristic method employed does not find Model selection may be unduly affected by outliers There is a \"selection bias\" because a parameter is more likely to be selected if it is above its expected value than if it is below its expected value Standard methods of inference for the final model are invalid in the model selection context However, certain features of GLMSELECT , in particular the procedure\u2019s extensive capabilities for customizing the selection and its flexibility and power in specifying complex potential effects , can partially mitigate these problems.","title":"Model Selection"},{"location":"other-analysis/model-selection/#traditional-model-selection-algoritms","text":"The GLMSELECT procedure extends the familiar forward, backward, and stepwise methods as implemented in the REG procedure to GLM-type models. Quite simply, FORWARD selection adds parameters one at a time, BACKWARD elimination deletes them, and STEPWISE selection switches between adding and deleting them.","title":"Traditional Model Selection Algoritms"},{"location":"other-analysis/model-selection/#forward-method-forward","text":"It is important to keep in mind that forward selection bases the decision about what effect to add at any step by considering models that differ by one effect from the current model . This search paradigm cannot guarantee reaching a \"best\" subset model. Furthermore, the add decision is greedy in the sense that the effect deemed most significant is the effect that is added. However, if your goal is to find a model that is best in terms of some selection criterion other than the significance level of the entering effect, then even this one step choice might not be optimal. For example, the effect you would add to get a model with the smallest value of the PRESS statistic at the next step is not necessarily the same effect that has the most significant entry F statistic. Note that in the case where all effects are variables (that is, effects with one degree of freedom and no hierarchy), using ADJRSQ , AIC , AICC , BIC , CP , RSQUARE , or SBC as the selection criterion for forward selection produces the same sequence of additions. However, if the degrees of freedom contributed by different effects are not constant, or if an out-of-sample prediction-based criterion is used, then different sequences of additions might be obtained.","title":"Forward Method (FORWARD)"},{"location":"other-analysis/model-selection/#backward-method-backward","text":"The backward elimination technique starts from the full model including all independent effects. Then effects are deleted one by one until a stopping condition is satisfied. At each step, the effect showing the smallest contribution to the model is deleted. In traditional implementations of backward elimination, the contribution of an effect to the model is assessed by using an F statistic. At any step, the predictor producing the least significant F statistic is dropped and the process continues until all effects remaining in the model have F statistics significant at a stay significance level ( SLS ).","title":"Backward Method (BACKWARD)"},{"location":"other-analysis/model-selection/#stepwise-method-stepwise","text":"In the traditional implementation of stepwise selection method, the same entry and removal F statistics for the forward selection and backward elimination methods are used to assess contributions of effects as they are added to or removed from a model. If at a step of the stepwise method, any effect in the model is not significant at the SLSTAY= level, then the least significant of these effects is removed from the model and the algorithm proceeds to the next step. This ensures that no effect can be added to a model while some effect currently in the model is not deemed significant. Only after all necessary deletions have been accomplished can another effect be added to the model. For selection criteria other than significance level, PROC GLMSELECT optionally supports a further modification in the stepwise method. In the standard stepwise method, no effect can enter the model if removing any effect currently in the model would yield an improved value of the selection criterion. In the modification, you can use the DROP=COMPETITIVE option to specify that addition and deletion of effects should be treated competitively. The selection criterion is evaluated for all models obtained by deleting an effect from the current model or by adding an effect to this model. The action that most improves the selection criterion is the action taken.","title":"Stepwise Method (STEPWISE)"},{"location":"other-analysis/model-selection/#code-examples","text":"1 selection = forward adds effects that at each step give the lowest value of the SBC statistic and stops at the step where adding any effect would increase the SBC statistic. 1 selection = forward ( select = SL ) adds effects based on significance level and stops when all candidate effects for entry at a step have a significance level greater than the default entry significance level of 0.50. 1 selection = forward ( select = ADJRSQ stop = SL SLE = 0 . 2 ) adds effects that at each step give the largest value of the adjusted R-square statistic and stops at the step where the significance level corresponding to the addition of this effect is greater than 0.2. 1 selection = forward ( select = SL stop = AIC ) terminates at the step where the effect to be added at the next step would produce a model with an AIC statistic larger than the AIC statistic of the current model. Provided that the entry significance level is large enough that the local extremum of the named criterion occurs before the final step, specifying any of these options the same model is selected, but more steps are done in the second case: 1 2 selection = forward ( select = SL choose = CRITERION ) selection = forward ( select = SL stop = CRITERION ) In some cases there might be a better local extremum that cannot be reached if you specify the STOP= option but can be found if you use the CHOOSE= option. Also, you can use the CHOOSE= option in preference to the STOP= option if you want examine how the named criterion behaves as you move beyond the step where the first local minimum of this criterion occurs. Note that you can specify both the CHOOSE= and STOP= options. You might want to consider models generated by forward selection that have at most some fixed number of effects but select from within this set based on a criterion you specify. The following example requests that forward selection continue until there are 20 effects in the final model and chooses among the sequence of models the one that has the largest value of the adjusted R-square statistic. 1 selection = forward ( stop = 20 choose = ADJRSQ ) You can also combine these options to select a model where one of two conditions is met. The following example chooses whatever occurs first between a local minimum of the predicted residual sum of squares ( PRESS ) and a local minimum of corrected Akaike\u2019s information criterion ( AICC ). 1 selection = forward ( stop = AICC choose = PRESS ) PROC GLMSELECT enables you to specify the criterion to optimize at each step by using the SELECT= option. For example, the following example requests that at each step the effect that is added be the one that gives a model with the smallest value of the Mallows\u2019 $C_p$ statistic. 1 selection = forward ( select = CP ) You can use SELECT= together with CHOOSE= and STOP= . If you specify only the SELECT= criterion, then this criterion is also used as the stopping criterion. In the previous example where only the selection criterion is specified, not only do effects enter based on the Mallows\u2019 $C_p$ statistic, but the selection terminates when the $C_p$ statistic first increases. 1 selection = backward removes effects that at each step produce the largest value of the Schwarz Bayesian information criterion ( SBC ) statistic and stops at the step where removing any effect increases the SBC statistic. 1 selection = backward ( stop = press ) removes effects based on the SBC statistic and stops at the step where removing any effect increases the predicted residual sum of squares ( PRESS ). 1 selection = backward ( select = SL ) removes effects based on significance level and stops when all candidate effects for removal at a step have a significance level less than the default stay significance level of 0.10. 1 selection = backward ( select = SL choose = validate SLS = 0 . 1 ) removes effects based on significance level and stops when all effects in the model are significant at the 0.1 level. Finally, from the sequence of models generated, choose the one that gives the smallest average square error when scored on the validation data.","title":"Code Examples"},{"location":"other-analysis/model-selection/#penalized-regression-methods-for-linear-models","text":"","title":"Penalized Regression Methods for Linear Models"},{"location":"other-analysis/model-selection/#least-absolute-selection-shrinkage-operator-lasso","text":"Lasso regression is what is called the Penalized regression method, often used in machine learning to select the subset of variables. It is a supervised machine learning method. Specifically, LASSO is a Shrinkage and Variable Selection method for linear regression models . The LASSO algorithm imposes a constraint on the sum of the absolute values of the model parameters , where the sum has a specified constant as an upper bound. This constraint causes regression coefficients for some variables to shrink towards zero allowing for a better interpretation of the model and to identifiying the variables most strongly associated with the response variable by obtaining the subset of predictors that minimizes prediction error. So why use Lasso instead of just using ordinary least squares (OLS) multiple regression? It can provide greater prediction accuracy . If the true relationship between the response variable and the predictors is approximately linear and you have a large number of observations, then OLS regression parameter estimates will have low bias and low variance. However, if you have a relatively small number of observations and a large number of predictors, then the variance of the OLS parameter estimates will be higher . In this case, LASSO regression is useful because shrinking the regression coefficients can reduce variance without a substantial increase in bias. LASSO regression can increase model interpretability . Often times, at least some of the explanatory variables in an OLS multiple regression analysis are not really associated with the response variable resulting in overfitted models which are more difficult to interpret. With Lasso Regression, the regression coefficients for unimportant variables are reduced to zero which effectively removes them from the model and produces a simpler one. In Lasso Regression, a tuning parameter, $\\lambda$, is included in the model to control the strength of the penalty. As $\\lambda$ increases, more coefficients are reduced to zero, that is fewer predictors are selected and there is more shrinkage of the non-zero coefficient. With Lasso Regression when $\\lambda=0$ we have an OLS regression analysis. Bias increases and variance decreases as $\\lambda$ increases. Although LASSO regression models can handle categorical variables with more than two levels, you can also create a serie of auxiliary binary categorical varaibles in order to improve the interpretability of the selected model. Binary substitutes variables for measure with individual questions.","title":"Least Absolute Selection Shrinkage Operator (LASSO)"},{"location":"other-analysis/model-selection/#least-angle-regression-lar","text":"The LAR algorithm starts with no predictors in the model and adds a predictor at each step. It first adds a predictor that is most correlated with the response variable and moves it towards least square estimate, until there is another predictor that is equally correlated with the model residual. It adds this predictor to the model and starts the least square estimation process over again, with both variables. The LAR algorithm continues with this process until it has tested all the predictors. Parameter estimates at any step are shrunk and predictors with coefficients that are shrunk to zero are removed from the model so the process starts all over again.","title":"Least Angle Regression (LAR)"},{"location":"other-analysis/model-selection/#code-examples_1","text":"1 2 3 4 5 * LASSO multiple regression with LARS algorithm k = 10 fold validation ; PROG GLMSELECT DATA = SAS - data - set PLOTS = ALL SEED = 12345 ; PARTITION ROLE = selected ( train = ' 1 ' test = ' 0 ' ) ; MODEL response = predictor1 predictor2 ... predictorN / SELECTION = LAR ( CHOOSE = CV STOP = NONE ) CVMETHOD = RANDOM ( 10 ) ; RUN ; The PARTITION statement assigns each observation a role, based on the variable called selected, to indicate whether the observation is a training or test observation. After the slash of the MODEL statement, we specify the options we want to use to test the model. The SELECTION option tells us which method to use to compute the parameters for variable selection ( LAR algorithm in this example). The CHOOSE=CV option, asks SAS to use cross validation to choose the final statistical model. STOP=NONE ensures that the model doesn't stop running until each of the candidate predictor variables is tested. CVMETHOD=RANDOM(10) , specifies that a K-fold cross-validation method with ten randomly selected folds will be used. The model with the lowest average means square error is selected by SAS as the best model. Tip In LASSO regression, the penalty term is not fair if the predictive variables are not on the same scale. Meaning that not all the predictors will get the same penalty. The SAS GLMSELECT procedure handles this by automatically standardizing the predictor variables, so that they all have a mean equal to zero and a standard deviation equal to one, which places them all on the same scale.","title":"Code Examples"},{"location":"other-analysis/model-selection/#lasso-regression-limitations","text":"As with any statistical methods, the LASSO regression has some limitations. Selection of variables is 100% statistically driven. The LASSO selection process does not think like a human being, who take into account theory and other factors in deciding which predictors to include. There might be a good rational for including a predictor, even if it appears to have no association with response variable. If predictors are strongly correlated with each other, the Lasso will arbitrarily select one of them . You may have different ideas about which of the predictors you would choose include or whether it's important to keep more than one. Estimating p-values for LASSO regression is not so straightforward , although methods to calculate p-values have been developed. Different selection methods and even different software packages can produce different results. There's no guarantee that the model selected by the LASSO regression will not be overfitted or the best model. If you find yourself in a position which you have a large number of potential predictors of a response variable, what do you do? The best solution may be a combination of machine learning, human intervention, and independent application .","title":"LASSO Regression Limitations"},{"location":"other-analysis/model-selection/#model-selection-algorithms-pros-and-cons","text":"Note that while the model selection question seems reasonable, trying to answer it for real data can lead to problematic pitfalls, including The selected model is not guaranteed to be the \"best\"; there may be other, more parsimonious or more intuitively reasonable models that may provide nearly as good or even better models, but which the particular heuristic method employed does not find Model selection may be unduly affected by outliers There is a \"selection bias\" because a parameter is more likely to be selected if it is above its expected value than if it is below its expected value Standard methods of inference for the final model are invalid in the model selection context However, certain features of GLMSELECT , in particular the procedure\u2019s extensive capabilities for customizing the selection and its flexibility and power in specifying complex potential effects , can partially mitigate these problems.","title":"Model Selection Algorithms Pros and Cons"},{"location":"other-analysis/normality-tests/","text":"Saphiro-Wilk normality test Link In statistics, the Shapiro-Wilk test is used to test the normality of a data set. It is considered one of the most powerful tests for normality contrast, especially for small samples ($n<50$). Monte Carlo simulations have found that Shapiro\u2013Wilk has the best power for a given significance , followed closely by Anderson\u2013Darling when comparing to the Kolmogorov\u2013Smirnov, Lilliefors, and Anderson\u2013Darling tests. The null-hypothesis of this test is that the population is normally distributed. Thus, if the p-value is less than the chosen alpha level, then the null hypothesis is rejected and there is evidence that the data tested are not from a normally distributed population if the p-value is greater than the chosen alpha level, then the null hypothesis cannot be rejected However, since the test is biased by sample size , it may be statistically significant from a normal distribution in any large samples and a Q\u2013Q plot would be required for verification in addition to the test. PROC UNIVARIATE Link The UNIVARIATE procedure provides a variety of descriptive statistics, and draws Q-Q, stem-and-leaf, normal probability, and box plots. This procedure also conducts Shapiro-Wilk, Kolmogorov-Smirnov, Anderson-Darling and Cramer-von Misers tests. Note The Shapiro-Wilk W will be reported only if $N<2000$. 1 2 3 4 5 PROC UNIVARIATE DATA = SAS - data - set NORMAL PLOT ; VAR variable ( s ); QQPLOT variable / NORMAL ( MU = EST SIGMA = EST COLOR = RED L = 1 ); OUTPUT OUT = normality PROBN = probn ; RUN ; NORMAL performs normality tests PLOT draws a stem-and-leaf and a box plots QQPLOT draws a Q-Q plot Note You must provide a VAR statement when you use an OUTPUT statement. To store the same statistic for several analysis variables in the OUT= data set, you specify a list of names in the OUTPUT statement. PROC UNIVARIATE makes a one-to-one correspondence between the order of the analysis variables in the VAR statement and the list of names that follow a statistic keyword. PROC CAPABILITY Link Like UNIVARIATE , the CAPABILITY procedure also produces various descriptive statistics and plots. CAPABILITY can draw a P-P plot using the PPPLOT option but does not support stem-and-leaf, box, and normal probability plots (it does not have the PLOT option). 1 2 3 4 5 6 7 PROC CAPABILITY DATA = SAS - data - set NORMAL ; VAR variable ( s ); QQPLOT variable / NORMAL ( MU = EST SIGMA = EST COLOR = RED L = 1 ); PPPLOT variable / NORMAL ( MU = EST SIGMA = EST COLOR = RED L = 1 ); HISTOGRAM / NORMAL ( COLOR = MAROON W = 4 ) CFILL = BLUE CFRAME = LIGR ; INSET MEAN STD / CFILL = BLANK FORMAT = 5 . 2 ; RUN ; NORMAL performs normality tests QQPLOT , PPPLOT and HISTOGRAM statements respectively draw a Q-Q plot, a P-P plot, and a histogram INSET statement adds summary statistics to graphs such as a histogram and a Q-Q plot","title":"Normality Tests"},{"location":"other-analysis/normality-tests/#saphiro-wilk-normality-test","text":"In statistics, the Shapiro-Wilk test is used to test the normality of a data set. It is considered one of the most powerful tests for normality contrast, especially for small samples ($n<50$). Monte Carlo simulations have found that Shapiro\u2013Wilk has the best power for a given significance , followed closely by Anderson\u2013Darling when comparing to the Kolmogorov\u2013Smirnov, Lilliefors, and Anderson\u2013Darling tests. The null-hypothesis of this test is that the population is normally distributed. Thus, if the p-value is less than the chosen alpha level, then the null hypothesis is rejected and there is evidence that the data tested are not from a normally distributed population if the p-value is greater than the chosen alpha level, then the null hypothesis cannot be rejected However, since the test is biased by sample size , it may be statistically significant from a normal distribution in any large samples and a Q\u2013Q plot would be required for verification in addition to the test.","title":"Saphiro-Wilk normality test"},{"location":"other-analysis/normality-tests/#proc-univariate","text":"The UNIVARIATE procedure provides a variety of descriptive statistics, and draws Q-Q, stem-and-leaf, normal probability, and box plots. This procedure also conducts Shapiro-Wilk, Kolmogorov-Smirnov, Anderson-Darling and Cramer-von Misers tests. Note The Shapiro-Wilk W will be reported only if $N<2000$. 1 2 3 4 5 PROC UNIVARIATE DATA = SAS - data - set NORMAL PLOT ; VAR variable ( s ); QQPLOT variable / NORMAL ( MU = EST SIGMA = EST COLOR = RED L = 1 ); OUTPUT OUT = normality PROBN = probn ; RUN ; NORMAL performs normality tests PLOT draws a stem-and-leaf and a box plots QQPLOT draws a Q-Q plot Note You must provide a VAR statement when you use an OUTPUT statement. To store the same statistic for several analysis variables in the OUT= data set, you specify a list of names in the OUTPUT statement. PROC UNIVARIATE makes a one-to-one correspondence between the order of the analysis variables in the VAR statement and the list of names that follow a statistic keyword.","title":"PROC UNIVARIATE"},{"location":"other-analysis/normality-tests/#proc-capability","text":"Like UNIVARIATE , the CAPABILITY procedure also produces various descriptive statistics and plots. CAPABILITY can draw a P-P plot using the PPPLOT option but does not support stem-and-leaf, box, and normal probability plots (it does not have the PLOT option). 1 2 3 4 5 6 7 PROC CAPABILITY DATA = SAS - data - set NORMAL ; VAR variable ( s ); QQPLOT variable / NORMAL ( MU = EST SIGMA = EST COLOR = RED L = 1 ); PPPLOT variable / NORMAL ( MU = EST SIGMA = EST COLOR = RED L = 1 ); HISTOGRAM / NORMAL ( COLOR = MAROON W = 4 ) CFILL = BLUE CFRAME = LIGR ; INSET MEAN STD / CFILL = BLANK FORMAT = 5 . 2 ; RUN ; NORMAL performs normality tests QQPLOT , PPPLOT and HISTOGRAM statements respectively draw a Q-Q plot, a P-P plot, and a histogram INSET statement adds summary statistics to graphs such as a histogram and a Q-Q plot","title":"PROC CAPABILITY"},{"location":"other-analysis/post-hoc-tests/","text":"Fuera de contexto pero puede valer para localizar m\u00e9todos: http://www.redalyc.org/articulo.oa?id=30235107 The objective of this study was to compare the ability of the Tukey and Duncan procedures and the Dunnett method (multiple comparisons versus a control) to select means with the methods of Bechhofer and Hsu, which are specifically designed to this end. Comparisons were done in a one-way balanced model using the Bechhofer method as a reference. Results showed that Bechhofer was the most suitable method for selection of means. The Dunnett method coincided in numerical results with that of Hsu (comparisons against the best), and both surpassed Tukey's and Duncan's methods. Hence, Dunnett was a good alternative. Between the Tukey and Duncan methods, Duncan was better since Tukey was too conservative. In statistics, family-wise error rate (FWER) is the probability of making one or more false discoveries, or type I errors when performing multiple hypotheses tests. https://www.researchgate.net/post/The_choice_of_post-hoc_test ANOVA is not interesting (and sensible) at all when you anyway want to compare individual groups. ANOVA is only relevant to asses the impact of a predictor (or a set of predictors) in a more complex model. It is only very, very indirectly and marginally related to the question if expected values differ between levels of a categorical predictor. Almost all post-hoc tests control the FWER independent of the ANOVA, and the \"two-step-procedure\" (first check ANOVA, then do post-hoc-tests) leads to additional problems.Note that \"post-hoc\" doe not mean \"after ANOVA\" but \"after knowing the data (of the other groups)\". This is often misinterpreted in many textbooks. To my experience, differences between the different post-hoc tests are very \"academic\" and typically of only little practical relevance. Practically, there are only two tests I consider: Tukey's HSD for all-pairwise comparisons and Dunnett's procedure for multiple-to-one comparisons. They are quite generally applicable and have no severe problems (as far as I know). Dunnett is superior for multiple-to-one comparisons, because Tukey assumes that k*(k-1) tests are performed (each of which can produce a false positive result) whereas Dunnett's procedure has to consider only k-1 tests (k is the number of groups). There is an intermediate between Tukey and Dunnett: Hsu's multiple comparisons with best ( MCB ). This test is like Dunnett's test but without a defined control group. This test first determines the \"control group\" as the \"best group\", which is the group with the most extreme mean and then compared all other groups to this one. It thus makes (k-1) tests (better than Tukey) but first has to determine the \"best\", what is negatively affecting the FWER (worse than Dunnett). (Note that selecting the \"best\" group by eye and then do Dunnett's test ignores the uncertainty associated with the selection of the \"best\" group, so the FWER is not really controlled then). If I test only very few and specific pairs of many possible pairs, I prefer Holm adjustments to control the FWER . This can outperform Tukey's test w.r.t. (with regards/respect to) power. For screenings it is often more sensible to control the FDR, what is done by the procedure of Benjamini/Hochberg. However, in research it is more important - to my opinion - to think if and what error rate should be reasonably controlled at what level. The rather mindless control of a FWER at 5% is quite stupid, although typical... http://ftp.sas.com/samples/A56648 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 1569 1570 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584 1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 1598 1599 1600 1601 1602 1603 1604 1605 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616 1617 1618 1619 1620 1621 1622 1623 1624 1625 1626 1627 1628 1629 1630 1631 1632 1633 1634 1635 1636 1637 1638 1639 1640 1641 1642 1643 1644 1645 1646 1647 1648 1649 1650 1651 1652 1653 1654 1655 1656 1657 1658 1659 1660 1661 1662 1663 1664 1665 1666 1667 1668 1669 1670 1671 1672 1673 1674 1675 1676 1677 1678 1679 1680 1681 1682 1683 1684 1685 1686 1687 1688 1689 1690 1691 1692 1693 1694 1695 1696 1697 1698 1699 1700 1701 1702 1703 1704 1705 1706 1707 1708 1709 1710 1711 1712 1713 1714 1715 1716 1717 1718 1719 1720 1721 1722 1723 1724 1725 1726 1727 1728 1729 1730 1731 1732 1733 1734 1735 1736 1737 1738 1739 1740 1741 1742 1743 1744 1745 1746 1747 1748 1749 1750 1751 1752 1753 1754 1755 1756 1757 1758 1759 1760 1761 1762 1763 1764 1765 1766 1767 1768 1769 1770 1771 1772 1773 1774 1775 1776 1777 1778 1779 1780 1781 1782 1783 1784 1785 1786 1787 1788 1789 1790 1791 1792 1793 1794 1795 1796 1797 1798 1799 1800 1801 1802 1803 1804 1805 1806 1807 1808 1809 1810 1811 1812 1813 1814 1815 1816 1817 1818 1819 1820 1821 1822 1823 1824 1825 1826 1827 1828 1829 1830 1831 1832 1833 1834 1835 1836 1837 1838 1839 1840 1841 1842 1843 1844 1845 1846 1847 1848 1849 1850 1851 1852 1853 1854 1855 1856 1857 1858 1859 1860 1861 1862 1863 1864 1865 1866 1867 1868 1869 1870 1871 1872 1873 1874 1875 1876 1877 1878 1879 1880 1881 1882 1883 1884 1885 1886 1887 1888 1889 1890 1891 1892 1893 1894 1895 1896 1897 1898 1899 1900 1901 1902 1903 1904 1905 1906 1907 1908 1909 1910 1911 1912 1913 1914 1915 1916 1917 1918 1919 1920 1921 1922 1923 1924 1925 1926 1927 1928 1929 1930 1931 1932 1933 1934 1935 1936 1937 1938 1939 1940 1941 1942 1943 1944 1945 1946 1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025 2026 2027 2028 2029 2030 2031 2032 2033 2034 2035 2036 2037 2038 2039 2040 2041 2042 2043 2044 2045 2046 2047 2048 2049 2050 2051 2052 2053 2054 2055 2056 2057 2058 2059 2060 2061 2062 2063 2064 2065 2066 2067 2068 2069 2070 2071 2072 2073 2074 2075 2076 2077 2078 2079 2080 2081 2082 2083 2084 2085 2086 2087 2088 2089 2090 2091 2092 2093 2094 2095 2096 2097 2098 2099 2100 2101 2102 2103 2104 2105 2106 2107 2108 2109 2110 2111 2112 2113 2114 2115 2116 2117 2118 2119 2120 2121 2122 2123 2124 2125 2126 2127 2128 2129 2130 2131 2132 2133 2134 2135 2136 2137 2138 2139 2140 2141 2142 2143 2144 2145 2146 2147 2148 2149 2150 2151 2152 2153 2154 2155 2156 2157 2158 2159 2160 2161 2162 2163 2164 2165 2166 2167 2168 2169 2170 2171 2172 2173 2174 2175 2176 2177 2178 2179 2180 2181 2182 2183 2184 2185 2186 2187 2188 2189 2190 2191 2192 2193 2194 2195 2196 2197 2198 2199 2200 2201 2202 2203 2204 2205 2206 2207 2208 2209 2210 2211 2212 2213 2214 2215 2216 2217 2218 2219 2220 2221 2222 2223 2224 2225 2226 2227 2228 2229 2230 2231 2232 2233 2234 2235 2236 2237 2238 2239 2240 2241 2242 2243 2244 2245 2246 2247 2248 2249 2250 2251 2252 2253 2254 2255 2256 2257 2258 2259 2260 2261 2262 2263 2264 2265 2266 2267 2268 2269 2270 2271 2272 2273 2274 2275 2276 2277 2278 2279 2280 2281 2282 2283 2284 2285 2286 2287 2288 2289 2290 2291 2292 2293 2294 2295 2296 2297 2298 2299 2300 2301 2302 2303 2304 2305 2306 2307 2308 2309 2310 2311 2312 2313 2314 2315 2316 2317 2318 2319 2320 2321 2322 2323 2324 2325 2326 2327 2328 2329 2330 2331 2332 2333 2334 2335 2336 2337 2338 2339 2340 2341 2342 2343 2344 2345 2346 2347 2348 2349 2350 2351 2352 2353 2354 2355 2356 2357 2358 2359 2360 2361 2362 2363 2364 2365 2366 2367 2368 2369 2370 2371 2372 2373 2374 2375 2376 2377 2378 2379 2380 2381 2382 2383 2384 2385 2386 2387 2388 2389 2390 2391 2392 2393 2394 2395 2396 2397 2398 2399 2400 2401 2402 2403 2404 2405 2406 2407 2408 2409 2410 2411 2412 2413 2414 2415 2416 2417 2418 2419 2420 2421 2422 2423 2424 2425 2426 2427 2428 2429 2430 2431 2432 2433 2434 2435 2436 2437 2438 2439 2440 2441 2442 2443 2444 2445 2446 2447 2448 2449 2450 2451 2452 2453 2454 2455 2456 2457 2458 2459 2460 2461 2462 2463 2464 2465 2466 2467 2468 2469 2470 2471 2472 2473 2474 2475 2476 2477 2478 2479 2480 2481 2482 2483 2484 2485 2486 2487 2488 2489 2490 2491 2492 2493 2494 2495 2496 2497 2498 2499 2500 2501 2502 2503 2504 2505 2506 2507 2508 2509 2510 2511 2512 2513 2514 2515 2516 2517 2518 2519 2520 2521 2522 2523 2524 2525 2526 2527 2528 2529 2530 2531 2532 2533 2534 2535 2536 2537 2538 2539 2540 2541 2542 2543 2544 2545 2546 2547 2548 2549 2550 2551 2552 2553 2554 2555 2556 2557 2558 2559 2560 2561 2562 2563 2564 2565 2566 2567 2568 2569 2570 2571 2572 2573 2574 2575 2576 2577 2578 2579 2580 2581 2582 2583 2584 2585 2586 2587 2588 2589 2590 2591 2592 2593 2594 2595 2596 2597 2598 2599 2600 2601 2602 2603 2604 2605 2606 2607 2608 2609 2610 2611 2612 2613 2614 2615 2616 2617 2618 2619 2620 2621 2622 2623 2624 2625 2626 2627 2628 2629 2630 2631 2632 2633 2634 2635 2636 2637 2638 2639 2640 2641 2642 2643 2644 2645 2646 2647 2648 2649 2650 2651 2652 2653 2654 2655 2656 2657 2658 2659 2660 2661 2662 2663 2664 2665 2666 2667 2668 2669 2670 2671 2672 2673 2674 2675 2676 2677 2678 2679 2680 2681 2682 2683 2684 2685 2686 2687 2688 2689 2690 2691 2692 2693 2694 2695 2696 2697 2698 2699 2700 2701 2702 2703 2704 2705 2706 2707 2708 2709 2710 2711 2712 2713 2714 2715 2716 2717 2718 2719 2720 2721 2722 2723 2724 2725 2726 2727 2728 2729 2730 2731 2732 2733 2734 2735 2736 2737 2738 2739 2740 2741 2742 2743 2744 2745 2746 2747 2748 2749 2750 2751 2752 2753 2754 2755 2756 2757 2758 2759 2760 2761 2762 2763 2764 2765 2766 2767 2768 2769 2770 2771 2772 2773 2774 2775 2776 2777 2778 2779 2780 2781 2782 2783 2784 2785 2786 2787 2788 2789 2790 2791 2792 2793 2794 2795 2796 2797 2798 2799 2800 2801 2802 2803 2804 2805 2806 2807 2808 2809 2810 2811 2812 2813 2814 2815 2816 2817 2818 2819 2820 2821 2822 2823 2824 2825 2826 2827 2828 2829 2830 2831 2832 2833 2834 2835 2836 2837 2838 2839 2840 2841 2842 2843 2844 2845 2846 2847 2848 2849 2850 2851 2852 2853 2854 2855 2856 2857 2858 2859 2860 2861 2862 2863 2864 2865 2866 2867 2868 2869 2870 2871 2872 2873 2874 2875 2876 2877 2878 2879 2880 2881 2882 2883 2884 2885 2886 2887 2888 2889 2890 2891 2892 2893 2894 2895 2896 2897 2898 2899 2900 2901 2902 2903 2904 2905 2906 2907 2908 2909 2910 2911 2912 2913 2914 2915 2916 2917 2918 2919 2920 2921 2922 2923 2924 2925 2926 2927 2928 2929 2930 2931 2932 2933 2934 2935 2936 2937 2938 2939 2940 2941 2942 2943 2944 2945 2946 2947 2948 2949 2950 2951 2952 2953 2954 2955 2956 2957 2958 2959 2960 2961 2962 2963 2964 2965 2966 2967 2968 2969 2970 2971 2972 2973 2974 2975 2976 2977 2978 2979 2980 2981 2982 2983 2984 2985 2986 2987 2988 2989 2990 2991 2992 2993 2994 2995 2996 2997 2998 2999 3000 3001 3002 3003 3004 3005 3006 3007 3008 3009 3010 3011 3012 3013 3014 3015 3016 3017 3018 3019 3020 3021 3022 3023 3024 3025 3026 3027 3028 3029 3030 3031 3032 3033 3034 3035 3036 3037 3038 3039 3040 3041 3042 3043 3044 3045 3046 3047 3048 3049 3050 3051 3052 3053 3054 3055 3056 3057 3058 3059 3060 3061 3062 3063 3064 3065 3066 3067 3068 3069 3070 3071 3072 3073 3074 3075 3076 3077 3078 3079 3080 3081 3082 3083 3084 3085 3086 3087 3088 3089 3090 3091 3092 3093 3094 3095 3096 3097 3098 3099 3100 3101 3102 3103 3104 3105 3106 3107 3108 3109 3110 3111 3112 3113 3114 3115 3116 3117 3118 3119 3120 3121 3122 3123 3124 3125 3126 3127 3128 3129 3130 3131 3132 3133 3134 3135 3136 3137 3138 3139 3140 3141 3142 3143 3144 3145 3146 3147 3148 3149 3150 3151 3152 3153 3154 3155 3156 3157 3158 3159 3160 3161 3162 3163 3164 3165 3166 3167 3168 3169 3170 3171 3172 3173 3174 3175 3176 3177 3178 3179 3180 3181 3182 3183 3184 3185 3186 3187 3188 3189 3190 3191 3192 3193 3194 3195 3196 3197 3198 3199 3200 3201 3202 3203 3204 3205 3206 3207 3208 3209 3210 3211 3212 3213 3214 3215 3216 3217 3218 3219 3220 3221 3222 3223 3224 3225 3226 3227 3228 3229 3230 3231 3232 3233 3234 3235 3236 3237 3238 3239 3240 3241 3242 3243 3244 3245 3246 3247 3248 3249 3250 3251 3252 3253 3254 3255 3256 3257 3258 3259 3260 3261 3262 3263 3264 3265 3266 3267 3268 3269 3270 3271 3272 3273 3274 3275 3276 3277 3278 3279 3280 3281 3282 3283 3284 3285 3286 3287 3288 3289 3290 3291 3292 3293 3294 3295 3296 3297 3298 3299 3300 3301 3302 3303 3304 3305 3306 3307 3308 3309 3310 3311 3312 3313 3314 3315 3316 3317 3318 3319 3320 3321 3322 3323 3324 3325 3326 3327 3328 3329 3330 3331 3332 3333 3334 3335 3336 3337 3338 3339 3340 3341 3342 3343 3344 3345 3346 3347 3348 3349 3350 3351 3352 3353 3354 3355 3356 3357 3358 3359 3360 3361 3362 3363 3364 3365 3366 3367 3368 3369 3370 3371 3372 3373 3374 3375 3376 3377 3378 3379 3380 3381 3382 3383 3384 3385 3386 3387 3388 3389 3390 3391 3392 3393 3394 3395 3396 3397 3398 3399 3400 3401 3402 3403 3404 3405 3406 3407 3408 3409 3410 3411 3412 3413 3414 3415 3416 3417 3418 3419 3420 3421 3422 3423 3424 3425 3426 3427 3428 3429 3430 3431 3432 3433 3434 3435 3436 3437 3438 3439 3440 3441 3442 3443 3444 3445 3446 3447 3448 3449 3450 3451 3452 3453 3454 3455 3456 3457 3458 3459 3460 3461 3462 3463 3464 3465 3466 3467 3468 3469 3470 3471 3472 3473 3474 3475 3476 3477 3478 3479 3480 3481 3482 3483 3484 3485 3486 3487 3488 3489 3490 3491 3492 3493 3494 3495 3496 3497 3498 3499 3500 3501 3502 3503 3504 3505 3506 3507 3508 3509 3510 3511 3512 3513 3514 3515 3516 3517 3518 3519 3520 3521 3522 3523 3524 3525 3526 3527 3528 3529 3530 3531 3532 3533 3534 3535 3536 3537 3538 3539 3540 3541 3542 3543 3544 3545 3546 3547 3548 3549 3550 3551 3552 3553 3554 3555 3556 3557 3558 3559 3560 3561 3562 3563 3564 3565 3566 3567 3568 3569 3570 3571 3572 3573 3574 3575 3576 3577 3578 3579 3580 3581 3582 3583 3584 3585 3586 3587 3588 3589 3590 3591 3592 3593 3594 3595 3596 3597 3598 3599 3600 3601 3602 3603 3604 3605 3606 3607 3608 3609 3610 3611 3612 3613 3614 3615 3616 3617 3618 3619 3620 3621 3622 3623 3624 3625 3626 3627 3628 3629 3630 3631 3632 3633 3634 3635 3636 3637 3638 3639 3640 3641 3642 3643 3644 3645 3646 3647 3648 3649 3650 3651 3652 3653 3654 3655 3656 3657 3658 3659 3660 3661 3662 3663 3664 3665 3666 3667 3668 3669 3670 3671 3672 3673 3674 3675 3676 3677 3678 3679 3680 3681 3682 3683 3684 3685 3686 3687 3688 3689 3690 3691 3692 3693 3694 3695 3696 3697 3698 3699 3700 3701 3702 3703 3704 3705 3706 3707 3708 3709 3710 3711 3712 3713 3714 3715 3716 3717 3718 3719 3720 3721 3722 3723 3724 3725 3726 3727 3728 3729 3730 3731 3732 3733 3734 3735 3736 3737 3738 3739 3740 3741 3742 3743 3744 3745 3746 3747 3748 3749 3750 3751 3752 3753 3754 3755 3756 3757 3758 3759 3760 3761 3762 3763 3764 3765 3766 3767 3768 3769 3770 3771 3772 3773 3774 3775 3776 3777 3778 3779 3780 3781 3782 3783 3784 3785 3786 3787 3788 3789 3790 3791 3792 3793 3794 3795 3796 3797 3798 3799 3800 3801 3802 3803 3804 3805 3806 3807 3808 3809 3810 3811 3812 3813 3814 3815 3816 3817 3818 3819 3820 3821 3822 3823 3824 3825 3826 3827 3828 3829 3830 3831 3832 3833 3834 3835 3836 3837 3838 3839 3840 3841 3842 3843 3844 3845 3846 3847 3848 3849 3850 3851 3852 3853 3854 3855 3856 3857 3858 3859 3860 3861 3862 3863 3864 3865 3866 3867 3868 3869 3870 3871 3872 3873 3874 3875 3876 3877 3878 3879 3880 3881 3882 3883 3884 3885 3886 3887 3888 3889 3890 3891 3892 3893 3894 3895 3896 3897 3898 3899 3900 3901 3902 3903 3904 3905 3906 3907 3908 3909 3910 3911 3912 3913 3914 3915 3916 3917 3918 3919 3920 3921 3922 3923 3924 3925 3926 3927 3928 3929 3930 3931 3932 3933 3934 3935 3936 3937 3938 3939 3940 3941 3942 3943 3944 3945 3946 3947 3948 3949 3950 3951 3952 3953 3954 3955 3956 3957 3958 3959 3960 3961 3962 3963 3964 3965 3966 3967 3968 3969 3970 3971 3972 3973 3974 3975 3976 3977 3978 3979 3980 3981 3982 3983 3984 3985 3986 3987 3988 3989 3990 3991 3992 3993 3994 3995 3996 3997 3998 3999 4000 4001 4002 4003 4004 4005 4006 4007 4008 4009 4010 4011 4012 4013 4014 4015 4016 4017 4018 4019 4020 4021 4022 4023 4024 4025 4026 4027 4028 4029 4030 4031 4032 4033 4034 4035 4036 4037 4038 4039 4040 4041 4042 4043 4044 4045 4046 4047 4048 4049 4050 4051 4052 4053 4054 4055 4056 4057 4058 4059 4060 4061 4062 4063 4064 4065 4066 4067 4068 4069 4070 4071 4072 4073 4074 4075 4076 4077 4078 4079 4080 4081 4082 4083 4084 4085 4086 4087 4088 4089 4090 4091 4092 4093 4094 4095 4096 4097 4098 4099 4100 4101 4102 4103 4104 4105 4106 4107 4108 4109 4110 4111 4112 4113 4114 4115 4116 4117 4118 4119 4120 4121 4122 4123 4124 4125 4126 4127 4128 4129 4130 4131 4132 4133 4134 4135 4136 4137 4138 4139 4140 4141 4142 4143 4144 4145 4146 4147 4148 4149 4150 4151 4152 4153 4154 4155 4156 4157 4158 4159 4160 4161 4162 4163 4164 4165 4166 4167 4168 4169 4170 4171 4172 4173 4174 4175 4176 4177 4178 4179 4180 4181 4182 4183 4184 4185 4186 4187 4188 4189 4190 4191 4192 4193 4194 4195 4196 4197 4198 4199 4200 4201 4202 4203 4204 4205 4206 4207 4208 4209 4210 4211 4212 4213 4214 4215 4216 4217 4218 4219 4220 4221 4222 4223 4224 4225 4226 4227 4228 4229 4230 4231 4232 4233 4234 4235 4236 4237 4238 4239 4240 4241 4242 4243 4244 4245 4246 4247 4248 4249 4250 4251 4252 4253 4254 4255 4256 4257 4258 4259 4260 4261 4262 4263 4264 4265 4266 4267 4268 4269 4270 4271 4272 4273 4274 4275 4276 4277 4278 4279 4280 4281 4282 4283 4284 4285 4286 4287 4288 4289 4290 4291 4292 4293 4294 4295 4296 4297 4298 4299 4300 4301 4302 4303 4304 4305 4306 4307 4308 4309 4310 4311 4312 4313 4314 4315 4316 4317 4318 4319 4320 4321 4322 4323 4324 4325 4326 4327 4328 4329 4330 4331 4332 4333 4334 4335 4336 4337 4338 4339 4340 4341 4342 4343 4344 4345 4346 4347 4348 4349 4350 4351 4352 4353 4354 4355 4356 4357 4358 4359 4360 4361 4362 4363 4364 4365 4366 4367 4368 4369 4370 4371 4372 4373 4374 4375 4376 4377 4378 4379 4380 4381 4382 4383 4384 4385 4386 4387 4388 4389 4390 4391 4392 4393 4394 4395 4396 4397 4398 4399 4400 4401 4402 4403 4404 4405 4406 4407 4408 4409 4410 4411 4412 4413 4414 4415 4416 4417 4418 4419 4420 4421 4422 4423 4424 4425 4426 4427 4428 4429 4430 4431 4432 4433 4434 4435 4436 4437 4438 4439 4440 4441 4442 4443 4444 4445 4446 4447 4448 4449 4450 4451 4452 4453 4454 4455 4456 4457 4458 4459 4460 4461 4462 4463 4464 4465 4466 4467 4468 4469 4470 4471 4472 4473 4474 4475 4476 4477 4478 4479 4480 4481 4482 4483 4484 4485 4486 4487 4488 4489 4490 4491 4492 4493 4494 4495 4496 4497 4498 4499 4500 4501 4502 4503 4504 4505 4506 4507 4508 4509 4510 4511 4512 4513 4514 4515 4516 4517 4518 4519 4520 4521 4522 4523 4524 4525 4526 4527 4528 4529 4530 4531 4532 4533 4534 4535 4536 4537 4538 4539 4540 4541 4542 4543 4544 4545 4546 4547 4548 4549 4550 4551 4552 4553 4554 4555 4556 4557 4558 4559 4560 4561 4562 4563 4564 4565 4566 4567 4568 4569 4570 4571 4572 4573 4574 4575 4576 4577 4578 4579 4580 4581 4582 4583 4584 4585 4586 4587 4588 4589 4590 4591 4592 4593 4594 4595 4596 4597 4598 4599 4600 4601 4602 4603 4604 4605 4606 4607 4608 4609 4610 4611 4612 4613 4614 4615 4616 4617 4618 4619 4620 4621 4622 4623 4624 4625 4626 4627 4628 4629 4630 4631 4632 4633 4634 4635 4636 4637 4638 4639 4640 4641 4642 4643 4644 4645 4646 4647 4648 4649 4650 4651 4652 4653 4654 4655 4656 4657 4658 4659 4660 4661 4662 4663 4664 4665 4666 4667 4668 4669 4670 4671 4672 4673 4674 4675 4676 4677 4678 4679 4680 4681 4682 4683 4684 4685 4686 4687 4688 4689 4690 4691 4692 4693 4694 4695 4696 4697 4698 4699 4700 4701 4702 4703 4704 4705 4706 4707 4708 4709 4710 4711 4712 4713 4714 4715 4716 4717 4718 4719 4720 4721 4722 4723 4724 4725 4726 4727 4728 4729 4730 4731 4732 4733 4734 4735 4736 4737 4738 4739 4740 4741 4742 4743 4744 4745 4746 4747 4748 4749 4750 4751 4752 4753 4754 4755 4756 4757 4758 4759 4760 4761 4762 4763 4764 4765 4766 4767 4768 4769 4770 4771 4772 4773 4774 4775 4776 4777 4778 4779 4780 4781 4782 4783 4784 4785 4786 4787 4788 4789 4790 4791 4792 4793 4794 4795 4796 4797 4798 4799 4800 4801 4802 4803 4804 4805 4806 4807 4808 4809 4810 4811 4812 4813 4814 4815 4816 4817 4818 4819 4820 4821 4822 4823 4824 4825 4826 4827 4828 4829 4830 4831 4832 4833 4834 4835 4836 4837 4838 4839 4840 4841 4842 4843 4844 4845 4846 4847 4848 4849 4850 4851 4852 4853 4854 4855 4856 4857 4858 4859 4860 4861 4862 4863 4864 4865 4866 4867 4868 4869 4870 4871 4872 4873 4874 4875 4876 4877 4878 4879 4880 4881 4882 4883 4884 4885 4886 4887 4888 4889 4890 4891 4892 4893 4894 4895 4896 4897 4898 4899 4900 4901 4902 4903 4904 4905 4906 4907 4908 4909 4910 4911 4912 4913 4914 4915 4916 4917 4918 4919 4920 4921 4922 4923 4924 4925 4926 4927 4928 4929 4930 4931 4932 4933 4934 4935 4936 4937 4938 4939 4940 4941 4942 4943 4944 4945 4946 4947 4948 4949 4950 4951 4952 4953 4954 4955 4956 4957 4958 4959 4960 4961 4962 4963 4964 4965 4966 4967 4968 4969 4970 4971 4972 4973 4974 4975 4976 4977 4978 4979 4980 4981 4982 4983 4984 4985 4986 4987 4988 4989 4990 4991 4992 4993 4994 4995 4996 4997 4998 4999 5000 5001 5002 5003 5004 5005 5006 5007 5008 5009 5010 5011 5012 5013 5014 5015 5016 5017 5018 5019 5020 5021 5022 5023 5024 5025 5026 5027 5028 5029 5030 5031 5032 5033 5034 5035 5036 5037 5038 5039 5040 5041 5042 5043 5044 5045 5046 5047 5048 5049 5050 5051 5052 5053 5054 5055 5056 5057 5058 5059 5060 5061 5062 5063 5064 5065 5066 5067 5068 5069 5070 5071 5072 5073 5074 5075 5076 5077 5078 5079 5080 5081 5082 5083 5084 5085 5086 5087 5088 5089 5090 5091 5092 5093 5094 5095 5096 5097 5098 5099 5100 5101 5102 5103 5104 5105 5106 5107 5108 5109 5110 5111 5112 5113 5114 5115 5116 5117 5118 5119 5120 5121 5122 5123 5124 5125 5126 5127 5128 5129 5130 5131 5132 5133 5134 5135 5136 5137 5138 5139 5140 5141 5142 5143 5144 5145 5146 5147 5148 5149 5150 5151 5152 5153 5154 5155 5156 5157 5158 5159 5160 5161 5162 5163 5164 5165 5166 5167 5168 5169 5170 5171 5172 5173 5174 5175 5176 5177 5178 5179 5180 5181 5182 5183 5184 5185 5186 5187 5188 5189 5190 5191 5192 5193 5194 5195 5196 5197 5198 5199 5200 5201 5202 5203 5204 5205 5206 5207 5208 5209 5210 5211 5212 5213 5214 5215 5216 5217 5218 5219 5220 5221 5222 5223 5224 5225 5226 5227 5228 5229 5230 5231 5232 5233 5234 5235 5236 5237 5238 5239 5240 5241 5242 5243 5244 5245 5246 5247 5248 5249 5250 5251 5252 5253 5254 5255 5256 5257 5258 5259 5260 5261 5262 5263 5264 5265 5266 5267 5268 5269 5270 5271 5272 5273 5274 5275 5276 5277 5278 5279 5280 5281 5282 5283 5284 5285 5286 5287 5288 5289 5290 5291 5292 5293 5294 5295 5296 5297 5298 5299 5300 5301 5302 5303 5304 5305 5306 5307 5308 5309 5310 5311 5312 5313 5314 5315 5316 5317 5318 5319 5320 5321 5322 5323 5324 5325 5326 5327 5328 5329 5330 5331 5332 5333 5334 5335 5336 5337 5338 5339 5340 5341 5342 5343 5344 5345 5346 5347 5348 5349 5350 5351 5352 5353 5354 5355 5356 5357 5358 5359 5360 5361 5362 5363 5364 5365 5366 5367 5368 5369 5370 5371 5372 5373 5374 5375 5376 5377 5378 5379 5380 5381 5382 5383 5384 5385 5386 5387 5388 5389 5390 5391 5392 5393 5394 5395 5396 5397 5398 5399 5400 5401 5402 5403 5404 5405 5406 5407 5408 5409 5410 5411 5412 5413 5414 5415 5416 5417 5418 5419 5420 5421 5422 5423 5424 5425 5426 5427 5428 5429 5430 5431 5432 5433 5434 5435 5436 5437 5438 5439 5440 5441 5442 5443 5444 5445 5446 5447 5448 5449 5450 5451 5452 5453 5454 5455 5456 5457 5458 5459 5460 5461 5462 5463 5464 5465 5466 5467 5468 5469 5470 5471 5472 5473 5474 5475 5476 5477 5478 5479 5480 5481 5482 5483 5484 5485 5486 5487 5488 5489 5490 5491 5492 5493 5494 5495 5496 5497 5498 5499 5500 5501 5502 5503 5504 5505 5506 5507 5508 5509 5510 5511 5512 5513 5514 5515 5516 5517 5518 5519 5520 5521 5522 5523 5524 5525 5526 5527 5528 5529 5530 5531 5532 5533 5534 5535 5536 5537 5538 5539 5540 5541 5542 5543 5544 5545 5546 5547 5548 5549 5550 5551 5552 5553 5554 5555 5556 5557 5558 5559 5560 5561 5562 5563 5564 5565 5566 5567 5568 5569 5570 5571 5572 5573 5574 5575 5576 5577 5578 5579 5580 5581 5582 5583 5584 5585 5586 5587 5588 5589 5590 5591 5592 5593 5594 5595 5596 5597 5598 5599 5600 5601 5602 5603 5604 5605 5606 5607 5608 5609 5610 5611 5612 5613 5614 5615 5616 5617 5618 5619 5620 5621 5622 5623 5624 5625 5626 5627 5628 5629 5630 5631 5632 5633 5634 5635 5636 5637 5638 5639 5640 5641 5642 5643 5644 5645 5646 5647 5648 5649 5650 5651 5652 5653 5654 5655 5656 5657 5658 5659 5660 5661 5662 5663 5664 5665 5666 5667 5668 5669 5670 5671 5672 5673 5674 5675 5676 5677 5678 5679 5680 5681 5682 5683 5684 5685 5686 5687 5688 5689 5690 5691 5692 5693 5694 5695 5696 5697 5698 5699 5700 5701 5702 5703 5704 5705 5706 5707 5708 5709 5710 5711 5712 5713 5714 5715 5716 5717 5718 5719 5720 5721 5722 5723 5724 5725 5726 5727 5728 5729 5730 5731 5732 5733 5734 5735 5736 5737 5738 5739 5740 5741 5742 5743 5744 5745 5746 5747 5748 5749 5750 5751 5752 5753 5754 5755 5756 5757 5758 5759 5760 5761 5762 5763 5764 5765 5766 5767 5768 5769 5770 5771 5772 5773 5774 5775 5776 5777 5778 5779 5780 5781 5782 5783 5784 5785 5786 5787 5788 5789 5790 5791 5792 5793 5794 5795 5796 5797 5798 5799 5800 5801 5802 5803 5804 5805 5806 5807 5808 5809 5810 5811 5812 5813 5814 5815 5816 5817 5818 5819 5820 5821 5822 5823 5824 5825 5826 5827 5828 5829 5830 5831 5832 5833 5834 5835 5836 5837 5838 5839 5840 5841 5842 5843 5844 5845 5846 5847 5848 5849 5850 5851 5852 5853 5854 5855 5856 5857 5858 5859 5860 5861 5862 5863 5864 5865 5866 5867 5868 5869 5870 5871 5872 5873 5874 5875 5876 5877 5878 5879 5880 5881 5882 5883 5884 5885 5886 5887 5888 5889 5890 5891 5892 5893 5894 5895 5896 5897 5898 5899 5900 5901 5902 5903 5904 5905 5906 5907 5908 5909 5910 5911 5912 5913 5914 5915 5916 5917 5918 5919 5920 5921 5922 5923 5924 5925 5926 5927 5928 5929 5930 5931 5932 5933 5934 5935 5936 5937 5938 5939 5940 5941 5942 5943 5944 5945 5946 5947 5948 5949 5950 5951 5952 5953 5954 5955 5956 5957 5958 5959 5960 5961 5962 5963 5964 5965 5966 5967 5968 5969 5970 5971 5972 5973 5974 5975 5976 5977 5978 5979 5980 5981 5982 5983 5984 5985 5986 5987 5988 5989 5990 5991 5992 5993 5994 5995 5996 5997 5998 5999 6000 6001 6002 6003 6004 6005 6006 6007 6008 6009 6010 6011 6012 6013 6014 6015 6016 6017 6018 6019 6020 6021 6022 6023 6024 6025 6026 6027 6028 6029 6030 6031 6032 6033 6034 6035 6036 6037 6038 6039 6040 6041 6042 6043 6044 6045 6046 6047 6048 6049 6050 6051 6052 6053 6054 6055 6056 6057 6058 6059 6060 6061 6062 6063 6064 6065 6066 6067 6068 6069 6070 6071 6072 6073 6074 6075 6076 6077 6078 6079 6080 6081 6082 6083 6084 6085 6086 6087 6088 6089 6090 6091 6092 6093 6094 6095 6096 6097 6098 6099 6100 6101 6102 6103 6104 6105 6106 6107 6108 6109 6110 6111 6112 6113 6114 6115 6116 6117 6118 6119 6120 6121 6122 6123 6124 6125 6126 6127 6128 6129 6130 6131 6132 6133 6134 6135 6136 6137 6138 6139 6140 6141 6142 6143 6144 6145 6146 6147 6148 6149 6150 6151 6152 6153 6154 6155 6156 6157 6158 6159 6160 6161 6162 6163 6164 6165 6166 6167 6168 6169 6170 6171 6172 6173 6174 6175 6176 6177 6178 6179 6180 6181 6182 6183 6184 6185 6186 6187 6188 6189 6190 6191 6192 6193 6194 6195 6196 6197 6198 6199 6200 6201 6202 6203 6204 6205 6206 6207 6208 6209 6210 6211 6212 6213 6214 6215 6216 6217 6218 6219 6220 6221 6222 6223 6224 6225 6226 6227 6228 6229 6230 6231 6232 6233 6234 6235 6236 6237 6238 6239 6240 6241 6242 6243 6244 6245 6246 6247 6248 6249 6250 6251 6252 6253 6254 6255 6256 6257 6258 6259 6260 6261 6262 6263 6264 6265 6266 6267 6268 6269 6270 6271 6272 6273 6274 6275 6276 6277 6278 6279 6280 6281 6282 6283 6284 6285 6286 6287 6288 6289 6290 6291 6292 6293 6294 6295 6296 6297 6298 6299 6300 6301 6302 6303 6304 6305 6306 6307 6308 6309 6310 6311 6312 6313 6314 6315 6316 6317 6318 6319 6320 6321 6322 6323 6324 6325 6326 6327 6328 6329 6330 6331 6332 6333 6334 6335 6336 6337 6338 6339 6340 6341 6342 6343 6344 6345 6346 6347 6348 6349 6350 6351 6352 6353 6354 6355 6356 6357 6358 6359 6360 6361 6362 6363 6364 6365 6366 6367 6368 6369 6370 6371 6372 6373 6374 6375 6376 6377 6378 6379 6380 6381 6382 6383 6384 6385 6386 6387 6388 6389 6390 6391 6392 6393 6394 6395 6396 6397 6398 6399 6400 6401 6402 6403 6404 6405 6406 6407 6408 6409 6410 6411 6412 6413 6414 6415 6416 6417 6418 6419 6420 6421 6422 6423 6424 6425 6426 6427 6428 6429 6430 6431 6432 6433 6434 6435 6436 6437 6438 6439 6440 6441 6442 6443 6444 6445 6446 6447 6448 6449 6450 6451 6452 6453 6454 6455 6456 6457 6458 6459 6460 6461 6462 6463 6464 6465 6466 6467 6468 6469 6470 6471 6472 6473 6474 6475 6476 6477 6478 6479 6480 6481 6482 6483 6484 6485 6486 6487 6488 6489 6490 6491 6492 6493 6494 6495 6496 6497 6498 6499 6500 6501 6502 6503 6504 6505 6506 6507 6508 6509 6510 6511 6512 6513 6514 6515 6516 6517 6518 6519 6520 6521 6522 6523 6524 6525 6526 6527 6528 6529 6530 6531 6532 6533 6534 6535 6536 6537 6538 6539 6540 6541 6542 6543 6544 6545 6546 6547 6548 6549 6550 6551 6552 6553 6554 6555 6556 6557 6558 6559 6560 6561 6562 6563 6564 6565 6566 6567 6568 6569 6570 6571 6572 6573 6574 6575 6576 6577 6578 6579 6580 6581 6582 6583 6584 6585 6586 6587 6588 6589 6590 6591 6592 6593 6594 6595 6596 6597 6598 6599 6600 6601 6602 6603 6604 6605 6606 6607 6608 6609 6610 6611 6612 6613 6614 6615 6616 6617 6618 6619 6620 6621 6622 6623 6624 6625 6626 6627 6628 6629 6630 6631 6632 6633 6634 6635 6636 6637 6638 6639 6640 6641 6642 6643 6644 6645 6646 6647 6648 6649 6650 6651 6652 6653 6654 6655 6656 6657 6658 6659 6660 6661 6662 6663 6664 6665 6666 6667 6668 6669 6670 6671 6672 6673 6674 6675 6676 6677 6678 6679 6680 6681 6682 6683 6684 6685 6686 6687 6688 6689 6690 6691 6692 6693 6694 6695 6696 6697 6698 6699 6700 6701 6702 6703 6704 6705 6706 6707 6708 6709 6710 6711 6712 6713 6714 6715 6716 6717 6718 6719 6720 6721 6722 6723 6724 6725 6726 6727 6728 6729 6730 6731 6732 6733 6734 6735 6736 6737 6738 6739 6740 6741 6742 6743 6744 6745 6746 6747 6748 6749 6750 6751 6752 6753 6754 6755 6756 6757 6758 6759 6760 6761 6762 6763 6764 6765 6766 6767 6768 6769 6770 6771 6772 6773 6774 6775 6776 6777 6778 6779 6780 6781 6782 6783 6784 6785 6786 6787 6788 6789 6790 6791 6792 6793 6794 6795 6796 6797 6798 6799 6800 6801 6802 6803 6804 6805 6806 6807 6808 6809 6810 6811 6812 6813 6814 6815 6816 6817 6818 6819 6820 6821 6822 6823 6824 6825 6826 6827 6828 6829 6830 6831 6832 6833 6834 6835 6836 6837 6838 6839 6840 6841 6842 6843 6844 6845 6846 6847 6848 6849 6850 6851 6852 6853 6854 6855 6856 6857 6858 6859 6860 6861 6862 6863 6864 6865 6866 6867 6868 6869 6870 6871 6872 6873 6874 6875 6876 6877 6878 6879 6880 6881 6882 6883 6884 6885 6886 6887 6888 6889 6890 6891 6892 6893 6894 6895 6896 6897 6898 6899 6900 6901 6902 6903 6904 6905 6906 6907 6908 6909 6910 6911 6912 6913 6914 6915 6916 6917 6918 6919 6920 6921 6922 6923 6924 6925 6926 6927 6928 6929 6930 6931 6932 6933 6934 6935 6936 6937 6938 6939 6940 6941 6942 6943 6944 6945 6946 6947 6948 6949 6950 6951 6952 6953 6954 6955 6956 6957 6958 6959 6960 6961 6962 6963 6964 6965 6966 6967 6968 6969 6970 6971 6972 6973 6974 6975 6976 6977 6978 6979 6980 6981 6982 6983 6984 6985 6986 6987 6988 6989 6990 6991 6992 6993 6994 6995 6996 6997 6998 6999 7000 7001 7002 7003 7004 7005 7006 7007 7008 7009 7010 7011 7012 7013 7014 7015 7016 7017 7018 7019 7020 7021 7022 7023 7024 7025 7026 7027 7028 7029 7030 7031 7032 7033 7034 7035 7036 7037 7038 7039 7040 7041 7042 7043 7044 7045 7046 7047 7048 7049 7050 7051 7052 7053 7054 7055 7056 7057 7058 7059 7060 7061 7062 7063 7064 7065 7066 7067 7068 7069 7070 7071 7072 7073 7074 7075 7076 7077 7078 7079 7080 7081 7082 7083 7084 7085 7086 7087 7088 7089 7090 7091 7092 7093 7094 7095 7096 7097 7098 7099 7100 7101 7102 7103 7104 7105 7106 7107 7108 7109 7110 7111 7112 7113 7114 7115 7116 7117 7118 7119 7120 7121 7122 7123 7124 7125 7126 7127 7128 7129 7130 7131 7132 7133 7134 7135 7136 7137 7138 7139 7140 7141 7142 7143 7144 7145 7146 7147 7148 7149 7150 7151 7152 7153 7154 7155 7156 7157 7158 7159 7160 7161 7162 7163 7164 7165 7166 7167 7168 7169 7170 7171 7172 7173 7174 7175 7176 7177 7178 7179 7180 7181 7182 7183 7184 7185 7186 7187 7188 7189 7190 7191 7192 7193 7194 7195 7196 7197 7198 7199 7200 7201 7202 7203 7204 7205 7206 7207 7208 7209 7210 7211 7212 7213 7214 7215 7216 7217 7218 7219 7220 7221 7222 7223 7224 7225 7226 7227 7228 7229 7230 7231 7232 7233 7234 7235 7236 7237 7238 7239 7240 7241 7242 7243 7244 7245 7246 7247 7248 7249 7250 7251 7252 7253 7254 7255 7256 7257 7258 7259 7260 7261 7262 7263 7264 7265 7266 7267 7268 7269 7270 7271 7272 7273 7274 7275 7276 7277 7278 7279 7280 7281 7282 7283 7284 7285 7286 7287 7288 7289 7290 7291 7292 7293 7294 7295 7296 7297 7298 7299 7300 7301 7302 7303 7304 7305 7306 7307 7308 7309 7310 7311 7312 7313 7314 7315 7316 7317 7318 7319 7320 7321 7322 7323 7324 7325 7326 7327 7328 7329 7330 7331 7332 7333 7334 7335 7336 7337 7338 7339 7340 7341 7342 7343 7344 7345 7346 7347 7348 7349 7350 7351 7352 7353 7354 7355 7356 7357 7358 7359 7360 7361 7362 7363 7364 7365 7366 7367 7368 7369 7370 7371 7372 7373 7374 7375 7376 7377 7378 7379 7380 7381 7382 7383 7384 7385 7386 7387 7388 7389 7390 7391 7392 7393 7394 7395 7396 7397 7398 7399 7400 7401 7402 7403 7404 7405 7406 7407 7408 7409 7410 7411 7412 7413 7414 7415 7416 7417 7418 7419 7420 7421 7422 7423 7424 7425 7426 7427 7428 7429 7430 7431 7432 7433 7434 7435 7436 7437 7438 7439 7440 7441 7442 7443 7444 7445 7446 7447 7448 7449 7450 7451 7452 7453 7454 7455 7456 7457 7458 7459 7460 7461 7462 7463 7464 7465 7466 7467 7468 7469 7470 7471 7472 7473 7474 7475 7476 7477 7478 7479 7480 7481 7482 7483 7484 7485 7486 7487 7488 7489 7490 7491 7492 7493 7494 7495 7496 7497 7498 7499 7500 7501 7502 7503 7504 7505 7506 7507 7508 7509 7510 7511 7512 7513 7514 7515 7516 7517 7518 7519 7520 7521 7522 7523 7524 7525 7526 7527 7528 7529 7530 7531 7532 7533 7534 7535 7536 7537 7538 7539 7540 7541 7542 7543 7544 7545 7546 7547 7548 7549 7550 7551 7552 7553 7554 7555 7556 7557 7558 7559 7560 7561 7562 7563 7564 7565 7566 7567 7568 7569 7570 7571 7572 7573 7574 7575 7576 7577 7578 7579 7580 7581 7582 7583 7584 7585 7586 7587 7588 7589 7590 7591 7592 7593 7594 7595 7596 7597 7598 7599 7600 7601 7602 7603 7604 7605 7606 7607 7608 7609 7610 7611 7612 7613 7614 7615 7616 7617 7618 7619 7620 7621 7622 7623 7624 7625 7626 7627 7628 7629 7630 7631 7632 7633 7634 7635 7636 7637 7638 7639 7640 7641 7642 7643 7644 7645 7646 7647 7648 7649 7650 7651 7652 7653 7654 7655 7656 7657 7658 7659 7660 7661 7662 7663 7664 7665 7666 7667 7668 7669 7670 7671 7672 7673 7674 7675 7676 7677 7678 7679 7680 7681 7682 7683 7684 7685 7686 7687 7688 7689 7690 7691 7692 7693 7694 7695 7696 7697 7698 7699 7700 7701 7702 7703 7704 7705 7706 7707 7708 7709 7710 7711 7712 7713 7714 7715 7716 7717 7718 7719 7720 7721 7722 7723 7724 7725 7726 7727 7728 7729 7730 7731 7732 7733 7734 7735 7736 7737 7738 7739 7740 7741 7742 7743 7744 7745 7746 7747 7748 7749 7750 7751 7752 7753 7754 7755 7756 7757 7758 7759 7760 7761 7762 7763 7764 7765 7766 7767 7768 7769 7770 7771 7772 7773 7774 7775 7776 7777 7778 7779 7780 7781 7782 7783 7784 7785 7786 7787 7788 7789 7790 7791 7792 7793 7794 7795 7796 7797 7798 7799 7800 7801 7802 7803 7804 7805 7806 7807 7808 7809 7810 7811 7812 7813 7814 7815 7816 7817 7818 7819 7820 7821 7822 7823 7824 7825 7826 7827 7828 7829 7830 7831 7832 7833 7834 7835 7836 7837 7838 7839 7840 7841 7842 7843 7844 7845 7846 7847 7848 7849 7850 7851 7852 7853 7854 7855 7856 7857 7858 7859 7860 7861 7862 7863 7864 7865 7866 7867 7868 7869 7870 7871 7872 7873 7874 7875 7876 7877 7878 7879 7880 7881 7882 7883 7884 7885 7886 7887 7888 7889 7890 7891 7892 7893 7894 7895 7896 7897 7898 7899 7900 7901 7902 7903 7904 7905 7906 7907 7908 7909 7910 7911 7912 7913 7914 7915 7916 7917 7918 7919 7920 7921 7922 7923 7924 7925 7926 7927 7928 7929 7930 7931 7932 7933 7934 7935 7936 7937 7938 7939 7940 7941 7942 7943 7944 7945 7946 7947 7948 7949 7950 7951 7952 7953 7954 7955 7956 7957 7958 7959 7960 7961 7962 7963 7964 7965 7966 7967 7968 7969 7970 7971 7972 7973 7974 7975 7976 7977 7978 7979 7980 7981 7982 7983 7984 7985 7986 7987 7988 7989 7990 7991 7992 7993 7994 7995 7996 7997 7998 7999 8000 8001 8002 8003 8004 8005 8006 8007 8008 8009 8010 8011 8012 8013 8014 8015 8016 8017 8018 8019 8020 8021 8022 8023 8024 8025 8026 8027 8028 8029 8030 8031 8032 8033 8034 8035 8036 8037 8038 8039 8040 8041 8042 8043 8044 8045 8046 8047 8048 8049 8050 8051 8052 8053 8054 8055 8056 8057 8058 8059 8060 8061 8062 8063 8064 8065 8066 8067 8068 8069 8070 8071 8072 8073 8074 /*-------------------------------------------------------------------*/ /* Multiple Comparisons and Multiple Tests Using the SAS(r) System */ /* by Westfall, Tobias, Rom, Wolfinger, Hochberg */ /* Copyright(c) 1999 by SAS Institute Inc., Cary, NC, USA */ /* SAS Publications order # 56648 */ /* ISBN 1-58025-397-0 */ /*-------------------------------------------------------------------*/ /* */ /* This material is provided \"as is\" by SAS Institute Inc. There */ /* are no warranties, expressed or implied, as to merchantability or */ /* fitness for a particular purpose regarding the materials or code */ /* contained herein. The Institute is not responsible for errors */ /* in this material as it now exists or will exist, nor does the */ /* Institute provide technical support for it. */ /* */ /*-------------------------------------------------------------------*/ /* Questions or problem reports concerning this material may be */ /* addressed to the author: */ /* */ /* SAS Institute Inc. */ /* Books by Users */ /* Attn: Peter Westfall, et al. */ /* SAS Campus Drive */ /* Cary, NC 27513 */ /* */ /* */ /* If you prefer, you can send email to: sasbbu@sas.com */ /* Use this for subject field: */ /* Comments for Peter Westfall, et al. */ /* */ /*-------------------------------------------------------------------*/ /* Date Last Updated: 1Jun00 */ /* */ /*-------------------------------------------------------------------*/ /* NOTES */ /*-------------------------------------------------------------------*/ */ /* */ /* 1. The %SimTests macro provides closed testing in a conservative */ /* but not alpha-exhaustive sense for the case of two-sided alter- */ /* natives. For the case of one-sided alternatives, the tests are */ /* technically not closed at all, since the closure involves */ /* intersections of half-spaces and not point nulls. Nevertheless, */ /* the method is powerful and still controls the FWE as discussed by */ /* Westfall and Young (1993, p. 74). */ /* */ /* 2. In Version 8.1 of SAS, PROC MULTTEST will include adjusted */ /* p-values using Hommel's 1988 method. It also will include */ /* p-values using Fisher closed tests. Enhancements to the STRATA */ /* option have been included as well to make the unadjusted p-values */ /* conform exactly with either Type II or Type III tests. */ /* */ /*-------------------------------------------------------------------*/ /*-------------------------------------------------------------------*/ /* UPDATES */ /*-------------------------------------------------------------------*/ /* */ /* 1. 12/99 Renumbered and relabeled the Programs on this web page */ /* to correspond with the book. */ /* */ /* 2. 12/99 Updated the %SimTests macro as follows: */ /* i. Changed all SEAdjP values to 0 when reported as < 1E-8 */ /* ii. Fixed a bug that caused incorrect computations when there */ /* are pairwise collinear contrasts */ /* */ /* 3. 12/99 changes made to Program 4.4. The program as given in */ /* the book and initially on the web page did not work. The */ /* as given on this web page works. */ /* */ /* 4. The %Williams macro was updated on Dec 7, 1999. The original */ /* implementation used the isotonic regression estimate of the */ /* control group mean rather then the sample mean as in Williams */ /* (1972) second procedure which was found later by Marcus (1976) */ /* to be superior in most cases. However, PROBMC employs the */ /* critical points from the distribution that uses the sample mean */ /* of the control group, therefore, a revision to the %Williams */ /* macro is indicated. We would like to thank Agnes Kovacs who has */ /* brought up the issue. */ /* */ /*-------------------------------------------------------------------*/ /* Program 2.4: Bonferroni and Sidak Adjusted p-values Using the DATA Step */ data one ; input test pval @@ ; bon_adjp = min ( 1 , 10 * pval ); sid_adjp = 1 - ( 1 - pval ) ** 10 ; datalines ; 1 0.0911 2 0.8912 3 0.0001 4 0.5718 5 0.0132 6 0.9011 7 0.2012 8 0.0289 9 0.0498 10 0.0058 ; proc sort data = one out = one ; by pval ; proc print data = one ; run ; /* Program 2.5: Bonferroni and Sidak Adjusted p-values Using PROC MULTTEST */ data one ; set one ; rename pval = raw_p ; drop bon_adjp sid_adjp ; proc multtest pdata = one bon sid out = outp ; proc sort data = outp out = outp ; by raw_p ; proc print data = outp ; run ; /* Program 2.6: Conservative Simultaneous Confidence Intervals with Multivariate Data */ data _null_ ; call symput ( ' bonalpha ' , 0.05 / 9 ); call symput ( ' sidalpha ' , 1 - ( 1 - 0.05 ) ** ( 1 / 9 )); data HusbWive ; input HusbQ1 - HusbQ4 WifeQ1 - WifeQ4 @@ ; DiffQ1 = HusbQ1 - WifeQ1 ; DiffQ2 = HusbQ2 - WifeQ2 ; DiffQ3 = HusbQ3 - WifeQ3 ; DiffQ4 = HusbQ4 - WifeQ4 ; DiffQAvg = sum ( of HusbQ1 - HusbQ4 ) / 4 - sum ( of WifeQ1 - WifeQ4 ) / 4 ; DiffComp = sum ( of HusbQ1 - HusbQ2 ) / 2 - sum ( of WifeQ1 - WifeQ2 ) / 2 ; DiffPass = sum ( of HusbQ3 - HusbQ4 ) / 2 - sum ( of WifeQ3 - WifeQ4 ) / 2 ; DiffFFP = sum ( of HusbQ1 HusbQ3 ) / 2 - sum ( of WifeQ1 WifeQ3 ) / 2 ; DiffFFY = sum ( of HusbQ2 HusbQ4 ) / 2 - sum ( of WifeQ2 WifeQ4 ) / 2 ; datalines ; 2 3 5 5 4 4 5 5 5 5 4 4 4 5 5 5 4 5 5 5 4 4 5 5 4 3 4 4 4 5 5 5 3 3 5 5 4 4 5 5 3 3 4 5 3 3 4 4 3 4 4 4 4 3 5 4 4 4 5 5 3 4 5 5 4 5 5 5 4 4 5 4 4 4 3 3 3 4 4 4 4 4 5 5 4 5 5 5 5 5 4 4 5 5 5 5 4 4 4 4 4 4 5 5 4 3 5 5 4 4 4 4 4 4 5 5 4 4 5 5 3 3 4 5 3 4 4 4 4 5 4 4 5 5 5 5 5 5 5 5 4 5 4 4 5 5 4 4 3 4 4 4 4 4 4 4 5 3 4 4 4 4 4 4 5 3 4 4 4 4 4 4 4 5 4 4 3 4 5 5 2 5 5 5 5 3 5 5 3 4 5 5 5 5 3 3 4 3 5 5 3 3 4 4 4 4 4 4 4 4 4 4 4 4 5 5 3 3 5 5 3 4 4 4 4 4 3 3 4 4 5 4 4 4 5 5 4 4 5 5 ; proc glm ; model HusbQ1 - HusbQ4 WifeQ1 - WifeQ4 = / nouni ; repeated Spouse 2 , Question 4 identity ; run ; proc means alpha = 0.05 n mean lclm uclm ; title \"Unadjusted Confidence Intervals\" ; var DiffQ1 - DiffQ4 DiffQAvg DiffComp DiffPass DiffFFP DiffFFY ; proc means alpha =& sidalpha n mean lclm uclm ; title \"Simultaneous Sidak Intervals\" ; var DiffQ1 - DiffQ4 DiffQAvg DiffComp DiffPass DiffFFP DiffFFY ; proc means alpha =& bonalpha n mean lclm uclm ; title \"Simultaneous Bonferroni Intervals\" ; var DiffQ1 - DiffQ4 DiffQAvg DiffComp DiffPass DiffFFP DiffFFY ; run ; /* Program 2.7: Multiple Tests with Multivariate Data */ proc means data = HusbWive n mean std prt ; title \"Tests of Hypotheses With Husband/Wife Data\" ; var DiffQ1 - DiffQ4 DiffQAvg DiffComp DiffPass DiffFFP DiffFFY ; run ; /* Program 2.8: HOLM Adjusted p-values Using PROC MULTTEST */ data one ; set one ; rename pval = raw_p ; drop bon_adjp sid_adjp ; proc multtest pdata = one bon stepbon out = outp ; proc sort data = outp out = outp ; by raw_p ; proc print data = outp ; run ; /* Program 2.9. Sidak-Holm Adjusted p-values Using PROC MULTTEST */ data one ; set one ; rename pval = raw_p ; drop bon_adjp sid_adjp ; proc multtest pdata = one sid stepsid out = outp ; proc sort data = outp out = outp ; by raw_p ; proc print data = outp ; run ; /* Program 2.10: Hochberg's Adjusted p-values Using PROC MULTTEST */ data one ; set one ; rename pval = raw_p ; drop bon_adjp sid_adjp ; proc multtest pdata = one holm hoc out = outp ; proc sort data = outp out = outp ; by raw_p ; proc print data = outp ; run ; /* Program 2.11: FDR-controlling p-values Using PROC MULTTEST */ data one ; set one ; rename pval = raw_p ; drop bon_adjp sid_adjp ; proc multtest pdata = one hoc fdr out = outp ; proc sort data = outp out = outp ; by raw_p ; proc print data = outp ; run ; /* Program 2.12: Adjusted p-values from Fixed-sequence Tests */ data a ; input p @@ ; if ( _N_ = 1 ) then pseq = 0 ; pseq = max ( pseq , p ); retain pseq ; cards ; 0.021 0.043 0.402 0.004 ; proc print data = a ; run ; /* Program 2.13: Schweder-Spjotvoll p-value Plot */ data one ; set one ; proc sort out = pplot ; by descending pval ; run ; data pplot ; set pplot ; q = 1 - pval ; order = _n_ ; run ; goptions ftext = simplex hsize = 5 in vsize = 3.33 in ; axis1 style = 1 width = 2 major = ( number = 5 ) minor = none label = ( ' q = 1 - p ' ); axis2 style = 1 width = 2 major = ( number = 6 ) order = ( 0 2 4 6 8 10 ) minor = none label = ( ' Order ' ); proc gplot data = pplot ; title \"SCHWEDER-SPJOTVOLL PLOT\" ; plot order * q / vaxis = axis2 haxis = axis1 frame ; run ; /* Program 2.14: Hochberg and Benjamini Graphical analysis of Multiple p-values */ %hochben ( dataset = one , pv = pval ); /* Program 3.1: Studentized Range Critical Value */ data ; qval = probmc ( \"RANGE\" ,., .95 , 45 , 5 ); c_alpha = qval / sqrt ( 2 ); run ; proc print ; run ; /* Program 3.2: Simultaneous Intervals for Mean Differences */ data wloss ; do diet = 'A' , 'B' , 'C' , 'D' , 'E' ; do i = 1 to 10 ; input wloss @@ ; output ; end ; end ; datalines ; 12.4 10.7 11.9 11.0 12.4 12.3 13.0 12.5 11.2 13.1 9.1 11.5 11.3 9.7 13.2 10.7 10.6 11.3 11.1 11.7 8.5 11.6 10.2 10.9 9.0 9.6 9.9 11.3 10.5 11.2 8.7 9.3 8.2 8.3 9.0 9.4 9.2 12.2 8.5 9.9 12.7 13.2 11.8 11.9 12.2 11.2 13.7 11.8 11.5 11.7 ; proc glm data = wloss ; class diet ; model wloss = diet ; means diet / cldiff t bon tukey ; run ; /* Program 3.3: Graphical Presentation for Comparing Means: LINES */ proc glm data = wloss ; class diet ; model wloss = diet ; means diet / lines tukey ; run ; /* Program 3.4: \"Hand\" Calculation of Adjusted p-values */ data ; n = 10 ; g = 5 ; df = g * ( n - 1 ); MeanA = 12.05 ; MeanB = 11.02 ; mse = 0.993422 ; tab = ( MeanA - MeanB ) / ( sqrt ( mse ) * sqrt ( 2 / n )); p = 2 * ( 1 - probt ( abs ( tab ), df )); adjp = 1 - probmc ( ' RANGE ' , sqrt ( 2 ) * abs ( tab ),., df , g ); run ; proc print ; var tab p adjp ; run ; /* Program 3.5: PROC GLM Calculation of Adjusted p-values */ proc glm data = wloss ; class diet ; model wloss = diet ; lsmeans diet / pdiff adjust = tukey ; run ; /* Program 3.6: Dunnett Critical Value (two-sided) */ data ; c_alpha = probmc ( \"DUNNETT2\" ,., .95 , 21 , 6 ); run ; proc print ; run ; /* Program 3.7: Simultaneous Two-sided Comparisons with a Control */ data tox ; input g @ ; do j = 1 to 4 ; input gain @ ; output ; end ; datalines ; 0 97.76 102.56 96.08 125.12 1 91.28 129.20 90.80 72.32 2 67.28 85.76 95.60 73.28 3 80.24 64.88 64.88 78.56 4 96.08 98.24 77.84 95.36 5 57.68 89.84 98.48 92.72 6 68.72 85.28 68.72 74.24 ; proc glm data = tox ; class g ; model gain = g ; means g / dunnett ; run ; /* Program 3.8: Dunnett Critical Value (one-sided) */ data ; c_alpha = probmc ( \"DUNNETT1\" ,., .95 , 21 , 6 ); run ; proc print ; run ; /* Program 3.9: Simultaneous One-sided Comparisons with a Control */ proc glm data = tox ; class g ; model gain = g ; means g / dunnettl ; run ; /* Program 3.10: Simultaneous Confidence Intervals for Means */ proc glm data = wloss ; class diet ; model wloss = diet ; means diet / clm smm sidak ; run ; /* Program 3.11: Orthogonal Comparisons */ data coupon ; input discount purchase @@ ; datalines ; 0 32.39 10 98.47 15 71.62 20 60.85 0 38.32 10 74.80 15 59.92 20 46.45 0 35.66 10 52.97 15 75.37 20 68.49 0 74.24 10 46.72 15 77.04 20 63.83 0 63.05 10 76.81 15 72.84 20 75.38 0 66.53 10 69.01 15 52.53 20 70.60 0 46.36 10 53.77 15 80.47 20 52.23 0 41.90 10 54.21 15 72.55 20 57.14 0 44.94 10 83.14 15 78.94 20 60.17 0 41.09 10 49.00 15 64.00 20 60.46 ; run ; proc glm data = coupon ; class discount ; model purchase = discount ; estimate \"linear\" discount - 3 - 1 1 3 ; estimate \"quad\" discount - 2 2 2 - 2 ; estimate \"cubic\" discount - 1 3 - 3 1 ; run ; data alevel ; input FWE @@ ; qMM = probmc ( ' maxmod ' ,., 1 - FWE , 36 , 3 ); CER = 2 * ( 1 - probt ( qMM , 36 )); output ; datalines ; 0.05 0.10 ; proc print data = alevel noobs ; title1 \"CER is the ALPHA level for orthogonal contrasts\" ; title2 \"that yields the corresponding FWE\" ; run ; title1 ; title2 ; /* Program 4.1: Recovery Time Data Set */ data recover ; input blanket $ minutes @@ ; datalines ; b0 15 b0 13 b0 12 b0 16 b0 16 b0 17 b0 13 b0 13 b0 16 b0 17 b0 17 b0 19 b0 17 b0 15 b0 13 b0 12 b0 16 b0 10 b0 17 b0 12 b1 13 b1 16 b1 9 b2 5 b2 8 b2 9 b3 14 b3 16 b3 16 b3 12 b3 7 b3 12 b3 13 b3 13 b3 9 b3 16 b3 13 b3 18 b3 13 b3 12 b3 13 ; /* Program 4.2: Tukey-Kramer Simultaneous Intervals with Unbalanced ANOVA */ proc glm data = recover ; class blanket ; model minutes = blanket ; means blanket / tukey ; run ; /* Program 4.3: LINES option with unequal sample sizes */ proc glm data = recover ; class blanket ; model minutes = blanket ; means blanket / tukey lines ; run ; /* Program 4.4: Simulation of Correct Tukey-Kramer Critical Value */ data sim ; array nsize { 4 } ( 20 , 3 , 3 , 15 ); do rep = 1 to 500 ; do i = 1 to dim ( nsize ); do j = 1 to nsize { i }; y = rannor ( 121211 ); output ; end ; end ; end ; run ; ods listing close ; ods output Diff = GDiffs ; proc glm data = sim ; by rep ; class i ; model y = i ; lsmeans i / tdiff ; quit ; ods listing ; proc transpose data = GDiffs out = t ( where = ( _label_ > RowName )); by rep RowName ; var _1 _2 _3 _4 ; data t ; set t ; abst = abs ( COL1 ); keep rep abst ; proc means noprint data = t ; var abst ; by rep ; output out = maxt max = maxt ; run ; ods select Quantiles ; proc univariate ; var maxt ; run ; /* Program 4.5: Simulation-Based Critical Value and Intervals */ proc glm data = recover ; class blanket ; model minutes = blanket ; lsmeans blanket / cl adjust = simulate ( seed = 121211 report ); run ; /* Program 4.6: Tukey-Kramer Adjusted p-values in an Unbalanced ANOVA */ proc glm data = recover ; class blanket ; model minutes = blanket ; lsmeans blanket / pdiff cl adjust = tukey ; run ; /* Program 4.7: Getting Greater Simulation Accuracy Using the NSAMP= Option */ proc glm data = recover ; class blanket ; model minutes = blanket ; lsmeans blanket / pdiff cl adjust = simulate ( NSAMP = 4000000 seed = 121211 ); run ; /* Program 4.8: Dunnett's Exact Two-sided Critical Value for Unbalanced ANOVA */ data ; n0 = 20 ; n1 = 3 ; n2 = 3 ; n3 = 15 ; lambda1 = sqrt ( n1 / ( n0 + n1 )); lambda2 = sqrt ( n2 / ( n0 + n2 )); lambda3 = sqrt ( n3 / ( n0 + n3 )); c_alpha = probmc ( ' DUNNETT2 ' ,., .95 , 37 , 3 , lambda1 , lambda2 , lambda3 ); t3 = - 1.66666667 / 0.88477275 ; adjp_3 = 1 - probmc ( ' DUNNETT2 ' , abs ( t3 ),., 37 , 3 , lambda1 , lambda2 , lambda3 ); run ; /* Program 4.9: Dunnett's Two-Sided Comparisons with Unbalanced Data */ proc glm data = recover ; class blanket ; model minutes = blanket ; lsmeans blanket / pdiff cl adjust = dunnett ; run ; /* Program 4.10: Dunnett's Exact One-sided Critical Value for Unbalanced ANOVA */ data ; n0 = 20 ; n1 = 3 ; n2 = 3 ; n3 = 15 ; lambda1 = sqrt ( n1 / ( n0 + n1 )); lambda2 = sqrt ( n2 / ( n0 + n2 )); lambda3 = sqrt ( n3 / ( n0 + n3 )); c_alpha = probmc ( ' DUNNETT1 ' ,., .90 , 37 , 3 , lambda1 , lambda2 , lambda3 ); t3 = - 1.66666667 / 0.88477275 ; adjp_3 = 1 - probmc ( ' DUNNETT1 ' , - t3 ,., 37 , 3 , lambda1 , lambda2 , lambda3 ); run ; proc print ; var c_alpha adjp_3 ; run ; /* Program 4.11: Dunnett's One-Sided Comparisons with Unbalanced Data */ proc glm data = recover ; class blanket ; model minutes = blanket ; lsmeans blanket / pdiff = controll cl alpha = 0.10 ; run ; /* Program 5.1: Selling Prices of Homes */ data house ; input location $ price sqfeet age @@ ; datalines ; A 113.5 2374 4 A 119.9 2271 8 A 127.9 2088 5 A 92.5 1645 8 A 103.0 1814 6 A 142.1 2553 7 A 120.5 1921 9 A 105.5 1854 2 A 101.2 1536 9 A 94.7 1677 3 A 129.0 2342 5 A 108.7 1862 4 A 99.7 1894 7 A 112.0 1774 9 A 104.8 1476 8 A 86.1 1466 7 A 103.5 1800 8 A 93.0 1491 5 A 99.5 1749 8 A 98.1 1690 7 A 144.8 2741 5 A 96.3 1460 5 A 95.1 1614 6 A 125.8 2244 6 A 126.9 2165 6 A 104.7 1828 4 B 74.2 1503 6 B 69.9 1689 6 B 77.0 1638 2 B 67.0 1276 6 B 98.9 2101 9 B 81.2 1668 5 B 85.7 2123 4 B 99.8 2208 5 B 55.7 1273 8 B 120.1 2519 4 B 109.1 2303 6 B 82.4 1800 3 B 102.7 2336 8 B 92.0 2100 6 B 84.1 1697 4 C 90.8 1674 4 C 98.2 2307 7 C 94.6 2152 5 C 87.9 1948 9 D 102.5 2258 2 D 81.3 1965 6 D 86.1 1772 3 D 94.7 2385 1 D 64.7 1345 4 D 93.5 2220 8 D 80.1 1883 8 D 92.3 2012 6 D 80.6 1898 5 E 105.3 2362 7 E 106.3 2362 7 E 84.3 1963 9 E 76.6 1941 7 E 82.4 1975 5 E 98.8 2529 6 E 86.8 2079 5 E 88.5 2190 4 E 77.5 1897 5 E 86.9 1946 4 ; /* Program 5.2: Simultaneous Confidence Intervals for Mean Differences in ANCOVA */ ods select LSMeanDiffCL ; proc glm data = house ; class location ; model price = location sqfeet age ; lsmeans location / pdiff cl adjust = simulate ( seed = 12345 cvadjust ); run ; /* Program 5.3: Viewing Simulation Details when using adjust=simulate */ ods select SimDetails SimResults LSMeanDiffCL ; proc glm data = house ; class location ; model price = location sqfeet age ; lsmeans location / pdiff cl adjust = simulate ( seed = 12345 report cvadjust ); run ; /* Program 5.4: Invocation of %SimIntervals using Direct Specification of %Estimates and %Contrasts */ %macro Estimates ; EstPar = { 12.05 , 11.02 , 10.27 , 9.27 , 12.17 }; Mse = 0.9934 ; Cov = Mse * I ( 5 ) / 10 ; /* sample size is 10 per group */ df = 45 ; %mend ; %macro Contrasts ; C = { 1 - 1 0 0 0 , 1 0 - 1 0 0 , 1 0 0 - 1 0 , 1 0 0 0 - 1 , 0 1 - 1 0 0 , 0 1 0 - 1 0 , 0 1 0 0 - 1 , 0 0 1 - 1 0 , 0 0 1 0 - 1 , 0 0 0 1 - 1 }; C = C ` ; /* transposed to coincide with notation in 5.2.2 */ Clab = { \"1-2\" , \"1-3\" , \"1-4\" , \"1-5\" , \"2-3\" , \"2-4\" , \"2-5\" , \"3-4\" , \"3-5\" , \"4-5\" }; /* Contrast labels */ %mend ; % SimIntervals ( nsamp = 50000 , seed = 121211 , conf = 0.95 , side = B ); /* Program 5.5: Invocation of %SimIntervals using %MakeGLMStats */ % MakeGLMStats ( dataset = wloss , classvar = diet , yvar = wloss , model = diet ); %macro Contrasts ; C = { 0 1 - 1 0 0 0 , 0 1 0 - 1 0 0 , 0 1 0 0 - 1 0 , 0 1 0 0 0 - 1 , 0 0 1 - 1 0 0 , 0 0 1 0 - 1 0 , 0 0 1 0 0 - 1 , 0 0 0 1 - 1 0 , 0 0 0 1 0 - 1 , 0 0 0 0 1 - 1 }; C = C ` ; /* transposed to coincide with notation in 5.2.2 */ Clab = { \"1-2\" , \"1-3\" , \"1-4\" , \"1-5\" , \"2-3\" , \"2-4\" , \"2-5\" , \"3-4\" , \"3-5\" , \"4-5\" }; /* Contrast labels */ %mend ; % SimIntervals ( nsamp = 50000 , seed = 121211 ); /* Program 5.6: Invocation of %SimIntervals using %MakeGLMStats to create %Contrasts and %Estimates */ % MakeGLMStats ( dataset = wloss , classvar = diet , yvar = wloss , model = diet , contrasts = all ( diet )); % SimIntervals ( nsamp = 50000 , seed = 121211 ); /* Program 5.7: %SimIntervals Analysis for Comparing Covariate-Adjusted Means */ % MakeGLMStats ( dataset = house , classvar = location , yvar = price , model = location sqfeet age , contrasts = all ( location )); % SimIntervals ( seed = 121211 ); /* Program 5.8: Adjusted p-values from PROC GLM */ ods select DiffMat ; proc glm data = house ; class location ; model price = location sqfeet age ; lsmeans location / pdiff cl adjust = simulate ( seed = 12345 nsamp = 100000 ); run ; /* Program 5.9: Rat Growth Data */ data ratgrwth ; length trt $ 10 ; input trt $ W0 - W4 @@ ; datalines ; Control 46 70 102 131 153 Control 49 67 90 112 140 Control 49 67 100 129 164 Control 51 71 94 110 141 Control 52 77 111 144 185 Control 56 81 104 121 151 Control 57 82 110 139 169 Control 57 86 114 139 172 Control 60 93 123 146 177 Control 63 91 112 130 154 Thyroxin 52 70 105 138 171 Thyroxin 52 73 97 116 140 Thyroxin 54 71 90 110 138 Thyroxin 56 75 108 151 189 Thyroxin 57 72 97 120 144 Thyroxin 59 85 116 148 177 Thyroxin 59 85 121 156 191 Thiouracil 46 61 78 90 107 Thiouracil 51 75 92 100 119 Thiouracil 51 75 101 123 140 Thiouracil 53 79 100 106 133 Thiouracil 53 72 89 104 122 Thiouracil 56 78 95 103 108 Thiouracil 58 69 93 114 138 Thiouracil 59 80 101 111 122 Thiouracil 59 88 100 111 122 Thiouracil 61 86 109 120 129 ; /* Program 5.10: Covariate-Adjusted One-Sided Comparisons with a Control */ ods select LSMeans LSMeanDiffCL ; proc glm data = ratgrwth ; class trt ; model w4 = trt w0 - w3 ; lsmeans trt / pdiff = controll cl adjust = dunnett ; run ; /* Program 5.11: Using Hsu's Control-Variate Simulation Method for Reducing Monte Carlo Error */ ods select LSMeans LSMeanDiffCL ; proc glm data = ratgrwth ; class trt ; model w4 = trt w0 - w3 ; lsmeans trt / pdiff = controll cl adjust = simulate ( cvadjust nsamp = 100000 report seed = 121211 ); run ; /* Program 5.12: Multiple Comparisons at Fixed Covariate Levels */ data alz ; input therapy age since score @@ ; cards ; 1 69 22 66 1 68 14 60 1 66 14 55 1 68 31 70 1 71 27 50 1 55 28 56 1 71 24 62 1 68 28 67 1 71 25 72 1 71 16 48 1 88 123 59 1 67 27 58 1 71 25 70 2 72 54 47 2 90 121 44 2 72 44 64 2 75 19 65 2 69 28 56 2 65 27 71 2 76 22 68 2 68 29 53 2 61 18 60 2 65 20 65 2 71 20 60 2 72 21 64 3 70 27 54 3 70 23 45 3 64 11 60 3 66 45 30 3 67 19 46 3 68 24 45 3 69 16 49 3 79 23 49 3 68 31 34 3 72 15 40 4 67 28 89 4 65 17 100 4 66 32 81 4 75 23 76 4 83 60 64 4 70 12 58 4 63 16 85 4 68 17 86 4 65 18 89 4 68 27 76 4 63 18 95 4 67 37 85 4 75 18 96 4 66 27 90 4 71 23 83 4 70 36 75 5 66 18 68 5 69 25 77 5 76 13 87 5 70 9 95 5 70 11 95 5 71 13 82 ; proc glm data = alz outstat = stat ; ods select LSMeanDiffCL ; class therapy ; model score = therapy since age therapy * since ; lsmeans therapy / pdiff cl adjust = simulate at since = 10 ; run ; proc glm data = alz outstat = stat ; ods select LSMeanDiffCL ; class therapy ; model score = therapy since age therapy * since ; lsmeans therapy / pdiff cl adjust = simulate at since = 20 ; run ; /* Program 6.1: Litter weight data */ data litter ; input dose weight gesttime number @@ ; datalines ; 0 28.05 22.5 15 0 33.33 22.5 14 0 36.37 22.0 14 0 35.52 22.0 13 0 36.77 21.5 15 0 29.60 23.0 5 0 27.72 21.5 16 0 33.67 22.5 15 0 32.55 22.5 14 0 32.78 21.5 15 0 31.05 22.0 12 0 33.40 22.5 15 0 30.20 22.0 16 0 28.63 21.5 7 0 33.38 22.0 15 0 33.43 22.0 13 0 29.63 21.5 14 0 33.08 22.0 15 0 31.53 22.5 16 0 35.48 22.0 9 5 34.83 22.5 15 5 26.33 22.5 7 5 24.28 22.5 15 5 38.63 23.0 9 5 27.92 22.0 13 5 33.85 22.5 13 5 24.95 22.5 17 5 33.20 22.5 15 5 36.03 22.5 12 5 26.80 22.0 13 5 31.67 22.0 14 5 30.33 21.5 12 5 26.83 22.5 14 5 32.18 22.0 13 5 33.77 22.5 16 5 21.30 21.5 9 5 25.78 21.5 14 5 19.90 21.5 12 5 28.28 22.5 16 50 31.28 22.0 16 50 35.80 21.5 16 50 27.97 21.5 14 50 33.13 22.5 15 50 30.60 22.5 15 50 30.17 21.5 15 50 27.07 21.5 14 50 32.02 22.0 17 50 36.72 22.5 13 50 28.50 21.5 14 50 21.58 21.5 16 50 30.82 22.5 17 50 30.55 22.0 14 50 27.63 22.0 14 50 22.97 22.0 12 50 29.55 21.5 12 50 31.93 22.0 14 50 29.30 21.5 16 500 24.55 22.0 7 500 33.78 22.5 13 500 32.98 22.0 10 500 25.38 21.5 11 500 30.32 22.0 15 500 19.22 22.5 11 500 26.37 21.5 14 500 28.60 22.5 9 500 19.70 22.0 11 500 32.88 22.5 15 500 26.12 22.5 13 500 33.20 22.0 12 500 32.97 22.5 14 500 38.75 23.0 16 500 33.15 22.5 12 500 30.70 21.5 13 500 35.32 22.0 17 ; /* Program 6.2: Estimation of Contrasts using PROC GLM */ proc glm data = litter ; class dose ; model weight = dose gesttime number ; estimate ' cont - low ' dose 1 - 1 0 0 ; estimate ' cont - mid ' dose 1 0 - 1 0 ; estimate ' cont - high ' dose 1 0 0 - 1 ; estimate ' ordinal ' dose 0.750 0.250 - 0.250 - 0.750 ; estimate ' arith ' dose 0.384 0.370 0.246 - 1.000 ; estimate ' log ord ' dose 0.887 0.113 - 0.339 - 0.661 ; run ; /* Program 6.3: Simultaneous Intervals for General Contrasts in an ANCOVA Model */ % MakeGLMStats ( dataset = litter , classvar = dose , yvar = weight , model = dose gesttime number ); %macro Contrasts ; C = { 0 1 - 1 0 0 0 0 , 0 1 0 - 1 0 0 0 , 0 1 0 0 - 1 0 0 , 0 0.750 0.250 - 0.250 - 0.750 0 0 , 0 0.384 0.370 0.246 - 1.000 0 0 , 0 0.887 0.113 - 0.339 - 0.661 0 0 }; C = C ` ; Clab = { \"Control-Low\" , \"Control-Med\" , \"Control-High\" , \"Ordinal\" , \"Arithmetic\" , \"Log-Ordinal\" }; %mend ; % SimIntervals ( nsamp = 100000 , seed = 121221 , side = U ); /* Program 6.4: Overall F-test Significant but Pairwise Comparisons Insignificant */ data wlossnew ; set wloss ; wloss = wloss + 6 * rannor ( 121211 ); /* Random error added */ proc glm ; class diet ; model wloss = diet ; means diet / cldiff tukey ; run ; /* Program 6.5: Scheffe Intervals */ proc glm data = wlossnew ; class diet ; model wloss = diet ; means diet / cldiff scheffe ; run ; /* Program 6.6: Multiple Contrasts, Where Some are Suggested by the Data */ data ; fwe = 0.05 ; g = 5 ; df = 45 ; fcrit = finv ( 1 - fwe , g - 1 , df ); c_alpha = sqrt (( g - 1 ) * fcrit ); p_crit = 2 * ( 1 - probt ( c_alpha , df )); call symput ( ' scheffep ' , p_crit ); run ; proc glm data = wlossnew ; title \"Use &Scheffep to determine significance of contrasts\" ; class diet ; model wloss = diet ; lsmeans diet / pdiff adjust = scheffe ; estimate \"c1\" diet 1 1 - 1 - 1 0 / divisor = 2 ; estimate \"c2\" diet - 1 - 1 1 0 1 / divisor = 2 ; estimate \"c3\" diet 4 - 1 - 1 - 1 - 1 / divisor = 4 ; estimate \"c4\" diet 2 - 3 2 - 3 2 / divisor = 6 ; estimate \"c5\" diet 1 - 1 0 - 1 1 / divisor = 2 ; estimate \"c6\" diet 2 - 1 0 - 2 1 / divisor = 3 ; run ; /* Program 6.7: Finding the Most Significant Contrast in ANCOVA */ %let classvar = location ; proc glm data = house ; class location ; model price = location sqfeet age ; lsmeans location / out = stats cov ; proc iml ; use stats ; read all into alldata ; read all var { & classvar } into classvar ; read all var { LSMEAN } into ests ; classvar = classvar ` ; ncall = ncol ( alldata ); nclass = nrow ( ests ); ncstart = ncall - nclass + 1 ; covs = alldata [, ncstart : ncall ]; cont1 = j ( nclass - 1 , 1 , 1 ); cont2 = - i ( nclass - 1 ); cont = cont1 || cont2 ; nummat = ( cont * ests ) * ( ests ` * cont ` ); denmat = cont * covs * cont ` ; h = half ( denmat ); evec = eigvec ( inv ( h ` ) * nummat * inv ( h )); e1 = inv ( h ) * evec [, 1 ]; contrast = e1 ` * cont ; contrast = contrast / sum (( contrast > 0 ) #contrast ); print \"Most Significant Contrast\" , contrast [ label = \"&classvar\" colname = classvar ]; quit ; /* Program 6.8: Simultaneous Confidence Bounds for Regression Function */ % MakeGLMStats ( dataset = house_a , yvar = price , model = sqfeet ); %macro Contrasts ; free c clab ; do x = 1000 to 3000 by 200 ; c = c // (1 || x); clab = clab // x; end ; c = c ` ; %mend ; % SimIntervals ( nsamp = 50000 , seed = 121211 ); data xvalues ; do x = 1000 to 3000 by 200 ; output ; end ; run ; data confplot ; merge xvalues SimIntOut ; run ; goptions ftext = swissb hsize = 4 in vsize = 4 in ; axis1 style = 1 width = 2 major = ( number = 5 ) minor = none label = ( ' Square Feet ' ); axis2 style = 1 width = 2 major = ( number = 5 ) minor = none label = ( ' Price ' ); symbol1 w = 1 c = black i = spline v = none ; symbol2 w = 1 c = black i = spline v = none l = 2 ; proc gplot data = confplot ; title ; plot Estimate * x = 1 ( LowerCL UpperCL ) * x = 2 / haxis = axis1 vaxis = axis2 frame overlay ; run ; /* Program 6.9: Working-Hotelling Confidence Bounds for Regression Function */ %let conf = .95 ; data housplt ; set house_a end = eof ; output ; if eof then do ; call symput ( 'n' , left ( _n_ )); price = .; do sqfeet = 1000 to 3000 by 200 ; output ; end ; end ; run ; proc reg data = housplt ; model price = sqfeet ; output out = ests p = pred stdp = se ; run ; data plot ; set ests ; if _n_ > & n ; c_a = sqrt ( 2 * finv ( & conf , 2 , & n - 2 )); lower = pred - c_a * se ; upper = pred + c_a * se ; run ; goptions ftext = swissb hsize = 4 in vsize = 4 in ; axis1 style = 1 width = 2 major = ( number = 5 ) minor = none label = ( ' Square Feet ' ); axis2 style = 1 width = 2 major = ( number = 5 ) minor = none label = ( ' Price ' ); Symbol1 W = 1 C = Black I = Spline V = None ; Symbol2 W = 1 C = Black I = Spline V = None L = 2 ; proc gplot data = plot ; title ; plot pred * sqfeet = 1 ( lower upper ) * sqfeet = 2 / haxis = axis1 vaxis = axis2 frame overlay ; run ; /* Program 6.10: Patient Satisfaction Data */ data pat_sat ; input age severe anxiety satisf @@ ; cards ; 50 51 2.3 48 36 46 2.3 57 40 48 2.2 66 41 44 1.8 70 28 43 1.8 89 49 54 2.9 36 42 50 2.2 46 45 48 2.4 54 52 62 2.9 26 29 50 2.1 77 29 48 2.4 89 43 53 2.4 67 38 55 2.2 47 34 51 2.3 51 53 54 2.2 57 36 49 2.0 66 33 56 2.5 79 29 46 1.9 88 33 49 2.1 60 55 51 2.4 49 29 52 2.3 77 44 58 2.9 52 43 50 2.3 60 ; /* Program 6.11: Confidence Bounds for a Partial Function */ % MakeGLMStats ( dataset = pat_sat , yvar = satisf , model = age severe anxiety ); %macro Contrasts ; free c clab ; do x = 45 to 60 by 1 ; c = c // (1 || 39.6 || x || 2.30); clab = clab // x; end ; c = c ` ; %mend ; % SimIntervals ( nsamp = 100000 , seed = 121221 ); data xvalues ; do x = 45 to 60 by 1 ; output ; end ; run ; data confplot ; merge xvalues SimIntOut ; run ; goptions ftext = swissb hsize = 4 in vsize = 4 in ; axis1 style = 1 width = 2 major = ( number = 6 ) minor = none label = ( ' Illness Severity ' ); axis2 style = 1 width = 2 major = ( number = 6 ) minor = none label = ( ' Satisfaction ' ); Symbol1 W = 1 C = Black I = Spline V = None ; Symbol2 W = 1 C = Black I = Spline V = None L = 2 ; proc gplot data = confplot ; title ; plot estimate * x = 1 ( lowercl uppercl ) * x = 2 / haxis = axis1 vaxis = axis2 frame overlay ; run ; /* Program 6.12: Tire Wear Data data tire; input make$ mph cost @@; datalines; A 10 9.8 A 20 12.5 A 20 14.2 A 30 14.9 A 40 19.0 A 40 16.5 A 50 20.9 A 60 22.4 A 60 24.1 A 70 25.8 B 10 15.0 B 20 14.5 B 20 16.1 B 30 16.5 B 40 16.4 B 40 19.1 B 50 20.9 B 60 22.3 B 60 19.8 B 70 21.4 ; run; /* Program 6.13: Simultaneous Confidence Bounds for Difference of Regression Functions */ % MakeGLMStats ( dataset = tire , classvar = make , yvar = cost , model = make mph make * mph ); %macro Contrasts ; free c clab ; do x = 10 to 70 by 5 ; c = c // (0 || 1 || -1 || 0 || x || -x); clab = clab // x ; end ; c = c ` ; %mend ; % SimIntervals ( nsamp = 100000 , seed = 121211 ); data xvalues ; do x = 10 to 70 by 5 ; output ; end ; run ; data confplot ; merge xvalues SimIntOut ; run ; goptions ftext = swissb hsize = 5 in vsize = 4 in ; axis1 style = 1 width = 2 major = ( number = 7 ) minor = none label = ( ' Miles Per Hour ' ); axis2 style = 1 width = 2 major = ( number = 5 ) minor = none label = ( ' CostA - CostB ' ); symbol1 w = 1 c = black i = spline v = none ; symbol2 w = 1 c = black i = spline v = none l = 2 ; proc gplot data = confplot ; title ; plot estimate * x = 1 ( lowercl uppercl ) * x = 2 / haxis = axis1 vaxis = axis2 frame overlay vref = 0 ; run ; /* Program 6.14: Finding the Critical Value for Comparing Two Regression Lines Using the Continuous Method (Constrained Optimization) */ options nonotes ; %let nsamp = 100000 ; /* Number of simulations */ %let seed = 121021 ; %let dataset = tire ; %let yvar = cost ; %let classvar = make ; /* Must be a two-level variable */ %let xvar = mph ; %let lowerX = 10 ; %let upperX = 70 ; %let conf = 0.95 ; %let npoints = 100 ; /* Number of points to plot on the graph */ proc iml ; use & dataset ; read all var { & yvar } into Y ; read all var { & xvar } into X1 ; read all var { & classvar } into X2 ; D = design ( X2 )[, 1 ]; DX = D #X1 ; n = nrow ( X1 ); one = j ( n , 1 , 1 ); X = one || X1 || D || DX ; XPXI = INV ( X ` * X ); XPXIXP = XPXI * X ` ; b = XPXIXP * Y ; df = n - ncol ( X ); mse = ( Y ` - b ` * X ` ) * ( Y - X * b ) / df ; lowerX = & lowerX ; upperX = & upperX ; xbar = x1 [ + ,] / n ; optn = { 1 0 }; bc = lowerX //UpperX; maxt = j ( & nsamp , 1 , 0 ); inc = ( UpperX - LowerX ) /& npoints ; start tstat ( x0 ) global ( n , XPXI , bstar , msestar ); c0 = { 0 } || { 0 } || { 1 } || x0 ; est = c0 * bstar ; t = est / sqrt ( msestar * c0 * XPXI * c0 ` ); t = abs ( t ); return ( t ); finish ; do isim = 1 to & nsamp ; Ystar = rannor ( j ( n , 1 , & seed )); bstar = xpxixp * Ystar ; msestar = ( Ystar ` - bstar ` * X ` ) * ( Ystar - X * bstar ) / df ; call nlpqn ( rc , xmax , \"tstat\" , xbar , optn , bc ,,,,,); mx = tstat ( xmax ); maxt [ isim ] = mx ; end ; temp = maxt ; maxt [ rank ( maxt ),] = temp ; critindx = round ( & nsamp *& conf , 1 ); sim_crit = maxt [ critindx ]; wh_crit = sqrt ( 2 * finv ( & conf , 2 , df )); t_crit = tinv ( 1 - ( 1 -& conf ) / 2 , df ); print \"The simulation-based, Working-Hotelling, and t critical values are\" ; print sim_crit wh_crit t_crit ; /* Program 7.1: Simulation of ANOVA Data */ data a ; array mu { 5 } ( 10 , 5 , 5 , 0 , 0 ); do a = 1 to dim ( mu ); do i = 1 to 10 ; y = mu { a } + 6 * rannor ( 12345 ); output ; end ; end ; run ; proc glm data = a ; class a ; model y = a ; means a / tukey cldiff ; ods select CLDiffs ; run ; /* Program 7.2: Using the %IndividualPower Macro to Calculate Power Analytically for All Pairwise Comparisons */ % IndividualPower ( MCP = RANGE , /* RANGE, DUNNETT2, DUNNETT1, OR MAXMOD */ g = 5 , /* number of groups (exclude control for DUNNETT) */ d = 4 , /* meaningful mean difference */ s = 3 /* estimate (guess) of standard deviation */ ); /* Program 7.3: Using the %IndividualPower Macro to Calculate Power Analytically for Dunnett's Two-sided Comparisons with a Control */ % IndividualPower ( MCP = DUNNETT2 , g = 6 , d = 5 , s = 3.5 ); /* Program 7.4: Simulating Combined Power Measures for all Pairwise Comparisons */ % SimPower ( TrueMeans = ( 10 , 5 , 5 , 0 , 0 ), s = 5 , n = 10 , seed = 12345 ); /* Program 7.5: Evaluating Directional FWE When There Are No Null Effects */ % SimPower ( TrueMeans = ( - .1 , - .2 , .1 , .05 ), s = 500 , n = 2 , nrep = 4000 , seed = 12345 ); /* Program 7.6: Simulating combined power of two-sided comparisons with a control */ % SimPower ( TrueMeans = ( 10 , 5 , 5 , 0 , 0 ), s = 5 , n = 10 , seed = 12345 , method = DUNNETT ); /* Program 7.7: Simulating Combined Power of One-sided Comparisons with a Control */ % SimPower ( TrueMeans = ( 10 , 5 , 5 , 0 , 0 ), s = 5 , n = 10 , seed = 12345 , method = DUNNETTL ); /* Program 7.8: Plotting Simulated Complete Power of Two-sided Comparisons with a Control */ % PlotSimPower ( TrueMeans = ( 10 , 5 , 5 , 0 , 0 ), s = 5 , seed = 12345 , method = Dunnett ); /* Program 8.1: Tukey-Welch (REGWQ) Comparisons for Balanced ANOVA */ data Cholesterol ; do trt = 'A' , 'B' , 'C' , 'D' , 'E' ; do i = 1 to 10 ; input response @@ ; output ; end ; end ; datalines ; 3.8612 10.3868 5.9059 3.0609 7.7204 2.7139 4.9243 2.3039 7.5301 9.4123 10.3993 8.6027 13.6320 3.5054 7.7703 8.6266 9.2274 6.3159 15.8258 8.3443 13.9621 13.9606 13.9176 8.0534 11.0432 12.3692 10.3921 9.0286 12.8416 18.1794 16.9819 15.4576 19.9793 14.7389 13.5850 10.8648 17.5897 8.8194 17.9635 17.6316 21.5119 27.2445 20.5199 15.7707 22.8850 23.9527 21.5925 18.3058 20.3851 17.3071 ; proc glm data = Cholesterol ; class trt ; model response = trt ; means trt / regwq ; run ; /* Program 8.2: Power Calculation for the REGWQ Method */ % SimPower ( TrueMeans = ( 10 , 5 , 5 , 0 , 0 ), s = 5 , n = 10 , seed = 12345 , method = REGWQ ); /* Program 8.3: The Begun and Gabriel Method} */ %beggab ( dataset = Cholesterol , groups = trt , response = response ); /* Program 8.4: Closed Testing Using Fisher's Combination Test */ data fishcomb ; input p1 p2 p3 ; t123 = - 2 * ( log ( p1 ) + log ( p2 ) + log ( p3 )); p123 = 1 - probchi ( t123 , 6 ); t12 = - 2 * ( log ( p1 ) + log ( p2 )); p12 = 1 - probchi ( t12 , 4 ); t13 = - 2 * ( log ( p1 ) + log ( p3 )); p13 = 1 - probchi ( t13 , 4 ); t23 = - 2 * ( log ( p2 ) + log ( p3 )); p23 = 1 - probchi ( t23 , 4 ); datalines ; 0.076 0.081 0.0201 ; run ; proc print ; var p123 p12 p13 p23 p1 p2 p3 ; run ; /* Program 8.5: Step-down Dunnett Critical Points */ data ; do i = 1 to 6 ; c_i =- probmc ( \"DUNNETT1\" ,., .95 , 21 , i ); diff_i = c_i * ( 210.0048 * 2 / 4 ) ** 0.5 ; output ; end ; proc print ; run ; /* Program 8.6: Dose-Response Contrasts for the Analysis of Angina Drug Data */ data angina ; do dose = 0 to 4 ; do i = 1 to 10 ; input response @@ ; output ; end ; end ; datalines ; 12.03 19.06 14.24 11.17 16.19 10.80 13.18 10.35 15.99 18.01 17.54 15.48 21.26 9.63 14.53 15.51 16.20 12.86 23.78 15.18 18.97 18.96 18.92 13.51 16.27 17.49 15.67 14.41 17.93 22.86 20.60 19.19 23.38 18.52 17.45 14.93 21.16 13.03 21.51 21.20 25.29 32.32 24.08 18.25 26.98 28.29 25.39 21.36 23.91 20.14 ; proc glm data = angina ; class dose ; model response = dose ; contrast ' all doses ' dose - 2 - 1 0 1 2 ; contrast ' next to highest ' dose - 3 - 1 1 3 0 ; contrast ' middle dose ' dose - 1 0 1 0 0 ; contrast ' low dose ' dose - 1 1 0 0 0 ; run ; /* Program 8.7: Rom-Costello-Connell Closed Dose-Response Testing */ %rcc ( DataSet = angina , Groups = dose , Response = Response , FWE = 0.05 ); /* Program 8.8: Williams' Test on Heart Rate Data */ data hr ; do trt = 1 to 5 ; do j = 1 to 12 ; input response @@ ; output ; end ; end ; cards ; 5 7 10 3 - 6 10 11 13 - 2 - 4 5 8 - 3 - 5 7 6 - 8 - 5 7 2 - 4 - 5 - 1 3 - 1 6 - 2 - 3 5 6 - 1 - 5 5 7 0 - 2 12 8 0 - 3 7 9 2 - 2 5 2 - 1 10 5 11 9 9 7 0 10 11 14 9 8 10 ; %macro Williams ( dataset = , trt = , response = ); /* Program 8.9: Logically Constrained Tests on Cholesterol Reduction Data */ %macro Contrasts ; C = { - 1 1 0 0 0 , - 1 0 1 0 0 , - 1 0 0 1 0 , - 1 0 0 0 1 , 0 - 1 1 0 0 , 0 - 1 0 1 0 , 0 - 1 0 0 1 , 0 0 - 1 1 0 , 0 0 - 1 0 1 , 0 0 0 - 1 1 }; C = C ` ; clab = { \"2-1\" , \"3-1\" , \"4-1\" , \"5-1\" , \"3-2\" , \"4-2\" , \"5-2\" , \"4-3\" , \"5-3\" , \"5-4\" }; %mend ; %macro Estimates ; mse = 10.41668 ; df = 45 ; EstPar = { 20.948 , 15.361 , 12.375 , 9.225 , 5.782 }; cov = mse * ( 1 / 10 ) * I ( 5 ); %mend ; % SimTests ( seed = 121211 , type = LOGICAL ); /* Program 8.10: Simplifying the %Contrasts Macro for Pairwise Comparisons */ %macro Contrasts ; g = 5 ; free c clab ; do i = 1 to g - 1 ; do j = i + 1 to g ; c = c // ((1:g)=i) - ((1:g)=j); clab = clab // ( trim(left(char(i,5))) + '-' + trim ( left ( char ( j , 5 )))); end ; end ; c = c ` ; %mend ; /* Program 8.11: Analysis of More General Contrasts using %SimTests */ %macro Contrasts ; C = { - 1 1 0 0 0 , - 1 0 1 0 0 , 0 - 1 1 0 0 , - 1 - 1 - 1 3 0 , - 1 - 1 - 1 0 3 }; C = C ` ; clab = { \"2-1\" , \"3-1\" , \"3-2\" , \"D-test\" , \"E-test\" }; %mend ; %macro Estimates ; mse = 10.41668 ; df = 45 ; EstPar = { 20.948 , 15.361 , 12.375 , 9.225 , 5.782 }; cov = mse * ( 1 / 10 ) * I ( 5 ); %mend ; % SimTests ( seed = 121211 , type = LOGICAL , nsamp = 50000 ); /* Program 8.12: Multiple Tests with General Contrasts and Correlations */ proc glm data = litter outstat = stat ; class dose ; model weight = dose gesttime number ; lsmeans dose / out = ests cov ; run ; %macro Estimates ; use ests ; read all var { lsmean } into EstPar ; read all var { cov1 cov2 cov3 cov4 } into Cov ; use stat ( where = ( _TYPE_ = ' ERROR ' )); read all var { df } into df ; %mend ; %macro Contrasts ; c = { 1 - 1 0 0 , 1 0 - 1 0 , 1 0 0 - 1 , 0.750 0.250 - 0.250 - 0.750 , 0.384 0.370 0.246 - 1.000 , 0.887 0.113 - 0.339 - 0.661 }; c = c ` ; clab = { \"cont-low \" , \"cont-mid \" , \"cont-high\" , \"ordinal \" , \"arith \" , \"log ord \" }; %mend ; % SimTests ( nsamp = 50000 , seed = 121211 , type = LOGICAL , side = U ); /* Program 9.1: Industrial Waste Output */ data waste ; do temp = 1 to 3 ; do envir = 1 to 5 ; do rep = 1 to 2 ; input waste @@ ; output ; end ; end ; end ; datalines ; 7.09 5.90 7.94 9.15 9.23 9.85 5.43 7.73 9.43 6.90 7.01 5.82 6.18 7.19 7.86 6.33 8.49 8.67 9.62 9.07 7.78 7.73 10.39 8.78 9.27 8.90 12.17 10.95 13.07 9.76 ; run ; /* Program 9.2: Multiple Comparisons of Main Effects in Balanced ANOVA */ proc glm data = waste ; class envir temp ; model waste = envir temp envir * temp ; lsmeans temp envir / pdiff cl adjust = tukey ; run ; /* Program 9.3: Simultaneous Tests of Both Sets of Main Effects Contrasts */ % MakeGLMStats ( dataset = waste , classvar = envir temp , yvar = waste , model = envir * temp ); %macro Contrasts ; C = { 0 1 1 1 - 1 - 1 - 1 0 0 0 0 0 0 0 0 0 , 0 1 1 1 0 0 0 - 1 - 1 - 1 0 0 0 0 0 0 , 0 1 1 1 0 0 0 0 0 0 - 1 - 1 - 1 0 0 0 , 0 1 1 1 0 0 0 0 0 0 0 0 0 - 1 - 1 - 1 , 0 0 0 0 1 1 1 - 1 - 1 - 1 0 0 0 0 0 0 , 0 0 0 0 1 1 1 0 0 0 - 1 - 1 - 1 0 0 0 , 0 0 0 0 1 1 1 0 0 0 0 0 0 - 1 - 1 - 1 , 0 0 0 0 0 0 0 1 1 1 - 1 - 1 - 1 0 0 0 , 0 0 0 0 0 0 0 1 1 1 0 0 0 - 1 - 1 - 1 , 0 0 0 0 0 0 0 0 0 0 1 1 1 - 1 - 1 - 1 }; C = C / 3 ; C1 = { 0 1 - 1 0 1 - 1 0 1 - 1 0 1 - 1 0 1 - 1 0 , 0 1 0 - 1 1 0 - 1 1 0 - 1 1 0 - 1 1 0 - 1 , 0 0 1 - 1 0 1 - 1 0 1 - 1 0 1 - 1 0 1 - 1 }; C1 = C1 / 5 ; C = C //C1; C = C ` ; Clab = { \"e1-e2\" , \"e1-e3\" , \"e1-e4\" , \"e1-e5\" , \"e2-e3\" , \"e2-e4\" , \"e2-e5\" , \"e3-e4\" , \"e3-e5\" , \"e4-e5\" , \"t1-t2\" , \"t1-t3\" , \"t2-t3\" }; %mend ; % SimTests ( seed = 121211 , type = LOGICAL ); /* Program 9.4: Simultaneous Confidence Intervals for Interaction Contrasts */ % MakeGLMStats ( dataset = waste , classvar = envir temp , yvar = waste , model = envir * temp ); %let a = 5 ; /* Levels of first CLASS variable */ %let b = 3 ; /* Levels or second CLASS variable */ %macro Contrasts ; start tlc ( n ); return ( trim ( left ( char ( n , 20 )))); finish ; idi = ( 1 :& a ); idj = ( 1 :& b ); free C clab ; do i1 = 1 to & a - 1 ; do i2 = i1 + 1 to & a ; do j1 = 1 to & b - 1 ; do j2 = j1 + 1 to & b ; C = C // (0 || ( ((idi=i1) - (idi=i2)) @ (( idj = j1 ) - ( idj = j2 )))); clab = clab // \"(\"+tlc(i1)+tlc(j1)+\"-\"+tlc(i1)+tlc(j2)+\")\" + \"-(\" + tlc ( i2 ) + tlc ( j1 ) + \"-\" + tlc ( i2 ) + tlc ( j2 ) + \")\" ; end ; end ; end ; end ; C = C ` ; %mend ; % SimIntervals ( nsamp = 100000 , seed = 12345 ); /* Program 9.5: Multiple Comparisons With One Observation Per Cell */ data waste1 ; set waste ; if rep = 1 ; run ; proc glm ; class envir temp ; model waste = envir temp ; means envir temp / regwq lines ; ods select MCLinesInfo MCLines ; run ; /* Program 9.6: Comparisons With One Observation Per Cell: Global Family */ % MakeGLMStats ( dataset = waste1 , classvar = envir temp , yvar = waste , model = envir temp ); %macro Contrasts ; C = { 0 1 - 1 0 0 0 0 0 0 , 0 1 0 - 1 0 0 0 0 0 , 0 1 0 0 - 1 0 0 0 0 , 0 1 0 0 0 - 1 0 0 0 , 0 0 1 - 1 0 0 0 0 0 , 0 0 1 0 - 1 0 0 0 0 , 0 0 1 0 0 - 1 0 0 0 , 0 0 0 1 - 1 0 0 0 0 , 0 0 0 1 0 - 1 0 0 0 , 0 0 0 0 1 - 1 0 0 0 , 0 0 0 0 0 0 1 - 1 0 , 0 0 0 0 0 0 1 0 - 1 , 0 0 0 0 0 0 0 1 - 1 }; C = C ` ; Clab = { \"e1-e2\" , \"e1-e3\" , \"e1-e4\" , \"e1-e5\" , \"e2-e3\" , \"e2-e4\" , \"e2-e5\" , \"e3-e4\" , \"e3-e5\" , \"e4-e5\" , \"t1-t2\" , \"t1-t3\" , \"t2-t3\" }; %mend ; % SimTests ( seed = 121211 , type = LOGICAL ); /* Program 9.7: Drug Study Data: Comparisons of LS-Means with Unbalanced Data */ data drug ; input drug disease @ ; do i = 1 to 6 ; input response @ ; output ; end ; cards ; 1 1 42 44 36 13 19 22 1 2 33 . 26 . 33 21 1 3 31 - 3 . 25 25 24 2 1 28 . 23 34 42 13 2 2 . 34 33 31 . 36 2 3 3 26 28 32 4 16 3 1 . . 1 29 . 19 3 2 . 11 9 7 1 - 6 3 3 21 1 . 9 3 . 4 1 24 . 9 22 - 2 15 4 2 27 12 12 - 5 16 15 4 3 22 7 25 5 12 . ; proc glm ; class drug disease ; model response = drug disease drug * disease / ss3 ; lsmeans drug / pdiff cl adjust = simulate ( seed = 121211 acc = .001 ); ods select OverallANOVA ModelANOVA LSMeanDiffCL ; run ; /* Program 9.8: BIBD Data for Detergents */ data detergent ; do detergent = 1 to 5 ; do block = 1 to 10 ; input plates @@ ; output ; end ; end ; datalines ; 27 28 30 31 29 30 . . . . 26 26 29 . . . 30 21 26 . 30 . . 34 32 . 34 31 . 33 . 29 . 33 . 34 31 . 33 31 . . 26 . 24 25 . 23 24 26 ; /* Program 9.9: Simultaneous Confidence Intervals for Treatment Differences */ proc glm data = detergent ; class block detergent ; model plates = block detergent ; lsmeans detergent / pdiff cl adjust = simulate ( acc = .001 report seed = 121211 ); run ; /* Program 9.10: Step-down Comparisons of BIBD Means */ % MakeGLMStats ( dataset = detergent , classvar = block detergent , yvar = plates , model = block detergent , contrasts = all ( detergent ) ); % SimTests ( nsamp = 100000 , seed = 121211 , type = LOGICAL ); /* Program 9.11: Weight Gain in Pigs */ data pigs ; do pen = 1 to 5 ; do feed = 1 to 3 ; do sex = 'M' , 'F' ; input gain initial @@ ; output ; end ; end ; end ; datalines ; 9.52 38 9.94 48 8.51 39 10.00 48 9.11 48 9.75 48 8.21 35 9.48 32 9.95 38 9.24 32 8.50 37 8.66 28 9.32 41 9.32 35 8.43 46 9.34 41 8.90 42 7.63 33 10.56 48 10.90 46 8.86 40 9.68 46 9.51 42 10.37 50 10.42 43 8.82 32 9.20 40 9.67 37 8.76 40 8.57 30 ; /* Program 9.12: Simultaneous Intervals in a Three-Factor ANCOVA with Interaction */ % MakeGLMStats ( dataset = pigs , classvar = pen feed sex , yvar = gain , model = pen feed sex feed * sex initial ); %macro Contrasts ; Cfeed = { 0 0 0 0 0 0 2 - 2 0 0 0 1 1 - 1 - 1 0 0 0 , 0 0 0 0 0 0 2 0 - 2 0 0 1 1 0 0 - 1 - 1 0 , 0 0 0 0 0 0 0 2 - 2 0 0 0 0 1 1 - 1 - 1 0 }; Cfeed = Cfeed / 2 ; Csex = { 0 0 0 0 0 0 0 0 0 3 - 3 1 - 1 1 - 1 1 - 1 0 }; Csex = Csex / 3 ; Cinit = { 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 }; C = Cfeed //Csex//Cinit; C = C ` ; Clab = { \"Feed1-Feed2\" , \"Feed1-Feed3\" , \"Feed2-Feed3\" , \"F-M\" , \"Init\" }; %mend ; % SimIntervals ( nsamp = 50000 , seed = 121211 ); /* Program 9.13: Simultaneous Tests in a Three-Factor ANCOVA with Interaction */ % SimTests ( nsamp = 50000 , seed = 121211 , type = LOGICAL ); /* Program 9.14: Frequency Tabulation of Respiratory Health Data */ data respiratory ; input T $ Age R0 - R4 @@ ; Score = ( R1 + 2 * R2 + 3 * R3 + 4 * R4 ) / 10 ; if ( T = 'A' ) then Treatment = ' Active ' ; else Treatment = ' Placebo ' ; drop T ; if ( Age > 30 ) then AgeGroup = ' Older ' ; else AgeGroup = ' Younger ' ; if ( R0 > 2 ) then InitHealth = ' Good ' ; else InitHealth = ' Poor ' ; datalines ; A 32 1 2 2 4 2 A 47 2 2 3 4 4 A 11 4 4 4 4 2 A 14 2 3 3 3 2 A 15 0 2 3 3 3 A 20 3 3 2 3 1 A 22 1 2 2 2 3 A 22 2 1 3 4 4 A 23 3 3 4 4 3 A 23 2 3 4 4 4 A 25 2 3 3 2 3 A 26 1 2 2 3 2 A 26 2 2 2 2 2 A 26 2 4 1 4 2 A 28 1 2 2 1 2 A 28 0 0 1 2 1 A 30 3 3 4 4 2 A 30 3 4 4 4 3 A 31 1 2 3 1 1 A 31 3 3 4 4 4 A 31 0 2 3 2 1 A 32 3 4 4 3 3 A 34 1 1 2 1 1 A 46 4 3 4 3 4 A 48 2 3 2 0 2 A 50 2 2 2 2 2 A 57 3 3 4 3 4 P 13 4 4 4 4 4 P 31 2 1 0 2 2 P 35 1 0 0 0 0 P 36 2 3 3 2 2 P 45 2 2 2 2 1 P 13 3 4 4 4 4 P 14 2 2 1 2 3 P 15 2 2 3 3 2 P 19 2 3 3 0 0 P 20 4 4 4 4 4 P 23 3 3 1 1 1 P 23 4 4 2 4 4 P 24 3 4 4 4 3 P 25 1 1 2 2 2 P 26 2 4 2 4 3 P 26 1 2 1 2 2 P 27 1 2 2 1 2 P 27 3 3 4 3 3 P 23 2 1 1 1 1 P 28 2 0 0 0 0 P 30 1 0 0 0 0 P 37 1 0 0 0 0 P 37 3 2 3 3 2 P 43 2 3 2 4 4 P 43 1 1 1 3 2 P 44 3 4 3 4 2 P 46 2 2 2 2 2 P 49 2 2 2 2 2 P 63 2 2 2 2 2 A 37 1 3 4 4 4 A 39 2 3 4 4 4 A 60 4 4 3 3 4 A 63 4 4 4 4 4 A 13 4 4 4 4 4 A 14 1 4 4 4 4 A 19 3 3 2 3 3 A 20 2 4 4 4 3 A 20 2 1 1 0 0 A 21 3 3 4 4 4 A 24 4 4 4 4 4 A 25 3 4 3 3 1 A 25 3 4 4 3 3 A 25 2 2 4 4 4 A 26 2 3 4 4 4 A 28 2 3 2 2 1 A 31 4 4 4 4 4 A 34 2 4 4 2 4 A 35 4 4 4 4 4 A 37 4 3 2 2 4 A 41 3 4 4 3 4 A 43 3 3 4 4 2 A 52 1 2 1 2 2 A 55 4 4 4 4 4 A 55 2 2 3 3 1 A 58 4 4 4 4 4 A 68 2 3 3 3 4 P 31 3 4 4 4 4 P 32 3 2 2 3 4 P 36 3 3 2 1 3 P 38 1 2 0 0 0 P 39 1 2 1 1 2 P 39 3 2 3 0 0 P 44 3 4 4 4 4 P 47 2 3 3 2 3 P 48 2 2 1 0 0 P 48 2 2 2 2 2 P 51 3 4 2 4 4 P 58 1 4 2 2 0 P 11 3 4 4 4 4 P 14 2 1 2 3 2 P 15 3 2 2 3 3 P 15 4 3 3 3 4 P 19 4 2 2 3 3 P 20 3 2 4 4 4 P 20 1 4 4 4 4 P 33 3 3 3 2 3 P 36 2 4 3 3 4 P 38 4 3 0 0 0 P 42 3 2 2 2 2 P 43 2 1 0 0 0 P 45 3 4 2 1 2 P 48 4 4 0 0 0 P 52 2 3 4 3 4 P 66 3 3 3 4 4 ; proc freq ; tables Treatment * AgeGroup * InitHealth / nocum list ; run ; /* Program 9.15: Multiple Tests for Treatment Efficacy in Subgroups */ % MakeGLMStats ( dataset = respiratory , classvar = Treatment AgeGroup InitHealth , yvar = Score , model = Treatment * AgeGroup * InitHealth ); %macro Contrasts ; CA = { 0 13 13 11 17 0 0 0 0 }; CA = CA / sum ( CA ); CP = { 0 0 0 0 0 14 19 12 12 }; CP = CP / sum ( CP ); C1 = CA - CP ; CAO = { 0 13 13 0 0 0 0 0 0 }; CAO = CAO / sum ( CAO ); CPO = { 0 0 0 0 0 14 19 0 0 }; CPO = CPO / sum ( CPO ); C2 = CAO - CPO ; CAY = { 0 0 0 11 17 0 0 0 0 }; CAY = CAY / sum ( CAY ); CPY = { 0 0 0 0 0 0 0 12 12 }; CPY = CPY / sum ( CPY ); C3 = CAY - CPY ; CAG = { 0 13 0 11 0 0 0 0 0 }; CAG = CAG / sum ( CAG ); CPG = { 0 0 0 0 0 14 0 12 0 }; CPG = CPG / sum ( CPG ); C4 = CAG - CPG ; CAP = { 0 0 13 0 17 0 0 0 0 }; CAP = CAP / sum ( CAP ); CPP = { 0 0 0 0 0 0 19 0 12 }; CPP = CPP / sum ( CPP ); C5 = CAP - CPP ; C6 = { 0 1 0 0 0 - 1 0 0 0 }; C7 = { 0 0 1 0 0 0 - 1 0 0 }; C8 = { 0 0 0 1 0 0 0 - 1 0 }; C9 = { 0 0 0 0 1 0 0 0 - 1 }; C = C1 //C2//C3//C4//C5//C6//C7//C8//C9; C = C ` ; Clab = { \"Overall\" , \"Older\" , \"Younger\" , \"Good\" , \"Poor\" , \"OldGood\" , \"OldPoor\" , \"YoungGood\" , \"YoungPoor\" }; %mend ; % SimTests ( nsamp = 50000 , seed = 121211 , type = LOGICAL , side = U ); /* Program 9.16: Multiple F tests in a Multifactorial ANOVA */ data wine ; input Purchase CustomerType light music handle examine @@ ; datalines ; 0 4 0 0 0 0 0 4 0 0 0 0 0 3 0 0 0 0 0 3 0 0 0 16 0 3 0 0 1 1 0 4 0 0 0 1 1 3 0 0 3 11 0 4 0 0 0 0 0 3 0 0 0 0 0 3 0 0 2 8 0 4 0 0 0 0 0 3 0 0 0 2 0 3 0 0 0 2 0 3 0 0 0 0 0 3 0 0 0 0 0 . 0 1 0 3 0 3 0 1 0 0 0 3 0 1 5 8 0 3 0 1 0 1 0 4 0 1 0 2 0 3 0 1 0 0 0 . 0 1 0 0 0 4 0 1 0 0 0 4 0 1 1 3 0 4 0 1 0 1 1 3 0 1 2 2 0 3 0 1 1 2 0 3 0 1 0 2 0 1 0 1 0 0 0 4 0 1 2 2 0 3 1 1 1 2 0 4 1 1 0 0 3 1 1 1 9 11 0 3 1 1 0 0 0 3 1 1 0 3 1 4 1 1 0 0 0 3 1 1 5 8 0 . 1 1 1 1 0 4 1 1 3 7 0 3 1 1 0 0 0 3 1 1 0 0 0 2 1 0 0 2 0 3 1 0 0 0 0 1 1 0 0 4 0 4 1 0 0 0 0 4 1 0 12 21 0 3 1 0 1 1 0 3 0 0 0 1 0 3 0 0 1 0 0 3 0 0 1 1 0 3 0 0 3 8 0 3 0 0 0 8 0 1 0 0 1 1 0 4 0 0 0 0 0 3 0 0 0 4 1 4 0 0 1 2 0 3 0 0 1 4 1 3 0 0 1 6 0 4 0 1 0 0 0 2 0 1 0 0 0 4 0 1 0 2 0 3 0 1 0 0 0 4 0 1 0 2 0 3 0 1 2 4 0 3 0 1 0 0 0 3 0 1 0 0 0 4 0 1 0 2 0 3 0 1 0 0 1 1 0 1 1 2 0 3 0 1 0 2 0 3 0 1 0 0 0 3 0 1 1 1 0 4 0 1 0 8 0 4 0 1 0 0 0 4 1 0 0 7 0 3 1 0 0 2 0 4 1 0 1 4 0 4 1 0 2 5 0 4 1 0 0 2 0 3 1 0 1 2 0 4 1 0 0 2 0 3 1 0 2 6 0 4 1 0 1 2 0 3 1 0 0 0 0 3 0 1 0 4 0 4 0 1 0 3 0 4 0 1 1 7 0 3 0 1 0 1 0 2 0 1 4 7 0 3 0 1 2 2 0 . 0 1 0 0 0 3 0 1 1 7 0 4 0 1 1 11 0 4 0 1 3 13 0 1 1 0 0 2 0 3 1 0 1 6 0 3 1 0 3 4 0 3 1 0 5 3 0 3 1 0 2 10 0 4 1 0 6 7 0 3 1 0 4 3 0 3 0 0 3 12 0 4 0 0 0 1 1 3 0 0 0 2 0 3 0 0 3 5 0 4 0 0 3 3 2 3 0 0 1 3 0 3 0 0 4 6 0 3 0 0 0 1 0 1 0 0 1 4 1 3 0 0 0 5 0 3 0 0 3 8 0 3 1 1 0 0 0 3 1 1 0 3 0 3 1 1 1 7 0 3 1 1 0 14 0 3 1 1 0 14 0 . 1 1 0 6 6 3 1 1 6 6 0 4 1 1 3 3 0 4 1 1 1 3 0 3 1 1 0 3 0 3 1 1 0 2 1 4 0 0 0 1 0 3 0 0 0 0 0 3 0 0 0 11 0 4 0 0 0 1 0 4 0 0 0 4 0 4 1 1 1 4 0 1 1 1 0 4 0 3 1 1 0 0 1 3 1 1 0 0 1 3 1 1 1 15 0 3 1 1 0 7 0 4 1 1 1 3 0 4 1 1 0 5 0 3 1 1 0 4 0 4 1 1 0 2 0 4 1 1 5 12 0 4 1 1 2 8 0 4 1 1 1 1 0 3 0 1 0 3 0 3 0 1 0 1 1 1 0 1 3 5 0 1 0 1 1 10 0 4 0 1 0 3 0 4 0 1 0 3 0 3 0 1 0 0 0 4 0 1 0 6 0 3 0 1 0 5 0 3 0 1 0 0 0 4 1 0 0 3 0 3 1 0 1 5 0 3 1 0 1 1 2 3 1 0 13 6 0 3 1 0 0 2 0 3 1 0 0 2 0 3 1 0 0 4 0 3 1 0 0 4 1 3 1 0 0 2 0 3 1 0 4 8 0 4 1 0 2 6 0 4 1 1 0 5 0 4 1 1 0 1 0 3 1 1 1 11 0 4 1 1 13 9 0 3 1 1 0 2 0 4 1 1 0 3 0 4 1 1 0 2 ; proc glm data = wine ; class CustomerType light music ; model purchase = CustomerType light music CustomerType * light CustomerType * music light * music CustomerType * light * music handle examine / SS3 ; run ; /* Program 10.1: Multiple Comparisons with Nonconstant Variance */ data wloss ; do diet = 'A' , 'B' , 'C' , 'D' , 'E' ; do i = 1 to 10 ; input wloss @@ ; output ; end ; end ; cards ; 12.4 10.7 11.9 11.0 12.4 12.3 13.0 12.5 11.2 13.1 9.1 11.5 11.3 9.7 13.2 10.7 10.6 11.3 11.1 11.7 8.5 11.6 10.2 10.9 9.0 9.6 9.9 11.3 10.5 11.2 8.7 9.3 8.2 8.3 9.0 9.4 9.2 12.2 8.5 9.9 12.7 13.2 11.8 11.9 12.2 11.2 13.7 11.8 11.5 11.7 ; proc mixed data = wloss ; class diet ; model wloss = diet / ddfm = satterth ; repeated / group = diet ; lsmeans diet / adjust = simulate ( seed = 121211 ) cl ; ods select tests3 ; ods output diffs = diffs ; run ; proc print data = diffs noobs ; title \"Multiple Heteroscedastic Comparisons\" ; var diet _diet Estimate StdErr DF tValue Probt Adjp AdjLow AdjUpp ; run ; /* Program 10.2: Logically Constrained Step-Down Tests in Heteroscedastic ANOVA */ % MakeGLMStats ( dataset = wloss , classvar = diet , yvar = wloss , model = diet , contrasts = all ( diet )); ods output Tests3 = Tests3 SolutionF = SolutionF CovB = CovB ; proc mixed data = wloss ; class diet ; model wloss = diet / ddfm = satterth solution covb ; repeated / group = diet ; run ; %macro Estimates ; use Tests3 ; read all var { DenDf } into df ; use CovB ; read all var ( \"Col1\" : \"Col6\" ) into cov ; use SolutionF ; read all var { Estimate } into EstPar ; %mend ; % SimTests ( seed = 121211 , type = LOGICAL ); /* Program 10.3: Logically Constrained Step-Down Tests in Homoscedastic ANOVA */ % MakeGLMStats ( dataset = wloss , classvar = diet , yvar = wloss , model = diet , contrasts = all ( diet )); % SimTests ( seed = 121211 , type = LOGICAL ); /* Program 10.4: Multiple Comparisons in the Random Block RCBD */ data waste ; do temp = 1 to 3 ; do envir = 1 to 5 ; do rep = 1 to 2 ; input waste @@ ; output ; end ; end ; end ; datalines ; 7.09 5.90 7.94 9.15 9.23 9.85 5.43 7.73 9.43 6.90 7.01 5.82 6.18 7.19 7.86 6.33 8.49 8.67 9.62 9.07 7.78 7.73 10.39 8.78 9.27 8.90 12.17 10.95 13.07 9.76 ; data waste1 ; set waste ; if rep = 1 ; run ; proc mixed data = waste1 ; class envir temp ; model waste = temp / ddfm = satterth ; random envir ; lsmeans temp / cl adjust = tukey ; ods output diffs = diffs ; run ; proc print data = diffs noobs ; title \"Multiple Comparisons in Random Block Model\" ; var temp _temp Estimate StdErr df AdjLow AdjUpp ; run ; /* Program 10.5: Multiple Comparisons with Random Block Levels: Incomplete Blocks */ data detergent ; do detergent = 1 to 5 ; do block = 1 to 10 ; input plates @@ ; output ; end ; end ; datalines ; 27 28 30 31 29 30 . . . . 26 26 29 . . . 30 21 26 . 30 . . 34 32 . 34 31 . 33 . 29 . 33 . 34 31 . 33 31 . . 26 . 24 25 . 23 24 26 ; proc mixed data = detergent ; class block detergent ; model plates = detergent / ddfm = satterth ; random block ; lsmeans detergent / cl adjust = simulate ( seed = 121211 ); ods output diffs = diffs ; run ; proc print data = diffs noobs ; title \"Multiple Comparisons in Random Block Model - Incomplete Blocks\" ; var detergent _detergent Estimate StdErr df AdjLow AdjUpp ; run ; /* Program 10.6: Multiple Comparisons in Random Block Model with Interaction */ proc mixed data = waste ; class envir temp ; model waste = temp / ddfm = satterth ; random envir envir * temp ; lsmeans temp / cl adjust = tukey ; ods output diffs = diffs ; run ; proc print data = diffs noobs ; title \"Multiple Comparisons in Random Block Model with Interaction\" ; var temp _temp Estimate StdErr df AdjLow AdjUpp ; run ; /* Program 10.7: Comparison of Repeated Measures Means */ data Halothane ; do Dog = 1 to 19 ; do Treatment = ' HA ',' LA ',' HP ',' LP ' ; input Rate @@ ; output ; end ; end ; datalines ; 426 609 556 600 253 236 392 395 359 433 349 357 432 431 522 600 405 426 513 513 324 438 507 539 310 312 410 456 326 326 350 504 375 447 547 548 286 286 403 422 349 382 473 497 429 410 488 547 348 377 447 514 412 473 472 446 347 326 455 468 434 458 637 524 364 367 432 469 420 395 508 531 397 556 645 625 ; proc mixed data = Halothane ; class Dog Treatment ; model Rate = Treatment / ddfm = satterth ; repeated / type = un subject = Dog ; lsmeans Treatment / adjust = simulate ( nsamp = 200000 seed = 121211 ) cl pdiff ; ods output Diffs = Diffs ; run ; proc print data = Diffs noobs ; title \"Multiple Comparisons in Repeated Measures Model\" ; var Treatment _Treatment Estimate StdErr df AdjLow AdjUpp ; run ; /* Program 10.8: Logically Constrained Step-Down Tests with Repeated Measures Data */ proc mixed data = Halothane ; class Dog Treatment ; model Rate = Treatment / ddfm = satterth ; repeated / type = un subject = Dog ; lsmeans Treatment / cov ; ods output LSmeans = LSmeans ; ods output Tests3 = Tests3 ; run ; %macro Contrasts ; C = { 1 - 1 0 0 , 1 0 - 1 0 , 1 0 0 - 1 , 0 1 - 1 0 , 0 1 0 - 1 , 0 0 1 - 1 , - .5 - .5 .5 .5 , .5 - .5 .5 - .5 , 1 - 1 - 1 1 }; C = C ` ; Clab = { \"HA-HP\" , \"HA-LA\" , \"HA-LP\" , \"HP-LA\" , \"HP-LP\" , \"LA-LP\" , \"Halo\" , \"CO2\" , \"Interaction\" }; /* Contrast labels */ %mend ; %macro Estimates ; use tests3 ; read all var { DenDf } into df ; use lsmeans ; read all var { Cov1 Cov2 Cov3 Cov4 } into cov ; read all var { Estimate } into EstPar ; %mend ; % SimTests ( nsamp = 40000 , seed = 121211 , type = LOGICAL ); /* Program 10.9: Multiple Comparisons of Multiple Outcomes in the MANOVA Framework */ data MultipleEndpoints ; Treatment = ' Placebo ' ; do Subject = 1 to 54 ; do Endpoint = 1 to 4 ; input y @@ ; output ; end ; end ; Treatment = ' Drug ' ; do Subject = 54 + 1 to 54 + 57 ; do Endpoint = 1 to 4 ; input y @@ ; output ; end ; end ; datalines ; 4 3 3 5 5 0 1 7 1 0 1 9 4 0 3 5 3 0 2 9 4 1 2 6 2 0 4 6 2 2 5 5 3 0 1 7 2 0 1 9 4 6 5 5 2 0 2 8 2 7 1 7 1 2 2 9 4 0 3 7 3 0 1 6 3 0 1 6 4 1 4 6 6 0 4 7 3 0 1 8 3 0 1 9 2 1 2 7 6 2 3 5 3 0 4 7 3 0 1 9 2 0 1 9 6 9 6 3 4 9 2 6 2 0 1 7 1 0 1 9 4 0 4 7 3 1 4 6 3 0 3 7 1 0 1 8 6 7 5 4 4 6 2 5 6 19 7 5 6 3 6 6 3 0 5 6 2 4 2 8 1 0 1 8 4 21 5 5 2 0 2 9 4 7 3 5 3 1 2 8 3 3 3 8 4 3 4 6 1 0 1 10 1 0 2 9 3 0 4 5 3 1 1 6 3 4 4 6 5 8 5 5 5 1 5 4 1 0 4 8 1 0 1 10 1 0 1 9 2 1 2 7 4 1 2 5 5 0 5 6 1 4 5 6 5 6 4 6 2 0 2 9 2 2 2 5 1 0 1 10 3 2 3 6 5 4 6 6 2 1 2 8 2 1 2 6 2 1 1 8 3 0 3 9 3 1 2 6 1 0 2 9 1 0 1 9 3 0 3 9 1 0 1 10 1 0 1 9 1 0 1 10 2 0 4 7 5 1 2 6 4 0 5 7 4 0 4 6 2 1 3 6 2 1 1 6 4 0 4 6 1 0 1 8 1 0 2 9 4 1 3 6 4 3 4 5 4 2 5 5 1 0 1 10 3 0 2 8 4 2 2 8 3 0 2 9 1 0 1 10 1 0 1 9 2 0 2 9 2 1 2 8 3 0 3 8 2 4 2 6 2 1 1 9 2 2 2 9 4 0 1 4 3 3 1 8 4 4 3 6 2 0 1 10 4 2 3 6 1 0 1 8 2 0 2 8 5 1 5 5 4 0 4 6 ; proc mixed data = MultipleEndpoints ; title \"Two-Sample Multivariate Mean Comparisons\" ; class Endpoint Treatment Subject ; model y = Treatment * Endpoint / ddfm = satterth ; repeated / type = un subject = Subject ; lsmeans Treatment * Endpoint / cov ; contrast ' F test ' Treatment * Endpoint 1 - 1 0 0 0 0 0 0 , Treatment * Endpoint 0 0 1 - 1 0 0 0 0 , Treatment * Endpoint 0 0 0 0 1 - 1 0 0 , Treatment * Endpoint 0 0 0 0 0 0 1 - 1 ; ods output LSmeans = LSmeans ; ods output Contrasts = Contrasts ; run ; %macro Contrasts ; C = { 1 - 1 0 0 0 0 0 0 , 0 0 1 - 1 0 0 0 0 , 0 0 0 0 1 - 1 0 0 , 0 0 0 0 0 0 1 - 1 }; C = C ` ; Clab = { \"Endpoint 1\" , \"Endpoint 2\" , \"Endpoint 3\" , \"Endpoint 4\" }; %mend ; %macro Estimates ; use Contrasts ; read all var { DenDf } into df ; use LSmeans ; read all var { Cov1 Cov2 Cov3 Cov4 Cov5 Cov6 Cov7 Cov8 } into cov ; read all var { Estimate } into EstPar ; %mend ; % SimTests ( seed = 121211 , nsamp = 100000 ); /* Program 10.10: Multiple Comparisons of Means in MANCOVA */ data Obesity ; input Group $ Creatinine Chloride Volume @@ ; Subject = _n_ ; datalines ; LU 17.6 5.15 205 LU 13.4 5.75 160 LU 20.3 4.35 480 LU 22.3 7.55 230 LU 20.5 8.50 235 LU 18.5 10.25 215 LU 12.1 5.95 215 LU 12.0 6.30 190 LU 10.1 5.45 190 LU 14.7 3.75 175 LU 14.8 5.10 145 LU 14.4 4.05 155 HU 18.1 9.00 220 HU 19.7 5.30 300 HU 16.9 9.85 305 HU 23.7 3.60 275 HU 19.2 4.05 405 HU 18.0 4.40 210 HU 14.8 7.15 170 HU 15.6 7.25 235 HU 16.2 5.30 185 HU 14.1 3.10 255 HU 17.5 2.40 265 HU 14.1 4.25 305 HU 19.1 5.80 440 HU 22.5 1.55 430 LO 17.0 4.55 350 LO 12.5 2.65 475 LO 21.5 6.50 195 LO 22.2 4.85 375 LO 13.0 8.75 160 LO 13.0 5.20 240 LO 10.9 4.75 205 LO 12.0 5.85 270 LO 22.8 2.85 475 LO 16.5 6.55 430 LO 18.4 6.60 490 HO 12.5 2.90 105 HO 8.7 3.00 115 HO 9.4 3.40 97 HO 15.0 5.40 325 HO 12.9 4.45 310 HO 12.1 4.30 245 HO 13.2 5.00 170 HO 11.5 3.40 220 ; data ObesityU ; /* Change multivariate data format to MIXED data format */ set Obesity ; Compound = ' Creatinine ' ; Amount = Creatinine ; output ; Compound = ' Chloride ' ; Amount = Chloride ; output ; keep Subject Group Compound Amount Volume ; run ; proc mixed data = ObesityU order = data ; class Group Compound Subject ; model Amount = Group * Compound Volume * Compound / ddfm = satterth s ; repeated / type = un subject = Subject ; lsmeans Group * Compound / cov ; contrast ' F test ' Group * Compound 1 0 - 1 0 0 0 0 0 , Group * Compound 1 0 0 0 - 1 0 0 0 , Group * Compound 1 0 0 0 0 0 - 1 0 , Group * Compound 0 1 0 - 1 0 0 0 0 , Group * Compound 0 1 0 0 0 - 1 0 0 , Group * Compound 0 1 0 0 0 0 0 - 1 ; ods output LSmeans = LSmeans ; ods output Contrasts = Contrasts ; run ; %macro Contrasts ; C = { 1 0 - 1 0 0 0 0 0 , 1 0 0 0 - 1 0 0 0 , 1 0 0 0 0 0 - 1 0 , 0 0 1 0 - 1 0 0 0 , 0 0 1 0 0 0 - 1 0 , 0 0 0 0 1 0 - 1 0 , 0 1 0 - 1 0 0 0 0 , 0 1 0 0 0 - 1 0 0 , 0 1 0 0 0 0 0 - 1 , 0 0 0 1 0 - 1 0 0 , 0 0 0 1 0 0 0 - 1 , 0 0 0 0 0 1 0 - 1 }; C = C ` ; Clab = { \"Creatine,LU-HU\" , \"Creatine,LU-LO\" , \"Creatine,LU-HO\" , \"Creatine,HU-LO\" , \"Creatine,HU-HO\" , \"Creatine,LO-HO\" , \"Chloride,LU-HU\" , \"Chloride,LU-LO\" , \"Chloride,LU-HO\" , \"Chloride,HU-LO\" , \"Chloride,HU-HO\" , \"Chloride,LO-HO\" }; %mend ; %macro Estimates ; use Contrasts ; read all var { DenDf } into df ; use LSmeans ; read all var { Cov1 Cov2 Cov3 Cov4 Cov5 Cov6 Cov7 Cov8 } into cov ; read all var { Estimate } into EstPar ; %mend ; % SimIntervals ( seed = 121211 , nsamp = 50000 ); /* Program 10.11: Global Tests of 'Sliced' Effects */ data heart ; do drug = ' ax23 ' , ' bww9 ' , ' ctrl ' ; do person = 1 to 8 ; do time = ' t1 ' , ' t2 ' , ' t3 ' , ' t4 ' ; input hr @@ ; output ; end ; end ; end ; datalines ; 72 86 81 77 78 83 88 81 71 82 81 75 72 83 83 69 66 79 77 66 74 83 84 77 62 73 78 70 69 75 76 70 85 86 83 80 82 86 80 84 71 78 70 75 83 88 79 81 86 85 76 76 85 82 83 80 79 83 80 81 83 84 78 81 69 73 72 74 66 62 67 73 84 90 88 87 80 81 77 72 72 72 69 70 65 62 65 61 75 69 69 68 71 70 65 63 ; proc mixed data = heart ; class time drug person ; model hr = time * drug / ddfm = satterth ; repeated time / type = un subject = person ( drug ); lsmeans time * drug / slice = time ; run ; /* Program 10.12: Multiple Comparisons of 'Sliced' Effects */ proc mixed data = heart ; class time drug person ; model hr = time * drug / ddfm = satterth ; repeated / type = un subject = person ( drug ); lsmeans time * drug / cov ; contrast ' F test ' time * drug 1 - 1 0 0 0 0 0 0 0 0 0 0 , time * drug 1 0 - 1 0 0 0 0 0 0 0 0 0 , time * drug 0 0 0 1 - 1 0 0 0 0 0 0 0 , time * drug 0 0 0 1 0 - 1 0 0 0 0 0 0 , time * drug 0 0 0 0 0 0 1 - 1 0 0 0 0 , time * drug 0 0 0 0 0 0 1 0 - 1 0 0 0 , time * drug 0 0 0 0 0 0 0 0 0 1 - 1 0 , time * drug 0 0 0 0 0 0 0 0 0 1 0 - 1 ; ods output LSmeans = LSmeans ; ods output Contrasts = Contrasts ; run ; %macro Contrasts ; C = { 1 - 1 0 0 0 0 0 0 0 0 0 0 , 1 0 - 1 0 0 0 0 0 0 0 0 0 , 0 1 - 1 0 0 0 0 0 0 0 0 0 , 0 0 0 1 - 1 0 0 0 0 0 0 0 , 0 0 0 1 0 - 1 0 0 0 0 0 0 , 0 0 0 0 1 - 1 0 0 0 0 0 0 , 0 0 0 0 0 0 1 - 1 0 0 0 0 , 0 0 0 0 0 0 1 0 - 1 0 0 0 , 0 0 0 0 0 0 0 1 - 1 0 0 0 , 0 0 0 0 0 0 0 0 0 1 - 1 0 , 0 0 0 0 0 0 0 0 0 1 0 - 1 , 0 0 0 0 0 0 0 0 0 0 1 - 1 }; C = C ` ; Clab = { \"Time1, a-b\" , \"Time1, a-c\" , \"Time1, b-c\" , \"Time2, a-b\" , \"Time2, a-c\" , \"Time2, b-c\" , \"Time3, a-b\" , \"Time3, a-c\" , \"Time3, b-c\" , \"Time4, a-b\" , \"Time4, a-c\" , \"Time4, b-c\" }; %mend ; %macro Estimates ; use Contrasts ; read all var { DenDf } into df ; use LSmeans ; read all var { Cov1 Cov2 Cov3 Cov4 Cov5 Cov6 Cov7 Cov8 Cov9 Cov10 Cov11 Cov12 } into cov ; read all var { Estimate } into EstPar ; %mend ; % SimTests ( seed = 121211 , nsamp = 50000 ); /* Program 11.1: Bootstrap Multiple Comparisons of Means in the ANOVA */ proc multtest data = wloss bootstrap seed = 121211 n = 50000 ; class diet ; test mean ( wloss ); contrast \"A-B\" 1 - 1 0 0 0 ; contrast \"A-C\" 1 0 - 1 0 0 ; contrast \"A-D\" 1 0 0 - 1 0 ; contrast \"A-E\" 1 0 0 0 - 1 ; contrast \"B-C\" 0 1 - 1 0 0 ; contrast \"B-D\" 0 1 0 - 1 0 ; contrast \"B-E\" 0 1 0 0 - 1 ; contrast \"C-D\" 0 0 1 - 1 0 ; contrast \"C-E\" 0 0 1 0 - 1 ; contrast \"D-E\" 0 0 0 1 - 1 ; ods select continuous pValues ; run ; /* Program 11.2: Bootstrap Two-Sample t-Test Using PROC MULTTEST */ data wlossBE ; set wloss ; if diet = \"B\" or diet = \"E\" ; run ; proc multtest data = wlossBE bootstrap n = 100000 ; class diet ; test ( mean ); contrast \"B-E\" 1 - 1 ; run ; /* Program 11.3: Two-Sample Permutation Test and Rank Test Using PROC MULTTEST */ proc multtest data = wlossBE permutation ; title \"Permutation Test using Raw Data\" ; class diet ; test mean ( wloss ); contrast \"B-E\" 1 - 1 ; ods select continuous pValues ; run ; proc rank data = wlossBE ; var wloss ; ranks wlossranks ; run ; proc multtest data = wlossBE permutation ; title \"Permutation Test using Ranks\" ; class diet ; test mean ( wlossranks ); contrast \"B-E\" 1 - 1 ; ods select continuous pValues ; run ; /* Program 11.4: Pairwise Comparisons using the Global Permutation Distribution */ proc multtest data = wloss permutation seed = 121211 n = 50000 ; class diet ; test mean ( wloss ); contrast \"A-B\" 1 - 1 0 0 0 ; contrast \"A-C\" 1 0 - 1 0 0 ; contrast \"A-D\" 1 0 0 - 1 0 ; contrast \"A-E\" 1 0 0 0 - 1 ; contrast \"B-C\" 0 1 - 1 0 0 ; contrast \"B-D\" 0 1 0 - 1 0 ; contrast \"B-E\" 0 1 0 0 - 1 ; contrast \"C-D\" 0 0 1 - 1 0 ; contrast \"C-E\" 0 0 1 0 - 1 ; contrast \"D-E\" 0 0 0 1 - 1 ; ods select pValues ; run ; /* Program 11.5: Testing Multiple Endpoints in Clinical Trials Using PROC MULTTEST */ proc multtest data = multiple_endp_mv stepboot n = 100000 seed = 121211 ; class tx ; test mean ( y1 - y4 ); contrast \"t vs c\" - 1 1 ; run ; /* Program 11.6: Inferences over Multiple Variables and Subgroup Contrasts} */ data respiratory1 ; set respiratory ; treat_x_age = compress ( Treatment || '_' || AgeGroup , ' ' ); score_control = score - r0 ; run ; proc sort data = respiratory1 out = respiratory2 ; by treat_x_age ; run ; proc multtest data = respiratory2 stepboot n = 100000 seed = 121211 ; class treat_x_age ; test mean ( score score_control / upper ); contrast \"Active-Placebo\" 1 1 - 1 - 1 ; contrast \"Active-Placebo, Old\" 1 0 - 1 0 ; contrast \"Active-Placebo, Young\" 0 1 0 - 1 ; run ; /* Program 12.1: Multiple Two-Sample Binary Data Tests */ data Adverse ; keep Group AE1 - AE28 ; array AE { 28 }; length Group $ 9 ; input Group nTotal nNone ; do i = 1 to dim ( AE ); AE { i } = 0 ; end ; do iobs = 1 to nNone ; output ; end ; do iobs = 1 to nTotal - nNone ; input nAE @@ ; do i = 1 to dim ( AE ); AE { i } = 0 ; end ; do i = 1 to nAE ; input iAE @@ ; AE { iAE } = 1 ; end ; output ; end ; datalines ; Control 80 46 4 2 3 17 28 2 18 28 2 2 28 3 4 22 28 3 1 3 28 2 1 28 4 2 3 11 28 2 2 28 3 12 27 28 2 1 28 3 2 19 28 3 1 9 28 2 14 28 2 7 28 2 4 28 2 4 28 2 2 28 2 3 28 4 1 4 9 28 3 1 26 28 2 1 28 3 5 12 28 2 2 28 2 4 28 3 5 13 28 2 16 28 2 9 28 3 1 2 28 2 24 28 2 2 28 2 7 28 2 7 28 2 25 28 5 3 14 19 21 28 Treatment 80 44 2 23 28 2 1 28 3 1 4 28 2 2 28 2 1 28 4 1 3 6 28 4 1 5 8 28 3 1 21 28 3 1 10 28 3 3 8 28 5 1 2 3 10 28 3 2 15 28 2 1 28 3 2 6 28 4 1 5 9 28 3 1 5 28 3 1 15 28 2 7 28 2 7 28 3 1 8 28 3 1 6 28 3 1 3 28 3 1 6 28 3 2 8 28 3 1 4 28 3 1 2 28 3 1 20 28 3 1 4 28 3 1 2 28 2 1 28 4 1 5 16 28 3 2 8 28 2 1 28 4 1 4 5 28 2 3 28 2 3 28 ; proc multtest data = Adverse stepperm seed = 121211 ; class Group ; test fisher ( AE1 - AE28 / upper ); contrast \"Treatment-Control\" - 1 1 ; ods output Discrete = Discrete ; ods output pValues = pValues ; run ; proc sort data = Discrete out = Discrete ; by Variable ; proc sort data = pValues out = pValues ; by Variable ; data both ; merge Discrete pValues ; by Variable ; run ; proc sort data = both out = both ; by Raw ; data best5 ; set both ; if _n_ <= 10 ; run ; proc print noobs data = best5 ; title \"Counts and Percentages for the Most Significant AEs\" ; var Variable Group count NumObs Percent ; run ; proc print noobs data = best5 ( where = ( Group = ' Control ' ) rename = ( Raw = RawPValue StepdownPermutation = AdjustedPValue )); title \"Fisher Exact (Raw) and Multivariate Permutation-Adjusted p-Values\" ; var Variable RawPValue AdjustedPValue ; run ; /* Program 12.2: Multiple Cochran-Armitage Permutation Tests, with Permutation Distribution Output */ proc multtest data = Adverse outperm = permdists ; class Group ; test ca ( AE1 - AE28 / upper permutation = 100 ); contrast \"Treatment-Control\" - 1 1 ; ods output pValues = pValues ; run ; proc sort data = pValues out = pValues ; by Raw ; proc print noobs data = pValues ( obs = 5 rename = ( Raw = ExactCAPValue )); title \"Exact Permutation pValues for CA Tests\" ; run ; proc print noobs data = permdists ; where ( _var_ = \"AE6\" ); title \"Permutation Distribution of CA test for AE6\" ; run ; /* Program 12.3: Discrete Bonferroni-Based Multiple Tests Using Binary Data */ proc multtest data = Adverse stepbon ; class Group ; test ca ( AE1 - AE28 / upper permutation = 100 ); contrast \"Treatment-Control\" - 1 1 ; ods output pValues = pValues ; run ; proc sort data = pValues out = pValues ; by Raw ; proc print data = pValues ( obs = 5 rename = ( Raw = ExactCAPValue )); title \"Exact Raw pValues and Discrete Bonferroni Adjustments\" ; var variable ExactCAPValue StepdownBonferroni ; run ; /* Program 12.4: All Binary Pairwise Comparisons Using the Global Permutation Distribution: Example 1 */ data rabbits ; input died freq Penicillin $ @@ ; datalines ; 0 0 1 / 8 1 11 1 / 8 0 3 1 / 4 1 9 1 / 4 0 8 1 / 2 1 4 1 / 2 0 11 1 1 1 1 0 7 4 1 0 4 ; proc multtest order = data stepperm ; class Penicillin ; freq freq ; test fisher ( died ); contrast \"1/8 vs 1/4\" - 1 1 0 0 0 ; contrast \"1/8 vs 1/2\" - 1 0 1 0 0 ; contrast \"1/8 vs 1\" - 1 0 0 1 0 ; contrast \"1/8 vs 4\" - 1 0 0 0 1 ; contrast \"1/4 vs 1/2\" 0 - 1 1 0 0 ; contrast \"1/4 vs 1\" 0 - 1 0 1 0 ; contrast \"1/4 vs 4\" 0 - 1 0 0 1 ; contrast \"1/2 vs 1\" 0 0 - 1 1 0 ; contrast \"1/2 vs 4\" 0 0 - 1 0 1 ; contrast \"1 vs 4\" 0 0 0 - 1 1 ; ods select Discrete pValues ; run ; /* Program 12.5: All Binary Pairwise Comparisons Using the Global Permutation Distribution: Example 2 */ data trouble ; input b f g ; datalines ; 0 2000 1 0 1 2 1 3 2 0 3 3 1 1 3 ; proc multtest data = trouble stepperm n = 1000 ; class g ; freq f ; test fisher ( b ); contrast \"1 vs 2\" 1 - 1 0 ; contrast \"1 vs 3\" 1 0 - 1 ; contrast \"2 vs 3\" 0 1 - 1 ; run ; /* Program 12.6: A ``Workaround\" for the Subset Pivotality Problem: Discrete Bonferroni Method for Pairwise Binary Comparisons */ data Test1vs2 ; set trouble ( where = ( g in ( 1 , 2 ))); Test1vs2 = b ; DummyGroup = ( g = 2 ); data Test1vs3 ; set trouble ( where = ( g in ( 1 , 3 ))); Test1vs3 = b ; DummyGroup = ( g = 3 ); data Test2vs3 ; set trouble ( where = ( g in ( 2 , 3 ))); Test2vs3 = b ; DummyGroup = ( g = 3 ); data TroubleNoMore ; set Test1vs2 Test1vs3 Test2vs3 ; run ; proc multtest data = TroubleNoMore stepbon ; class DummyGroup ; freq f ; test ca ( Test1vs2 Test1vs3 Test2vs3 / permutation = 10 ); contrast \"Pairwise Test\" - 1 1 ; run ; /* Program 12.7: Toxicity Data and Incorrect Comparisons Against Control */ data Toxicity ; do Group = ' Hist ',' Curr ',' Low ',' High ' ; do Outcome = 0 to 1 ; input Freq @@ ; output ; end ; end ; datalines ; 326 4 49 1 42 7 44 4 ; proc multtest data = Toxicity order = data stepperm seed = 121211 ; class Group ; freq Freq ; test fisher ( Outcome / lower ); contrast \"Hist vs Curr\" - 1 1 0 0 ; contrast \"Low vs Curr\" 0 1 - 1 0 ; contrast \"High vs Curr\" 0 1 0 - 1 ; run ; /* Program 12.8: Adjunct Program Used for Closed Pairwise Testing */ proc multtest data = Toxicity ( where = ( Group ^= \"High\" )) order = data seed = 121211 permutation ; title \"Hist=Curr=Low\" ; class Group ; freq Freq ; test fisher ( Outcome / lower ); contrast \"Hist vs Curr\" - 1 1 0 ; contrast \"Low vs Curr\" 0 1 - 1 ; ods select pValues ; run ; proc multtest data = Toxicity ( where = ( Group ^= \"Low\" )) order = data seed = 121211 permutation ; title \"Hist=Curr=High\" ; class Group ; freq Freq ; test fisher ( Outcome / lower ); contrast \"Hist vs Curr\" - 1 1 0 ; contrast \"High vs Curr\" 0 1 - 1 ; ods select pValues ; run ; proc multtest data = Toxicity ( where = ( Group ^= \"Hist\" )) order = data seed = 121211 permutation ; title \"Curr=Low=High\" ; class Group ; freq Freq ; test fisher ( outcome / lower ); contrast \"Low vs Curr\" 1 - 1 0 ; contrast \"High vs Curr\" 1 0 - 1 ; ods select pValues ; run ; /* Program 12.9: Closed Testing of Pairwise vs. Control with Binary Data Using Discrete Bonferroni */ data HistVsCurr ; set Toxicity ( where = ( Group in ( ' Hist ',' Curr ' ))); HistVsCurr = Outcome ; DummyGroup = ( Group = ' Hist ' ); data Low_VsCurr ; set Toxicity ( where = ( Group in ( ' Low ' , ' Curr ' ))); Low_VsCurr = Outcome ; DummyGroup = ( Group = ' Low ' ); data HighVsCurr ; set Toxicity ( where = ( Group in ( ' High ',' Curr ' ))); HighVsCurr = Outcome ; DummyGroup = ( Group = ' High ' ); data TrickToxicity ; set HistVsCurr Low_VsCurr HighVsCurr ; run ; proc multtest data = TrickToxicity stepbon ; title \"Complete Null\" ; class DummyGroup ; freq Freq ; test ca ( HistVsCurr Low_VsCurr HighVsCurr / lower permutation = 20 ); contrast \"PairTest\" 1 - 1 ; ods select pValues ; run ; proc multtest data = TrickToxicity stepbon ; title \"HistVsCurrLow\" ; class DummyGroup ; freq Freq ; test ca ( HistVsCurr Low_VsCurr / lower permutation = 20 ); contrast \"PairTest\" 1 - 1 ; ods select pValues ; run ; proc multtest data = TrickToxicity stepbon ; title \"HistVsCurrHigh\" ; class DummyGroup ; freq Freq ; test ca ( HistVsCurr HighVsCurr / lower permutation = 20 ); contrast \"PairTest\" 1 - 1 ; ods select pValues ; run ; proc multtest data = TrickToxicity stepbon ; title \"CurrLowHigh \" ; class DummyGroup ; freq Freq ; test ca ( Low_VsCurr HighVsCurr / lower permutation = 20 ); contrast \"PairTest\" 1 - 1 ; ods select pValues ; run ; /* Program 12.10: Improving the Power of Binary Tests */ data fungal ; input ep1 ep2 ep3 treat1 treat2 ; datalines ; 1 0 0 1 2 0 1 0 1 2 0 0 1 0 2 1 1 0 0 0 1 0 1 1 2 0 1 1 0 1 1 1 1 57 61 0 0 0 10 0 ; data par ; nep = 3 ; nt = 2 ; run ; %romex ( 2 , fungal , par ); /* Program 12.11: Joint Test of Endpoint 1 and Endpoint 3 */ proc sort data = fungal out = fungal ; by ep1 ep3 ; proc summary data = fungal ; by ep1 ep3 ; var treat1 treat2 ; output out = fungal13 sum = treat1 treat2 ; run ; data par ; nep = 2 ; nt = 2 ; run ; %romex ( 2 , fungal13 , par ); /* Program 12.12: Monte Carlo Calculation of P(H0) */ data par ; nep = 3 ; nt = 2 ; data mcn ; n_sample = 1000 ; seed = 1235 ; run ; %rommc ( 2 , fungal , par , mcn ); /* Program 12.13: Analysis of Fungal Data using PROC MULTTEST */ data fungal_mult ; set fungal ; Freq = treat1 ; Treatment = 1 ; if ( Freq ) then output ; Freq = treat2 ; Treatment = 2 ; if ( Freq ) then output ; run ; proc multtest data = fungal_mult stepperm n = 100000 seed = 121211 ; class Treatment ; freq Freq ; test fisher ( ep1 - ep3 / upper ); contrast \"New-Old\" - 1 1 ; ods select pValues ; run ; /* Program 12.14: Comparing Multiple Contrasts for Multiple Binary Variables */ data Doctors ; keep Doctor HDeath MI_EKG RFB Infect Neuro Pulm RenFail ; array AE { 7 } HDeath MI_EKG RFB Infect Neuro Pulm RenFail ; input Doctor nTotal nNone @@ ; do i = 1 to dim ( AE ); AE { i } = 0 ; end ; do iobs = 1 to nNone ; output ; end ; do iobs = 1 to nTotal - nNone ; input nAE @@ ; do i = 1 to dim ( AE ); AE { i } = 0 ; end ; do i = 1 to nAE ; input iAE @@ ; AE { iAE } = 1 ; end ; output ; end ; datalines ; 1 38 32 1 7 1 2 2 2 6 1 1 1 1 3 1 5 6 2 26 20 1 6 1 6 1 6 2 3 6 3 2 5 6 2 2 3 3 36 32 1 3 1 3 1 3 1 2 4 52 45 1 7 1 5 1 2 2 2 5 2 1 3 2 1 2 5 1 2 3 4 7 5 43 36 1 7 1 5 1 5 1 5 1 2 1 2 1 2 6 30 26 1 6 1 6 1 4 1 1 ; proc multtest data = Doctors stepbon stepperm seed = 121211 n = 50000 ; class Doctor ; test ca ( HDeath MI_EKG RFB Infect Neuro Pulm RenFail / lower permutation = 50 ); contrast \"1 vs rest\" - 5 1 1 1 1 1 ; contrast \"2 vs rest\" 1 - 5 1 1 1 1 ; contrast \"3 vs rest\" 1 1 - 5 1 1 1 ; contrast \"4 vs rest\" 1 1 1 - 5 1 1 ; contrast \"5 vs rest\" 1 1 1 1 - 5 1 ; contrast \"6 vs rest\" 1 1 1 1 1 - 5 ; ods output pValues = pValues ; proc sort data = pValues out = pValsort ; by raw ; data top5 ; set pValsort ; if _n_ <= 5 ; proc print noobs data = top5 ; run ; /* Program 12.15: Multiple Peto Mortality-Prevalence Carcinogenicity Tests, Using Discrete Distributions and Discrete Bonferroni Multiplicity Adjustments */ data Carcenogenicity ; keep TGroup Day Tumor1 - Tumor44 ; array Tumor { 44 }; input TGroup nTotal @@ ; do iobs = 1 to nTotal ; input Day nTumor @@ ; do i = 1 to dim ( Tumor ); Tumor { i } = 0 ; end ; do i = 1 to nTumor ; input iTumor Tumori @@ ; Tumor { iTumor } = Tumori ; end ; output ; end ; datalines ; 1 60 729 0 729 1 7 1 564 0 675 1 10 1 598 1 22 2 613 2 5 1 10 1 729 1 21 1 729 2 1 1 36 1 505 1 5 2 689 1 44 2 704 0 682 1 5 2 697 2 10 1 34 1 729 1 10 1 556 0 729 2 10 1 14 1 617 0 661 1 5 1 112 1 42 2 729 2 10 1 27 1 729 1 10 1 729 3 10 1 20 1 25 1 465 0 729 1 25 1 729 0 588 0 729 1 5 1 595 0 532 2 10 1 38 1 620 1 5 2 680 1 5 2 561 0 578 2 6 1 10 1 682 2 5 1 8 2 729 0 713 1 10 1 729 1 5 1 541 1 5 1 689 1 32 1 729 1 6 1 729 1 10 1 638 0 693 1 9 1 729 1 26 1 729 2 15 1 21 1 729 1 42 1 729 1 33 1 729 0 602 3 10 1 14 1 42 2 556 3 5 1 10 1 11 2 576 0 729 0 623 0 729 1 1 1 639 1 10 1 638 0 729 2 10 1 34 1 729 0 729 1 6 1 575 0 2 60 732 1 10 1 415 0 732 1 10 1 732 0 576 0 581 1 5 2 634 2 5 1 10 1 595 0 667 1 5 1 618 3 1 1 10 1 42 2 732 1 10 1 586 0 640 1 9 1 493 0 426 1 5 2 419 1 10 1 658 1 10 1 661 1 10 1 689 1 19 2 643 0 697 0 648 1 10 1 706 0 566 0 732 1 6 1 451 1 27 1 568 1 42 2 686 3 5 1 18 2 31 1 508 1 10 1 732 0 508 0 662 0 732 0 217 1 12 2 732 2 5 1 10 1 485 0 644 1 10 1 732 2 5 1 10 1 683 1 6 1 678 1 10 1 732 4 1 1 5 1 9 1 10 1 556 2 5 2 10 1 732 1 13 1 732 1 6 1 581 0 536 0 732 1 10 1 732 1 6 1 544 0 591 1 5 2 615 0 290 1 42 2 732 1 10 1 732 1 9 1 732 1 1 1 446 2 1 1 10 1 473 0 667 1 10 1 531 0 683 3 5 1 10 1 27 1 3 60 562 2 5 1 10 1 590 0 514 1 5 2 543 0 543 1 10 1 731 0 641 2 10 1 28 1 731 2 5 1 29 1 588 0 580 0 645 2 10 1 40 2 633 1 10 1 674 0 718 2 2 1 5 1 578 2 4 2 25 1 644 3 5 2 10 1 31 1 679 0 596 1 5 2 702 1 5 1 731 2 10 1 25 1 651 0 402 0 569 1 10 1 600 1 10 1 711 0 702 1 10 1 731 0 731 2 3 1 10 1 599 0 576 1 10 1 470 0 729 1 10 1 548 1 9 1 729 1 10 1 710 2 5 1 42 2 613 1 43 2 731 1 10 1 616 0 731 2 10 1 25 1 724 1 10 1 570 1 39 1 731 1 5 1 731 2 1 1 41 1 708 2 5 1 10 1 534 1 10 1 497 0 718 0 652 2 10 1 30 1 727 1 5 2 573 0 663 0 731 0 510 1 10 1 579 1 10 1 686 1 5 1 693 1 10 1 731 2 10 1 24 1 731 0 731 0 573 0 4 60 700 3 1 1 10 1 11 2 475 0 566 1 10 1 617 2 10 1 37 1 476 0 542 1 10 1 581 0 655 0 446 2 10 1 35 1 547 2 5 1 10 1 719 2 10 1 36 1 678 1 10 1 603 2 5 1 10 1 683 1 8 1 543 1 10 1 730 2 1 1 10 1 624 1 10 1 449 1 10 1 639 0 475 0 609 1 10 1 511 0 696 1 10 1 556 1 10 1 620 1 5 2 392 1 10 1 661 1 32 1 676 1 10 1 556 1 16 1 605 0 496 0 532 1 5 2 505 0 482 0 591 0 556 1 10 1 730 1 10 1 635 0 669 0 730 2 10 1 42 1 568 1 10 1 702 1 17 2 618 0 630 3 10 1 14 1 23 1 730 2 6 1 10 1 730 0 519 0 382 0 633 1 10 1 451 0 576 0 549 1 10 1 610 1 23 2 654 1 10 1 524 2 10 1 42 2 669 2 5 1 10 1 593 1 10 1 730 0 659 2 1 1 10 1 24 0 ; data Carcenogenicity ; set Carcenogenicity ; select ; when ( Day <= 365 ) Stratum = 1 ; when ( Day <= 455 ) Stratum = 2 ; when ( Day <= 546 ) Stratum = 3 ; when ( Day <= 637 ) Stratum = 4 ; when ( Day <= 730 ) Stratum = 5 ; otherwise Stratum = 6 ; end ; run ; ods listing close ; proc multtest data = Carcenogenicity stepbon ; class TGroup ; strata Stratum ; test peto ( Tumor1 - Tumor44 / upper time = day permutation = 10 continuity = .5 ); contrast \"Dose trend\" 0 1 2 3 ; ods output pValues = pValues ; run ; ods listing ; proc sort data = pValues out = pvalsort ; by raw ; data top5 ; set pvalsort ; if _n_ <= 5 ; proc print data = top5 noobs ; run ; /* Program 12.16: Multiple Binary Comparisons Using Freeman-Tukey Tests */ proc multtest data = Toxicity order = data stepboot seed = 121211 ; class Group ; freq Freq ; test ft ( Outcome / lower ); contrast \"Hist vs Curr\" - 1 1 0 0 ; contrast \"Low vs Curr\" 0 1 - 1 0 ; contrast \"High vs Curr\" 0 1 0 - 1 ; run ; /* Program 12.17: Mixing Continuous and Discrete Variables */ data Adverse ; set Adverse ; TotalAdverse = sum ( of AE1 - AE27 ); proc multtest data = Adverse stepperm seed = 121211 ; class Group ; test mean ( TotalAdverse / upper ) fisher ( ae1 - ae28 / upper ); contrast \"Treatment-Control\" - 1 1 ; ods output pValues = pValues ; run ; proc sort data = pValues out = pValues ; by raw ; data Top5 ; set pValues ; if _n_ <= 5 ; proc print noobs data = Top5 ; run ; /* Program 12.18: Corresponding PROC MULTTEST and PROC LIFETEST */ title1 ' Lifetimes of Rats ' ; data DMBA_LIFETEST ; input Group Days @@ ; Censored = ( Days < 0 ); Days = abs ( Days ); datalines ; 1 143 1 164 1 188 1 188 1 190 1 192 1 206 1 209 1 213 1 216 1 220 1 227 1 230 1 234 1 246 1 265 1 304 1 - 216 1 - 244 2 142 2 156 2 163 2 198 2 205 2 232 2 232 2 233 2 233 2 233 2 233 2 239 2 240 2 261 2 280 2 280 2 296 2 296 2 323 2 - 204 2 - 344 ; proc lifetest data = DMBA_LIFETEST ; title2 \"Comparisons of Survival Curves via the LIFETEST Procedure\" ; time Days * Censored ( 1 ); strata Group ; ods select HomTests ; run ; title2 ' Logrank Test via the MULTTEST Procedure ' ; data DMBA_MULTTEST ; set DMBA_LIFETEST ; t = 2 * ( 1 - Censored ); run ; proc multtest data = DMBA_MULTTEST ; title3 \"Asymptotic Analysis\" ; class Group ; test peto ( t / time = Days ); ods select pValues ; run ; proc multtest data = DMBA_MULTTEST ; title3 \"Permutation Analysis\" ; class Group ; test peto ( t / time = Days permutation = 10 ); ods select pValues ; run ; /* Program 13.1 Bayesian Sample for an Incomplete Block Design */ data detergent ; do detergent = 1 to 5 ; do block = 1 to 10 ; input plates @@ ; output ; end ; end ; datalines ; 27 28 30 31 29 30 . . . . 26 26 29 . . . 30 21 26 . 30 . . 34 32 . 34 31 . 33 . 29 . 33 . 34 31 . 33 31 . . 26 . 24 25 . 23 24 26 run ; proc mixed data = detergent ; class block detergent ; model plates = detergent ; random block ; lsmeans detergent / cl adjust = simulate ( seed = 121211 ); prior / out = sample seed = 1283470 nsample = 10000 ; run ; /* Program 13.2: Calculating Average Differences in Losses */ %let k = 100 ; data s ; set sample ; array lsm [ 5 ] lsm1 - lsm5 ; array Loss [ 5 , 5 ] Loss1 - Loss25 ; do i = 1 to 5 ; do j = 1 to 5 ; delta = lsm [ i ] - lsm [ j ]; if ( delta > 0 ) then Loss [ i , j ] = delta ; else Loss [ i , j ] = & k * delta ; end ; end ; run ; proc means data = s mean noprint ; var Loss1 - Loss25 ; output out = o mean = mean1 - mean25 ; run ; data o1 ; set o ; array mean [ 5 , 5 ] mean1 - mean25 ; do i = 1 to 5 ; do j = 1 to 5 ; if ( i ne j ) then do ; LossDiff = mean [ i , j ]; output ; end ; end ; end ; keep i j LossDiff ; proc print noobs ; run ; /* Program 13.3: Probabilities of Meaningful Differences */ data s1 ; set sample ; array lsm [ 5 ]; array M [ 5 , 5 ]; do i = 1 to 4 ; do j = i + 1 to 5 ; M [ i , j ] = ( abs ( lsm [ i ] - lsm [ j ]) > 2 ); end ; end ; proc summary data = s1 ; var M :; output out = s2 ( where = ( _STAT_ = ' MEAN ' )); proc iml ; use s2 ; read all var ( \"M1\" : \"M25\" ) into M ; title \"Probabilities of Meaningful Differences\" ; print ( shape ( M , 5 , 5 )) [ rowname = ( \"1\" : \"5\" ) colname = ( \"1\" : \"5\" )]; /* Program 13.4: Multiple Bayes Tests of Point Null Hypotheses */ data MultipleEndpoints ; Treatment = ' Placebo ' ; do Subject = 1 to 54 ; input Endpoint1 - Endpoint4 @@ ; output ; end ; Treatment = ' Drug ' ; do Subject = 54 + 1 to 54 + 57 ; input Endpoint1 - Endpoint4 @@ ; output ; end ; datalines ; 4 3 3 5 5 0 1 7 1 0 1 9 4 0 3 5 3 0 2 9 4 1 2 6 2 0 4 6 2 2 5 5 3 0 1 7 2 0 1 9 4 6 5 5 2 0 2 8 2 7 1 7 1 2 2 9 4 0 3 7 3 0 1 6 3 0 1 6 4 1 4 6 6 0 4 7 3 0 1 8 3 0 1 9 2 1 2 7 6 2 3 5 3 0 4 7 3 0 1 9 2 0 1 9 6 9 6 3 4 9 2 6 2 0 1 7 1 0 1 9 4 0 4 7 3 1 4 6 3 0 3 7 1 0 1 8 6 7 5 4 4 6 2 5 6 19 7 5 6 3 6 6 3 0 5 6 2 4 2 8 1 0 1 8 4 21 5 5 2 0 2 9 4 7 3 5 3 1 2 8 3 3 3 8 4 3 4 6 1 0 1 10 1 0 2 9 3 0 4 5 3 1 1 6 3 4 4 6 5 8 5 5 5 1 5 4 1 0 4 8 1 0 1 10 1 0 1 9 2 1 2 7 4 1 2 5 5 0 5 6 1 4 5 6 5 6 4 6 2 0 2 9 2 2 2 5 1 0 1 10 3 2 3 6 5 4 6 6 2 1 2 8 2 1 2 6 2 1 1 8 3 0 3 9 3 1 2 6 1 0 2 9 1 0 1 9 3 0 3 9 1 0 1 10 1 0 1 9 1 0 1 10 2 0 4 7 5 1 2 6 4 0 5 7 4 0 4 6 2 1 3 6 2 1 1 6 4 0 4 6 1 0 1 8 1 0 2 9 4 1 3 6 4 3 4 5 4 2 5 5 1 0 1 10 3 0 2 8 4 2 2 8 3 0 2 9 1 0 1 10 1 0 1 9 2 0 2 9 2 1 2 8 3 0 3 8 2 4 2 6 2 1 1 9 2 2 2 9 4 0 1 4 3 3 1 8 4 4 3 6 2 0 1 10 4 2 3 6 1 0 1 8 2 0 2 8 5 1 5 5 4 0 4 6 ; data multend1 ; set MultipleEndpoints ; Endpoint4 = - Endpoint4 ; run ; ods listing close ; proc glm data = multend1 ; class Treatment ; model Endpoint1 - Endpoint4 = Treatment ; estimate \"Treatment vs Control\" Treatment - 1 1 ; manova h = Treatment / printe ; ods output Estimates = Estimates PartialCorr = PartialCorr ; run ; ods listing ; %macro Estimates ; use Estimates ; read all var { tValue } into EstPar ; use PartialCorr ; read all var ( \"Endpoint1\" : \"Endpoint4\" ) into cov ; %mend ; % BayesTests ( rho = .5 , Pi0 = .5 ); /* Program 13.5: Evaluating Sensitivity to Priors-Recalibrating the Joint Prior */ % BayesTests ( rho = .5 , PiAll = .5 ); /* Program 14.1: Multiple Comparisons in Logistic Regression */ data uti ; format diagnosis $13 .; do Diagnosis = \"complicated\" , \"uncomplicated\" ; do treatment = \"A\" , \"B\" , \"C\" ; input cured total @@ ; AminusC = ( treatment = \"A\" ); BminusC = ( treatment = \"B\" ); CompminusUnComp = ( Diagnosis = \"complicated\" ); output ; end ; end ; datalines ; 78 106 101 112 68 114 40 45 54 59 34 40 ; proc logistic data = uti outest = stats covout ; model cured / total = AminusC BminusC CompminusUnComp ; run ; %macro Contrasts ; C = { 0 1 0 0 , 0 0 1 0 , 0 1 - 1 0 , 0 0 0 1 }; C = C ` ; Clab = { \"trt(A-C)\" , \"trt(B-C)\" , \"trt(A-B)\" , \"Diag(Comp-UnComp)\" }; %mend ; %macro estimates ; use stats ( where = ( _TYPE_ = ' PARMS ' )); read all var { INTERCEPT AminusC BminusC CompminusUnComp } into EstPar ; EstPar = EstPar ` ; use stats ( where = ( _TYPE_ = ' COV ' )); read all var { INTERCEPT AminusC BminusC CompminusUnComp } into Cov ; df = 0 ; %mend ; % SimIntervals ( seed = 121211 , nsamp = 100000 ); /* Program 14.2: MCB Analysis of Water Filters */ data Filter ; do Brand = 1 to 7 ; do i = 1 to 3 ; input NColony @@ ; output ; end ; end ; cards ; 69 122 95 118 154 102 171 132 182 122 119 . 204 225 190 140 130 127 170 165 . ; % MCB ( Filter , NColony , Brand ); /* Program 14.3: Finding the Most Significant Linear Combination with Multivariate Data */ ods select Spouse_Question . Canonical . CanCoefficients ; proc glm data = husbwive ; model HusbQ1 - HusbQ4 WifeQ1 - WifeQ4 = / nouni ; repeated Spouse 2 , Question 4 identity / canonical ; run ; /* Program 14.4: Confidence Interval and Test for Most Significant Linear Combination */ data _null_ ; tcrit = sqrt ( 4 * ( 30 - 1 ) * finv ( 1 - 0.05 , 4 , 30 - 4 ) / ( 30 - 4 )); alpha = 2 * ( 1 - probt ( tcrit , 29 )); call symput ( ' alpha ' , alpha ); run ; data one ; set HusbWive ; maxdiff = - 0.26708818 * DiffQ1 + 0.63289897 * DiffQ2 + 2.65384153 * DiffQ3 - 1.77626607 * DiffQ4 ; proc means alpha =& alpha n mean lclm uclm prt ; title \"Interval and Test for max Diff, Alpha=&alpha\" ; var maxdiff ; run ; /* Macros used in \"Multiple Comparisons and Multiple Tests Ssing the SAS(R) System,\" */ /* by Westfall, Tobias, Rom, Wolfinger, and Hochberg, SAS Books By Users series. */ /* The %Rom macro */ /* This macro computes multiplicity adjustments usings Rom's (1990) method. */ /*---------------------------------------------------------------*/ /* Name: Rom */ /* Title: Rom Step-Up procedure */ /* Author: Dror Rom, rom@prosof.com */ /* Reference: Rom, D.M. (1990). A sequentially rejective test */ /* procedure based on a modified Bonferroni */ /* inequality. Biometrika, 77, 663--665. */ /* Release: Version 6.11 */ /*---------------------------------------------------------------*/ /* Input: */ /* */ /* DATASET= the SAS data set containing the data to be */ /* analyzed (required) */ /* */ /* PV= the p-values (required) */ /* */ /* FWE= the level of significance for comparisons */ /* among the means. The default is 0.05. */ /* */ /* Output: */ /* */ /* The output dataset contains one observation for each */ /* P-value in the dataset. The output dataset contains the */ /* following variables: */ /* */ /* i - The index of the ordered P-value */ /* */ /* CRIT - The critical value */ /* */ /* PVALUE - The P-value */ /* */ /* ADJP - The adjusted P-value */ /* */ /* DEC - The decision on the corresponding hypothesis */ /*---------------------------------------------------------------*/ % MACRO ROM ( dataset = , pv = , FWE = 0.05 ); data a ; set & dataset ; p =& pv ; keep p ; proc sort ; by descending p ; proc means noprint data = a ; var p ; output out = b n = N ; proc transpose data = a prefix = pv out = a ; data adjp ; merge b a ; array critv { 200 }; array pv { 200 }; minim = 1 ; do z = 1 to n ; w = 0 ; converge = ' false ' ; do while (( converge = ' false ' ) or ( w <= 6 )); w = w + 1 ; if ( w = 1 ) then do ; if ( z = 1 ) then alpha = pv ( z ); else if ( z = 2 ) then alpha = 2 * pv ( 2 ); else alpha = ( - 1 + ( 1 + 4 * z * pv ( z )) ** 0.5 ) / 2 ; end ; else do ; if abs ( alpha - adjp ) <= 0.0001 then converge = ' true ' ; alpha = adjp ; end ; critv ( 2 ) = alpha ; critv ( 1 ) = alpha / 2 ; critv ( n ) = alpha ; critv ( n - 1 ) = alpha / 2 ; do i = 3 to n ; m = n - i + 1 ; do j = 1 to i - 1 ; critv ( i + 1 - j ) = critv ( i - j ); end ; critv ( 1 ) = 0 ; do j = 1 to i - 2 ; *** calculates n choose m ***** ; c = 1 ; k = j ; jj = i - j ; do ii = 1 to k ; jj = jj + 1 ; c = c * jj / ii ; end ; comb = c ; critv ( 1 ) = critv ( 1 ) + critv ( i ) ** j - comb * critv ( i - j ) ** ( i - j ); end ; critv ( 1 ) = ( critv ( 1 ) + critv ( i ) ** ( i - 1 )) / i ; end ; adjp = alpha * pv [ z ] / critv [ n - z + 1 ]; end ; minim = min ( adjp , minim ); adjp = minim ; output ; end ; data adjp ; set adjp ; i = _N_ ; keep adjp i ; data critp ; merge b a ; array critv { 200 }; alpha =& fwe ; critv ( 2 ) = alpha ; critv ( 1 ) = alpha / 2 ; critv ( n ) = alpha ; critv ( n - 1 ) = alpha / 2 ; do i = 3 to n ; m = n - i + 1 ; do j = 1 to i - 1 ; critv ( i + 1 - j ) = critv ( i - j ); end ; critv ( 1 ) = 0 ; do j = 1 to i - 2 ; *** calculates n choose m ***** ; c = 1 ; k = j ; jj = i - j ; do ii = 1 to k ; jj = jj + 1 ; c = c * jj / ii ; end ; comb = c ; critv ( 1 ) = critv ( 1 ) + critv ( i ) ** j - comb * critv ( i - j ) ** ( i - j ); end ; critv ( 1 ) = ( critv ( 1 ) + critv ( i ) ** ( i - 1 )) / i ; end ; data c ; merge a b critp ; array pv { 200 }; array critv { 200 }; alpha =& fwe ; dec = ' retain ' ; do i = 1 to n ; crit = critv ( n + 1 - i ); pvalue = pv ( i ); if ( pv ( i ) <= crit ) then dec = ' reject ' ; output ; end ; run ; data c ; set c ; i = _N_ ; title1 ' ' ; TITLE2 ' ROM STEP - UP PROCEDURE ' ; title3 ' ' ; DATA _NULL_ ; FILE PRINT ; merge c adjp END = EOF ; by i ; IF _N_ = 1 THEN DO ; PUT @ 8 'I' @ 12 ' CRITICAL VALUE ' @ 30 ' P - VALUE ' @ 45 ' ADJUSTED P ' @ 60 ' DECISION ' ; PUT @ 5 65 * '-' ; END ; put @ 8 i @ 12 crit f7 .6 @ 30 pvalue @ 45 adjp f7 .6 @ 60 dec ; if EOF = 1 then do ; PUT @ 5 65 * '-' ; put @ 8 ' ALPHA = ' @ 16 alpha ; end ; RUN ; % MEND ROM ; /* The %HochBen Macro */ /* */ /* This macro performs multiplicity adjustments using Hochberg */ /* and Benjamini's (1990) graphical method. */ /*---------------------------------------------------------------*/ /* Name: HochBen */ /* Title: Hochberg and Benjamini graphical analysis of */ /* multiple P-values */ /* Author: Dror Rom, rom@prosof.com */ /* Reference: Hochberg, Y., and Benjamini, Y. (1990). More */ /* Powerful Procedures for Multiple Significance */ /* Testing. Statistics in Medicine, 9, 811-818. */ /* Release: Version 6.11 */ /*---------------------------------------------------------------*/ /* Input: */ /* */ /* DATASET= the SAS data set containing the data to be */ /* analyzed (required) */ /* */ /* PV= the p-values (required) */ /* */ /* FWE= the level of significance for comparisons */ /* among the means. The default is 0.05. */ /* */ /* Output: */ /* */ /* The output dataset contains one observation for each */ /* P-value in the dataset. The output dataset contains the */ /* following variables: */ /* */ /* i - The index of the ordered P-value */ /* */ /* CRIT - The critical value */ /* */ /* PVALUE - The P-value */ /* */ /* ADJP - The adjusted P-value */ /* */ /* DEC - The decision on the corresponding hypothesis */ /* */ /* NHAT - The estimated number of true hypotheses */ /*---------------------------------------------------------------*/ % MACRO HOCHBEN ( dataset = , pv = , FWE = 0.05 ); data a ; set & dataset ; p =& pv ; q = 1 - p ; proc sort ; by q ; data b ; set a ; i = _N_ ; slope = q / i ; nhat = 1 / slope - 1 ; title1 ' ' ; title2 ' HOCHBERG & BENJAMINI GRAPHICAL ANALYSIS OF MULTIPLE P - VALUES ' ; proc sort ; by p ; data c ; set b ; lags = lag1 ( slope ); data c ; set c ; diff = slope - lags ; if ( diff = '.' ) or ( diff >= 0 ) then diff = '.' ; else diff = 0 ; data d ; set c ; if ( diff = 0 ); ii = 1 ; data d ; set d ; by diff ; if first . diff ; keep nhat q i ii ; data c1 ; set c ; if ( diff = '.' ); data c1 ; set c1 ; by diff ; if last . diff ; ii = 1 ; keep nhat q i ii ; run ; data d ; set c1 d ; by ii ; if last . ii ; keep nhat q i ; data d1 ; set d ; i = nhat + 1 ; q = 1 ; keep i q ; data d3 ; set d ; i = nhat ; drop q ; do j = 0 , 10 ; qqq = j / 10 ; output ; end ; data d2 ; q = 0 ; i = 0 ; data d1 ; set d1 d2 d ; qq = q ; keep qq i ; data c ; set c ; drop nhat ; proc sort ; by i ; data e ; merge c d ; by i ; data f ; set e d1 d3 ; symbol1 v = PLUS i = none ; symbol2 v = none i = join ; symbol3 v = none i = join ; title1 ' ' ; title2 ' HOCHBERG & BENJAMINI GRAPHICAL ANALYSIS OF MULTIPLE P - VALUES ' ; title3 ' PLOT of 1 - PVALUES VS . THEIR ORDER ' ; goptions colors = ( black ) cback = white ; label q = 'q' ; proc gplot data = f ; plot q * i = 1 qq * i = 2 qqq * i = 3 / overlay frame ; run ; data a ; set a ; keep p ; proc sort ; by p ; proc means noprint ; var p ; output out = b n = N ; data b ; set b ; data c1 ; set f ; nhat = round ( nhat ); if not ( nhat = '.' ); data c2 ; merge b c1 ; if not ( n = '.' ); alpha =& fwe ; keep n alpha nhat ; proc transpose data = a prefix = pv out = a ; run ; data c ; merge a c2 ; data c ; set c ; array pv { 200 }; dec = ' reject ' ; do i = 1 to n ; crit1 = alpha / ( n + 1 - i ); if ( nhat = 0 ) then crit2 = alpha ; else crit2 = alpha / nhat ; crit = max ( crit1 , crit2 ); if ( nhat < 1 ) then adjp = pv ( i ); else adjp = max ( min (( n + 1 - i ) * pv ( i ), nhat * pv ( i ), 1 ), adjp ); pvalue = pv ( i ); if (( pv ( i ) <= crit ) and ( dec = ' reject ' )) then dec = ' reject ' ; else dec = ' retain ' ; output ; end ; run ; title1 ' ' ; title2 ' HOCHBERG & BENJAMINI GRAPHICAL ANALYSIS OF MULTIPLE P - VALUES ' ; title3 ' ' ; title4 ' CRITICAL VALUES ADJUSTED BY ESTIMATED NUMBER OF TRUE HYPOTHESES ' ; title5 ' ' ; DATA _NULL_ ; FILE PRINT ; SET c END = EOF ; I = _N_ ; IF _N_ = 1 THEN DO ; PUT @ 8 'I' @ 12 ' CRITICAL VALUE ' @ 30 ' P - VALUE ' @ 45 ' ADJUSTED P ' @ 60 ' DECISION ' ; PUT @ 5 65 * '-' ; END ; put @ 8 i @ 12 crit f7 .6 @ 30 pvalue @ 45 adjp f7 .6 @ 60 dec ; if EOF = 1 then do ; PUT @ 5 65 * '-' ; put @ 8 ' ALPHA = ' @ 16 alpha ; put @ 8 ' ESTIMATED NUMBER OF TRUE HYPOTHESES : ' @ 56 nhat ; end ; RUN ; % MEND HOCHBEN ; /* The %SimIntervals Macro */ /* This macro computes simultaneous confidence intervals for a general */ /* collection of linear functions of parameters, using Edwards and Berry */ /* (1987). */ /*--------------------------------------------------------------*/ /* Name: SimIntervals */ /* Title: Simultaneous Confidence Intervals for General */ /* Linear Functions */ /* Author: Randy Tobias, sasrdt@sas.com, */ /* Reference: Edwards and Berry (1987). The efficiency of */ /* simulation-based multiple comparisons. */ /* Biometrics 43, 913-928. */ /* Release: Version 7.01 */ /*--------------------------------------------------------------*/ /* Inputs: */ /* */ /* NSAMP = simulation size, with 20000 as default */ /* */ /* SEED = random number seed, with 0 (clock time) */ /* as default */ /* */ /* CONF = desired confidence level, with 0.95 as default */ /* */ /* SIDE = U, L or B, for upper-tailed, lower-tailed */ /* or two-tailed, respectively. SIDE=B is default. */ /* */ /* Additionally, %SimIntervals requires two further macros to */ /* be defined that use SAS/IML to construct the estimates and */ /* the contrasts of interest. In particular, make sure the */ /* following two macros are defined before invoking */ /* %SimIntervals: */ /* */ /* %Estimate: Uses SAS/IML code to define */ /* EstPar - (column) vector of estimated parameters */ /* Cov - covariance matrix for the for the estimates */ /* df - error degrees of freedom; set to 0 for */ /* asymptotic analysis */ /* */ /* %Contrasts: Uses SAS/IML code to define */ /* C - matrix whose columns define the contrasts of */ /* interest between the parameters */ /* CLab - (column) character vector whose elements */ /* label the respective contrasts in C */ /* */ /* You can either define these macros directly, or use the */ /* %MakeGLMStats macro to define them. */ /* */ /*--------------------------------------------------------------*/ /* Output: */ /* The output is a dataset with one observation for each */ /* contrast and the following variables: */ /* */ /* Contrast - contrast label */ /* Estimate - contrast estimated value */ /* StdErr - standard error of estimate */ /* tValue - normalized estimate, Estimate/StdErr */ /* RawP - non-multiplicity-adjusted p-value */ /* OneP - one-step multiplicity-adjusted p-value */ /* LowerCL - multiplicity-adjusted lower confidence limit */ /* UpperCL - multiplicity-adjusted upper confidence limit */ /* */ /* This dataset is also displayed as a formatted table, using */ /* the ODS system. */ /*--------------------------------------------------------------*/ %macro SimIntervals ( nsamp = 20000 , seed = 0 , conf = 0.95 , side = B , options = ); %global ANORM quant ; options nonotes ; proc iml ; % Estimates ; if ( df <= 0 ) then call symput ( ' ANORM ',' 1 ' ); else call symput ( ' ANORM ',' 0 ' ); % Contrasts ; Cov = C ` * Cov * C ; D = diag ( 1 / sqrt ( vecdiag ( Cov ))); R = D * Cov * D ; evec = eigvec ( R ); eval = eigval ( R ) <> 0 ; U = ( diag ( sqrt ( eval )) * evec ` ) ` ; dimU = sum ( eval > 1e-8 ); U = U [, 1 : dimU ]; ests = C ` * EstPar ; ses = sqrt ( vecdiag ( Cov )); tvals = ests / ses ; %if ( & side = B ) %then %do ; if df > 0 then rawp = 2 * ( 1 - probt ( abs ( tvals ), df )); else rawp = 2 * ( 1 - probnorm ( abs ( tvals ))); %end ; %else %if ( & side = L ) %then %do ; if df > 0 then rawp = probt ( tvals , df ) ; else rawp = probnorm ( tvals ); %end ; %else %do ; if df > 0 then rawp = 1 - probt ( tvals , df ) ; else rawp = 1 - probnorm ( tvals ); %end ; adjp = j ( ncol ( C ), 1 , 0 ); maxt = j ( & nsamp , 1 , 0 ); do isim = 1 to & nsamp ; Z = U * rannor ( j ( dimU , 1 , & seed )); if df > 0 then do ; V = cinv ( ranuni ( & seed ), df ); tvalstar = Z / sqrt ( V / df ); end ; else do ; tvalstar = Z ; end ; %if ( & side = B ) %then %do ; mx = max ( abs ( tvalstar )); %end ; %else %do ; mx = max ( tvalstar ); %end ; maxt [ isim ] = mx ; %if ( & side = B ) %then %do ; adjp = adjp + ( mx > abs ( tvals )); %end ; %else %if ( & side = L ) %then %do ; adjp = adjp + ( mx > - tvals ); %end ; %else %do ; adjp = adjp + ( mx > tvals ); %end ; end ; adjp = adjp /& nsamp ; confindx = round ( & nsamp *& conf , 1 ); sorttemp = maxt ; maxt [ rank ( maxt ),] = sorttemp ; c_alpha = maxt [ confindx ]; start tlc ( n , d ); return ( trim ( left ( char ( n , d )))); finish ; %if ( & side = B ) %then %do ; LowerCL = ests - c_alpha * ses ; UpperCL = ests + c_alpha * ses ; %end ; %else %if ( & side = L ) %then %do ; LowerCL = j ( ncol ( C ), 1 ,. M ); UpperCL = ests + c_alpha * ses ; %end ; %else %do ; LowerCL = ests - c_alpha * ses ; UpperCL = j ( ncol ( C ), 1 ,. I ); %end ; create SimIntOut var { \"Estimate\" \"StdErr\" \"tValue\" \"RawP\" \"OneP\" \"LowerCL\" \"UpperCL\" }; data = ests || ses || tvals || rawp || adjp || LowerCL || UpperCL ; append from data ; call symput ( ' confpct ' , tlc ( 100 *& conf , 4 )); call symput ( ' quant ' , tlc ( c_alpha , 8 )); create labels from clab ; append from clab ; data SimIntOut ; merge labels ( rename = ( COL1 = Contrast )) SimIntOut ; run ; %if ( ^ %index ( %upcase ( & options ), NOPRINT )) %then %do ; proc template ; delete MCBook . SimIntervals ; define table MCBook . SimIntervals ; column Contrast Estimate StdErr tValue RawP OneP LowerCL UpperCL ; define header h1 ; text \"Estimated &confpct% Quantile = &quant\" ; spill_margin ; %if ( ^& ANORM ) %then %do ; space = 1 ; %end ; end ; %if ( & ANORM ) %then %do ; define header h2 ; text \"Asymptotic Normal Approximations\" ; space = 1 ; end ; %end ; define column Contrast ; header = \"Contrast\" ; end ; define column Estimate ; header = \"Estimate\" format = D8 . space = 1 ; translate _val_ = . _ into '' ; end ; define column StdErr ; header = \"Standard Error\" format = D8 . space = 1 ; translate _val_ = . _ into '' ; end ; define column tValue ; header = \"#t Value\" format = 7.2 ; translate _val_ = . I into ' Infty ' , _val_ = . M into ' - Infty ' , _val_ = . _ into '' ; end ; %if ( & side = B ) %then %do ; define header ProbtHead ; text \" Pr > |t| \" ; start = RawP end = OneP just = c expand = '-' ; end ; %end ; %else %if ( & side = L ) %then %do ; define header ProbtHead ; text \" Pr < t \" ; start = RawP end = OneP just = c expand = '-' ; end ; %end ; %else %do ; define header ProbtHead ; text \" Pr > t \" ; start = RawP end = OneP just = c expand = '-' ; end ; %end ; define column RawP ; space = 1 glue = 10 parent = Common . PValue header = \"Raw\" ; translate _val_ = . _ into '' ; end ; define column OneP ; parent = Common . PValue header = \"Adjusted\" ; translate _val_ = . _ into '' ; end ; define header CLHead ; text \"&confpct% Confidence Interval\" ; start = LowerCL end = UpperCL just = c ; end ; define LowerCL ; translate _val_ = . M into ' - Infty ' ; space = 1 glue = 10 format = D8 . print_headers = off ; end ; define UpperCL ; format = D8 . print_headers = off ; translate _val_ = . I into ' Infty ' ; end ; end ; run ; data _null_ ; set SimIntOut ; file print ods = ( template = ' MCBook . SimIntervals ' ); put _ods_ ; run ; %end ; options notes ; %mend ; /* The %MakeGLMStats Macro */ /* This macro creates the %Estimates and %Contrasts */ /* macros that are needed for %SimIntervals and %SimTests. */ /*--------------------------------------------------------------*/ /* Name: MakeGLMStats */ /* Title: Macro to create %Estimates and %Contrasts macros */ /* needed for %SimIntervals and %SimTests */ /* Author: Randy Tobias, sasrdt@sas.com */ /* Release: Version 7.01 */ /*--------------------------------------------------------------*/ /* Inputs: */ /* */ /* DATASET = Data set to be analyzed (required) */ /* */ /* CLASSVAR = Listing of classification variables. If absent, */ /* no classification variables are assumed */ /* */ /* YVAR = response variable (required) */ /* */ /* MODEL = GLM model specification (required) */ /* */ /* CONTRASTS = CONTROL(effect), ALL(effect), or USER. This */ /* creates the %Contrasts macro unless you specify */ /* USER (the default), in which case you create */ /* the %Contrasts macro yourself */ /* */ /*--------------------------------------------------------------*/ /* Output: This macro creates the %Estimates macro needed for */ /* the %SimIntervals and %SimTests macros. Additionally, if */ /* you specify CONTRASTS = ALL or CONTROL, it also creates the */ /* %Contrasts macro. There is no other output. */ /*--------------------------------------------------------------*/ %macro MakeGLMStats ( dataset = , classvar = , yvar = , model = , contrasts = USER ); %global nx yvar1 nlev icntl ; options nonotes ; %let yvar1 = & yvar ; proc glmmod data =& dataset noprint outparm = parm outdesign = design ; %if ( %length ( & classvar )) %then %do ; class & classvar ; %end ; model & yvar = & model ; data _null_ ; set parm ; call symput ( ' nx ' , _n_ ); run ; %macro Estimates ; use design ; read all var ( \"col1\" : \"col&nx\" ) into X ; read all var ( \"&yvar1\" ) into Y ; XpXi = ginv ( X ` * X ); rankX = trace ( XpXi * ( X ` * X )); n = nrow ( X ); df = n - rankX ; EstPar = XpXi * X ` * Y ; mse = ssq ( Y - X * EstPar ) / df ; Cov = mse * XpXi ; %mend ; %let ctype = %upcase ( %scan ( & contrasts , 1 )); %if ( & ctype ^= USER ) %then %do ; %let effect = %scan ( & contrasts , 2 ); %if ( & ctype = CONTROL ) %then %do ; %let icntl = %scan ( & contrasts , 3 ); %end ; %end ; %if ( & ctype ^= USER ) %then %do ; ods listing close ; ods output LSMeanCoef = LSMeanCoef ; proc glm data =& dataset ; %if ( %length ( & classvar )) %then %do ; class & classvar ; %end ; model & yvar = & model ; lsmeans & effect / e ; quit ; ods listing ; proc transpose data = LSMeanCoef out = temp ; var Row :; data _null_ ; set temp ; call symput ( ' nlev ' , _n_ ); run ; %end ; %if ( & ctype = ALL ) %then %do ; %macro Contrasts ; %global nlev ; use LSMeanCoef ; read all var ( \"Row1\" : \"Row&nlev\" ) into L ; free C clab ; do i = 1 to ncol ( L ) - 1 ; do j = i + 1 to ncol ( L ); C = C // L[,i]` - L[,j]`; clab = clab // ( trim(left(char(i,5))) + '-' + trim ( left ( char ( j , 5 )))); end ; end ; C = C ` ; %mend ; %end ; %if ( & ctype = CONTROL ) %then %do ; %macro Contrasts ; %global icntl ; use LSMeanCoef ; read all var ( \"Row1\" : \"Row&nlev\" ) into L ; free C clab ; j = & icntl ; do i = 1 to ncol ( L ); if ( i ^= j ) then do ; C = C // L[,i]` - L[,j]`; clab = clab // ( trim(left(char(i,5))) + '-' + trim ( left ( char ( j , 5 )))); end ; end ; C = C ` ; %mend ; %end ; options notes ; %mend ; /* The %IndividualPower Macro */ /* This macro computes power for various multiple comparisons tests */ /* using the ``Individual Power\" definition. */ /*--------------------------------------------------------------*/ /* Name: IndividualPower */ /* Title: Macro to evaluate individual power of multiple */ /* comparisons */ /* Author: Randy Tobias, sasrdt@sas.com */ /* Release: Version 7.01 */ /*--------------------------------------------------------------*/ /* Inputs: */ /* */ /* MCP = RANGE, DUNNETT2, DUNNETT1, or MAXMOD (required) */ /* */ /* G = Number of groups (excluding control for */ /* DUNNETT2 and DUNNETT1; required) */ /* */ /* D = Meaningful mean difference (required) */ /* */ /* S = Standard deviation (required) */ /* */ /* FWE = Desired Familywise Error (0.05 default) */ /* */ /* TARGET = Target power level (0.80 default) */ /* */ /*--------------------------------------------------------------*/ /* Output: This macro plots individual power for a variety of */ /* Multiple comparisons methods, and plots it as a function of */ /* n, the within-group sample size */ /*--------------------------------------------------------------*/ %macro IndividualPower ( mcp = , g = , d = , s = , FWE = 0.05 , target = 0.80 ); %let mcp = %upcase ( & mcp ); options nonotes ; data power ; keep C_a N NCP DF Power ; label N = \"Group size, N\" ; ntarget = 1 ; nactual = .; dtarget = 1000 ; do N = 2 to 1000 until ( Power > .99 ); %if ( & mcp = MAXMOD ) %then %do ; ncp = sqrt ( N ) * ( & d /& s ); %end ; %else %do ; ncp = sqrt ( N / 2 ) * ( & d /& s ); %end ; %if ( ( & mcp = DUNNETT1 ) or ( & mcp = DUNNETT2 )) %then %do ; df = ( & g + 1 ) * ( N - 1 ); %end ; %else %do ; df = ( & g ) * ( N - 1 ); %end ; conf = 1 -& fwe ; %if ( & mcp = RANGE ) %then %do ; c_a = probmc ( \"&mcp\" ,., conf , df , & g ) / sqrt ( 2 ); %end ; %else %do ; c_a = probmc ( \"&mcp\" ,., conf , df , & g ); %end ; %if ( & mcp = DUNNETT1 ) %then %do ; Power = 1 - probt ( c_a , df , ncp ); %end ; %else %do ; Power = 1 - probf ( c_a ** 2 , 1 , df , ncp ** 2 ); %end ; if ( abs ( Power - & target ) < dtarget ) then do ; ntarget = N ; nactual = Power ; dtarget = abs ( Power - & target ); end ; output ; end ; call symput ( ' ntarget ' , trim ( left ( ntarget ))); call symput ( ' nactual ' , trim ( left ( nactual ))); run ; data target ; length xsys ysys position $ 1 ; retain xsys ysys hsys color ; xsys = '2' ; ysys = '2' ; color = ' black ' ; x = 0 ; y = & nactual ; function = ' MOVE ' ; output ; x = & ntarget ; y = & nactual ; function = ' DRAW ' ; line = 1 ; size = 1 ; output ; x = & ntarget ; y = 0 ; function = ' DRAW ' ; line = 1 ; size = 1 ; output ; x = & ntarget + 2 ; y = & nactual / 2 ; function = ' LABEL ' ; style = ' swissb ' ; text = \"Power(N=&ntarget)\" ; position = '0' ; output ; x = & ntarget + 2 ; y = & nactual / 2 - 0.12 ; function = ' LABEL ' ; style = ' swissb ' ; text = \" = \" || put ( & nactual , pvalue6 .); position = '0' ; output ; goptions ftext = swissb vsize = 6 in hsize = 6 in ; axis1 style = 1 width = 2 minor = none order = 0 to 1 by 0.2 ; axis2 style = 1 width = 2 minor = none ; symbol1 i = join ; proc gplot data = power annotate = target ; title2 \"Power for detecting an individual difference of &d\" ; title3 \"Using the &mcp method with FWE=&FWE\" ; title4 \"With &g groups and standard deviation = &s\" ; plot power * n = 1 / vaxis = axis1 haxis = axis2 frame ; run ; quit ; title2 ; title3 ; title4 ; options notes ; %mend ; /* The %SimPower Macro */ /* This macro computes several versions of power for multiple comparisons */ /* procedures, in addition to FWE and directional FWE. */ /*--------------------------------------------------------------*/ /* Name: SimPower */ /* Title: Macro to simulate power of multiple comparisons */ /* using various definitions */ /* Author: Randy Tobias, sasrdt@sas.com */ /* Release: Version 7.01 */ /*--------------------------------------------------------------*/ /* Inputs: */ /* */ /* METHOD = TUKEY, DUNNETT, DUNNETTL, DUNNETTU, or REGWQ */ /* (TUKEY is the default) */ /* */ /* NREP = number of Monte Carlo samples (1000 default) */ /* */ /* N = A Listing of within-group sample sizes */ /* or a single common sample size (no default) */ /* */ /* S = Standard deviation (required) */ /* */ /* FWE = Desired Familywise Error (0.05 default) */ /* */ /* TRUEMEANS = A listing of the true group means (no default) */ /* */ /* SEED = Seed value for random numbers (0 default) */ /* */ /*--------------------------------------------------------------*/ /* Output: This macro simulates multiple power, using the */ /* complete, minimal, and proportional definitions. It also */ /* simulates FWE (ordinary and directional), for a variety */ /* of MCPs. The results are presented in a formatted table. */ /*--------------------------------------------------------------*/ options ls = 76 nodate generic ; /* / Use the binomial estimation options in PROC FREQ to compute / confidence limits for the Complete and Minimal power, which are / proportions; and use PROC MEANS to compute confidence limits for / the proportional power. /---------------------------------------------------------------------*/ %macro EstBin ( Input , Var , Output , Label ); proc freq data =& Input noprint ; table & Var / measures bin out = Freq ; output out =& Output bin ; data _null_ ; set Freq ; if ( _N_ = 1 ) then call symput ( ' First ' , & Var ); data & Output ; set & Output ; keep Quantity Estimate LowerCL UpperCL ; Quantity = & Label ; Estimate = _BIN_ ; LowerCL = L_BIN ; label LowerCL = \"Lower 95% CL\" ; UpperCL = U_BIN ; label UpperCL = \"Upper 95% CL\" ; if ( & First = 0 ) then do ; Estimate = 1 - Estimate ; temp = LowerCL ; LowerCL = UpperCL ; UpperCL = temp ; LowerCL = 1 - LowerCL ; UpperCL = 1 - UpperCL ; end ; run ; %mend ; %macro SimPower ( method = TUKEY , nrep = 1000 , n = , s = , FWE = 0.05 , TrueMeans = = , seed = 0 ); %let method = %upcase ( & method ); /* / Determine the number of groups from the true means. /---------------------------------------------------------------------*/ %let g = 1 ; %do %while ( %length ( %bquote ( %scan ( %bquote ( & TrueMeans ), & g , %bquote ( ',' ))))); %let g = %eval ( & g + 1 ); %end ; %let g = %eval ( & g - 1 ); options nonotes ; /* / Create &nrep random normal data sets using the true means. /---------------------------------------------------------------------*/ %if ( \"%substr(&n,1,1)\" = \"(\" ) %then %do ; data a ; array mu { & g } & TrueMeans ; array _n { & g } & n ; do rep = 1 to & nrep ; do a = 1 to dim ( mu ); do i = 1 to _n { a }; y = mu { a } + & s * rannor ( & seed ); output ; end ; end ; end ; run ; %end ; %else %do ; data a ; array mu { & g } & TrueMeans ; do rep = 1 to & nrep ; do a = 1 to dim ( mu ); do i = 1 to & n ; y = mu { a } + & s * rannor ( & seed ); output ; end ; end ; end ; run ; %end ; /* / Analyze the random data sets. For the methods that return pairwise / results, we put the confidence limits in the CLDiffs dataset; for / REGWQ we assemble all the LINES results in the MCLines dataset. /---------------------------------------------------------------------*/ ods listing close ; %if ( & method = REGWQ ) %then %do ; ods output MCLines ( match_all = mv ) = MCLines ; %end ; %else %do ; ods output CLDiffs = CLDiffs ; %end ; proc glm data = a ; by rep ; class a ; model y = a / nouni ; means a / alpha =& fwe & method %if ( & method ^= REGWQ ) %then cldiff ; ; quit ; ods listing ; %if ( & method = REGWQ ) %then %do ; /* / Combine the MCLines# datasets, putting all the Lines info into a / single L variable. /---------------------------------------------------------------------*/ data temp ; set & mv ; data MCLines ; set temp ( drop = Effect Dependent Method N ); length _Name_ $ 8 ; array Line { 26 }; where ( Mean ^= . _ ); L = ' ' ; do i = 1 to 26 ; substr ( L , i , 1 ) = Line { i }; end ; _Name_ = trim ( left ( ' Level ' || trim ( left ( Level )))); run ; /* / Turn the results from REGWQ into a data set that has a variable / \"Correct\" with the number of correctly rejected hypotheses / for each random data set. Each of the three combined definitions / of power can be computed from this number. /---------------------------------------------------------------------*/ proc sort data = MCLines out = MCLines ; by rep Level ; proc transpose data = MCLines out = Lines ; by rep ; var L ; proc transpose data = MCLines out = Means prefix = Mn ; by rep ; var Mean ; data True ; merge Lines Means ; array Level { & g }; array eq { & g , & g }; array mu { & g } & TrueMeans ; array Mn { & g }; if ( _n_ = 1 ) then do ; ndiff = 0 ; nsame = 0 ; do a1 = 1 to & g - 1 ; do a2 = a1 + 1 to & g ; if ( mu { a1 } ^= mu { a2 }) then ndiff = ndiff + 1 ; if ( mu { a1 } = mu { a2 }) then nsame = nsame + 1 ; end ; end ; retain ndiff nsame ; end ; do i = 1 to & g ; do j = i + 1 to & g ; eq { i , j } = 0 ; do k = 1 to 26 ; lik = substr ( Level { i }, k , 1 ); ljk = substr ( Level { j }, k , 1 ); if (( lik ^= ' ' ) & ( ljk ^= ' ' ) & ( lik = ljk )) then eq { i , j } = 1 ; end ; end ; end ; CorrectA = 0 ; Correct0 = 0 ; DirectionalError = 0 ; do i = 1 to & g - 1 ; do j = i + 1 to & g ; if (( mu { i } ^= mu { j }) & ( ^ eq { i , j })) then CorrectA = CorrectA + 1 ; if (( mu { i } = mu { j }) & eq { i , j } ) then Correct0 = Correct0 + 1 ; if ( (( mu { i } = mu { j }) & ^ eq { i , j } ) | (( mu { i } < mu { j }) & ( ^ eq { i , j }) & ( Mn { i } > Mn { j })) | (( mu { i } > mu { j }) & ( ^ eq { i , j }) & ( Mn { i } < Mn { j }))) then DirectionalError = 1 ; end ; end ; %end ; %else %do ; /* / Turn the results from the tests into a data set that has a / variable \"Correct\" with the number of correctly rejected hypotheses / for each random data set. Each of the three combined definitions / of power can be computed from this number. /---------------------------------------------------------------------*/ proc transpose data = CLDiffs out = Sig prefix = Reject ; var Significance ; by rep ; proc transpose data = CLDiffs out = Comp prefix = Comp ; var Comparison ; by rep ; proc transpose data = CLDiffs out = Diff prefix = Diff ; var Difference ; by rep ; run ; %if ( & method = TUKEY ) %then %let npair = %eval ( & g * ( & g - 1 )); %else %if ( & method = DUNNETT ) %then %let npair = %eval ( & g - 1 ); %else %if ( & method = DUNNETTU ) %then %let npair = %eval ( & g - 1 ); %else %if ( & method = DUNNETTL ) %then %let npair = %eval ( & g - 1 ); data True ; merge Sig ( keep = Reject :) Comp ( keep = Comp :) Diff ( keep = Diff :); array mu { & g } & TrueMeans ; array Comp { & npair }; array Reject { & npair }; array Diff { & npair }; if ( _n_ = 1 ) then do ; ndiff = 0 ; nsame = 0 ; %if ( & method = TUKEY ) %then %do ; do a1 = 1 to & g ; do a2 = 1 to & g ; if ( a1 ^= a2 ) then do ; if ( mu { a1 } ^= mu { a2 }) then ndiff = ndiff + 1 ; if ( mu { a1 } = mu { a2 }) then nsame = nsame + 1 ; end ; end ; end ; %end ; %else %if ( & method = DUNNETT ) %then %do ; do a2 = 2 to & g ; if ( mu { 1 } ^= mu { a2 }) then ndiff = ndiff + 1 ; if ( mu { 1 } = mu { a2 }) then nsame = nsame + 1 ; end ; %end ; %else %if ( & method = DUNNETTL ) %then %do ; do a2 = 2 to & g ; if ( mu { a2 } - mu { 1 } < 0 ) then ndiff = ndiff + 1 ; if ( mu { a2 } - mu { 1 } >= 0 ) then nsame = nsame + 1 ; end ; %end ; %else %if ( & method = DUNNETTU ) %then %do ; do a2 = 2 to & g ; if ( mu { a2 } - mu { 1 } > 0 ) then ndiff = ndiff + 1 ; if ( mu { a2 } - mu { 1 } <= 0 ) then nsame = nsame + 1 ; end ; %end ; retain ndiff nsame ; /* put \"For G=&g and TrueMeans=&TrueMeans, NDiff=\" ndiff; */ end ; CorrectA = 0 ; Correct0 = 0 ; DirectionalError = 0 ; do i = 1 to dim ( Reject ); a1 = 1 * scan ( Comp { i }, 1 , ' ' ); a2 = 1 * scan ( Comp { i }, 3 , ' ' ); if (( mu { a1 } ^= mu { a2 }) & Reject { i }) then CorrectA = CorrectA + 1 ; %if ( & method = DUNNETTL ) %then %do ; if (( mu { a1 } - mu { a2 } >= 0 ) & ^ Reject { i }) then Correct0 = Correct0 + 1 ; if (( mu { a1 } - mu { a2 } >= 0 ) & Reject { i }) then DirectionalError = 1 ; %end ; %else %if ( & method = DUNNETTU ) %then %do ; if (( mu { a1 } - mu { a2 } <= 0 ) & ^ Reject { i }) then Correct0 = Correct0 + 1 ; if (( mu { a1 } - mu { a2 } <= 0 ) & Reject { i }) then DirectionalError = 1 ; %end ; %else %do ; if ( ( mu { a1 } = mu { a2 }) & ^ Reject { i } ) then Correct0 = Correct0 + 1 ; if ( (( mu { a1 } = mu { a2 }) & Reject { i } ) | (( mu { a1 } < mu { a2 }) & Reject { i } & ( Diff { i } > 0 )) | (( mu { a1 } > mu { a2 }) & Reject { i } & ( Diff { i } < 0 ))) then DirectionalError = 1 ; %end ; end ; %end ; call symput ( ' ndiff ' , trim ( left ( put ( ndiff , 20. )))); call symput ( ' nsame ' , trim ( left ( put ( nsame , 20. )))); if ( ndiff ) then do ; CompletePower = CorrectA = ndiff ; MinimalPower = CorrectA > 0 ; ProportionalPower = CorrectA / ndiff ; end ; if ( nsame ) then do ; TrueFWE = 1 - ( Correct0 = nsame ); end ; run ; data Sim ; if ( 0 ); run ; %if ( & ndiff ) %then %do ; % EstBin ( True , CompletePower , CompletePower , ' Complete Power ' ); data Sim ; set Sim CompletePower ; run ; % EstBin ( True , MinimalPower , MinimalPower , ' Minimal Power ' ); data Sim ; set Sim MinimalPower ; run ; proc means data = True noprint ; var ProportionalPower ; output out = ProportionalPower ( keep = Estimate LowerCL UpperCL ) mean = Estimate lclm = LowerCL uclm = UpperCL ; data ProportionalPower ; set ProportionalPower ; keep Quantity Estimate LowerCL UpperCL ; Quantity = ' Proportional Power ' ; data Sim ; set Sim ProportionalPower ; run ; %end ; %if ( & nsame ) %then %do ; % EstBin ( True , TrueFWE , TrueFWE , ' True FWE ' ); data Sim ; set Sim TrueFWE ; run ; %end ; % EstBin ( True , DirectionalError , DirectionalError , ' Directional FWE ' ); data Sim ; set Sim DirectionalError ; run ; data Sim ; set Sim ; CI = \"(\" || put ( LowerCL , 5.3 ) || ',' || put ( UpperCL , 5.3 ) || \")\" ; label CI = \"---95% CI----\" ; run ; options ls = 76 nodate generic ; proc print data = Sim noobs label ; var Quantity Estimate CI ; title1 \"Method=&method, Nominal FWE=&FWE, nrep=&nrep, Seed=&seed\" ; title2 \"True means = &TrueMeans, n=&n, s=&s\" ; run ; title1 ; title2 ; options notes ; %mend ; /* The %PlotSimPower Macro */ /* This macro computes power using various definitions, for various */ /* sample sizes, and plots the results. */ /*--------------------------------------------------------------*/ /* Name: PlotSimPower */ /* Title: Macro to plot the simulated power of multiple */ /* comparisons procedures */ /* Author: Randy Tobias, sasrdt@sas.com */ /* Release: Version 7.01 */ /*--------------------------------------------------------------*/ /* Inputs: */ /* */ /* METHOD = TUKEY, DUNNETT, DUNNETTL, DUNNETTU, or REGWQ */ /* (TUKEY is the default) */ /* */ /* NREP = number of Monte Carlo samples (100 default) */ /* */ /* S = Standard deviation (required) */ /* */ /* FWE = Desired Familywise Error (0.05 default) */ /* */ /* TRUEMEANS = A listing of the true group means (no default) */ /* */ /* SEED = Seed for random numbers (0 default) */ /* */ /* STOP = type/maxpower, specifies the type of power */ /* whether COMPLETE, MINIMAL, or PROPORTIONAL, */ /* and the maximum power to stop simulation. */ /* The default is COMPLETE/0.9 */ /* */ /* TARGET = desired power level (default 0.8) */ /* */ /*--------------------------------------------------------------*/ /* Output: A graph of the power function, indicating the n for */ /* which the target power is acheived. */ /* */ /*--------------------------------------------------------------*/ %macro PlotSimPower ( method = TUKEY , nrep = 100 , s = , FWE = 0.05 , TrueMeans = , seed = 0 , stop = Complete / 0.9 , target = 0.8 ); %let StopType = %scan ( & stop , 1 , '/' ); %let StopValue = %scan ( & stop , 2 , '/' ); data plot ; if ( 0 ); run ; %do n = 2 %to 100 ; % SimPower ( TrueMeans = & TrueMeans , s = & s , n = & n , nrep = & nrep , method = & method , seed = & seed ); data Sim ; set Sim ( where = ( scan ( Quantity , 1 ) = \"%scan(&stop,1,'/')\" )); n = & n ; put \"For N=&n, &StopType power = \" Estimate ; if ( LowerCL >= & StopValue ) then call symput ( 'n' , ' 1001 ' ); data plot ; set plot Sim ; %end ; data probit ; set plot ; Estimate = round ( 100 * Estimate ); c = 100 ; sqrtn = sqrt ( n ); call symput ( ' MaxN ' , trim ( left ( put ( n , best20 .)))); proc probit data = probit outest = ests noprint ; model Estimate / c = sqrtn ; data _null_ ; set ests ; call symput ( ' IParm ' , trim ( left ( put ( Intercept , best20 .)))); call symput ( ' NParm ' , trim ( left ( put ( sqrtn , best20 .)))); run ; %put IParm =& IParm ; %put NParm =& NParm ; data plot ; set plot ; EPower = probnorm ( & IParm + sqrt ( n ) *& NParm ); label EPower = \"Power\" ; run ; data target ; length xsys ysys position $ 1 ; retain hsys color ; sqrtn = ( probit ( & target ) - & IParm ) /& NParm ; ntarget = int ( sqrtn * sqrtn + 1 ); actual = probnorm ( & IParm + sqrt ( ntarget ) *& NParm ); color = ' black ' ; xsys = '1' ; ysys = '2' ; x = 0 ; y = actual ; function = ' MOVE ' ; output ; xsys = '2' ; ysys = '2' ; x = ntarget ; y = actual ; function = ' DRAW ' ; line = 1 ; size = 1 ; output ; xsys = '2' ; ysys = '1' ; x = ntarget ; y = 0 ; function = ' DRAW ' ; line = 1 ; size = 1 ; output ; xsys = '2' ; ysys = '2' ; x = ( & MaxN + ntarget ) / 2 ; y = actual / 2 ; function = ' LABEL ' ; style = ' swissb ' ; text = \"Power(N=\" || trim ( left ( put ( ntarget , best6 .))) || \")\" ; position = '0' ; output ; xsys = '2' ; ysys = '2' ; x = ( & MaxN + ntarget ) / 2 ; y = actual / 2 - 0.1 ; function = ' LABEL ' ; style = ' swissb ' ; text = \" = \" || put ( actual , 5.3 ); position = '0' ; output ; goptions ftext = swissb vsize = 6 in hsize = 6 in ; axis1 style = 1 width = 2 minor = none ; axis2 style = 1 width = 2 minor = none ; symbol1 color = BLACK i = join ; symbol2 color = BLACK i = none v = dot height = 0.5 ; proc gplot data = plot ( where = ( scan ( Quantity , 1 ) = \"&StopType\" )) annotate = target ; title2 \"&StopType power using the &method method with FWE=&FWE\" ; title4 \"With true means &TrueMeans and standard deviation = &s\" ; plot EPower * n = 1 Estimate * n = 2 / vaxis = axis1 haxis = axis2 frame overlay ; run ; quit ; title2 ; title3 ; title4 ; %mend ; /* The %BegGab Macro */ /* This macro performs multiple comparisons using the Begun and Gabriel */ /* (1981) method. */ /*--------------------------------------------------------------*/ /* Name: BegGab */ /* Title: Begun and Gabriel Closed Testing Procedure */ /* Author: Dror Rom, rom@prosof.com */ /* Reference: Begun, J., and Gabriel, K. R. (1981). Closure of */ /* the Newman-Keuls multiple comparisons procedure.*/ /* Journal of the American Statistical Association,*/ /* 76, 241-245. */ /* Release: Version 6.11 */ /*--------------------------------------------------------------*/ /* Input: */ /* */ /* DATASET= the SAS data set containing the data to be */ /* analyzed (required) */ /* */ /* GROUPS= the grouping variable (required) */ /* */ /* RESPONSE= the response variable (required) */ /* */ /* FWE= the level of significance for comparisons */ /* among the means. The default is 0.05. */ /* */ /* Output: */ /* */ /* The output dataset contains one observation for each */ /* pairwise comparison in the dataset. The output dataset */ /* contains the following variables: */ /* */ /* GRi - The index of a (smaller) mean being compared */ /* */ /* Grj - The index of a (larger) mean being compared */ /* */ /* PVALUE - The P-value for the comparison */ /* */ /* DECISION - Reject or Retain the corresponding hypothesis */ /*--------------------------------------------------------------*/ %macro BegGab ( dataset = , groups = , response = , FWE = 0.05 ); options nonotes ; proc sort data =& dataset ; by & groups ; proc means data =& dataset noprint ; var & response ; by & groups ; output out = bg mean = mean std = sd n = n ; proc print ; var trt mean sd n ; data bg ; set bg ; samps = n ; drop n ; proc sort ; by mean ; proc means noprint ; var samps ; output out = b n = m ; data a ; set b ; call symput ( 'm' , m ); run ; %let mm = %eval ( & m *& m ); %let m = %eval ( & m ); data a ; set bg ; i = _N_ ; grp = trt ; trt = i ; m =& m ; array t ( & m ); array gr ( & m ) $ ; t ( i ) = trt ; gr ( i ) = grp ; array meanss ( & m ); meanss ( i ) = mean ; array stds ( & m ); stds ( i ) = sd ; array samp ( & m ); samp ( i ) = samps ; array p ( & m , & m ); retain meanss1 - meanss & m ; retain stds1 - stds & m ; retain samp1 - samp & m ; retain p1 - p & mm ; retain t1 - t & m ; retain gr1 - gr & m ; df = 0 ; mse = 0 ; do i = 1 to m ; mse = mse + stds ( i ) ** 2 * ( samp ( i ) - 1 ); df = df + samp ( i ) - 1 ; end ; mse = mse / df ; do i = 1 to ( m - 1 ); do j = ( i + 1 ) to m ; msamp = 0 ; do k = i to j ; msamp = msamp + samp ( k ); end ; msamp = msamp / ( j - i + 1 ); tr = j - i + 1 ; if ( tr > 2 ) then do ; tval = abs (( meanss ( j ) - meanss ( i ))) / (( mse / 2 ) * (( 1 / samp ( i )) + ( 1 / samp ( j )))) ** 0.5 ; if not ( tval = '.' ) then p ( i , j ) = 1 - probmc ( ' RANGE ' , tval ,., df , tr ); end ; else do ; tval = abs (( meanss ( j ) - meanss ( i ))) / ( mse * (( 1 / ( samp ( i ))) + ( 1 / ( samp ( j ))))) ** 0.5 ; if not ( tval = '.' ) then p ( i , j ) = 2 * ( 1 - probt ( tval , df )); end ; end ; end ; run ; data a ; set a ; by j ; if last . j ; data a ; set a ; j = m ; array p { & m , & m }; array g { & m }; array dec { & m , & m } $ ; do i = 1 to m ; g ( i ) = 0 ; end ; do i = 1 to ( m - 1 ); do j = ( i + 1 ) to m ; dec ( i , j ) = '.' ; end ; end ; do i = 1 to ( m - 1 ); do j = ( i + 1 ) to m ; if ( not ( dec ( i , j ) = ' Retain ' )) then do ; if p ( i , j ) >& FWE then do ; dec ( i , j ) = ' Retain ' ; do k = i to ( j - 1 ); do l = ( k + 1 ) to j ; dec ( k , l ) = ' Retain ' ; end ; end ; end ; else if ( p ( i , j ) <= ( & FWE * ( j - i + 1 ) / m )) then dec ( i , j ) = ' Reject ' ; else if ( 0.05 >= p ( i , j ) > ( & FWE * ( j - i + 1 )) / m ) then do ; if ((( i > 2 ) and ( p ( 1 ,( i - 1 )) <= ( & FWE * ( i - 1 )) / m )) and (( j < ( m - 1 )) and ( p (( j + 1 ), m ) <= ( & FWE * ( m - j )) / m ))) then do ; dec ( i , j ) = ' Reject ' ; do k = 1 to ( i - 2 ); do l = ( k + 1 ) to ( i - 1 ); if ( p ( k , l ) > ( & FWE * ( l - k + 1 ) / m )) then dec ( i , j ) = ' Retain ' ; end ; end ; do k = ( j + 1 ) to ( m - 1 ); do l = ( k + 1 ) to m ; if ( p ( k , l ) > ( & FWE * ( l - k + 1 ) / m )) then dec ( i , j ) = ' Retain ' ; end ; end ; end ; if ((( i > 2 ) and ( p ( 1 ,( i - 1 )) <= ( & FWE * ( i - 1 )) / m )) and (( j >= ( m - 1 )))) then do ; dec ( i , j ) = ' Reject ' ; do k = 1 to ( i - 2 ); do l = ( k + 1 ) to ( i - 1 ); if ( p ( k , l ) > ( & FWE * ( l - k + 1 ) / m )) then dec ( i , j ) = ' Retain ' ; end ; end ; end ; if ((( i <= 2 )) and (( j < ( m - 1 )) and ( p (( j + 1 ), m ) <= ( & FWE * ( m - j )) / m ))) then do ; dec ( i , j ) = ' Reject ' ; do k = ( j + 1 ) to ( m - 1 ); do l = ( k + 1 ) to m ; if ( p ( k , l ) > ( & FWE * ( l - k + 1 ) / m )) then dec ( i , j ) = ' Retain ' ; end ; end ; end ; if (( i <= 2 ) and ( j >= ( m - 1 ))) then dec ( i , j ) = ' Reject ' ; if ( not ( dec ( i , j ) = ' Reject ' )) then do k = i to ( j - 1 ); do l = ( k + 1 ) to j ; dec ( k , l ) = ' Retain ' ; end ; end ; end ; end ; end ; end ; run ; data a1 ; set a ; array t { & m }; array p { & m , & m }; array dec { & m , & m } $ ; array gr ( & m ) $ ; do i = 1 to m ; do j = 1 to m ; ti = t ( i ); tj = t ( j ); gri = gr ( i ); grj = gr ( j ); decision = dec ( i , j ); pvalue = p ( i , j ); if not ( pvalue = '.' ) then output ; end ; end ; title1 ' ' ; title2 ' Begun - Gabriel Closed Testing Procedure ' ; title3 \"Based on the Range Statistic, FWE=&FWE\" ; TITLE4 ' ' ; DATA _NULL_ ; FILE PRINT ; set a1 end = eof ; IF _N_ = 1 THEN DO ; PUT @ 20 ' t_i ' @ 26 '-' @ 30 ' t_j ' @ 37 ' P - VALUE ' @ 48 ' DECISION ' ; PUT @ 20 36 * '-' ; END ; if pvalue >= 0.0001 then put @ 20 gri @ 26 '-' @ 30 grj @ 37 pvalue 6.4 @ 48 decision ; else put @ 20 gri @ 26 '-' @ 30 grj @ 37 ' < .0001 ' @ 48 decision ; if EOF = 1 then do ; PUT @ 20 36 * '-' ; end ; RUN ; data b ; set a ; array dec ( & m , & m ) $ ; array g ( & m ); array gr ( & m ) $ ; do i = 1 to ( m - 1 ); do j = ( i + 1 ) to m ; if (( dec ( i , j ) = ' Reject ' )) then g ( j ) = g ( j ) + 1 ; end ; end ; data b1 ; set b ; array g ( & m ); array gr ( & m ) $ ; do j = 1 to m ; if ( j = 1 ) then hfb = 0 ; else hfb = g ( j ); output ; end ; data c ; set b ; array dec ( & m , & m ) $ ; array g ( & m ); do i = 1 to ( m - 1 ); do j = ( i + 1 ) to m ; if (( dec ( i , j ) = ' Retain ' )) then do l = i to j ; g ( l ) = g ( j ); end ; end ; end ; data c1 ; set c ; array g ( & m ); array gr ( & m ) $ ; do j = 1 to m ; tj = gr ( j ); if ( j = 1 ) then hfa = 0 ; else hfa = g ( j ); output ; end ; data d ; merge b1 c1 ; by j ; keep hfa hfb j tj ; goptions colors = ( black ) cback = white ; proc gplot ; label tj = ' Treatment ' ; title1 ' ' ; title2 ' Begun - Gabriel Closed Testing Procedure ' ; title3 \"Based on the Range Statistic, FWE=&FWE\" ; title4 ' Schematic Plot of Significant Differences ' ; title5 ' ' ; symbol1 i = join value = dot ; symbol2 i = join value = dot ; axis1 label = ( angle = 90 rotate = 0 ' Response ' ) value = none major = none minor = none ; axis2 minor = none ; plot ( hfa hfb ) * tj / vaxis = axis1 haxis = axis2 overlay frame ; run ; options notes ; % MEND BEGGAB ; /* The %RCC Macro */ /* This macro performs closed dose-response tests using the Rom, */ /* Costello, and Connell (1994) method. */ /*--------------------------------------------------------------*/ /* Name: RCC */ /* Title: Rom, Costello, and Connell Closed Testing */ /* Procedure for Dose Response analysis */ /* Author: Dror Rom, rom@prosof.com */ /* Reference: Rom, D. M., Costello, R. and Connell, L. (1994). */ /* On closed test procedures for dose response */ /* analysis. Statistics in Medicine, 13, 1583-1596. */ /* Release: Version 6.11 */ /*--------------------------------------------------------------*/ /* Input: */ /* */ /* DATASET= the SAS data set containing the data to be */ /* analyzed (required) */ /* */ /* GROUPS= the grouping variable (required) */ /* */ /* RESPONSE= the response variable (required) */ /* */ /* FWE= the level of significance for comparisons */ /* among the means. The default is 0.05. */ /* */ /* Output: */ /* */ /* The output dataset contains one observation for each */ /* trend test among successive means. The output dataset */ /* contains the following variables: */ /* */ /* Di - The index of a (smaller) dose being compared */ /* */ /* Dj - The index of a (larger) dose being compared */ /* */ /* PVALUE - The P-value for the comparison */ /* */ /* DECISION - Reject or Retain the corresponding hypothesis */ /*--------------------------------------------------------------*/ % MACRO rcc ( dataset = , groups = , response = , FWE = 0.05 ); proc means data =& dataset noprint ; var & response ; by & groups ; output out = rcc n = samps std = sd mean = mean ; data meanss ; set rcc ; proc sort ; by & groups ; proc means noprint ; var samps ; output out = b n = m ; data a ; set b ; call symput ( 'm' , m ); run ; %let mm = %eval ( & m *& m ); %let m = %eval ( & m ); data a ; set meanss ; i = _N_ ; m =& m ; array d ( & m ); d ( i ) =& groups ; array meanss ( & m ); meanss ( i ) = mean ; array stds ( & m ); stds ( i ) = sd ; array samp ( & m ); samp ( i ) = samps ; array p ( & m , & m ); retain meanss1 - meanss & m ; retain stds1 - stds & m ; retain samp1 - samp & m ; retain p1 - p & mm ; retain d1 - d & m ; df = 0 ; mse = 0 ; do i = 1 to m ; mse = mse + stds ( i ) ** 2 * ( samp ( i ) - 1 ); df = df + samp ( i ) - 1 ; end ; mse = mse / df ; do i = 1 to ( m - 1 ); do j = ( i + 1 ) to m ; expec = 0 ; do k = i to j ; expec = expec + k ; end ; expec = expec / ( j - i + 1 ); num = 0 ; den = 0 ; do k = i to j ; num = num + ( k - expec ) * meanss ( k ); den = den + ( k - expec ) ** 2 / samp ( k ); end ; t = num / ( mse * den ) ** 0.5 ; p ( i , j ) = 1 - probt ( t , df ); end ; end ; run ; data a ; set a ; by j ; if last . j ; data a ; set a ; alpha = 0.05 ; j = m ; array p { & m , & m }; array g { & m }; array dec { & m , & m } $ ; do i = 1 to m ; g ( i ) = 0 ; end ; do i = 1 to ( m - 1 ); do j = ( i + 1 ) to m ; dec ( i , j ) = '.' ; end ; end ; do i = 1 to ( m - 1 ); do j = ( i + 1 ) to m ; if ( not ( dec ( i , j ) = ' Retain ' )) then do ; if p ( i , j ) > alpha then do ; dec ( i , j ) = ' Retain ' ; do k = i to ( j - 1 ); do l = ( k + 1 ) to j ; dec ( k , l ) = ' Retain ' ; end ; end ; end ; else if ( p ( i , j ) <= ( alpha * ( j - i + 1 ) / m )) then dec ( i , j ) = ' Reject ' ; else if ( 0.05 >= p ( i , j ) > ( alpha * ( j - i + 1 )) / m ) then do ; if ((( i > 2 ) and ( p ( 1 ,( i - 1 )) <= ( alpha * ( i - 1 )) / m )) and (( j < ( m - 1 )) and ( p (( j + 1 ), m ) <= ( alpha * ( m - j )) / m ))) then do ; dec ( i , j ) = ' Reject ' ; do k = 1 to ( i - 2 ); do l = ( k + 1 ) to ( i - 1 ); if ( p ( k , l ) > ( alpha * ( l - k + 1 ) / m )) then dec ( i , j ) = ' Retain ' ; end ; end ; do k = ( j + 1 ) to ( m - 1 ); do l = ( k + 1 ) to m ; if ( p ( k , l ) > ( alpha * ( l - k + 1 ) / m )) then dec ( i , j ) = ' Retain ' ; end ; end ; end ; if ((( i > 2 ) and ( p ( 1 ,( i - 1 )) <= ( alpha * ( i - 1 )) / m )) and (( j >= ( m - 1 )))) then do ; dec ( i , j ) = ' Reject ' ; do k = 1 to ( i - 2 ); do l = ( k + 1 ) to ( i - 1 ); if ( p ( k , l ) > ( alpha * ( l - k + 1 ) / m )) then dec ( i , j ) = ' Retain ' ; end ; end ; end ; if ((( i <= 2 )) and (( j < ( m - 1 )) and ( p (( j + 1 ), m ) <= ( alpha * ( m - j )) / m ))) then do ; dec ( i , j ) = ' Reject ' ; do k = ( j + 1 ) to ( m - 1 ); do l = ( k + 1 ) to m ; if ( p ( k , l ) > ( alpha * ( l - k + 1 ) / m )) then dec ( i , j ) = ' Retain ' ; end ; end ; end ; if (( i <= 2 ) and ( j >= ( m - 1 ))) then dec ( i , j ) = ' Reject ' ; if ( not ( dec ( i , j ) = ' Reject ' )) then do k = i to ( j - 1 ); do l = ( k + 1 ) to j ; dec ( k , l ) = ' Retain ' ; end ; end ; end ; end ; end ; end ; data a1 ; set a ; array d { & m }; array p { & m , & m }; array dec { & m , & m } $ ; do i = 1 to m ; do j = 1 to m ; di = d ( i ); dj = d ( j ); decision = dec ( i , j ); pvalue = p ( i , j ); if not ( pvalue = '.' ) then output ; end ; end ; title1 ' ' ; title2 ' ROM - COSTELLO - CONNELL CLOSED TESTING PROCEDURE ' ; title3 ' FOR UPPER - TAILED ' ; TITLE4 ' DOSE RESPONSE ANALYSIS ' ; title5 ' ' ; DATA _NULL_ ; FILE PRINT ; set a1 end = eof ; IF _N_ = 1 THEN DO ; PUT @ 15 ' d_i ' @ 21 '-' @ 25 ' d_j ' @ 32 ' P - VALUE ' @ 43 ' DECISION ' ; PUT @ 15 36 * '-' ; END ; if pvalue >= 0.0001 then put @ 15 di @ 21 '-' @ 25 dj @ 32 pvalue 6.4 @ 43 decision ; else put @ 15 di @ 21 '-' @ 25 dj @ 32 ' < .0001 ' @ 43 decision ; if EOF = 1 then do ; PUT @ 15 36 * '-' ; put @ 15 ' ALPHA = ' @ 23 alpha ; end ; RUN ; data b ; set a ; array dec ( & m , & m ) $ ; array g ( & m ); do i = 1 to ( m - 1 ); do j = ( i + 1 ) to m ; if (( dec ( i , j ) = ' Reject ' )) then g ( j ) = g ( j ) + 1 ; end ; end ; data b1 ; set b ; array g ( & m ); do j = 1 to m ; if ( j = 1 ) then hfb = 0 ; else hfb = g ( j ); output ; end ; data c ; set b ; array dec ( & m , & m ) $ ; array g ( & m ); do i = 1 to ( m - 1 ); do j = ( i + 1 ) to m ; if (( dec ( i , j ) = ' Retain ' )) then do l = i to j ; g ( l ) = g ( j ); end ; end ; end ; data c1 ; set c ; array g ( & m ); array d ( & m ); do j = 1 to m ; dj = d ( j ); if ( j = 1 ) then hfa = 0 ; else hfa = g ( j ); output ; end ; data d ; merge b1 c1 ; by j ; keep hfa hfb j dj ; goptions colors = ( black ) cback = white ; proc gplot ; label dj = ' TREATMENT ' ; title1 ' ' ; title2 ' ROM - COSTELLO - CONNELL CLOSED TESTING PROCEDURE ' ; title3 ' ' ; title4 ' SCHEMATIC PLOT OF THE DOSE RESPONSE ' ; title5 ' ' ; symbol1 i = join value = dot ; symbol2 i = join value = dot ; axis1 label = ( angle = 90 rotate = 0 ' Response ' ) value = none major = none minor = none ; axis2 minor = none ; plot ( hfa hfb ) * dj / vaxis = axis1 haxis = axis2 overlay frame ; run ; % MEND rcc ; /* The %Williams Macro */ /* This macro performs a step-down test for the ``Minimal Effective Dose\" */ /* using Williams (1971, 1972) method. */ /*---------------------------------------------------------------*/ /* Name: Williams */ /* Title: Williams step-down test for minimal effective dose */ /* Author: Dror Rom, rom@prosof.com */ /* Reference: Williams, D. A. (1971). A test for difference */ /* between treatment means when several dose levels */ /* are compared with a zero dose level. */ /* Biometrics, 27, 103-117. */ /* Williams, D. A. (1972). The comparison of several */ /* dose levels with a zero dose control. */ /* Biometrics, 28, 519-531 */ /* Release: Version 6.12 */ /*---------------------------------------------------------------*/ /* Input: */ /* */ /* DATASET= the SAS data set containing the data to be */ /* analyzed (required) */ /* */ /* TRT= the grouping variable (required) */ /* */ /* RESPONSE= the response variable (required) */ /* */ /* */ /* Output: */ /* */ /* The output dataset contains one observation for each */ /* pairwise comparison in the dataset. The output dataset */ /* contains the following variables: */ /* */ /* i - The index of a dose being compared with the */ /* control (zero dose) */ /* */ /* M1 - The mean of the control */ /* */ /* M - The mean of the dose being compared with the */ /* control */ /* */ /* W - The Williams statistics for the dose being */ /* compared with the control */ /* */ /* PV - P-value for the comparison */ /*---------------------------------------------------------------*/ %macro Williams ( dataset = , trt = , response = ); options nonotes ; ************************ Get Stats ***************** ; proc glm data =& dataset noprint outstat = stat ; class & trt ; model & response =& trt ; run ; data stat ; set stat ; keep _source_ mse ss df ; if _source_ = ' ERROR ' ; mse = ss / df ; proc means data =& dataset noprint ; var & response ; by & trt ; output out = means mean = mean n = samp ; data means ; set means ; proc means noprint ; var samp ; output out = a n = m ; data a ; set a ; keep m ; data b ; merge a stat ; d = df ; e = mse ; call symput ( 'm' , m ); run ; data b ; set b ; %let m = %eval ( & m ); proc sort ; by m ; data c ; set means ; m =& m ; array means ( & m ); array samps ( & m ); means ( & trt ) = mean ; samps ( & trt ) = samp ; retain means1 - means & m ; retain samps1 - samps & m ; data c ; set c ; if & trt = m ; data d ; merge c b ; array means ( & m ); array samps ( & m ); array will ( & m ); *************** Pool adjacent violators ********** ; control = means ( 1 ); do j = 1 to m - 1 ; if ( means ( j ) > means ( j + 1 )) then do ; emean = means ( j + 1 ); l = 0 ; do k = j to 1 by - 1 ; l = l + 1 ; if emean < means ( k ) then do ; emean = emean * l / ( l + 1 ) + means ( k ) / ( l + 1 ); do r = k to j + 1 ; means ( r ) = emean ; end ; end ; end ; end ; do k = 2 to m ; will ( k ) = ( means ( k ) - control ) / ( mse * (( 1 / ( samps ( 1 )) + ( 1 / ( samps ( k )))))) ** 0.5 ; end ; end ; *************** Get P - values ********** ; data williams ; set d ; array means ( & m ); array will ( & m ); array p ( & m ); do i = 2 to m ; p ( i ) = 1 - probmc ( \"williams\" , will ( i ),., df , i - 1 ); *** P - values are based on number of doses *** ; end ; data williams ; set williams ; format means1 - means & m f4 .3 will2 - will & m f4 .3 p2 - p & m f5 .4 ; proc print ; var means1 - means & m will2 - will & m p2 - p & m ; run ; options notes ; % MEND Williams ; /* The %SimTests Macro */ /* This macro performs closed multiple tests for general functions */ /* parameters, incorporating logical constraints and correlations using */ /* Westfall's (1997) method. */ /*--------------------------------------------------------------*/ /* Name: SimTests */ /* Title: Simultaneous Hypothesis Tests for General Linear */ /* Functions, using Correlations and Constraints */ /* Author: Peter Westfall, westfall@ttu.edu */ /* Reference: Westfall, P.H. (1997). Multiple testing of */ /* general contrasts using logical constraints and */ /* correlations. JASA 92, 299-306 */ /* Release: Version 7.01 */ /*--------------------------------------------------------------*/ /* Inputs: */ /* */ /* NSAMP = simulation size, with 20000 as default */ /* */ /* SEED = random number seed, with 0 (clock time) */ /* as default */ /* */ /* SIDE = U, L or B, for upper-tailed, lower-tailed */ /* or two-tailed, respectively. SIDE=B is default. */ /* */ /* TYPE = LOGICAL or FREE, for logically constrained or */ /* unconstrained tests, respectively. TYPE=FREE */ /* is the default. */ /* */ /* Additionally, %SimTests requires two further macros to be */ /* defined that use SAS/IML to construct the estimates and */ /* the contrasts of interest. In particular, make sure the */ /* following two macros are defined before invoking */ /* %SimTests: */ /* */ /* %Estimate: Uses SAS/IML code to define */ /* EstPar - (column) vector of estimated parameters */ /* Cov - covariance matrix for the for the estimates */ /* df - error degrees of freedom; set to 0 for */ /* asymptotic analysis */ /* */ /* %Contrasts: Uses SAS/IML code to define */ /* C - matrix whose columns define the contrasts of */ /* interest between the parameters */ /* CLab - (column) character vector whose elements */ /* label the respective contrasts in C */ /* */ /* You can either define these macros directly, or use the */ /* %MakeGLMStats macro to define them. */ /* */ /*--------------------------------------------------------------*/ /* Output: */ /* The primary output is a dataset with one observation for */ /* each contrast and the following variables: */ /* */ /* Contrast - contrast label */ /* Estimate - contrast estimated value */ /* StdErr - standard error of estimate */ /* tValue - normalized estimate, Estimate/StdErr */ /* RawP - non-multiplicity-adjusted p-value */ /* BonP - Bonferroni multiplicity-adjusted p-value */ /* BonMult - corresponding Bonferroni multiplier */ /* AdjP - stepwise multiplicity-adjusted p-value */ /* SEAdjP - standard error for AdjP */ /* */ /* This dataset is also displayed as a formatted table, using */ /* the ODS system. */ /* */ /* This macro also produces a data set called SUBSETS that has */ /* has a variable STEPJ indicating the particular (ordered) */ /* hypothesis being considered; as well as variables */ /* (TEST1--TESTk) identifying the particular subset hypotheses */ /* that contain the hypothesis indicated by the STEPJ variable,*/ /* that do not contradict falsehood of the previous hypotheses.*/ /* The order of the TEST1--TESTk variables is from most to */ /* least significant. */ /*--------------------------------------------------------------*/ %macro SimTests ( nsamp = 20000 , seed = 0 , side = B , type = FREE , options = ); %global ANORM ; options nonotes ; proc iml ; % Estimates ; if ( df <= 0 ) then call symput ( ' ANORM ',' 1 ' ); else call symput ( ' ANORM ',' 0 ' ); % Contrasts ; C = C ` ; side = \"&side\" ; type = \"&type\" ; if side = \"U\" then C =- C ; EstCont = C * EstPar ; CovCont = C * Cov * C ` ; SECont = sqrt ( vecdiag ( CovCont )); tvals = EstCont / SECont ; if side = \"B\" then do ; tvals = - abs ( tvals ); if df = 0 then pvals = 2 * probnorm ( tvals ); else pvals = 2 * probt ( tvals , df ); end ; else do ; if df = 0 then pvals = probnorm ( tvals ); else pvals = probt ( tvals , df ); end ; k = nrow ( c ); nests = nrow ( EstPar ); call symput ( 'k' , char ( k )); call symput ( 'g' , char ( nests )); r = rank ( Pvals ` ); ir = r ; ir [, r ] = 1 : nrow ( PVals ); origord = ir ` ; cord = c [ ir ,]; clabord = clab [ ir ,]; tvalsord = tvals [ ir ,]; pvalsord = pvals [ ir ,]; ccord = CovCont [ ir , ir ]; crrccord = inv ( sqrt ( diag ( ccord ))) * ccord * inv ( sqrt ( diag ( ccord ))); ct = t ( cord ); start ztrail ; ii = 1 ; zz = kk ; do while ( mod ( zz , 2 ) = 0 ); ii = ii + 1 ; zz = zz / 2 ; end ; finish ; if type = \"LOGICAL\" then do ; do iout = 1 to k - 2 ; limit = 2 ** ( k - iout - 1 ); in = J ( k - iout - 1 , 1 , 0 ); zero = J ( k , 1 , 0 ); in1 = zero ; y = ct [, 1 : iout ]; do kk = 1 to limit ; if kk = limit then in = j ( k - iout - 1 , 1 , 0 ); else do ; run ztrail ; in [ ii ,] =^ in [ ii ,]; end ; locbin = j ( iout , 1 , 0 ) // {1} // in; loc1 = loc ( locbin ); x = ct [, loc1 ]; res = y - x * ginv ( x ` * x ) * x ` * y ; ssemat = vecdiag ( res ` * res ); if ssemat > .00000001 then do ; if in1 = 0 then in1 = locbin ; else do ; check = in1 - repeat ( locbin , 1 , ncol ( in1 )); diff = check [ <> ,] - check [ >< ,]; if min ( diff ) = 2 then in1 = in1 || locbin ; else do ; mindx = diff [, >:< ]; if check [ + , mindx ] =- 1 then in1 [, mindx ] = locbin ; end ; end ; end ; end ; in1 = in1 ` ; ncont = nrow ( in1 ); in1 = j ( ncont , 1 , iout + 1 ) || in1 ; if iout = 1 then inbig = in1 ; else inbig = inbig //in1; end ; end ; big = j ( 1 , k + 1 , 1 ) //inbig; lastset = j ( 1 , 1 , k ) || j ( 1 , k - 1 , 0 ) || { 1 }; big = big //lastset; stepj = big [, 1 ]; if type = \"FREE\" then do ; stepj = 1 : k ; stepj = stepj ` ; end ; SubsetK = big [, 2 : ncol ( big )]; if type = \"FREE\" then do ; m = j ( 1 , k , 1 ); do i = 2 to k ; r = j ( 1 , i - 1 , 0 ) || j ( 1 , k - i + 1 , 1 ); m = m //r; end ; SubsetK = m ; end ; subsets = subsetk || stepj ; create subsets var (( \"t1\" : \"t&k\" ) || \"StepJ\" ); append from subsets ; nbig = nrow ( big ); if type = \"LOGICAL\" then des = design ( big [, 1 ]); else des = design ( stepj ); if type = \"LOGICAL\" then contonly = big [, 2 : k + 1 ]; else contonly = subsetk ; tcmpr = des * tvalsord ; h = root ( crrccord ); if type = \"FREE\" then nbig = k ; count = j ( nbig , 1 , 0 ); countc = count ; countc2 = count ; contonly = (( contonly + des ) > 0 ); if type = \"LOGICAL\" then totals = contonly [, + ]; else do ; totals = k : 1 ; totals = totals ` ; end ; if side = \"B\" then do ; if df = 0 then bon = 2 * ( probnorm ( tcmpr )) #totals ; else bon = 2 * ( probt ( tcmpr , df )) #totals ; end ; else do ; if df = 0 then bon = ( probnorm ( tcmpr )) #totals ; else bon = ( probt ( tcmpr , df )) #totals ; end ; if & nsamp > 0 then do ; file log ; do isim = 1 to & nsamp ; if mod ( isim , 5000 ) = 0 then put isim ; z = h ` * rannor ( j ( k , 1 , & seed )); if df = 0 then s = 1 ; else do ; chi = 2 * rangam ( & seed , df / 2 ); s = sqrt ( chi / df ); end ; t = z / s ; if side = \"B\" then t = - abs ( t ); try = ( contonly # ( j ( nbig , 1 , 1 ) * t ` )); try1 = ( 10000 * ( try = 0 )) + try ; maxind = ( try1 [, >< ] <= tcmpr ); sumind = ( try1 < (( tcmpr ) * j ( 1 , ncol ( try ), 1 )))[, + ]; countc = countc + sumind ; countc2 = countc2 + sumind ## 2 ; count = count + maxind ; end ; smpl = count /& nsamp ; cv = bon + smpl - countc /& nsamp ; avec = countc /& nsamp ; avec2 = countc2 /& nsamp ; varx = smpl # ( j ( nrow ( smpl ), 1 , 1 ) - smpl ); varz = avec2 - avec ## 2 + smpl - smpl ## 2 - 2 * avec # ( j ( nrow ( smpl ), 1 , 1 ) - smpl ); covzx = ( avec - smpl ) # ( j ( nrow ( smpl ), 1 , 1 ) - smpl ); a1 = varz + covzx ; a2 = varx + covzx ; atot = a1 + a2 ; atot = ( atot = 0 ) + atot ; a1 = a1 / atot ; a2 = a2 / atot ; atot = a1 + a2 ; a2 = a2 + ( atot = 0 ); gls = a1 #smpl + a2 #cv ; stdgls = sqrt ( abs (( a1 ## 2 #varx + a2 ## 2 #varz - 2 * a1 #a2#covzx ) /& nsamp )); stdsmpl = sqrt ( varx /& nsamp ); stdcv = sqrt ( abs ( varz /& nsamp )); glsbig = des # ( gls * j ( 1 , k , 1 )); glsp = glsbig [ <> ,]; glsin = glsbig [ <:> ,]; stdgls = stdgls [ glsin ,]; glsptry = glsp ` ; smplbig = des # ( smpl * j ( 1 , k , 1 )); smplp = smplbig [ <> ,]; smplin = smplbig [ <:> ,]; stdsmpl = stdsmpl [ smplin ,]; cvbig = des # ( cv * j ( 1 , k , 1 )); cvp = cvbig [ <> ,]; cvin = cvbig [ <:> ,]; stdcv = stdcv [ cvin ,]; do i = 2 to k ; if smplp [ 1 , i ] < smplp [ 1 , i - 1 ] then do ; smplp [ 1 , i ] = smplp [ 1 , i - 1 ]; stdsmpl [ i , 1 ] = stdsmpl [ i - 1 , 1 ]; end ; if cvp [ 1 , i ] < cvp [ 1 , i - 1 ] then do ; cvp [ 1 , i ] = cvp [ 1 , i - 1 ]; stdcv [ i , 1 ] = stdcv [ i - 1 , 1 ]; end ; if glsp [ 1 , i ] < glsp [ 1 , i - 1 ] then do ; glsp [ 1 , i ] = glsp [ 1 , i - 1 ]; stdgls [ i , 1 ] = stdgls [ i - 1 , 1 ]; end ; end ; adjpsmpl = smplp ` ; adjpcv = cvp ` ; adjpgls = glsp ` ; adjp = adjpgls ; SEAdjp = stdgls # ( stdgls > .00000001 ); end ; bonbig = des # ( bon * j ( 1 , k , 1 )); bonp = bonbig [ <> ,]; bonmult = bonp ` / pvalsord ; do i = 2 to k ; if bonp [ 1 , i ] < bonp [ 1 , i - 1 ] then bonp [ 1 , i ] = bonp [ 1 , i - 1 ]; end ; rawp = pvalsord ; estimate = EstCont [ ir ,]; if side = \"U\" then estimate =- estimate ; stderr = SECont [ ir ,]; contrast = cord ; if side = \"U\" then contrast =- contrast ; adjpbon = bonp ` ; adjpbon = ( adjpbon < 1 ) #adjpbon + ( adjpbon >= 1 ); if & nsamp > 0 then do ; outres = origord || contrast || estimate || stderr || rawp || bonmult || adjpbon || adjp || SEAdjp ; create SimTestOut var ( \"OrigOrd\" || ( \"Est1\" : \"Est&g\" ) || \"Estimate\" || \"StdErr\" || \"RawP\" || \"BonMult\" || \"BonP\" || \"AdjP\" || \"SEAdjP\" ); append from outres ; end ; else do ; outres = origord || contrast || estimate || stderr || rawp || bonmult || adjpbon ; create SimTestOut var ( \"OrigOrd\" || ( \"Est1\" : \"Est&g\" ) || \"Estimate\" || \"StdErr\" || \"RawP\" || \"BonMult\" || \"BonP\" ); append from outres ; end ; create labels from clabord ; append from clabord ; data SimTestOut ; merge SimTestOut labels ; rename col1 = Contrast ; proc sort data = SimTestOut out = SimTestOut ; by origord ; data SimTestOut ; set SimTestOut ; drop origord ; run ; %if ( ^ %index ( %upcase ( & options ), NOPRINT )) %then %do ; proc template ; delete MCBook . SimTests ; define table MCBook . SimTests ; column Contrast Estimate StdErr RawP BonP AdjP SEAdjP ; define header h1 ; spill_margin ; %if ( %upcase ( & type ) = LOGICAL ) %then %do ; text \"Logically Constrained (Restricted Combinations) Step-Down Tests\" ; %end ; %else %do ; text \"Unconstrained (Free Combinations) Step-Down Tests\" ; %end ; %if ( ^& ANORM ) %then %do ; space = 1 ; %end ; end ; %if ( & ANORM ) %then %do ; define header h2 ; text \"Asymptotic Normal Approximations\" ; space = 1 ; end ; %end ; define column Contrast ; header = \"Contrast\" ; end ; define column Estimate ; header = \"Estimate\" format = D8 . space = 1 ; translate _val_ = . _ into '' ; end ; define column StdErr ; header = \"Standard Error\" format = D8 .; translate _val_ = . _ into '' ; end ; %if ( & nsamp ) %then %let LastPValCol = AdjP ; %else %let LastPValCol = BonP ; %if ( & side = B ) %then %do ; define header ProbtHead ; text \" Pr > |t| \" ; start = Rawp end =& LastPValCol just = c expand = '-' ; end ; %end ; %else %if ( & side = L ) %then %do ; define header ProbtHead ; text \" Pr < t \" ; start = Rawp end =& LastPValCol just = c expand = '-' ; end ; %end ; %else %do ; define header ProbtHead ; text \" Pr > t \" ; start = Rawp end =& LastPValCol just = c expand = '-' ; end ; %end ; define column RawP ; space = 1 glue = 10 parent = Common . PValue header = \"Raw\" ; translate _val_ = . _ into '' ; end ; define column BonP ; space = 1 glue = 10 parent = Common . PValue header = \"Bon\" ; translate _val_ = . _ into '' ; end ; define column AdjP ; parent = Common . PValue header = \"Adj\" ; translate _val_ = . _ into '' ; end ; define column SEAdjP ; header = \"SE(AdjP)\" format = d8 .; translate _val_ = . _ into '' ; end ; end ; run ; data _null_ ; set SimTestOut ; file print ods = ( template = ' MCBook . SimTests ' ); put _ods_ ; run ; %end ; options notes ; %mend ; /* The %RomEx Macro */ /* This macro computes exact discrete tests using Rom's (1992) method. */ /*---------------------------------------------------------------*/ /* Name: RomEx */ /* Title: Rom exact discrete multiple comparison procedure */ /* Author: Chung-Kuei Chang, prosof@prosof.com */ /* Reference: Rom, D. M. (1992). Strengthening some common */ /* multiple test procedures for discrete data. */ /* Statistics in Medicine, 11, 511-514. */ /* Release: Version 6.11 */ /*---------------------------------------------------------------*/ /* Input: */ /* */ /* The following arguments are required and must be in this */ /* order. */ /* */ /* - the tail of the test 1-lower, 2-upper, 3-two-sided */ /* - the SAS data set containing the data to be analyzed */ /* - the SAS data set containing the number of endpoints and */ /* number of treatments. */ /* */ /* Output: */ /* */ /* The output dataset contains one observation for each */ /* P-value in the dataset. The output dataset contains the */ /* following variables: */ /* */ /* i - The index of the ordered P-value */ /* */ /* ENDPOINT - The index of the corresponding endpoint */ /* */ /* P_VALUE - The Fisher Exact P-value for the endpoint */ /* */ /* ADJP - The P-value for the global null hypothesis */ /*---------------------------------------------------------------*/ % MACRO ROMEX ( TAIL , dsdat , dspar ); %let con = 1e-10 ; % GLOBAL ADJP ; DATA PAR ; SET & DSPAR ; DIGNEP = INT ( LOG10 ( NEP )) + 1 ; DIGNT = INT ( LOG10 ( NT )) + 1 ; CALL SYMPUT ( ' DIGNEP ' , DIGNEP ); CALL SYMPUT ( ' DIGNT ' , DIGNT ); proc print ; run ; RUN ; DATA PAR ; KEEP NEP NT ; SET PAR ; LENGTH TT $&DIGNEP . TG $&DIGNT .; TT = NEP ; TG = NT ; CALL SYMPUT ( ' TT ' , TT ); CALL SYMPUT ( ' TG ' , TG ); TITLE ' DATA = MULTCOMP . PAR ' ; RUN ; DATA DISMULEP ; N = _N_ ; SET & DSDAT END = EOF ; IF EOF = 1 THEN DO ; CALL SYMPUT ( ' NC ' , N ); END ; proc print ; RUN ; title1 ' MULTIPLE ENDPOINTS DATA ' ; title2 ' ' ; proc print data = dismulep ; id ep1 ; VAR EP2 - EP & TT TREAT1 - TREAT & TG ; run ; PROC TRANSPOSE DATA = DISMULEP OUT = TEMP1 ; BY N ; VAR EP1 - EP & TT TREAT1 - TREAT & TG ; * PROC PRINT ; TITLE ' TEMP1 ' ; RUN ; DATA TEMP1 ; SET TEMP1 ; DROP _NAME_ N ; RUN ; PROC TRANSPOSE DATA = TEMP1 OUT = TEMP2 PREFIX = TAO ; VAR COL1 ; * PROC PRINT DATA = TEMP2 ; TITLE ' TEMP2 ' ; RUN ; % LET NCOLO =% EVAL ( & TT +& TG ); % LET NROW =& NC ; % LET NCOL =& TG ; % LET TOT =% EVAL ( & NC *& NCOLO ); DATA ZPZ ( KEEP = P_VALUE ENDPOINT ); SET TEMP2 ; ARRAY P ( & TT ); ARRAY POBS ( & TT ); ARRAY TAO ( & NROW , & NCOLO ); ARRAY Q ( & NROW , & NCOL ); ARRAY MINM ( & NROW , & NCOL ); ARRAY C ( & NCOL ); ARRAY R ( & NROW ); ARRAY NN ( & TT , 2 , & TG ); ARRAY BR { & NCOL }; ARRAY BC { & NROW }; ARRAY CY { & NROW , & NCOL }; ARRAY RY { & NROW , & NCOL }; ARRAY SR ( 2 ); ARRAY SRY ( 2 , & TG ); ARRAY SMINM ( & TG ); % MACRO SMTABLE ( B ); DO SJ =& B TO & K ; IF SJ > 1 THEN DO ; DO SJI = 1 TO 2 ; SRY ( SJI , SJ ) = SRY ( SJI , SJ - 1 ) + NN ( I , SJI , SJ - 1 ); END ; END ; MMR = SR ( 1 ) - SRY ( 1 , SJ ); NN ( I , 1 , SJ ) = MIN ( MMR , C ( SJ )); NN ( I , 2 , SJ ) = C ( SJ ) - NN ( I , 1 , SJ ); MMR2 = SR ( 2 ) - SRY ( 2 , SJ ); MMC = C ( SJ ) - MMR2 ; SMINM ( SJ ) = MAX ( 0 , MMC ); END ; % MEND SMTABLE ; % MACRO SPROB ; SPP = SFIX ; DO SI = 1 TO 2 ; DO SJ = 1 TO & K ; DO SS = 2 TO NN ( I , SI , SJ ); SPP = SPP - LOG ( SS ); END ; END ; END ; SPP = EXP ( SPP ); % MEND SPROB ; % MACRO TWOXK ( K ); SR ( 1 ) = 0 ; SR ( 2 ) = 0 ; STOTAL = TOTAL ; DO II = 1 TO 2 ; DO J = 1 TO & K ; SR ( II ) = SR ( II ) + NN ( I , II , J ); END ; END ; STOBS = 0 ; EXPMAR = 0 ; DO K = 2 TO & K ; STOBS = STOBS + ( K - 1 ) * NN ( I , 1 , K ); EXPMAR = EXPMAR + ( K - 1 ) * SR ( 1 ) * C ( K ); END ; EXPMAR = EXPMAR / TOTAL ; DISOBS = ABS ( STOBS - EXPMAR ); SFIX = 0 ; DO J = 1 TO 2 ; DO S = 2 TO SR ( J ); SFIX = SFIX + LOG ( S ); END ; END ; DO K = 1 TO & K ; DO S = 2 TO C ( K ); SFIX = SFIX + LOG ( S ); END ; END ; DO S = 2 TO STOTAL ; SFIX = SFIX - LOG ( S ); END ; SRY ( 1 , 1 ) = 0 ; SRY ( 2 , 1 ) = 0 ; P_VALUE = 0 ; % SMTABLE ( 1 ) * N_TABLE = 0 ; DO UNTIL ( NN ( I , 1 , 1 ) < SMINM ( 1 )); DO UNTIL ( NN ( I , 1 , & K - 1 ) < SMINM ( & K - 1 )); STPRO = 0 ; DO SI = 2 TO & K ; STPRO = STPRO + ( SI - 1 ) * NN ( I , 1 , SI ); END ; DISPRO = ABS ( STPRO - EXPMAR ); % IF & TAIL = 1 % THEN % DO ; IF STPRO - STOBS <& con THEN DO ; % END ; % ELSE % IF & TAIL = 2 % THEN % DO ; IF STPRO - STOBS >-& con THEN DO ; % END ; % ELSE % IF & TAIL = 3 % THEN % DO ; IF DISPRO - DISOBS >-& con THEN DO ; % END ; % SPROB P_VALUE = P_VALUE + SPP ; END ; NN ( I , 1 , & K - 1 ) = NN ( I , 1 , & K - 1 ) - 1 ; NN ( I , 2 , & K - 1 ) = NN ( I , 2 , & K - 1 ) + 1 ; NN ( I , 1 , & K ) = NN ( I , 1 , & K ) + 1 ; NN ( I , 2 , & K ) = NN ( I , 2 , & K ) - 1 ; END ; *************** FOR DO UNTIL ; IF & K > 2 THEN DO ; U = ( & K - 1 ); DO UNTIL ( NN ( I , 1 , 1 ) < SMINM ( 1 ) OR NN ( I , 1 , U ) >= SMINM ( U )); U = U - 1 ; NN ( I , 1 , U ) = NN ( I , 1 , U ) - 1 ; IF NN ( I , 1 , U ) >= SMINM ( U ) THEN DO ; NN ( I , 2 , U ) = NN ( I , 2 , U ) + 1 ; U = U + 1 ; % SMTABLE ( U ) END ; END ; END ; END ; ******************* FOR THE DO UNTIL REPLACING SSTART ; % MEND TWOXK ; DO I = 1 TO & NROW ; R ( I ) = 0 ; END ; DO J = 1 TO & NCOL ; C ( J ) = 0 ; END ; TOTAL = 0 ; DO I = 1 TO & NROW ; DO J = 1 TO & NCOLO ; IF J >& TT THEN DO ; Q ( I , J -& TT ) = TAO ( I , J ); R ( I ) = R ( I ) + TAO ( I , J ); C ( J -& TT ) = C ( J -& TT ) + TAO ( I , J ); TOTAL = TOTAL + TAO ( I , J ); END ; END ; END ; BR ( & NCOL ) = 0 ; BC ( & NROW ) = 0 ; DO I = ( & NCOL - 1 ) TO 1 BY - 1 ; BR ( I ) = BR ( I + 1 ) + C ( I + 1 ); END ; DO I = ( & NROW - 1 ) TO 1 BY - 1 ; BC ( I ) = BC ( I + 1 ) + R ( I + 1 ); END ; DO I = 1 TO & TT ; DO J = 1 TO & TG ; NN ( I , 1 , J ) = 0 ; DO K = 1 TO & NC ; IF TAO ( K , I ) = 1 THEN NN ( I , 1 , J ) = NN ( I , 1 , J ) + Q ( K , J ); END ; NN ( I , 2 , J ) = C ( J ) - NN ( I , 1 , J ); END ; % TWOXK ( & TG ) ENDPOINT = I ; OUTPUT ZPZ ; IF I = 1 THEN POBS ( I ) = P_VALUE ; ELSE DO ; LE = 0 ; DO B = 1 TO ( I - 1 ); IF P_VALUE - POBS ( B ) <& con THEN DO ; DO CC = ( I - 1 ) TO B BY - 1 ; POBS ( CC + 1 ) = POBS ( CC ); END ; POBS ( B ) = P_VALUE ; LE = 1 ; GOTO ORDER ; END ; END ; ORDER : ; IF LE = 0 THEN POBS ( I ) = P_VALUE ; END ; END ; FIX = 0 ; DO I = 1 TO & NROW ; DO S = 2 TO R ( I ); FIX = FIX + LOG ( S ); END ; END ; DO J = 1 TO & NCOL ; DO S = 2 TO C ( J ); FIX = FIX + LOG ( S ); END ; END ; DO S = 1 TO TOTAL ; FIX = FIX - LOG ( S ); END ; DO S = 1 TO & NROW ; RY ( S , 1 ) = 0 ; END ; DO S = 1 TO & NCOL ; CY ( 1 , S ) = 0 ; END ; % MACRO MTABLE ( A , B ); DO II =& A TO & NROW ; DO JJ = 1 TO & NCOL ; IF ( II >& A OR JJ >=& B ) THEN DO ; IF ( JJ =& NCOL AND II <& NROW ) THEN DO ; Q ( II , JJ ) = R ( II ) - RY ( II , & NCOL - 1 ) - Q ( II , & NCOL - 1 ); IF II > 1 THEN CY ( II , JJ ) = CY ( II - 1 , JJ ) + Q ( II - 1 , JJ ); END ; IF II =& NROW THEN DO ; IF JJ <& NCOL THEN Q ( II , JJ ) = C ( JJ ) - CY ( II - 1 , JJ ) - Q ( II - 1 , JJ ); ELSE DO ; LAST = R ( & NROW ); DO S = 1 TO ( & NCOL - 1 ); LAST = LAST - Q ( & NROW , S ); END ; Q ( & NROW , & NCOL ) = LAST ; END ; END ; IF ( II <& NROW AND JJ <& NCOL ) THEN DO ; IF II > 1 THEN CY ( II , JJ ) = CY ( II - 1 , JJ ) + Q ( II - 1 , JJ ); IF JJ > 1 THEN RY ( II , JJ ) = RY ( II , JJ - 1 ) + Q ( II , JJ - 1 ); MMR = R ( II ) - RY ( II , JJ ); MMC = C ( JJ ) - CY ( II , JJ ); Q ( II , JJ ) = MIN ( MMR , MMC ); MXR = MMR - BR ( JJ ); IF II > 1 THEN DO ; DO L = ( JJ + 1 ) TO & NCOL ; MXR = MXR + CY ( II - 1 , L ) + Q ( II - 1 , L ); END ; END ; MXC = MMC - BC ( II ); MINM ( II , JJ ) = MAX ( 0 , MXR , MXC ); END ; END ; END ; END ; % MEND MTABLE ; % MACRO PROB ; PP = FIX ; DO II = 1 TO & NROW ; DO J = 1 TO & NCOL ; DO S = 2 TO Q ( II , J ); PP = PP - LOG ( S ); END ; END ; END ; PP = EXP ( PP ); % MEND PROB ; % MTABLE ( 1 , 1 ) P_ONESID = 0 ; N_TABLE = 0 ; N_TABLE = N_TABLE + 1 ; START : ; DO UNTIL ( Q ( & NROW - 1 , & NCOL - 1 ) < MINM ( & NROW - 1 , & NCOL - 1 )); DO I = 1 TO & TT ; DO J = 1 TO & TG ; NN ( I , 1 , J ) = 0 ; DO K = 1 TO & NC ; IF TAO ( K , I ) = 1 THEN NN ( I , 1 , J ) = NN ( I , 1 , J ) + Q ( K , J ); END ; NN ( I , 2 , J ) = C ( J ) - NN ( I , 1 , J ); END ; % TWOXK ( & TG ) IF P_VALUE - POBS ( 1 ) <-& con THEN DO ; P ( 1 ) = P_VALUE ; GOTO CALP ; END ; ELSE DO ; IF I = 1 THEN P ( I ) = P_VALUE ; ELSE DO ; LE = 0 ; DO B = 1 TO ( I - 1 ); IF P_VALUE - P ( B ) <& con THEN DO ; DO CC = ( I - 1 ) TO B BY - 1 ; P ( CC + 1 ) = P ( CC ); END ; P ( B ) = P_VALUE ; LE = 1 ; GOTO ORDER2 ; END ; END ; IF LE = 0 THEN P ( I ) = P_VALUE ; ORDER2 : ; END ; IF P ( 1 ) - POBS ( 1 ) <-& con THEN GOTO CALP ; ELSE IF I <& TT THEN DO ; IF P ( 1 ) - POBS ( 1 ) >& con THEN GOTO NEXTI ; ELSE IF ABS ( P ( 1 ) - POBS ( 1 )) <& con THEN DO ; DO B = 2 TO I ; IF P ( B ) - POBS ( B ) >& con THEN GOTO NEXTI ; ELSE IF P ( B ) - POBS ( B ) <-& con THEN GOTO CALP ; ELSE IF ( ABS ( P ( B ) - POBS ( B )) <& con AND I <& TT ) THEN GOTO NEXTI ; ELSE IF ( ABS ( P ( B ) - POBS ( B )) <& con AND I =& TT ) THEN GOTO CALP ; END ; END ; END ; ELSE IF I =& TT THEN DO ; IF P ( 1 ) - POBS ( 1 ) >& con THEN GOTO SKIP ; ELSE IF ABS ( P ( 1 ) - POBS ( 1 )) <& con THEN DO ; DO B = 2 TO I ; IF P ( B ) - POBS ( B ) >& con THEN GOTO SKIP ; ELSE IF P ( B ) - POBS ( B ) <-& con THEN GOTO CALP ; ELSE IF ( ABS ( P ( B ) - POBS ( B )) <& con AND B <& TT ) THEN GOTO NEXTB ; ELSE IF ( ABS ( P ( B ) - POBS ( B )) <& con AND B =& TT ) THEN GOTO CALP ; NEXTB : ; END ; END ; END ; END ; NEXTI : ; END ; GOTO SKIP ; CALP : ; % PROB P_ONESID = P_ONESID + PP ; SKIP :; Q ( & NROW - 1 , & NCOL - 1 ) = Q ( & NROW - 1 , & NCOL - 1 ) - 1 ; Q ( & NROW - 1 , & NCOL ) = Q ( & NROW - 1 , & NCOL ) + 1 ; Q ( & NROW , & NCOL - 1 ) = Q ( & NROW , & NCOL - 1 ) + 1 ; Q ( & NROW , & NCOL ) = Q ( & NROW , & NCOL ) - 1 ; END ; DO I =& NROW - 1 TO 1 BY - 1 ; DO J =& NCOL - 1 TO 1 BY - 1 ; IF ( I <& NROW - 1 OR J <& NCOL - 1 ) THEN DO ; Q ( I , J ) = Q ( I , J ) - 1 ; IF Q ( 1 , 1 ) < MINM ( 1 , 1 ) THEN GOTO FINISH ; IF Q ( I , J ) >= MINM ( I , J ) THEN DO ; J = J + 1 ; % MTABLE ( I , J ) GOTO START ; END ; END ; END ; END ; FINISH : ; CALL SYMPUT ( ' ADJP ' , P_ONESID ); RUN ; PROC SORT DATA = ZPZ OUT = ZPZ ; BY P_VALUE ; RUN ; TITLE1 ' ' ; title2 ' ROM DISCRETE MULTIPLE ENDPOINTS ANALYSIS ' ; title3 ' ' ; title4 ' ' ; title5 ' ' ; DATA _NULL_ ; FILE PRINT ; SET ZPZ END = EOF ; ADJP =& ADJP ; I = _N_ ; IF _N_ = 1 THEN DO ; PUT @ 18 'I' @ 28 ' ENDPOINT ' @ 57 ' P - VALUE * ' ; PUT @ 15 51 * '-' ; END ; PUT @ 18 I @ 24 ENDPOINT 8. @ 55 P_VALUE 8.4 ; IF EOF = 1 THEN DO ; PUT @ 15 51 * '-' ; PUT @ 18 ' EXACT ADJUSTED P - VALUE ' @ 55 ADJP 8.4 ; PUT ///; % IF & TAIL = 1 % THEN % DO ; PUT @ 18 ' *: P - VALUE FOR LOWER - TAILED TREND ' ; % END ; % ELSE % IF & TAIL = 2 % THEN % DO ; PUT @ 18 ' *: P - VALUE FOR UPPER - TAILED TREND ' ; % END ; % IF & TAIL = 3 % THEN % DO ; PUT @ 18 ' *: P - VALUE FOR TWO - TAILED TREND ' ; % END ; END ; RUN ; % MEND ROMEX ; /* The %RomMC Macro */ /* This macro simulates discrete tests using Rom's (1992) method. */ /*---------------------------------------------------------------*/ /* Name: RomMc */ /* Title: Rom Monte-Carlo discrete multiple comparison */ /* procedure */ /* Author: Chung-Kuei Chang, prosof@prosof.com */ /* Reference: Rom, D. M. (1992). Strengthening some common */ /* multiple test procedures for discrete data. */ /* Statistics in Medicine, 11, 511-514. */ /* Release: Version 6.11 */ /*---------------------------------------------------------------*/ /* Input: */ /* */ /* The following arguments are must be in this order. */ /* */ /* - the tail of the test 1-lower, 2-upper, 3-two-sided */ /* - the SAS data set containing the data to be analyzed */ /* - the SAS data set containing the number of endpoints and */ /* number of treatments. */ /* - the SAS data set containing the number of Monte-Carlo */ /* samples. */ /* */ /* Output: */ /* */ /* The output dataset contains one observation for each */ /* P-value in the dataset. The output dataset contains the */ /* following variables: */ /* */ /* i - The index of the ordered P-value */ /* */ /* ENDPOINT - The index of the corresponding endpoint */ /* */ /* P_VALUE - The Fisher Exact P-value for the endpoint */ /* */ /* ADJP - The P-value for the global null hypothesis */ /* */ /* L95 - The lower limit of the confidence interval for */ /* the P-value */ /* */ /* U95 - The upper limit of the confidence interval for */ /* the P-value */ /*---------------------------------------------------------------*/ % MACRO ROMMC ( TAIL , dsdat , dspar , dsnmc ); %let con = 1e-10 ; % GLOBAL ADJP MM L95 U95 ; % MACRO ZSCORE ( K ); SR ( 1 ) = 0 ; ******************* SUM OF THE ELEMENTS IN THE FIRST ROW ; SR ( 2 ) = 0 ; ******************* SUM OF THE ELEMENTS IN THE SECOND ROW ; ******************* C ( J ) IS THE JTH COLUMN SUM ; STOTAL = TOTAL ; DO II = 1 TO 2 ; DO J = 1 TO & K ; SR ( II ) = SR ( II ) + NN ( I , II , J ); END ; END ; TOBS = 0 ; EXPMAR = 0 ; UR2 = SR ( 1 ); U2 = ( SR ( 1 ) ** 2 ) / TOTAL ; VC2 = 0 ; V2 = 0 ; DO K = 2 TO & K ; TOBS = TOBS + ( K - 1 ) * NN ( I , 1 , K ); EXPMAR = EXPMAR + SR ( 1 ) * C ( K - 1 ) * ( K - 1 ); VC2 = VC2 + ( K - 1 ) ** 2 * C ( K ); V2 = V2 + ( K - 1 ) * C ( K ); END ; EXPMAR = EXPMAR / TOTAL ; V2 = V2 ** 2 / TOTAL ; VAR = ( UR2 - U2 ) * ( VC2 - V2 ) / ( TOTAL - 1 ); IF VAR > 0 THEN DO ; Z = ( TOBS - EXPMAR ) / VAR ** 0.5 ; PVAL = PROBNORM ( Z ); % IF & TAIL = 1 % THEN % DO ; PZ = PVAL ; % END ; % ELSE % IF & TAIL = 2 % THEN % DO ; PZ = 1 - PVAL ; % END ; % ELSE % IF & TAIL = 3 % THEN % DO ; PZ = 2 * MIN ( PVAL , 1 - PVAL ); % END ; P_VALUE = PZ ; END ; ELSE PZ = 1 ; % MEND ZSCORE ; DATA N_SAMPLE ; SET & DSNMC ; IF SEED = . THEN SEED = 0 ; CALL SYMPUT ( ' MM ' , N_SAMPLE ); CALL SYMPUT ( ' SEED ' , SEED ); RUN ; DATA PAR ; SET & DSPAR ; DIGNEP = INT ( LOG10 ( NEP )) + 1 ; DIGNT = INT ( LOG10 ( NT )) + 1 ; CALL SYMPUT ( ' DIGNEP ' , DIGNEP ); CALL SYMPUT ( ' DIGNT ' , DIGNT ); RUN ; DATA PAR ; KEEP NEP NT ; SET PAR ; LENGTH TT $&DIGNEP . TG $&DIGNT .; TT = NEP ; TG = NT ; CALL SYMPUT ( ' TT ' , TT ); CALL SYMPUT ( ' TG ' , TG ); TITLE ' DATA = MULTCOMP . PAR ' ; RUN ; DATA DISMULEP ; SET & DSDAT END = EOF ; N = _N_ ; IF EOF = 1 THEN DO ; CALL SYMPUT ( ' NC ' , N ); END ; RUN ; PROC TRANSPOSE DATA = DISMULEP OUT = TEMP1 ; BY N ; VAR EP1 - EP & TT TREAT1 - TREAT & TG ; TITLE ' TEMP1 ' ; RUN ; DATA TEMP1 ; SET TEMP1 ; DROP _NAME_ N ; RUN ; title1 ' MULTIPLE ENDPOINTS DATA ' ; title2 ' ' ; proc print data = dismulep ; id ep1 ; VAR EP2 - EP & TT TREAT1 - TREAT & TG ; run ; PROC TRANSPOSE DATA = DISMULEP OUT = TEMP1 ; BY N ; VAR EP1 - EP & TT TREAT1 - TREAT & TG ; RUN ; PROC TRANSPOSE DATA = TEMP1 OUT = TEMP2 PREFIX = TAO ; VAR COL1 ; RUN ; % LET NCOLO =% EVAL ( & TT +& TG ); % LET NROW =& NC ; % LET NCOL =& TG ; % LET TOT =% EVAL ( & NC *& NCOLO ); DATA TEMP2 ; SET TEMP2 ; ARRAY TAO ( & NROW , & NCOLO ); ARRAY Q ( & NROW , & NCOL ); ARRAY C ( & NCOL ); ARRAY R ( & NROW ); DO I = 1 TO & NROW ; R ( I ) = 0 ; END ; DO J = 1 TO & NCOL ; C ( J ) = 0 ; END ; TOTAL = 0 ; DO I = 1 TO & NROW ; DO J = 1 TO & NCOLO ; IF J >& TT THEN DO ; Q ( I , J -& TT ) = TAO ( I , J ); R ( I ) = R ( I ) + TAO ( I , J ); C ( J -& TT ) = C ( J -& TT ) + TAO ( I , J ); TOTAL = TOTAL + TAO ( I , J ); END ; END ; END ; CALL SYMPUT ( 'N' , TOTAL ); RUN ; DATA ZPZ ( KEEP = P_VALUE ENDPOINT ); SET TEMP2 ; TRIAL =& MM ; % MACRO SMTABLE ( B ); DO SJ =& B TO & K ; IF SJ > 1 THEN DO ; DO SJI = 1 TO 2 ; SRY ( SJI , SJ ) = SRY ( SJI , SJ - 1 ) + NN ( I , SJI , SJ - 1 ); END ; END ; MMR = SR ( 1 ) - SRY ( 1 , SJ ); NN ( I , 1 , SJ ) = MIN ( MMR , C ( SJ )); NN ( I , 2 , SJ ) = C ( SJ ) - NN ( I , 1 , SJ ); MMR2 = SR ( 2 ) - SRY ( 2 , SJ ); MMC = C ( SJ ) - MMR2 ; SMINM ( SJ ) = MAX ( 0 , MMC ); END ; % MEND SMTABLE ; % MACRO SPROB ; SPP = SFIX ; DO SI = 1 TO 2 ; DO SJ = 1 TO & K ; DO SS = 2 TO NN ( I , SI , SJ ); SPP = SPP - LOG ( SS ); END ; END ; END ; SPP = EXP ( SPP ); % MEND SPROB ; ARRAY TAO ( & NROW , & NCOLO ); ARRAY Q ( & NROW , & NCOL ); ARRAY MINM ( & NROW , & NCOL ); ARRAY C ( & NCOL ); ARRAY R ( & NROW ); ARRAY NN ( & TT , 2 , & TG ); ARRAY BR { & NCOL }; ARRAY BC { & NROW }; ARRAY SR ( 2 ); ARRAY SRY ( 2 , & TG ); ARRAY SMINM ( & TG ); ARRAY P ( & TT ); ARRAY POBS ( & TT ); ARRAY CY { & NROW , & NCOL }; *** CY ( I , J ) = Q ( 1 , J ) + Q ( 2 , J ) + ... + Q ( I - 1 , J ); ARRAY RY { & NROW , & NCOL }; *** RY ( I , J ) = Q ( I , 1 ) + Q ( I , 2 ) + ... + Q ( I , J - 1 ); ARRAY acumr ( & NROW ); ARRAY acumc ( & NCOL ); acumr ( 1 ) = R ( 1 ); acumc ( 1 ) = C ( 1 ); DO I = 2 TO & NROW ; acumr ( I ) = acumr ( I - 1 ) + R ( I ); END ; DO I = 2 TO & NCOL ; acumc ( I ) = acumc ( I - 1 ) + C ( I ); END ; BR ( & NCOL ) = 0 ; BC ( & NROW ) = 0 ; DO I = ( & NCOL - 1 ) TO 1 BY - 1 ; BR ( I ) = BR ( I + 1 ) + C ( I + 1 ); END ; DO I = ( & NROW - 1 ) TO 1 BY - 1 ; BC ( I ) = BC ( I + 1 ) + R ( I + 1 ); END ; DO I = 1 TO & TT ; DO J = 1 TO & TG ; NN ( I , 1 , J ) = 0 ; DO K = 1 TO & NC ; IF TAO ( K , I ) = 1 THEN NN ( I , 1 , J ) = NN ( I , 1 , J ) + Q ( K , J ); END ; NN ( I , 2 , J ) = C ( J ) - NN ( I , 1 , J ); END ; /* %TWOXK(&TG) */ % ZSCORE ( & TG ) ENDPOINT = I ; OUTPUT ZPZ ; IF I = 1 THEN POBS ( I ) = P_VALUE ; ELSE DO ; LE = 0 ; DO B = 1 TO ( I - 1 ); IF P_VALUE - POBS ( B ) <& con THEN DO ; DO CC = ( I - 1 ) TO B BY - 1 ; POBS ( CC + 1 ) = POBS ( CC ); END ; POBS ( B ) = P_VALUE ; LE = 1 ; GOTO ORDER ; END ; END ; ORDER : ; IF LE = 0 THEN POBS ( I ) = P_VALUE ; END ; END ; ADJP = 0 ; N_TABLE = 0 ; ARRAY XX ( & N ); ARRAY YY ( & N ); DO KL = 1 TO & MM ; DO I = 1 TO & N ; XX ( I ) = I ; END ; DO I = 1 TO & N ; RR = 0 ; YY ( I ) = XX ( INT (( & N + 1 - I ) * RANUNI ( & SEED ) + 1 )); J = 0 ; DO UNTIL ( RR > 0 ); J = J + 1 ; IF XX ( J ) = YY ( I ) THEN RR = J ; END ; DO J = RR TO ( & N - I ); XX ( J ) = XX ( J + 1 ); END ; END ; DO I = 1 TO & NROW ; IF I = 1 THEN RMIN = 0 ; ELSE RMIN = acumr ( I - 1 ); RMAX = acumr ( I ) + 1 ; DO J = 1 TO & NCOL ; Q ( I , J ) = 0 ; IF J = 1 THEN CMIN = 1 ; ELSE CMIN = acumc ( J - 1 ) + 1 ; CMAX = acumc ( J ); DO KK = CMIN TO CMAX ; IF RMIN < YY ( KK ) < RMAX THEN Q ( I , J ) = Q ( I , J ) + 1 ; END ; END ; END ; EQUAL = ' NO ' ; DO I = 1 TO & TT ; DO J = 1 TO & TG ; NN ( I , 1 , J ) = 0 ; DO K = 1 TO & NC ; IF TAO ( K , I ) = 1 THEN NN ( I , 1 , J ) = NN ( I , 1 , J ) + Q ( K , J ); END ; NN ( I , 2 , J ) = C ( J ) - NN ( I , 1 , J ); END ; /* %TWOXK(&TG) */ % ZSCORE ( & TG ) IF I = 1 THEN P ( 1 ) = P_VALUE ; IF P_VALUE - POBS ( 1 ) <-& con THEN DO ; P ( 1 ) = P_VALUE ; ADJP = ADJP + 1 ; GOTO NEXTKL ; END ; ELSE IF ABS ( P_VALUE - POBS ( 1 )) <& con THEN DO ; EQUAL = ' YES ' ; IF & TT = 1 THEN DO ; ADJP = ADJP + 1 ; GOTO NEXTKL ; END ; ELSE IF & TT > 1 AND I > 1 THEN DO ; DO CC = ( I - 1 ) TO 1 BY - 1 ; P ( CC + 1 ) = P ( CC ); END ; P ( 1 ) = P_VALUE ; DO CC = 2 TO I ; IF P ( CC ) - POBS ( CC ) <-& con OR ( ABS ( P ( CC ) - POBS ( CC )) <& con AND CC =& TT ) THEN DO ; ADJP = ADJP + 1 ; GOTO NEXTKL ; END ; ELSE IF P ( CC ) - POBS ( CC ) >& con THEN GOTO NEXTI ; END ; END ; END ; ELSE IF P_VALUE - POBS ( 1 ) >& con THEN DO ; IF & TT = 1 THEN GOTO NEXTI ; ELSE IF & TT > 1 AND I > 1 THEN DO ; LE = 0 ; DO B = 1 TO ( I - 1 ); IF P_VALUE - P ( B ) <& con THEN DO ; DO CC = ( I - 1 ) TO B BY - 1 ; P ( CC + 1 ) = P ( CC ); END ; P ( B ) = P_VALUE ; LE = 1 ; ******** THE P_VALUE IS LESS THAN ONE OF THE PREVIOUS ONES .; GOTO ORDER2 ; END ; END ; END ; IF LE = 0 THEN P ( I ) = P_VALUE ; ORDER2 : ; IF EQUAL = ' YES ' THEN DO ; DO CC = 1 TO I ; IF P ( CC ) - POBS ( CC ) <-& con OR ( ABS ( P ( CC ) - POBS ( CC )) <& con AND CC =& TT ) THEN DO ; ADJP = ADJP + 1 ; GOTO NEXTKL ; END ; ELSE IF P ( CC ) - POBS ( CC ) >& con THEN GOTO NEXTI ; END ; END ; END ; NEXTI : ; END ; ************************* END FOR I ; NEXTKL : ; END ; ************************* END FOR KL ; ADJP = ADJP /& MM ; L95 = MAX ( 0 , ADJP - PROBIT ( .975 ) * ( ADJP * ( 1 - ADJP ) /& MM ) ** .5 ); U95 = MIN ( 1 , ADJP + PROBIT ( .975 ) * ( ADJP * ( 1 - ADJP ) /& MM ) ** .5 ); CALL SYMPUT ( ' ADJP ' , ADJP ); CALL SYMPUT ( ' L95 ' , L95 ); CALL SYMPUT ( ' U95 ' , U95 ); RUN ; PROC SORT DATA = ZPZ OUT = ZPZ ; BY P_VALUE ; * PROC PRINT ; RUN ; title1 ' ' ; TITLE2 ' ROM DISCRETE MULTIPLE ENDPOINTS ANALYSIS ' ; title3 ' ' ; TITLE4 ' ' ; title5 ' ' ; DATA _NULL_ ; FILE PRINT ; SET ZPZ END = EOF ; ADJP =& ADJP ; L95 =& L95 ; U95 =& U95 ; I = _N_ ; MM =& MM ; IF _N_ = 1 THEN DO ; PUT @ 18 'I' @ 28 ' ENDPOINT ' @ 57 ' P - VALUE * ' ; PUT @ 15 51 * '-' ; END ; PUT @ 18 I @ 24 ENDPOINT 8. @ 55 P_VALUE 8.4 ; IF EOF = 1 THEN DO ; PUT @ 15 51 * '-' ; PUT @ 18 ' MONTE CARLO GLOBAL P - VALUE ' @ 55 ADJP 8.4 ; PUT @ 18 ' 95 % CONFIDENCE INTERVAL ' @ 54 '(' @ 55 L95 8.4 @ 63 ',' @ 63 U95 8.4 @ 72 ')' ; PUT @ 18 ' NUMBER OF SAMPLES ' @ 50 MM 8. ; PUT //; % IF & TAIL = 1 % THEN % DO ; PUT @ 18 ' *: ASYMPTOTIC P - VALUE FOR LOWER - TAILED TREND ' ; % END ; % ELSE % IF & TAIL = 2 % THEN % DO ; PUT @ 18 ' *: ASYMPTOTIC P - VALUE FOR UPPER - TAILED TREND ' ; % END ; % ELSE % IF & TAIL = 3 % THEN % DO ; PUT @ 18 ' *: ASYMPTOTIC P - VALUE FOR TWO - TAILED TREND ' ; % END ; END ; RUN ; % MEND ROMMC ; /* The %BayesIntervals Macro */ /* This macro computes Bayesian simultaneous confidence intervals. */ /*---------------------------------------------------------------*/ /* Name: BayesIntervals */ /* Title: Bayesian simultaneous intervals based on */ /* percentiles */ /* Author: Russell D. Wolfinger, sasrdw@sas.com */ /* Release: Version 6 or later */ /*---------------------------------------------------------------*/ /* Inputs: */ /* */ /* data= data set representing a sample from the */ /* posterior distribution (required) */ /* */ /* vars= the variables in the data set for which to compute */ /* simultaneous intervals (required) */ /* */ /* alpha= the joint significance level, default=0.05 */ /* */ /* tail= the tail type of the intervals, must be L, U, or */ /* 2, default=2 */ /* */ /* maxit= maximum number of iterations in the search, */ /* default=50 */ /* */ /* tol= convergence tolerance, default=0.001 */ /* */ /* Output: */ /* */ /* The macro begins by computing the joint coverage of the */ /* naive unadjusted alpha-level intervals, computed as */ /* percentiles across the sample. It then decreases alpha */ /* using a bisection search until the joint coverage comes */ /* within the tol= value of alpha. A history of this search */ /* is printed and then the simultaneous intervals. */ /* */ /*---------------------------------------------------------------*/ %macro BayesIntervals ( data = , vars = , alpha = .05 , tail = 2 , maxit = 50 , tol = 0.001 ); options nonotes ; %if %bquote ( & data ) = %then %let data =& syslast ; /*---do bisection search to find adjusted alpha---*/ %let lowera = 0 ; data _null_ ; uppera = & alpha * 2 ; call symput ( ' uppera ' , uppera ); run ; %let iter = 0 ; %put %str ( ) The BayesIntervals Macro ; %put ; %put Iteration Alpha Coverage ; %do %while ( & iter < & maxit ); /*---compute quantiles---*/ data _null_ ; alf = ( & lowera + & uppera ) / 2 ; %if ( & tail = 2 ) %then %do ; lowerp = 100 * alf / 2 ; upperp = 100 * ( 1 - alf / 2 ); %end ; %else %if ( & tail = L ) %then %do ; lowerp = 100 * alf ; upperp = 100 ; %end ; %else %do ; lowerp = 0 ; upperp = 100 * ( 1 - alf ); %end ; call symput ( ' alf ' , left ( alf )); call symput ( ' lowerp ' , left ( lowerp )); call symput ( ' upperp ' , left ( upperp )); run ; proc univariate data =& data pctldef = 1 noprint ; var & vars ; output pctlpts =& lowerp , & upperp pctlpre =& vars pctlname = l u out = p ; run ; proc transpose data = p out = pt ; run ; /*---load limits and variable names into macro variables---*/ data _null_ ; set pt nobs = count end = last ; retain i 0 ; if ( mod ( _n_ , 2 ) = 1 ) then do ; i = i + 1 ; mname = \"v\" || left ( put ( i , 8. )); len = length ( _NAME_ ); vname = substr ( _NAME_ , 1 , len - 1 ); call symput ( mname , left ( vname )); mname = \"lv\" || left ( put ( i , 8. )); call symput ( mname , left ( put ( COL1 , best8 .))); end ; else do ; mname = \"uv\" || left ( put ( i , 8. )); call symput ( mname , left ( put ( COL1 , best8 .))); end ; if last then do ; call symput ( ' nvar ' , left ( put ( count / 2 , 8. ))); end ; run ; /*---pass through data and determine simultaneous coverage---*/ data _null_ ; set & data nobs = no end = last ; retain count 0 ; bad = 0 ; %do i = 1 %to & nvar ; if ( && v & i < && lv & i ) or ( && v & i > && uv & i ) then bad = 1 ; %end ; if ( bad = 0 ) then count = count + 1 ; if last then do ; coverage = count / no ; target = 1 - & alpha ; if ( abs ( coverage - target ) < & tol ) then conv = 1 ; else do ; conv = 0 ; alf = & alf ; if ( coverage < target ) then do ; call symput ( ' uppera ' , left ( alf )); end ; else do ; call symput ( ' lowera ' , left ( alf )); end ; end ; call symput ( ' conv ' , left ( conv )); call symput ( ' coverage ' , left ( coverage )); end ; run ; %let iter = %eval ( & iter + 1 ); %put %str ( ) & iter %str ( ) & alf %str ( ) & coverage ; %if ( & conv = 1 ) %then %let iter =& maxit ; %end ; options notes ; %if ( & conv = 1 ) %then %do ; data BayesIntervals ; %do i = 1 %to & nvar ; _NAME_ = \"&&v&i\" ; Lower = && lv & i ; Upper = && uv & i ; output ; %end ; run ; proc print data = BayesIntervals ; run ; %end ; %else %do ; %put Did not converge ; %end ; %mend BayesIntervals ; /* The %BayesTests Macro */ /* This macro computes Bayesian posterior probabilities for a set of */ /* free-combination tests, using Gonen and Westfall's (1998) method. */ /*---------------------------------------------------------------*/ /* Name: BayesTests */ /* Title: Multiple tests of hypotheses using Bayesian */ /* posterior probabilities */ /* Author: Peter H. Westfall, westfall@ttu.edu, */ /* Reference: Gonen, M. and Westfall, P.H. (1998). Bayesian */ /* multiple testing for multiple endpoints in clinical trials. */ /* Proceedings of the American Statistical Association, */ /* Biopharmaceutical Subsection. */ /* Release: Version 7.01 */ /*---------------------------------------------------------------*/ /* Inputs: */ /* */ /* MEANMUZ = The mean vector of the prior distribution for the */ /* noncentrality parameters of the test statistics, */ /* given that they are nonzero. Default is 2.5 2.5. */ /* */ /* SIGMAMUZ = The variance of the prior distribution of the */ /* noncentrality parameters (assumed constant), */ /* given that the noncentrality parameters are */ /* nonzero. The default is 2.0. */ /* */ /* You must specify two out of three of the following */ /* parameters. The third will be calculated from the */ /* two that you specify. */ /* */ /* RHO = The prior correlation (assumed constant) of the */ /* prior distribution of the noncentrality */ /* parameters (assumed constant), given that the */ /* noncentrality parameters are nonzero; AND ALSO */ /* the tetrachoric correlation of the binary */ /* outcomes (Hi true, Hj true). */ /* */ /* Piall = The prior probability that all null hypotheses */ /* are true. */ /* */ /* Pi0 = The probability that an individual hypothesis */ /* is true (assumed identical for all hypotheses) */ /* */ /* Additionally, %BayesTests requires a further macro to be */ /* defined that uses SAS/IML to construct the estimates and */ /* their covariance, as follows: */ /* */ /* %Estimate: Uses SAS/IML code to define */ /* EstPar - (column) vector of estimated parameters */ /* Cov - covariance matrix for the for the estimates */ /* */ /* You can either define this macro directly, or use the */ /* %MakeGLMStats macro to define it. */ /* */ /* Output: */ /* */ /* The output shows the values of Pi0, PiAll, and Rho (two of */ /* which were input and the third calculated.) */ /* The formatted output contains the following variables: */ /* */ /* Z Statistic - The values of the test statistics defined in */ /* the %Estimates macro */ /* */ /* Prior Mean */ /* Effect Size - The prior mean of the noncentrality parameter */ /* (meanmuz) */ /* */ /* Prior Std Dev */ /* Effect Size - The prior std dev of the noncentrality */ /* parameter (sqrt(sigmamuz)) */ /* */ /* Posterior */ /* Probability - The probability that the null hypothesis is */ /* true, given the data (and the prior inputs) */ /* */ /* Cov1-Covk - The correlation matrix of the test statistics, */ /* from the %Estimates macro */ /* */ /*---------------------------------------------------------------*/ %macro BayesTests ( meanmuz = j ( 1 , k , 2.5 ) , sigmamuz = 2.0 , rho = , Piall = , Pi0 = ); proc iml ; % Estimates ; zsample = EstPar ` ; sigma = cov ; %if & rho ^= %then %let flag1 = 1 ; %else %let flag1 = 0 ; %if & piall ^= %then %let flag2 = 1 ; %else %let flag2 = 0 ; %if & pi0 ^= %then %let flag3 = 1 ; %else %let flag3 = 0 ; %if %eval ( & flag1 +& flag2 +& flag3 ) ^= 2 %then %do ; print \"Please specify exactly two of the three inputs: rho, PIall, pi0\" ; print \"The other will be implied by the two you specify\" ; %end ; k = ncol ( zsample ); mu = & meanmuz ; sig2 = & sigmamuz ; pstd = j ( k , 1 , sqrt ( sig2 )); if ssq ( sigma - sigma ` ) > .0000001 then print \"Warning: Asymmetric Cov Matrix\" ; in = j ( 1 , k , 0 ); START FUN ( Z ) GLOBAL ( K1 , K2 , Z0 , RHO ); V1 = SQRT ( RHO ); V2 = SQRT ( 1 - RHO ); V3 = Z0 - V1 * Z ; V4 = PROBNORM ( V3 / V2 ); V5 = 1 - V4 ; V6 = ( 1 / SQRT ( 2 * 3.14159265 )) * EXP ( - ( Z ** 2 ) / 2 ); IF K1 = 0 THEN V = ( V4 ** K2 ) * V6 ; ELSE IF K2 = 0 THEN V = ( V5 ** K1 ) * V6 ; ELSE V = ( V4 ** K2 ) * ( V5 ** K1 ) * V6 ; RETURN ( V ); FINISH FUN ; START JPROB ( PROB , IN , CRIT , CORR ) GLOBAL ( K1 , K2 , Z0 , RHO ) ; Z0 = CRIT ; RHO = CORR ; K1 = SUM ( IN ); K2 = NCOL ( IN ) - K1 ; A = {. M . P }; CALL QUAD ( PROB , \"FUN\" , A ); FINISH JPROB ; start mnorm ( x , mu , sig ); p = nrow ( sig ); log1 = - ( p / 2 ) * log ( 2 * 3.14159265 ); log2 = - .5 * log ( det ( sig )); log3 = - .5 * ( x - mu ) ` * inv ( sig ) * ( x - mu ); log = log1 + log2 + log3 ; f = exp ( log ); return ( f ); finish mnorm ; %if & Pi0 = %then %do ; Piall = & Piall ; corr = & rho ; clower = probit ( Piall ); cupper = probit ( Piall ** ( 1 / K )); CALL JPROB ( PROBl , IN , Clower , CORR ); CALL JPROB ( PROBu , IN , Cupper , CORR ); do t = 1 to 50 ; cchk = ( clower + cupper ) / 2 ; call JPROB ( PROBchk , IN , Cchk , CORR ); if cupper - clower < .0000001 then do ; Pi0 = probnorm ( cchk ); t = 51 ; end ; if probchk < PIall then clower = cchk ; else cupper = cchk ; end ; crit = cchk ; call symput ( ' Pi0 ' , char ( Pi0 )); %end ; %if & rho = %then %do ; Piall = & Piall ; Pi0 = & Pi0 ; crit = probit ( Pi0 ); pilower = pi0 ** k ; piupper = pi0 ; * print \"critical value is \" crit ; if Piall <= pilower then print \"error: PIAll is too low or Pi is too high \"; if Piall >= piupper then print \"error: PIAll is too high or Pi is too low \"; Corrl = 0 ; Corru = .999 ; if ( Piall > pilower ) & ( Piall < piupper ) then do t = 1 to 50 ; corrchk = ( corrl + corru ) / 2 ; call JPROB ( PROBchk , IN , crit , CORRchk ); if corru - corrl < .0000001 then do ; corr = corrchk ; t = 51 ; end ; if probchk < PIAll then corrl = corrchk ; else corru = corrchk ; end ; call symput ( ' Rho ' , char ( corr )); %end ; %if & Piall = %then %do ; corr = & rho ; Pi0 = & Pi0 ; crit = probit ( Pi0 ); CALL JPROB ( Piall , IN , CRIT , CORR ); call symput ( ' Piall ' , char ( Piall )); %end ; priprob = j ( k , 1 , Pi0 ); call symput ( 'k' , char ( k )); sumnum = j ( k , 1 , 0 ); rho1 = corr ; sumdenom = 0 ; %do ii = 1 %to & k ; do i & ii = 0 to 1 ; %end ; in = i1 %do ii = 2 %to & k ; || i & ii %end ; ; call jprob ( prob , in , crit , rho1 ); mean = in #mu ; cov = sig2 * ( rho1 * in ` * in + ( 1 - rho1 ) * diag ( in )) + sigma ; f = mnorm ( zsample ` , mean ` , cov ); %do ii = 1 %to & k ; if i & ii = 0 then sumnum [ & ii ] = sumnum [ & ii ] + prob * f ; %end ; sumdenom = sumdenom + prob * f ; %do ii = 1 %to & k ; end ; %end ; postprob = sumnum / sumdenom ; bf = ( postprob / ( 1 - postprob )) # (( 1 - priprob ) / priprob ); todata = zsample ` || mu ` || pstd || postprob || sigma ; create imlout from todata ; append from todata ; quit ; proc print data = imlout noobs label ; title \"Prior Probability on Individual Nulls is &Pi0\" ; title2 \"Prior Probability on Joint Null is &Piall\" ; title3 \"Prior Correlation Between Nulls is &Rho\" ; label col1 = ' Z Statistic ' col2 = ' Prior Mean Effect Size ' col3 = ' Prior StdDev Effect Size ' col4 = ' Posterior Probability ' %do ii = 1 %to & k ; %let ii1 = %eval ( & ii + 4 ); col & ii1 = Cov & ii %end ; ; run ; quit ; %mend ; /* The %MCB Macro */ /* This macro computes confidence intervals for Hsu's (1984, 1996) */ /* multiple comparisons with the best. */ /*--------------------------------------------------------------*/ /* Name: MCB */ /* Title: Multiple Comparisons with the Best */ /* Author: Randy Tobias, sasrdt@sas.com */ /* Reference: Hsu, Jason C. (1996). _Multiple_Comparisons:_ */ /* _Theory_and_methods_, Chapman & Hall, NY. */ /* Release: Version 7.01 */ /*--------------------------------------------------------------*/ /* Input: */ /* */ /* The following arguments are required. They must be the */ /* first three arguments and they must be in this order. Do */ /* not use keywords for these arguments. */ /* */ /* - the SAS data set containing the data to be analyzed */ /* - the response variable */ /* - the grouping variable */ /* */ /* The following additional arguments may be listed in any */ /* order, separated by commas: */ /* */ /* MODEL= a linear model for the response, specified */ /* using the effects syntax of GLM. The default */ /* is a one-way model in the required grouping */ /* variable. */ /* */ /* CLASS= classification variables involved in the */ /* linear model. The default is the required */ /* grouping variable. */ /* */ /* ALPHA= the level of significance for comparisons */ /* among the means. The default is 0.05. */ /* */ /* OUT= the name of the output dataset containing the */ /* MCB analysis. The default is MCBOUT. */ /* */ /* OPTIONS= a string containing either of the following */ /* options */ /* */ /* NOPRINT - suppresses printed output of */ /* results */ /* NOCLEAN - suppresses deletion of temporary */ /* datasets */ /* */ /* Output: */ /* */ /* The output dataset contains one observation for each */ /* group in the dataset. The output data set contains the */ /* following variables: */ /* */ /* LEVEL - formatted value of this group */ /* */ /* LSMEAN - sample mean response within this group */ /* */ /* SE - standard error of the sample mean for this */ /* group */ /* */ /* CLLO - lower confidence limit for the difference */ /* between the population mean of this group and */ /* the best population mean */ /* */ /* CLHI - upper confidence limit for the difference */ /* between the population mean of this group and */ /* the best population mean */ /* */ /* RVAL - the smallest alpha level at which the */ /* population mean of this group can be rejected */ /* as the best, for all groups but the one with */ /* the best sample mean */ /* */ /* SVAL - the smallest alpha level at which the */ /* population mean of this group can be selected */ /* as the best treatment, for the group with the */ /* best sample mean */ /*--------------------------------------------------------------*/ %macro mcb ( data , resp , mean , model = & mean , class = & mean , alpha = 0.05 , out = mcbout , options = ); /* / Retrieve options. /---------------------------------------------------------------------*/ %let print = 1 ; %let clean = 1 ; %let iopt = 1 ; %do %while ( %length ( %scan ( & options , & iopt ))); %if ( %upcase ( %scan ( & options , & iopt )) = NOPRINT ) %then %let print = 0 ; %else %if ( %upcase ( %scan ( & options , & iopt )) = NOCLEAN ) %then %let clean = 0 ; %else %put Warning : Unrecognized option %scan ( & options , & iopt ).; %let iopt = %eval ( & iopt + 1 ); %end ; /* / Count number of variables in grouping effect. /---------------------------------------------------------------------*/ %let ivar = 1 ; %do %while ( %length ( %scan ( & mean , & ivar , * ))); %let var & ivar = %upcase ( %scan ( & mean , & ivar , * )); %let ivar = %eval ( & ivar + 1 ); %end ; %let nvar = %eval ( & ivar - 1 ); /* / Compute ANOVA and LSMEANS /---------------------------------------------------------------------*/ ods listing close ; proc mixed data =& data ; class & class ; model & resp = & model ; lsmeans & mean ; make ' LSMeans ' out =& out ; run ; ods listing ; data & out ; set & out ; orig_n = _n_ ; proc sort data =& out out =& out ; by & mean ; run ; /* / Retrieve the levels of the classification variable. /---------------------------------------------------------------------*/ data & out ; set & out ; drop tvalue probt ; length level $ 20 ; level = '' ; %do ivar = 1 %to & nvar ; level = trim ( left ( level )) || ' ' || trim ( left ( && var & ivar )); %end ; call symput ( ' nlev ' , trim ( left ( _n_ ))); call symput ( ' lev ' || trim ( left ( _n_ )), level ); run ; /* / Now, perform Dunnett's comparison-with-control test with each / level as the control. /---------------------------------------------------------------------*/ ods listing close ; proc mixed data =& data ; class & class ; model & resp = & model / dfm = sat ; %do ilev = 1 %to & nlev ; %let control = ; %do ivar = 1 %to & nvar ; %let control = & control \"%scan(&&lev&ilev,&ivar)\" ; %end ; lsmeans & mean / diff = controlu ( & control ) cl alpha =& alpha adjust = dunnett ; %end ; make ' Diffs ' out = _mcb ; run ; ods listing ; data _mcb ; set _mcb ; length level1 $ 20 level2 $ 20 ; level1 = '' ; level2 = '' ; %do ivar = 1 %to & nvar ; %let v1 = && var & ivar ; %let v2 = _ && var & ivar ; %if ( %length ( & v2 ) > 8 ) %then %let var2 = %substr ( & v2 , 1 , 8 ); level1 = trim ( left ( level1 )) || ' ' || trim ( left ( & v1 )); level2 = trim ( left ( level2 )) || ' ' || trim ( left ( & v2 )); %end ; run ; /* / Sort results by first and second level, respectively. /---------------------------------------------------------------------*/ proc sort data = _mcb out = _tmcb1 ; by level1 level2 ; proc transpose data = _tmcb1 out = _tmcb1 prefix = lo ; by level1 ; var adjlow ; data _tmcb1 ; set _tmcb1 ; ilev = _n_ ; proc sort data = _mcb out = _tmcb2 ; by level2 level1 ; proc transpose data = _tmcb2 out = _tmcb2 prefix = lo ; by level2 ; var adjlow ; data _tmcb2 ; set _tmcb2 ; ilev = _n_ ; run ; /* / From Hsu (1996), p. 94: / Di+ = +( min_{j!=i} m_i - m_j + d^i*s*sqrt(1/n_i + 1/n_j))^+ / = +(-max_{j!=i} m_j - m_i - d^i*s*sqrt(1/n_i + 1/n_j))^+ / G = {i : min_{j!=i} m_i - m_j + d^i*s*sqrt(1/n_i + 1/n_j) > 0} / Di- = 0 if G = {i} / = min_{j!=i} m_i - m_j + d^j*s*sqrt(1/n_i + 1/n_j) otherwise /---------------------------------------------------------------------*/ data clhi ; set _tmcb2 ; keep level2 clhi ilev ; rename level2 = level ; clhi = - max ( of lo1 - lo %eval ( & nlev - 1 )); if ( clhi < 0 ) then clhi = 0 ; data _g ; set clhi ; if ( clhi > 0 ); run ; %let ng = 0 ; %let g = 0 ; data _null_ ; set _g ; call symput ( ' ng ' , _n_ ); call symput ( 'g' , ilev ); run ; data cllo ; set _tmcb1 ; keep level1 cllo ilev ; rename level1 = level ; if (( & ng = 1 ) & ( & g = ilev )) then cllo = 0 ; else cllo = min ( of lo1 - lo %eval ( & nlev - 1 )); run ; data cl ; merge cllo clhi ; by level ; data & out ; merge & out cl ; drop df ilev ; run ; /* / Compute RVAL and SVAL. RVAL is just the p-value for Dunnett's / test for all means except the best, and SVAL is the maximum RVAL. /---------------------------------------------------------------------*/ data _slev ; set & out ; _i_ = _n_ ; proc sort data = _slev out = _slev ; by descending estimate ; %let ibest = 0 ; data _null_ ; set _slev ; if ( _n_ = 1 ) then call symput ( ' ibest ' , _i_ ); proc sort data = _mcb out = _pval ; by level2 adjp ; proc transpose data = _pval out = _pval prefix = p ; by level2 ; var adjp ; data _pval ; set _pval ; keep level2 rval ; rename level2 = level ; if ( _n_ = & ibest ) then rval = .; else rval = p1 ; proc sort data = _pval out = _spval ; by descending rval ; data _null_ ; set _spval ; if ( _n_ = 1 ) then call symput ( ' sval ' , rval ); data _pval ; set _pval ; if ( _n_ = & ibest ) then sval = & sval ; data & out ; merge & out _pval ; by level ; drop level ; proc sort data =& out out =& out ; by orig_n ; data & out ; set & out ; drop orig_n ; run ; /* / Print and clean up. /---------------------------------------------------------------------*/ %if ( & print ) %then %do ; proc print uniform data =& out noobs ; run ; %end ; %if ( & clean ) %then %do ; proc datasets library = work nolist ; delete cllo clhi cl _slev _spval _pval _mcb _tmcb1 _tmcb2 _g ; run ; %end ; %mend ; /*--------------------------------------------------------------*/ /* Name: MCW */ /* Title: Multiple Comparisons with the Worst */ /* Author: Randy Tobias, sasrdt@sas.com */ /* Reference: Hsu, Jason C. (1996). _Multiple_Comparisons:_ */ /* _Theory_and_methods_, Chapman & Hall, NY. */ /* Release: Version 7.01 */ /*--------------------------------------------------------------*/ /* Input: */ /* */ /* The following arguments are required. They must be the */ /* first three arguments and they must be in this order. Do */ /* not use keywords for these arguments. */ /* */ /* - the SAS data set containing the data to be analyzed */ /* - the response variable */ /* - the grouping variable */ /* */ /* The following additional arguments may be listed in any */ /* order, separated by commas: */ /* */ /* MODEL= a linear model for the response, specified */ /* using the effects syntax of GLM. The default */ /* is a one-way model in the required grouping */ /* variable. */ /* */ /* CLASS= classification variables involved in the */ /* linear model. The default is the required */ /* grouping variable. */ /* */ /* ALPHA= the level of significance for comparisons */ /* among the means. The default is 0.05. */ /* */ /* OUT= the name of the output dataset containing the */ /* MCB analysis. The default is MCBOUT. */ /* */ /* OPTIONS= a string containing either of the following */ /* options */ /* */ /* NOPRINT - suppresses printed output of */ /* results */ /* NOCLEAN - suppresses deletion of temporary */ /* datasets */ /* */ /* Output: */ /* */ /* The output dataset contains one observation for each */ /* group in the dataset. The output data set contains the */ /* following variables: */ /* */ /* LEVEL - formatted value of this group */ /* */ /* LSMEAN - sample mean response within this group */ /* */ /* SE - standard error of the sample mean for this */ /* group */ /* */ /* CLLO - lower confidence limit for the difference */ /* between the population mean of this group and */ /* the worst population mean */ /* */ /* CLHI - upper confidence limit for the difference */ /* between the population mean of this group and */ /* the worst population mean */ /* */ /* RVAL - the smallest alpha level at which the */ /* population mean of this group can be rejected */ /* as the worst, for all groups but the one with */ /* the worst sample mean */ /* */ /* SVAL - the smallest alpha level at which the */ /* population mean of this group can be selected */ /* as the worst treatment, for the group with the */ /* worst sample mean */ /*--------------------------------------------------------------*/ %macro mcw ( data , resp , mean , model = & mean , class = & mean , alpha = 0.05 , out = mcbout , options = ); /* / Retrieve options. /---------------------------------------------------------------------*/ %let print = 1 ; %let clean = 1 ; %let iopt = 1 ; %do %while ( %length ( %scan ( & options , & iopt ))); %if ( %upcase ( %scan ( & options , & iopt )) = NOPRINT ) %then %let print = 0 ; %else %if ( %upcase ( %scan ( & options , & iopt )) = NOCLEAN ) %then %let clean = 0 ; %else %put Warning : Unrecognized option %scan ( & options , & iopt ).; %let iopt = %eval ( & iopt + 1 ); %end ; /* / Copy the dataset but reverse the sign of the response, so that / the worst is the maximum response. /---------------------------------------------------------------------*/ data _tmpds ; set & data ; & resp = -& resp ; run ; %mcb ( _tmpds , & resp , & mean , model = & model , class = & class , alpha = & alpha , out = & out , options = & options ); /* / Reverse the sign of the results, so that the worst is again the / minimum response. /---------------------------------------------------------------------*/ data & out ; set & out ; rename cllo = cllo ; rename clhi = clhi ; estimate = - estimate ; tvalue = - tvalue ; _temp = - cllo ; cllo = - clhi ; clhi = _temp ; drop _temp ; run ; /* / Print and clean up. /---------------------------------------------------------------------*/ %if ( & print ) %then %do ; proc print uniform data =& out noobs ; run ; %end ; %if ( & clean ) %then %do ; proc datasets library = work nolist ; delete _tmpds ; run ; %end ; %mend ; /*--------------------------------------------------------------*/ /* Name: UMCB */ /* Title: Unconstrained Multiple Comparisons with the Best */ /* Author: Randy Tobias, sasrdt@sas.com */ /* Reference: Hsu, Jason C. (1996). _Multiple_Comparisons:_ */ /* _Theory_and_methods_, Chapman & Hall, NY. */ /* Release: Version 7.01 */ /*--------------------------------------------------------------*/ /* Input: */ /* */ /* The following arguments are required. They must be the */ /* first three arguments and they must be in this order. Do */ /* not use keywords for these arguments. */ /* */ /* - the SAS data set containing the data to be analyzed */ /* - the response variable */ /* - the grouping variable */ /* */ /* The following additional arguments may be listed in any */ /* order, separated by commas: */ /* */ /* MODEL= a linear model for the response, specified */ /* using the effects syntax of GLM. The default */ /* is a one-way model in the required grouping */ /* variable. */ /* */ /* CLASS= classification variables involved in the */ /* linear model. The default is the required */ /* grouping variable. */ /* */ /* ALPHA= the level of significance for comparisons */ /* among the means. The default is 0.05. */ /* */ /* OUT= the name of the output dataset containing the */ /* MCB analysis. The default is MCBOUT. */ /* */ /* OPTIONS= a string containing either of the following */ /* options */ /* */ /* NOPRINT - suppresses printed output of */ /* results */ /* NOCLEAN - suppresses deletion of temporary */ /* datasets */ /* */ /* Output: */ /* */ /* The output dataset contains one observation for each */ /* group in the dataset. The output data set contains the */ /* following variables: */ /* */ /* LEVEL - formatted value of this group */ /* */ /* LSMEAN - sample mean response within this group */ /* */ /* SE - standard error of the sample mean for this */ /* group */ /* */ /* CLLO - lower confidence limit for the difference */ /* between the population mean of this group and */ /* the best population mean */ /* */ /* CLHI - upper confidence limit for the difference */ /* between the population mean of this group and */ /* the best population mean */ /*--------------------------------------------------------------*/ %macro umcb ( data , resp , mean , model = & mean , class = & mean , alpha = 0.05 , out = mcbout , method = EH , options = ); /* / Retrieve options. /---------------------------------------------------------------------*/ %let print = 1 ; %let clean = 1 ; %let iopt = 1 ; %do %while ( %length ( %scan ( & options , & iopt ))); %if ( %upcase ( %scan ( & options , & iopt )) = NOPRINT ) %then %let print = 0 ; %else %if ( %upcase ( %scan ( & options , & iopt )) = NOCLEAN ) %then %let clean = 0 ; %else %put Warning : Unrecognized option %scan ( & options , & iopt ).; %let iopt = %eval ( & iopt + 1 ); %end ; /* / Count number of variables in grouping effect. /---------------------------------------------------------------------*/ %let ivar = 1 ; %do %while ( %length ( %scan ( & mean , & ivar , * ))); %let var & ivar = %upcase ( %scan ( & mean , & ivar , * )); %let ivar = %eval ( & ivar + 1 ); %end ; %let nvar = %eval ( & ivar - 1 ); /* / Compute ANOVA and LSMEANS /---------------------------------------------------------------------*/ ods listing close ; proc mixed data =& data ; class & class ; model & resp = & model ; lsmeans & mean ; make ' LSMeans ' out =& out ; run ; ods listing ; data & out ; set & out ; orig_n = _n_ ; proc sort data =& out out =& out ; by & mean ; run ; /* / Retrieve the levels of the classification variable. /---------------------------------------------------------------------*/ data & out ; set & out ; drop tvalue probt ; length level $ 20 ; level = '' ; %do ivar = 1 %to & nvar ; level = trim ( left ( level )) || ' ' || trim ( left ( && var & ivar )); %end ; call symput ( ' nlev ' , trim ( left ( _n_ ))); call symput ( ' lev ' || trim ( left ( _n_ )), level ); run ; %if ( %upcase ( & method ) = TK ) %then %do ; ods listing close ; proc mixed data =& data ; class & class ; model & resp = & model ; lsmeans & mean / diff = all cl alpha =& alpha adjust = tukey ; make ' Diffs ' out = _mcb ; run ; ods listing ; proc sort data = _mcb out = _mcb ; by & mean _ & mean ; run ; /* / Add reverse differences. /---------------------------------------------------------------------*/ data _mcb ; set _mcb ; keep level1 level2 adjlow adjupp adjp ; length level1 $ 20 level2 $ 20 ; level1 = '' ; level2 = '' ; %do ivar = 1 %to & nvar ; %let v1 = && var & ivar ; %let v2 = _ && var & ivar ; %if ( %length ( & v2 ) > 8 ) %then %let var2 = %substr ( & v2 , 1 , 8 ); level1 = trim ( left ( level1 )) || ' ' || trim ( left ( & v1 )); level2 = trim ( left ( level2 )) || ' ' || trim ( left ( & v2 )); %end ; output ; _tmplev = level1 ; level1 = level2 ; level2 = _tmplev ; _tmpcl = - adjlow ; adjlow = - adjupp ; adjupp = _tmpcl ; output ; run ; /* / Confidence limits are the minimum lower and upper CL's for each / level. /---------------------------------------------------------------------*/ proc sort data = _mcb out = _mcb ; by level1 level2 ; proc transpose data = _mcb out = cllo prefix = lo ; by level1 ; var adjlow ; proc transpose data = _mcb out = clhi prefix = hi ; by level1 ; var adjupp ; data cllo ; set cllo ; rename level1 = level ; cllo = min ( of lo1 - lo %eval ( & nlev - 1 )); data clhi ; set clhi ; rename level1 = level ; clhi = min ( of hi1 - hi %eval ( & nlev - 1 )); data cl ; merge cllo ( keep = level cllo ) clhi ( keep = level clhi ); run ; data & out ; merge & out cl ; drop level ; run ; %if ( & clean ) %then %do ; proc datasets library = work nolist ; delete _mcb cllo clhi cl ; run ; %end ; %end ; %else %do ; /* / Now, perform Dunnett's comparison-with-control test with each / level as the control. /---------------------------------------------------------------------*/ ods listing close ; proc mixed data =& data ; class & class ; model & resp = & model / dfm = sat ; %do ilev = 1 %to & nlev ; %let control = ; %do ivar = 1 %to & nvar ; %let control = & control \"%scan(&&lev&ilev,&ivar)\" ; %end ; lsmeans & mean / diff = control ( & control ) cl alpha =& alpha adjust = dunnett ; %end ; make ' Diffs ' out = _mcb ; run ; ods listing ; data _mcb ; set _mcb ; length level1 $ 20 level2 $ 20 ; level1 = '' ; level2 = '' ; %do ivar = 1 %to & nvar ; %let v1 = && var & ivar ; %let v2 = _ && var & ivar ; %if ( %length ( & v2 ) > 8 ) %then %let var2 = %substr ( & v2 , 1 , 8 ); level1 = trim ( left ( level1 )) || ' ' || trim ( left ( & v1 )); level2 = trim ( left ( level2 )) || ' ' || trim ( left ( & v2 )); %end ; proc sort data = _mcb out = _mcb ; by level2 level1 ; data cl ; keep cllo clhi ; array m { & nlev , & nlev }; /* m[i1]-m[i2] - |d|^i2*s[i1,i2] */ array p { & nlev , & nlev }; /* m[i1]-m[i2] + |d|^i2*s[i1,i2] */ array s { & nlev }; array l { & nlev }; array u { & nlev }; do i = 1 to & nlev ; do j = 1 to & nlev ; m [ i , j ] = .; p [ i , j ] = .; end ; end ; do obs = 1 to %eval ( & nlev * ( & nlev - 1 )); set _mcb point = obs ; j = mod (( obs - 1 ), %eval ( & nlev - 1 )) + 1 ; i2 = int (( obs - 1 ) / %eval ( & nlev - 1 )) + 1 ; if ( j < i2 ) then i1 = j ; else i1 = j + 1 ; m [ i1 , i2 ] = adjlow ; p [ i1 , i2 ] = adjupp ; end ; /* / From Hsu (1996), p. 120: / S = {i : min_{j!=i} m_i - m_j + |d|^i*s[i,j] > 0} / = {i : min_{j!=i} -(m_j - m_i - |d|^i*s[i,j]) > 0} / = {i : min_{j!=i} -m[j,i] > 0} /---------------------------------------------------------------------*/ ns = 0 ; do i = 1 to & nlev ; minmmji = 1e12 ; do j = 1 to & nlev ; if ( j ^= i ) then do ; if ( - m [ j , i ] < minmmji ) then minmmji = - m [ j , i ]; end ; end ; s [ i ] = ( minmmji > 0 ); ns = ns + s [ i ]; end ; /* / From Hsu (1996), p. 115: / Lij = (i ^= j) * (m_i - m_j + |d|^j*s[i,j]) / = (i ^= j) * p[i,j] / Li = min_{j in S} Lij / / Uij = (i ^= j) * -(m_i - m_j + |d|^j*s[i,j])^- / = (i ^= j) * min(0,p[i,j]) / Ui = max_{j in S} Uij put \"Edwards-Hsu intervals\"; do i = 1 to &nlev; li = 1e12; do j = 1 to &nlev; if (s[j]) then do; if (i = j) then lij = 0; else lij = m[i,j]; if (lij < li) then li = lij; end; end; ui = -1e12; do j = 1 to &nlev; if (s[j]) then do; if (i = j) then uij = 0; else uij = min(0,p[i,j]); if (uij > ui) then ui = uij; end; end; put li 7.3 \" < mu\" i 1. \" - max_j muj < \" ui 7.3; end; /---------------------------------------------------------------------*/ /* / From Hsu (1996), p. 120: / If S = {i} then / Li* = (min_{j!=i} m_i - m_j - |d|^i*s[i,j] )^+ / = (min_{j!=i} -(m_j - m_i + |d|^i*s[i,j]))^+ / = (min_{j!=i} -p[j,i])^+ / Otherwise / Li* = min_{j in S,j!=i} m_i - m_j - |d|^j*s[i,j] / = min_{j in S,j!=i} m[i,j] /---------------------------------------------------------------------*/ do i = 1 to & nlev ; if (( ns = 1 ) & s [ i ]) then do ; minmpji = 1e12 ; do j = 1 to & nlev ; if ( j ^= i ) then do ; if ( - p [ j , i ] < minmpji ) then minmpji = - p [ j , i ]; end ; end ; l [ i ] = max ( 0 , minmpji ); end ; else do ; minpmij = 1e12 ; do j = 1 to & nlev ; if ( s [ j ] & ( j ^= i )) then do ; if ( m [ i , j ] < minpmij ) then minpmij = m [ i , j ]; end ; end ; l [ i ] = minpmij ; end ; end ; /* / From Hsu (1996), p. 120: / If i in S then / Ui* = min_{j!=i} m_i - m_j + |d|^i*s[i,j] / = min_{j!=i} -(m_j - m_i - |d|^i*s[i,j]) / = min_{j!=i} -m[j,i] / Otherwise / Ui* = -(max_{j in S,} m_i - m_j + |d|^j*s[i,j])^- / = -(max_{j in S,} p[i,j])^- /---------------------------------------------------------------------*/ do i = 1 to & nlev ; if ( s [ i ]) then do ; minmmji = 1e12 ; do j = 1 to & nlev ; if ( j ^= i ) then do ; if ( - m [ j , i ] < minmmji ) then minmmji = - m [ j , i ]; end ; end ; u [ i ] = minmmji ; end ; else do ; minppij = - 1e12 ; do j = 1 to & nlev ; if ( s [ j ]) then do ; if ( p [ i , j ] > minppij ) then minppij = p [ i , j ]; end ; end ; u [ i ] = minppij ; end ; end ; do i = 1 to & nlev ; cllo = l { i }; clhi = u { i }; output ; end ; stop ; data & out ; merge & out cl ; drop level ; run ; %if ( & clean ) %then %do ; proc datasets library = work nolist ; delete _mcb cl ; run ; %end ; %end ; proc sort data =& out out =& out ; by orig_n ; data & out ; set & out ; drop orig_n ; run ; /* / Print and clean up. /---------------------------------------------------------------------*/ %if ( & print ) %then %do ; proc print uniform data =& out noobs ; run ; %end ; %mend ; /*--------------------------------------------------------------*/ /* Name: UMCW */ /* Title: Unconstrained Multiple Comparisons with the Worst */ /* Author: Randy Tobias, sasrdt@sas.com */ /* Reference: Hsu, Jason C. (1996). _Multiple_Comparisons:_ */ /* _Theory_and_methods_, Chapman & Hall, NY. */ /* Release: Version 7.01 */ /*--------------------------------------------------------------*/ /* Input: */ /* */ /* The following arguments are required. They must be the */ /* first three arguments and they must be in this order. Do */ /* not use keywords for these arguments. */ /* */ /* - the SAS data set containing the data to be analyzed */ /* - the response variable */ /* - the grouping variable */ /* */ /* The following additional arguments may be listed in any */ /* order, separated by commas: */ /* */ /* MODEL= a linear model for the response, specified */ /* using the effects syntax of GLM. The default */ /* is a one-way model in the required grouping */ /* variable. */ /* */ /* CLASS= classification variables involved in the */ /* linear model. The default is the required */ /* grouping variable. */ /* */ /* ALPHA= the level of significance for comparisons */ /* among the means. The default is 0.05. */ /* */ /* OUT= the name of the output dataset containing the */ /* MCB analysis. The default is MCBOUT. */ /* */ /* OPTIONS= a string containing either of the following */ /* options */ /* */ /* NOPRINT - suppresses printed output of */ /* results */ /* NOCLEAN - suppresses deletion of temporary */ /* datasets */ /* */ /* Output: */ /* */ /* The output dataset contains one observation for each */ /* group in the dataset. The output data set contains the */ /* following variables: */ /* */ /* LEVEL - formatted value of this group */ /* */ /* LSMEAN - sample mean response within this group */ /* */ /* SE - standard error of the sample mean for this */ /* group */ /* */ /* CLLO - lower confidence limit for the difference */ /* between the population mean of this group and */ /* the worst population mean */ /* */ /* CLHI - upper confidence limit for the difference */ /* between the population mean of this group and */ /* the worst population mean */ /*--------------------------------------------------------------*/ %macro umcw ( data , resp , mean , model = & mean , class = & mean , alpha = 0.05 , out = mcbout , method = EH , options = ); /* / Retrieve options. /---------------------------------------------------------------------*/ %let print = 1 ; %let clean = 1 ; %let iopt = 1 ; %do %while ( %length ( %scan ( & options , & iopt ))); %if ( %upcase ( %scan ( & options , & iopt )) = NOPRINT ) %then %let print = 0 ; %else %if ( %upcase ( %scan ( & options , & iopt )) = NOCLEAN ) %then %let clean = 0 ; %else %put Warning : Unrecognized option %scan ( & options , & iopt ).; %let iopt = %eval ( & iopt + 1 ); %end ; /* / Copy the dataset but reverse the sign of the response, so that / the worst is the maximum response. /---------------------------------------------------------------------*/ data _tmpds ; set & data ; & resp = -& resp ; run ; %umcb ( _tmpds , & resp , & mean , model = & model , class = & class , alpha = & alpha , out = & out , method = & method , options = & options ); /* / Reverse the sign of the results, so that the worst is again the / minimum response. /---------------------------------------------------------------------*/ data & out ; set & out ; rename cllo = cllo ; rename clhi = clhi ; estimate = - estimate ; tvalue = - tvalue ; _temp = - cllo ; cllo = - clhi ; clhi = _temp ; drop _temp ; run ; /* / Print and clean up. /---------------------------------------------------------------------*/ %if ( & print ) %then %do ; proc print uniform data =& out noobs ; run ; %end ; %if ( & clean ) %then %do ; proc datasets library = work nolist ; delete _tmpds ; run ; %end ; %mend ;","title":"Post-hoc Tests"},{"location":"other-analysis/propensity-score/","text":"Propensity Score Link What is it? Link The objective of randomization in statistics is to obtain groups that are comparable in terms of both observed and unobserved characteristics. When randomization is not possible, causal inference is complicated by the fact that a group that received a treatment or experienced an event maybe very different from another group that did not experience the event or receive the treatment. Thus, it is not clear whether a difference in certain outcome of interest is due to the treatment or is the product of prior differences among groups . Propensity score methods were developed to facilitate the creation of comparison groups that are similar in terms of the distribution of observed characteristics . The first step involves estimating the likelihood (the propensity score) that a person would have received the treatment given certain characteristics. More formally, the propensity score is the conditional probability of assignment to a particular treatment given a vector of observed covariates . Two key assumptions of propensity scores are that both the outcome of interest and the treatment assignment do not depend on unobservable characteristics . Computing the Propensity Score Link In order to program the corresponding model in SAS, the response variable is the group/arm to which the patient belongs and the predictors are all the baseline variables which could affect the assignment to a certain group. While the model is fitted the propensity score value can be computed and kept in an output: 1 2 3 4 5 PROC LOGISTIC DATA = input - SAS - data - set ; CLASS sex site ( param = ref ); MODEL arm = sex age weight site serologytests / FIRTH ; OUTPUT OUT = output - SAS - data - set PROB = ps - variable - customized - name ; RUN ; This kind of analysis are commonly used in observacional studies on which the patient is not randomized to a certain group but it belongs to it due to a certain diagnostic. In order to correct the possible effect of unbalanced population groups, the propensity score value can be included in the model as a way to isolate the effects due to the treatment from the baseline characteristics. Propensity Score Matching Link After estimating the propensity scores, they are used to group observations that are close to each other. One way of accomplishing this is to classify treated and untreated observations into subgroups and then separately compare the outcome for each subgroup. This method is usually referred as subclassification on the propensity scores (Rosenbaum and Rubin 1984). The other way is to match one treated unit to one or more untreated controls, which is usually referred as matching on the propensity score (Rosenbaum and Rubin 1983). Key in the implementation of matching using propensity scores is to decide what metric to use when evaluating the distance between scores (usually the absolute value or the Mahalanobis metric) and what type of algorithm to implement (local or global optimal). Pair-matching Methods Link The most common implementation is 1:1 (1 to 1) or pair-matching in which pairs of treated and untreated subjects are formed which allows to estimate for average treatment effect in the treated (ATT). Methods without Replacement Link We match each untreated subject to at most one treated subject. Once an untreated subject has been matched to a treated subject, that untreated subject is no longer eligible for consideration as a match for other treated subjects. Global optimal matching: forms matched pairs so as to minimize the average within-pair difference in propensity scores Local optimal, greedy or nearest available neighbor matching: selects a treated subject and then selects as a matched control subject the one whose propensity score is closest to that of the treated subject (if multiple untreated subjects are equally close to the treated subject, one of these untreated subjects is selected at random). In each iteration, the best (optimal) control is chosen, but this process does not guarantee that the total distance between propensity scores is minimized. Four different approaches: Sequentially treated subjects from highest to lowest propensity score Sequentially treated subjects from lowest to highest propensity score Sequentially treated subjects in the order of the best possible match (the first selected treated subject is that treated subject who is closest to an untreated subject and so on) Treated subjects in a random order Local optimal, greedy or nearest available neighbour matching within specified caliper widths: we can match treated and untreated subjects only if the absolute difference in their propensity scores is within a prespecified maximal distance (the caliper distance, defined as a proportion of the standard deviation of the logit of the propensity score) Note Although the propensity score is the natural metric to use, when using caliper matching, a reduction in bias due to the use of different caliper widths has been described when matching on the logit of the propensity score. Although it is difficult to know beforehand the optimal choice of caliper width, some researchers (Rosenbuam & Rubin, 1985; Austin, 2011) have recommended using a caliper width that is equal to 0.2 of the standard deviation of the logit of the propensity score, i.e., $0.2\\cdot\\sqrt\\left ( \\sigma^2_1+\\sigma^2_2 \\right )/2$. Sequentially treated subjects from highest to lowest propensity score Sequentially treated subjects from lowest to highest propensity score Sequentially treated subjects in the order of the best possible match (the first selected treated subject is that treated subject who is closest to an untreated subject and so on) Treated subjects in a random order Note Optimal matching and greedy nearest neighbor matching on the propensity score will result in all treated subjects being matched to an untreated subject (assuming that the number of untreated subjects is at least as large as the number of treated subjects). However, greedy nearest neighbor matching within specified caliper widths may not result in all treated subjects being matched to an untreated subject, because for some treated subjects, there may not be any untreated subjects who are unmatched and whose propensity score lies within the specified caliper distance of that of the treated subject. The objective of the caliper matching is to avoid bad matches. Methods with Replacement Link Permits the same untreated subject to be matched to multiple treated subjects (because untreated subjects are recycled or allowed to be included in multiple matched sets, the order in which the treated subjects are selected has no effect on the formation of matched pairs). Matching with replacement minimizes the propensity score distance between the matched units since each treated unit is matched to the closest control, even if the control has been selected before. Nearest neighbor matching with replacement: matches each treated subject to the nearest untreated subject Nearest neighbor matching within specified caliper widths with replacement: matches each treated subject to the nearest untreated subject (subject to possible caliper restrictions) 1 to N Matching Methods Link They include matching each treated unit to more than one control match. This can be done by creating N replicas of each treated unit and proceeding as described above. Radius Matching Link All the control units within a certain distance are chosen (Dehejia and Wahba 1999). Mahalanobis Metric Matching Link In this type of matching, the definition of distance is changed. The similarity between the propensity score of treated and untreated units is evaluated using the multidimensional Mahalanobis metric matching: $D_{ij}=\\sqrt{\\left ( x_i-y_j \\right )^TS^{-1}\\left ( x_i-y_j \\right )}$ where $S^{-1}$ is the pooled variance-covariance matrix and x and y are multivariate vectors. Note that if the variance-covariance matrix is an identity matrix the Mahalanobis metric is reduced to the familiar Euclidean metric. Usually the Mahalanobis metric matching includes the propensity score and other covariates that are considered to be important and are hoped to be balanced (Rosenbaum and Rubin 1985). PSMatching Macro Link With the macro PSMatching.sas (Coca-Perraillon, 2006) different methods can be applied to calculate the matching once you have the propensity score. First you have to prepare the following input: Treatment data set containing the treatment cases along with the patient number idT and the corresponding propensity score pscoreT Control data set containing the control cases along with the patient number idC and the corresponding propensity score pscoreC The parameters datatreatment and datacontrol refer to the Treatment and Control datasets and they do not need to be sorted. The method parameter can be NN (nearest available neighbor), caliper or radius . Caliper can be any number indicating the size of the caliper and the parameter replacement takes the values of yes or no. All the parameters are case insensitive. If the method is NN , the caliper parameter is ignored. 1 %PSMatching(datatreatment=treatment, datacontrol=control, method=caliper, numberofcontrols=1, caliper=0.2, replacement=no); Here is the macro code: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 %macro PSMatching ( datatreatment = , datacontrol = , method = , numberofcontrols = , caliper = , replacement = ); /* Create copies of the treated units if N > 1 */ ; data _Treatment0 ( drop = i ); set Treatment ; do i = 1 to & numberofcontrols ; RandomNumber = ranuni ( 12345 ); output ; end ; run ; /* Randomly sort both datasets */ proc sort data = _Treatment0 out = _Treatment ( drop = RandomNumber ); by RandomNumber ; run ; data _Control0 ; set Control ; RandomNumber = ranuni ( 45678 ); run ; proc sort data = _Control0 out = _Control ( drop = RandomNumber ); by RandomNumber ; run ; data Matched ( keep = IdSelectedControl MatchedToTreatID ); length pscoreC 8 ; length idC 8 ; /* Load Control dataset into the hash object */ if _N_ = 1 then do ; declare hash h ( dataset : \"_Control\" , ordered : ' no ' ); declare hiter iter ( 'h' ); h . defineKey ( ' idC ' ); h . defineData ( ' pscoreC ' , ' idC ' ); h . defineDone (); call missing ( idC , pscoreC ); end ; /* Open the treatment */ set _Treatment ; %if %upcase ( & method ) ~= RADIUS %then %do ; retain BestDistance 99 ; %end ; /* Iterate over the hash */ rc = iter . first (); if ( rc = 0 ) then BestDistance = 99 ; do while ( rc = 0 ); /* Caliper */ %if %upcase ( & method ) = CALIPER %then %do ; if ( pscoreT - & caliper ) <= pscoreC <= ( pscoreT + & caliper ) then do ; ScoreDistance = abs ( pscoreT - pscoreC ); if ScoreDistance < BestDistance then do ; BestDistance = ScoreDistance ; IdSelectedControl = idC ; MatchedToTreatID = idT ; end ; end ; %end ; /* NN */ %if %upcase ( & method ) = NN %then %do ; ScoreDistance = abs ( pscoreT - pscoreC ); if ScoreDistance < BestDistance then do ; BestDistance = ScoreDistance ; IdSelectedControl = idC ; MatchedToTreatID = idT ; end ; %end ; %if %upcase ( & method ) = NN or %upcase ( & method ) = CALIPER %then %do ; rc = iter . next (); /* Output the best control and remove it */ if ( rc ~= 0 ) and BestDistance ~= 99 then do ; output ; %if %upcase ( & replacement ) = NO %then %do ; rc1 = h . remove ( key : IdSelectedControl ); %end ; end ; %end ; /* Radius */ %if %upcase ( & method ) = RADIUS %then %do ; if ( pscoreT - & caliper ) <= pscoreC <= ( pscoreT + & caliper ) then do ; IdSelectedControl = idC ; MatchedToTreatID = idT ; output ; end ; rc = iter . next (); %end ; end ; run ; /* Delete temporary tables. Quote for debugging */ ods select none ; proc datasets ; delete _ :( gennum = all ); run ; ods select all ; %mend PSMatching ; PROC PSMATCH Link Here and here you can find the documentation of this procedure. Stratification on the Propensity Score Link Stratification on the propensity score involves stratifying subjects into mutually exclusive subsets based on their estimated propensity score . Subjects are ranked according to their estimated propensity score. Subjects are then stratified into subsets based on previously defined thresholds of the estimated propensity score. A common approach is to divide subjects into five equal-size groups using the quintiles of the estimated propensity score . Rosenbaum and Rubin (1984) stated that stratifying on the quintiles of the propensity score eliminates approximately 90% of the bias due to measured confounders when estimating a linear treatment effect. Within each propensity score stratum, treated and untreated subjects will have roughly similar values of the propensity score. Therefore, when the propensity score has been correctly specified, the distribution of measured baseline covariates will be approximately similar between treated and untreated subjects within the same stratum. More info on this topic here .","title":"Propensity Score"},{"location":"other-analysis/propensity-score/#propensity-score","text":"","title":"Propensity Score"},{"location":"other-analysis/propensity-score/#what-is-it","text":"The objective of randomization in statistics is to obtain groups that are comparable in terms of both observed and unobserved characteristics. When randomization is not possible, causal inference is complicated by the fact that a group that received a treatment or experienced an event maybe very different from another group that did not experience the event or receive the treatment. Thus, it is not clear whether a difference in certain outcome of interest is due to the treatment or is the product of prior differences among groups . Propensity score methods were developed to facilitate the creation of comparison groups that are similar in terms of the distribution of observed characteristics . The first step involves estimating the likelihood (the propensity score) that a person would have received the treatment given certain characteristics. More formally, the propensity score is the conditional probability of assignment to a particular treatment given a vector of observed covariates . Two key assumptions of propensity scores are that both the outcome of interest and the treatment assignment do not depend on unobservable characteristics .","title":"What is it?"},{"location":"other-analysis/propensity-score/#computing-the-propensity-score","text":"In order to program the corresponding model in SAS, the response variable is the group/arm to which the patient belongs and the predictors are all the baseline variables which could affect the assignment to a certain group. While the model is fitted the propensity score value can be computed and kept in an output: 1 2 3 4 5 PROC LOGISTIC DATA = input - SAS - data - set ; CLASS sex site ( param = ref ); MODEL arm = sex age weight site serologytests / FIRTH ; OUTPUT OUT = output - SAS - data - set PROB = ps - variable - customized - name ; RUN ; This kind of analysis are commonly used in observacional studies on which the patient is not randomized to a certain group but it belongs to it due to a certain diagnostic. In order to correct the possible effect of unbalanced population groups, the propensity score value can be included in the model as a way to isolate the effects due to the treatment from the baseline characteristics.","title":"Computing the Propensity Score"},{"location":"other-analysis/propensity-score/#propensity-score-matching","text":"After estimating the propensity scores, they are used to group observations that are close to each other. One way of accomplishing this is to classify treated and untreated observations into subgroups and then separately compare the outcome for each subgroup. This method is usually referred as subclassification on the propensity scores (Rosenbaum and Rubin 1984). The other way is to match one treated unit to one or more untreated controls, which is usually referred as matching on the propensity score (Rosenbaum and Rubin 1983). Key in the implementation of matching using propensity scores is to decide what metric to use when evaluating the distance between scores (usually the absolute value or the Mahalanobis metric) and what type of algorithm to implement (local or global optimal).","title":"Propensity Score Matching"},{"location":"other-analysis/propensity-score/#pair-matching-methods","text":"The most common implementation is 1:1 (1 to 1) or pair-matching in which pairs of treated and untreated subjects are formed which allows to estimate for average treatment effect in the treated (ATT).","title":"Pair-matching Methods"},{"location":"other-analysis/propensity-score/#methods-without-replacement","text":"We match each untreated subject to at most one treated subject. Once an untreated subject has been matched to a treated subject, that untreated subject is no longer eligible for consideration as a match for other treated subjects. Global optimal matching: forms matched pairs so as to minimize the average within-pair difference in propensity scores Local optimal, greedy or nearest available neighbor matching: selects a treated subject and then selects as a matched control subject the one whose propensity score is closest to that of the treated subject (if multiple untreated subjects are equally close to the treated subject, one of these untreated subjects is selected at random). In each iteration, the best (optimal) control is chosen, but this process does not guarantee that the total distance between propensity scores is minimized. Four different approaches: Sequentially treated subjects from highest to lowest propensity score Sequentially treated subjects from lowest to highest propensity score Sequentially treated subjects in the order of the best possible match (the first selected treated subject is that treated subject who is closest to an untreated subject and so on) Treated subjects in a random order Local optimal, greedy or nearest available neighbour matching within specified caliper widths: we can match treated and untreated subjects only if the absolute difference in their propensity scores is within a prespecified maximal distance (the caliper distance, defined as a proportion of the standard deviation of the logit of the propensity score) Note Although the propensity score is the natural metric to use, when using caliper matching, a reduction in bias due to the use of different caliper widths has been described when matching on the logit of the propensity score. Although it is difficult to know beforehand the optimal choice of caliper width, some researchers (Rosenbuam & Rubin, 1985; Austin, 2011) have recommended using a caliper width that is equal to 0.2 of the standard deviation of the logit of the propensity score, i.e., $0.2\\cdot\\sqrt\\left ( \\sigma^2_1+\\sigma^2_2 \\right )/2$. Sequentially treated subjects from highest to lowest propensity score Sequentially treated subjects from lowest to highest propensity score Sequentially treated subjects in the order of the best possible match (the first selected treated subject is that treated subject who is closest to an untreated subject and so on) Treated subjects in a random order Note Optimal matching and greedy nearest neighbor matching on the propensity score will result in all treated subjects being matched to an untreated subject (assuming that the number of untreated subjects is at least as large as the number of treated subjects). However, greedy nearest neighbor matching within specified caliper widths may not result in all treated subjects being matched to an untreated subject, because for some treated subjects, there may not be any untreated subjects who are unmatched and whose propensity score lies within the specified caliper distance of that of the treated subject. The objective of the caliper matching is to avoid bad matches.","title":"Methods without Replacement"},{"location":"other-analysis/propensity-score/#methods-with-replacement","text":"Permits the same untreated subject to be matched to multiple treated subjects (because untreated subjects are recycled or allowed to be included in multiple matched sets, the order in which the treated subjects are selected has no effect on the formation of matched pairs). Matching with replacement minimizes the propensity score distance between the matched units since each treated unit is matched to the closest control, even if the control has been selected before. Nearest neighbor matching with replacement: matches each treated subject to the nearest untreated subject Nearest neighbor matching within specified caliper widths with replacement: matches each treated subject to the nearest untreated subject (subject to possible caliper restrictions)","title":"Methods with Replacement"},{"location":"other-analysis/propensity-score/#1-to-n-matching-methods","text":"They include matching each treated unit to more than one control match. This can be done by creating N replicas of each treated unit and proceeding as described above.","title":"1 to N Matching Methods"},{"location":"other-analysis/propensity-score/#radius-matching","text":"All the control units within a certain distance are chosen (Dehejia and Wahba 1999).","title":"Radius Matching"},{"location":"other-analysis/propensity-score/#mahalanobis-metric-matching","text":"In this type of matching, the definition of distance is changed. The similarity between the propensity score of treated and untreated units is evaluated using the multidimensional Mahalanobis metric matching: $D_{ij}=\\sqrt{\\left ( x_i-y_j \\right )^TS^{-1}\\left ( x_i-y_j \\right )}$ where $S^{-1}$ is the pooled variance-covariance matrix and x and y are multivariate vectors. Note that if the variance-covariance matrix is an identity matrix the Mahalanobis metric is reduced to the familiar Euclidean metric. Usually the Mahalanobis metric matching includes the propensity score and other covariates that are considered to be important and are hoped to be balanced (Rosenbaum and Rubin 1985).","title":"Mahalanobis Metric Matching"},{"location":"other-analysis/propensity-score/#psmatching-macro","text":"With the macro PSMatching.sas (Coca-Perraillon, 2006) different methods can be applied to calculate the matching once you have the propensity score. First you have to prepare the following input: Treatment data set containing the treatment cases along with the patient number idT and the corresponding propensity score pscoreT Control data set containing the control cases along with the patient number idC and the corresponding propensity score pscoreC The parameters datatreatment and datacontrol refer to the Treatment and Control datasets and they do not need to be sorted. The method parameter can be NN (nearest available neighbor), caliper or radius . Caliper can be any number indicating the size of the caliper and the parameter replacement takes the values of yes or no. All the parameters are case insensitive. If the method is NN , the caliper parameter is ignored. 1 %PSMatching(datatreatment=treatment, datacontrol=control, method=caliper, numberofcontrols=1, caliper=0.2, replacement=no); Here is the macro code: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 %macro PSMatching ( datatreatment = , datacontrol = , method = , numberofcontrols = , caliper = , replacement = ); /* Create copies of the treated units if N > 1 */ ; data _Treatment0 ( drop = i ); set Treatment ; do i = 1 to & numberofcontrols ; RandomNumber = ranuni ( 12345 ); output ; end ; run ; /* Randomly sort both datasets */ proc sort data = _Treatment0 out = _Treatment ( drop = RandomNumber ); by RandomNumber ; run ; data _Control0 ; set Control ; RandomNumber = ranuni ( 45678 ); run ; proc sort data = _Control0 out = _Control ( drop = RandomNumber ); by RandomNumber ; run ; data Matched ( keep = IdSelectedControl MatchedToTreatID ); length pscoreC 8 ; length idC 8 ; /* Load Control dataset into the hash object */ if _N_ = 1 then do ; declare hash h ( dataset : \"_Control\" , ordered : ' no ' ); declare hiter iter ( 'h' ); h . defineKey ( ' idC ' ); h . defineData ( ' pscoreC ' , ' idC ' ); h . defineDone (); call missing ( idC , pscoreC ); end ; /* Open the treatment */ set _Treatment ; %if %upcase ( & method ) ~= RADIUS %then %do ; retain BestDistance 99 ; %end ; /* Iterate over the hash */ rc = iter . first (); if ( rc = 0 ) then BestDistance = 99 ; do while ( rc = 0 ); /* Caliper */ %if %upcase ( & method ) = CALIPER %then %do ; if ( pscoreT - & caliper ) <= pscoreC <= ( pscoreT + & caliper ) then do ; ScoreDistance = abs ( pscoreT - pscoreC ); if ScoreDistance < BestDistance then do ; BestDistance = ScoreDistance ; IdSelectedControl = idC ; MatchedToTreatID = idT ; end ; end ; %end ; /* NN */ %if %upcase ( & method ) = NN %then %do ; ScoreDistance = abs ( pscoreT - pscoreC ); if ScoreDistance < BestDistance then do ; BestDistance = ScoreDistance ; IdSelectedControl = idC ; MatchedToTreatID = idT ; end ; %end ; %if %upcase ( & method ) = NN or %upcase ( & method ) = CALIPER %then %do ; rc = iter . next (); /* Output the best control and remove it */ if ( rc ~= 0 ) and BestDistance ~= 99 then do ; output ; %if %upcase ( & replacement ) = NO %then %do ; rc1 = h . remove ( key : IdSelectedControl ); %end ; end ; %end ; /* Radius */ %if %upcase ( & method ) = RADIUS %then %do ; if ( pscoreT - & caliper ) <= pscoreC <= ( pscoreT + & caliper ) then do ; IdSelectedControl = idC ; MatchedToTreatID = idT ; output ; end ; rc = iter . next (); %end ; end ; run ; /* Delete temporary tables. Quote for debugging */ ods select none ; proc datasets ; delete _ :( gennum = all ); run ; ods select all ; %mend PSMatching ;","title":"PSMatching Macro"},{"location":"other-analysis/propensity-score/#proc-psmatch","text":"Here and here you can find the documentation of this procedure.","title":"PROC PSMATCH"},{"location":"other-analysis/propensity-score/#stratification-on-the-propensity-score","text":"Stratification on the propensity score involves stratifying subjects into mutually exclusive subsets based on their estimated propensity score . Subjects are ranked according to their estimated propensity score. Subjects are then stratified into subsets based on previously defined thresholds of the estimated propensity score. A common approach is to divide subjects into five equal-size groups using the quintiles of the estimated propensity score . Rosenbaum and Rubin (1984) stated that stratifying on the quintiles of the propensity score eliminates approximately 90% of the bias due to measured confounders when estimating a linear treatment effect. Within each propensity score stratum, treated and untreated subjects will have roughly similar values of the propensity score. Therefore, when the propensity score has been correctly specified, the distribution of measured baseline covariates will be approximately similar between treated and untreated subjects within the same stratum. More info on this topic here .","title":"Stratification on the Propensity Score"},{"location":"other-analysis/questionnaires/","text":"In order to use the global punctuation of a questionnaire to fit a model, thresholds between different representative punctuation ranges must be defined in advance to use it as a categorical response variable. If these thresholds levels are well defined and tested, they should give more information than the continuous numeric global punctuation. You can also use a ROC curve to obtain an estimation/candidates of the thresholds.","title":"Questionnaires analysis"},{"location":"other-analysis/randomization/","text":"The most compelling way to establish that an intervention definitively causes a clinical outcome is to randomly allocate patients into treatment groups. Randomization helps to ensure that a certain proportion of patients receive each treatment and that the treatment groups being compared are similar in both measured and unmeasured patient characteristics. Simple or unrestricted, equal randomization of patients between 2 treatment groups is equivalent to tossing a fair coin for each patient assignment. As the sample size increases, the 2 groups will become more perfectly balanced. However, this balance is not guaranteed when there are relatively few patients enrolled in a trial . In the coin toss scenario, obtaining several consecutive heads, for example, is more likely than typically perceived. If a long series of assignments to 1 group occurred when randomizing patients in a clinical trial, imbalances between the groups would occur. Imbalances between groups can be minimized in small sample\u2013size studies by restricting the randomization procedure . Restricted randomization means that simple randomization is applied within defined groups of patients. Permuted Block Randomization Link Permuted block randomization is a way to randomly allocate a participant to a treatment group, while maintaining a balance (at the end of every block) across treatment groups. Each \u201cblock\u201d has a specified number of randomly ordered treatment assignments. Random permuted blocks are blocks of different sizes , where the size of the next block is randomly chosen from the available block sizes. For example, here is a list of random permuted blocks of sizes 4 or 6: 1 2 3 4 5 6 A A B A B B A B A B B B A A B A A B A B A B B A B A A A B B Blocking can be used within strata , so that important prognostic characteristics (the stratification factors ) are balanced between the treatment groups. Assigning Blocks Link Two basic methods for assigning blocks are random number generation and permutations. Random Number Generation Link Randomly generate a number for each treatment assignment. For example, if you had a block with treatments AABB, you might get: 1 2 3 4 A = 4 A = 88 B = 9 B = 17 Rank the generated numbers from highest to lowest: 1 2 3 4 A = 88 B = 17 B = 9 A = 4 This gives you your first block, ABBA . Repeat the process to assign a new block. Permutations Link Write a list of all permutations for the block size ( b ). The number of possible arrangements is given by: b! / ((b/2)! (b/2!)) For example, for a block of size 4, you would have 6 possible arrangements: 1 2 3 4 5 6 AABB ABAB BAAB BABA BBAA ABBA Randomly choose one arrangement for each block. Choice of block size Link Block sizes must be multiples of the number of treatments and take the allocation ratio into account. * For 1:1 randomisation of 2 groups, blocks can be size 2, 4, 6, etc. * For 1:1:1 randomisation of 3 groups, blocks can be size 3, 6, 9, etc. * For 2:1 randomisation of 2 groups, blocks can be size 3, 6, 9, etc. The treatment allocation is predictable towards the end of a block . For this reason block sizes should be kept confidential and not shared with those randomising. Large blocks reduce predictability, but will not restrict the randomisation as closely as small blocks. If interim analyses are planned at particular sample sizes, it is desirable that the treatments are balanced at these points . If your experiment involves a number that isn't divisible by the block size, then your treatment groups may not have the exact same amounts. Having many stratification factors can lead to many incomplete blocks and thereby imbalance . For large trials, a small imbalance usually doesn't make a big difference, but this is something to take into consideration for smaller trials. Therefore, choice of block size(s) should take into account: * the sample size * planned interim analyses * number of stratification factors You can experiment with different block sizes and stratification factors on a simulation . This will show you how much imbalance to expect for various choices. Why Are Permuted Blocks and Stratified Randomization Important? Link The most efficient allocation of patients for maximizing statistical power is often equal allocation into groups . Power to detect a treatment effect is increased as the standard error of the treatmenteffect estimate is decreased. In a 2-group setting, allocating more patients to 1 group would reduce the standard error for that 1 group but doing so would decrease the sample size and increase the standard error in the other group. The standard error of the treatment effect or the difference between the groups is therefore minimized with equal allocation. Permuted block randomization avoids such imbalances . Stratified randomization ensures balance between treatment groups for the selected, measurable prognostic characteristics used to define the strata. Because stratified randomization essentially produces a randomized trial within each stratum, stratification can be used when different patient populations are being enrolled or if it is important to analyze results within the subgroups defined by the stratifying characteristics. For example, when there are concerns that an intervention is influenced by patient sex, stratification might occur by sex. Because patients are randomly allocated both in the male and female groups, the effect of the intervention can be tested for the entire population and \u2014assuming sufficient sample size\u2014 separately in men and women. Limitations of Permuted Block Randomization and Stratified Randomization Link The main limitation of permuted block randomization is the potential for bias if treatment assignments become known or predictable . For example, with a block size of 4, if an investigator knew the first 3 assignments in the block, the investigator also would know with certainty the assignment for the next patient enrolled. The use of reasonably large block sizes, random block sizes, and strong blinding procedures such as double-blind treatment assignments and identical-appearing placebos are strategies used to prevent this. In stratified randomization, the number of strata should be fairly limited , such as 3 or 4, but even fewer strata should be used in trials enrolling relatively few research participants. There is no particular statistical disadvantage to stratification, but strata do result in more complex randomization procedures . In some settings, stratified randomization may not be possible because it is simply not feasible to determine a patient's prognostic characteristics before getting a treatment assignment, such as in an emergency setting. An alternative to stratification is to prespecify a statistical adjustment for the key characteristics in the primary analysis that are thought to influence outcomes and may not be completely balanced between groups by the randomization procedure. Another alternative to stratification is minimization . Minimization considers the current balance of the key prognostic characteristics between treatment groups and if an imbalance exists, assigns future patients as necessary to rebalance the groups. For example, if the experimental group had a smaller proportion of women than did the control group and the next patient to be randomized is a woman, a minimization procedure might assign that patient to the experimental group. Minimization can bemore complex than stratification , but is effective and can accommodate more factors than stratification . How Does the Approach to Randomization Affect the Trial's Interpretation? Link In a clinical trial, the ultimate goal of the randomization procedure is to create similar treatment groups that allow an unbiased comparison . Restricted randomization procedures such as stratified randomization and permuted block randomization create balance between important prognostic characteristics and are useful when conducting randomized trials enrolling relatively few patients.","title":"Randomization"},{"location":"other-analysis/randomization/#permuted-block-randomization","text":"Permuted block randomization is a way to randomly allocate a participant to a treatment group, while maintaining a balance (at the end of every block) across treatment groups. Each \u201cblock\u201d has a specified number of randomly ordered treatment assignments. Random permuted blocks are blocks of different sizes , where the size of the next block is randomly chosen from the available block sizes. For example, here is a list of random permuted blocks of sizes 4 or 6: 1 2 3 4 5 6 A A B A B B A B A B B B A A B A A B A B A B B A B A A A B B Blocking can be used within strata , so that important prognostic characteristics (the stratification factors ) are balanced between the treatment groups.","title":"Permuted Block Randomization"},{"location":"other-analysis/randomization/#assigning-blocks","text":"Two basic methods for assigning blocks are random number generation and permutations.","title":"Assigning Blocks"},{"location":"other-analysis/randomization/#random-number-generation","text":"Randomly generate a number for each treatment assignment. For example, if you had a block with treatments AABB, you might get: 1 2 3 4 A = 4 A = 88 B = 9 B = 17 Rank the generated numbers from highest to lowest: 1 2 3 4 A = 88 B = 17 B = 9 A = 4 This gives you your first block, ABBA . Repeat the process to assign a new block.","title":"Random Number Generation"},{"location":"other-analysis/randomization/#permutations","text":"Write a list of all permutations for the block size ( b ). The number of possible arrangements is given by: b! / ((b/2)! (b/2!)) For example, for a block of size 4, you would have 6 possible arrangements: 1 2 3 4 5 6 AABB ABAB BAAB BABA BBAA ABBA Randomly choose one arrangement for each block.","title":"Permutations"},{"location":"other-analysis/randomization/#choice-of-block-size","text":"Block sizes must be multiples of the number of treatments and take the allocation ratio into account. * For 1:1 randomisation of 2 groups, blocks can be size 2, 4, 6, etc. * For 1:1:1 randomisation of 3 groups, blocks can be size 3, 6, 9, etc. * For 2:1 randomisation of 2 groups, blocks can be size 3, 6, 9, etc. The treatment allocation is predictable towards the end of a block . For this reason block sizes should be kept confidential and not shared with those randomising. Large blocks reduce predictability, but will not restrict the randomisation as closely as small blocks. If interim analyses are planned at particular sample sizes, it is desirable that the treatments are balanced at these points . If your experiment involves a number that isn't divisible by the block size, then your treatment groups may not have the exact same amounts. Having many stratification factors can lead to many incomplete blocks and thereby imbalance . For large trials, a small imbalance usually doesn't make a big difference, but this is something to take into consideration for smaller trials. Therefore, choice of block size(s) should take into account: * the sample size * planned interim analyses * number of stratification factors You can experiment with different block sizes and stratification factors on a simulation . This will show you how much imbalance to expect for various choices.","title":"Choice of block size"},{"location":"other-analysis/randomization/#why-are-permuted-blocks-and-stratified-randomization-important","text":"The most efficient allocation of patients for maximizing statistical power is often equal allocation into groups . Power to detect a treatment effect is increased as the standard error of the treatmenteffect estimate is decreased. In a 2-group setting, allocating more patients to 1 group would reduce the standard error for that 1 group but doing so would decrease the sample size and increase the standard error in the other group. The standard error of the treatment effect or the difference between the groups is therefore minimized with equal allocation. Permuted block randomization avoids such imbalances . Stratified randomization ensures balance between treatment groups for the selected, measurable prognostic characteristics used to define the strata. Because stratified randomization essentially produces a randomized trial within each stratum, stratification can be used when different patient populations are being enrolled or if it is important to analyze results within the subgroups defined by the stratifying characteristics. For example, when there are concerns that an intervention is influenced by patient sex, stratification might occur by sex. Because patients are randomly allocated both in the male and female groups, the effect of the intervention can be tested for the entire population and \u2014assuming sufficient sample size\u2014 separately in men and women.","title":"Why Are Permuted Blocks and Stratified Randomization Important?"},{"location":"other-analysis/randomization/#limitations-of-permuted-block-randomization-and-stratified-randomization","text":"The main limitation of permuted block randomization is the potential for bias if treatment assignments become known or predictable . For example, with a block size of 4, if an investigator knew the first 3 assignments in the block, the investigator also would know with certainty the assignment for the next patient enrolled. The use of reasonably large block sizes, random block sizes, and strong blinding procedures such as double-blind treatment assignments and identical-appearing placebos are strategies used to prevent this. In stratified randomization, the number of strata should be fairly limited , such as 3 or 4, but even fewer strata should be used in trials enrolling relatively few research participants. There is no particular statistical disadvantage to stratification, but strata do result in more complex randomization procedures . In some settings, stratified randomization may not be possible because it is simply not feasible to determine a patient's prognostic characteristics before getting a treatment assignment, such as in an emergency setting. An alternative to stratification is to prespecify a statistical adjustment for the key characteristics in the primary analysis that are thought to influence outcomes and may not be completely balanced between groups by the randomization procedure. Another alternative to stratification is minimization . Minimization considers the current balance of the key prognostic characteristics between treatment groups and if an imbalance exists, assigns future patients as necessary to rebalance the groups. For example, if the experimental group had a smaller proportion of women than did the control group and the next patient to be randomized is a woman, a minimization procedure might assign that patient to the experimental group. Minimization can bemore complex than stratification , but is effective and can accommodate more factors than stratification .","title":"Limitations of Permuted Block Randomization and Stratified Randomization"},{"location":"other-analysis/randomization/#how-does-the-approach-to-randomization-affect-the-trials-interpretation","text":"In a clinical trial, the ultimate goal of the randomization procedure is to create similar treatment groups that allow an unbiased comparison . Restricted randomization procedures such as stratified randomization and permuted block randomization create balance between important prognostic characteristics and are useful when conducting randomized trials enrolling relatively few patients.","title":"How Does the Approach to Randomization Affect the Trial's Interpretation?"},{"location":"other-analysis/roc-curve/","text":"Check these websites Generating Receiver Operating Characteristic (ROC) curve using SAS Macros In statistics, a receiver operating characteristic curve (ROC curve) , is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied. The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. The true-positive rate is also known as sensitivity . The false-positive rate is also known as the fall-out and can be calculated as (1 \u2212 specificity) . ROC analysis provides tools to select possibly optimal models and to discard suboptimal ones independently from (and prior to specifying) the cost context or the class distribution. Macro ROCPLOT Link Produce a plot of the Receiver Operating Characteristic (ROC) curve associated with a fitted binary-response model. Label points on the ROC curve using statistic or input variable values. Identify optimal cutpoints on the ROC curve using several optimality criteria such as correct classification, efficiency, cost, and others. Plot optimality criteria against a selected variable. When only an ROC plot with labeled points is needed, you can often produce the desired plot in PROC LOGISTIC without this macro. Using ODS graphics , PROC LOGISTIC can plot the ROC curve of a model whether applied to the data used to fit the model or to additional data scored using the fitted model. Specify the PLOTS=ROC option in the PROC LOGISTIC statement, or specify the OUTROC= option in the MODEL and/or SCORE statements. 1 2 3 PROC LOGISTIC ( \u2026 ) plots ( only ) = roc ( id = obs ); MODEL ( \u2026 ) / OUTROC = ROC_data ; RUN ; More information on this macro here .","title":"ROC Curve"},{"location":"other-analysis/roc-curve/#macro-rocplot","text":"Produce a plot of the Receiver Operating Characteristic (ROC) curve associated with a fitted binary-response model. Label points on the ROC curve using statistic or input variable values. Identify optimal cutpoints on the ROC curve using several optimality criteria such as correct classification, efficiency, cost, and others. Plot optimality criteria against a selected variable. When only an ROC plot with labeled points is needed, you can often produce the desired plot in PROC LOGISTIC without this macro. Using ODS graphics , PROC LOGISTIC can plot the ROC curve of a model whether applied to the data used to fit the model or to additional data scored using the fitted model. Specify the PLOTS=ROC option in the PROC LOGISTIC statement, or specify the OUTROC= option in the MODEL and/or SCORE statements. 1 2 3 PROC LOGISTIC ( \u2026 ) plots ( only ) = roc ( id = obs ); MODEL ( \u2026 ) / OUTROC = ROC_data ; RUN ; More information on this macro here .","title":"Macro ROCPLOT"},{"location":"other-analysis/sequence-design/","text":"Theory of Hypothesis-Based Adaptive Design Link An adaptive design is a design that allows adaptations or modifcations to some aspects of a trial after its initiation without undermining the validity and integrity of the trial. The adaptations may include, but are not limited to, sample-size reestimation, early stopping for efficacy or futility, response-adaptive randomization, and dropping inferior treatment groups. Adaptive designs usually require unblinding data and invoke a dependent sampling procedure. Therefore, theory behind adaptive design is much more complicated than that behind classical design. Many interesting methods for adaptive design have been developed. Virtually all methods can be viewed as some combination of stagewise p-values. The stagewise p-values are obtained based on the subsample from each stage; therefore, they are mutually independent and uniformly distributed over [0,1] under the null hypothesis. The first method uses the same stopping boundaries as a classical group sequential design ( O'Brien and Fleming , 1979; Pocock , 1977) and allows stopping for early efficacy or futility. Lan and DeMets (1983) proposed the error spending method (ESM) , in which the timing and number of analyses can be changed based on a prespecified error-spending function. ESM is derived from Brownian motion. The method has been extended to allow for sample-size reestimation (SSR) (Cui, Hung, and Wang, 1999). It can be viewed as a fixed-weight method (i.e., using fixed weights for z-scores from the first and second stages regardless of sample-size change). Lehmacher and Wassmer (1999) further degeneralized this weight method by using the inverse-normal method, in which the z-score is not necessarily taken from a normal endpoint, but from the inverse-normal function of stagewise p-values. Hence, the method can be used for any type of endpoint. O'Brien-Flemming Link Let us start considering a two-arm trial. The usual settings for randomized two-arm clinical trials are: Response is dichotomous and immediate They are single-phase trials, with sample sizes fixed in advance At the end of a trial, compare success rates (i.e. proportions) using a formal test of significance based on the usual Pearson, is chi-squared test. The aim is to form a multiple testing procedure that provides investigators with an opportunity to conduct periodic reviews of the data as they accumulate and thereby offers the chance for early termination should one treatment prove superior to the other early on while continuing to use essentially the single-phase decision rule should early termination not occur. The following is a brief description of the O'Brien-Flemming procedure : Investigators plan to test $k$ times, including the final comparison at the end of the trial. Data are reviewd periodically, with $m_1$ subjects receiving treatment 1 and $m_2$ subjects receiving treatment 2, between successive tests; there are a total of $k\\cdot (m_1+m_2)$ subjects. The constraint is to maintain an overall size $\\alpha$, say, $\\alpha = 0.05$. Rule: After the $n$th test, $1 \\le n \\le k$, the study is terminated and $H_0$ is rejected if $(n/k)X^2 \\ge P(k,\\alpha)$ where $X^2$ is the usual Pearson's chi-squared statistic. Using the theory of Brownian motion, O'Brien and Fleming (1979) obtained the values for $P(k,\\alpha)$ but, more importantly, they concluded that they are approximately the $(1-\\alpha)th$ percentile of the chi-squared distribution with 1 degree of freedom -- almost independent of $k$. SAS code Link We first calculate the boundaries for our study design according to the prerequisites. One-sided example 1 2 3 4 5 6 7 8 9 10 11 12 13 PROC SEQDESIGN ALTREF = 0 . 25 BOUNDARYSCALE = PVALUE PLOTS = BOUNDARY ( HSCALE = SAMPLESIZE ) ERRSPEND ; ONESIDEDOBRIENFLEMING : DESIGN NSTAGES = 3 METHOD = OBF STOP = BOTH ALT = UPPER ALPHA = 0 . 1 BETA = 0 . 2 INFO = EQUAL ; SAMPLESIZE MODEL = TWOSAMPLEFREQ ( NULLPROP = 0 . 15 TEST = PROP WEIGHT = 2 ); ODS OUTPUT BOUNDARY = output - boundaries - values ; RUN ; Two-sided example 1 2 3 4 5 6 7 8 9 10 11 12 13 PROC SEQDESIGN ALTREF = 0 . 25 BOUNDARYSCALE = PVALUE PLOTS = BOUNDARY ( HSCALE = SAMPLESIZE ) ERRSPEND ; ONESIDEDOBRIENFLEMING : DESIGN NSTAGES = 3 METHOD = OBF STOP = BOTH ALT = TWOSIDED ALPHA = 0 . 2 BETA = 0 . 2 INFO = EQUAL ; SAMPLESIZE MODEL = TWOSAMPLEFREQ ( NULLPROP = 0 . 15 TEST = PROP WEIGHT = 2 ); ODS OUTPUT BOUNDARY = output - boundaries - values ; RUN ; PROC SEQDESIGN parameters: * ALTREF = specifies alternative reference, 0.25 for a difference of 25% * BOUNDARYSCALE = MLE | SCORE | STDZ | PVALUE specifies statistic scale for the boundary * ERRSPEND displays cumulative error spending at each stage DESIGN statement parameters: * NSTAGES = is the number of stages in the design (including the final stage) * METHOD = specifies methods for boundary values ( OBF specifies the O'Brien-Fleming method) * STOP = ACCEPT | REJECT | BOTH specifies the condition of early stopping for the design * ALT = LOWER | UPPER | TWOSIDED specifies type of alternative hypothesis * ALPHA = and BETA= specify the Type I error probability level $\\alpha$ and the Type II error probability level $\\beta$ * INFO = specifies information levels ( EQUAL for equally spaced information levels and CUM for cumulative relative information levels) SAMPLESIZE statement parameters: * MODEL = TWOSAMPLEFREQ < ( options ) > specifies the two-sample test for binomial proportions. The available options are as follows: * The NULLPROP = option specifies proportions $p_a=p_{0a}$ and $p_b=p_{0b}$ in groups A and B, respectively, under the null hypothesis * The TEST = option specifies the null hypothesis $H_0:\\theta=0$ in the test ( PROP option uses the difference in proportions $\\theta=(p_a-p_b)-(p_{0a}-p_{0b})$, LOGOR option uses the log odds-ratio test and the LOGRR option uses the log relative risk test) * The WEIGHT= option specifies the sample size allocation weights for the two groups","title":"Sequence Design"},{"location":"other-analysis/sequence-design/#theory-of-hypothesis-based-adaptive-design","text":"An adaptive design is a design that allows adaptations or modifcations to some aspects of a trial after its initiation without undermining the validity and integrity of the trial. The adaptations may include, but are not limited to, sample-size reestimation, early stopping for efficacy or futility, response-adaptive randomization, and dropping inferior treatment groups. Adaptive designs usually require unblinding data and invoke a dependent sampling procedure. Therefore, theory behind adaptive design is much more complicated than that behind classical design. Many interesting methods for adaptive design have been developed. Virtually all methods can be viewed as some combination of stagewise p-values. The stagewise p-values are obtained based on the subsample from each stage; therefore, they are mutually independent and uniformly distributed over [0,1] under the null hypothesis. The first method uses the same stopping boundaries as a classical group sequential design ( O'Brien and Fleming , 1979; Pocock , 1977) and allows stopping for early efficacy or futility. Lan and DeMets (1983) proposed the error spending method (ESM) , in which the timing and number of analyses can be changed based on a prespecified error-spending function. ESM is derived from Brownian motion. The method has been extended to allow for sample-size reestimation (SSR) (Cui, Hung, and Wang, 1999). It can be viewed as a fixed-weight method (i.e., using fixed weights for z-scores from the first and second stages regardless of sample-size change). Lehmacher and Wassmer (1999) further degeneralized this weight method by using the inverse-normal method, in which the z-score is not necessarily taken from a normal endpoint, but from the inverse-normal function of stagewise p-values. Hence, the method can be used for any type of endpoint.","title":"Theory of Hypothesis-Based Adaptive Design"},{"location":"other-analysis/sequence-design/#obrien-flemming","text":"Let us start considering a two-arm trial. The usual settings for randomized two-arm clinical trials are: Response is dichotomous and immediate They are single-phase trials, with sample sizes fixed in advance At the end of a trial, compare success rates (i.e. proportions) using a formal test of significance based on the usual Pearson, is chi-squared test. The aim is to form a multiple testing procedure that provides investigators with an opportunity to conduct periodic reviews of the data as they accumulate and thereby offers the chance for early termination should one treatment prove superior to the other early on while continuing to use essentially the single-phase decision rule should early termination not occur. The following is a brief description of the O'Brien-Flemming procedure : Investigators plan to test $k$ times, including the final comparison at the end of the trial. Data are reviewd periodically, with $m_1$ subjects receiving treatment 1 and $m_2$ subjects receiving treatment 2, between successive tests; there are a total of $k\\cdot (m_1+m_2)$ subjects. The constraint is to maintain an overall size $\\alpha$, say, $\\alpha = 0.05$. Rule: After the $n$th test, $1 \\le n \\le k$, the study is terminated and $H_0$ is rejected if $(n/k)X^2 \\ge P(k,\\alpha)$ where $X^2$ is the usual Pearson's chi-squared statistic. Using the theory of Brownian motion, O'Brien and Fleming (1979) obtained the values for $P(k,\\alpha)$ but, more importantly, they concluded that they are approximately the $(1-\\alpha)th$ percentile of the chi-squared distribution with 1 degree of freedom -- almost independent of $k$.","title":"O'Brien-Flemming"},{"location":"other-analysis/sequence-design/#sas-code","text":"We first calculate the boundaries for our study design according to the prerequisites. One-sided example 1 2 3 4 5 6 7 8 9 10 11 12 13 PROC SEQDESIGN ALTREF = 0 . 25 BOUNDARYSCALE = PVALUE PLOTS = BOUNDARY ( HSCALE = SAMPLESIZE ) ERRSPEND ; ONESIDEDOBRIENFLEMING : DESIGN NSTAGES = 3 METHOD = OBF STOP = BOTH ALT = UPPER ALPHA = 0 . 1 BETA = 0 . 2 INFO = EQUAL ; SAMPLESIZE MODEL = TWOSAMPLEFREQ ( NULLPROP = 0 . 15 TEST = PROP WEIGHT = 2 ); ODS OUTPUT BOUNDARY = output - boundaries - values ; RUN ; Two-sided example 1 2 3 4 5 6 7 8 9 10 11 12 13 PROC SEQDESIGN ALTREF = 0 . 25 BOUNDARYSCALE = PVALUE PLOTS = BOUNDARY ( HSCALE = SAMPLESIZE ) ERRSPEND ; ONESIDEDOBRIENFLEMING : DESIGN NSTAGES = 3 METHOD = OBF STOP = BOTH ALT = TWOSIDED ALPHA = 0 . 2 BETA = 0 . 2 INFO = EQUAL ; SAMPLESIZE MODEL = TWOSAMPLEFREQ ( NULLPROP = 0 . 15 TEST = PROP WEIGHT = 2 ); ODS OUTPUT BOUNDARY = output - boundaries - values ; RUN ; PROC SEQDESIGN parameters: * ALTREF = specifies alternative reference, 0.25 for a difference of 25% * BOUNDARYSCALE = MLE | SCORE | STDZ | PVALUE specifies statistic scale for the boundary * ERRSPEND displays cumulative error spending at each stage DESIGN statement parameters: * NSTAGES = is the number of stages in the design (including the final stage) * METHOD = specifies methods for boundary values ( OBF specifies the O'Brien-Fleming method) * STOP = ACCEPT | REJECT | BOTH specifies the condition of early stopping for the design * ALT = LOWER | UPPER | TWOSIDED specifies type of alternative hypothesis * ALPHA = and BETA= specify the Type I error probability level $\\alpha$ and the Type II error probability level $\\beta$ * INFO = specifies information levels ( EQUAL for equally spaced information levels and CUM for cumulative relative information levels) SAMPLESIZE statement parameters: * MODEL = TWOSAMPLEFREQ < ( options ) > specifies the two-sample test for binomial proportions. The available options are as follows: * The NULLPROP = option specifies proportions $p_a=p_{0a}$ and $p_b=p_{0b}$ in groups A and B, respectively, under the null hypothesis * The TEST = option specifies the null hypothesis $H_0:\\theta=0$ in the test ( PROP option uses the difference in proportions $\\theta=(p_a-p_b)-(p_{0a}-p_{0b})$, LOGOR option uses the log odds-ratio test and the LOGRR option uses the log relative risk test) * The WEIGHT= option specifies the sample size allocation weights for the two groups","title":"SAS code"},{"location":"other-analysis/survival-analysis/","text":"What Is Survival Analysis? Link Check these websites An\u00e1lisis de supervivencia Introduction to survival analysis in SAS A SAS Macro to Generate Information Rich Kaplan-Meier Plots Kaplan-Meier Survival Plotting Macro %NEWSURV Application of Survival Analysis in Multiple Events Using SAS In many cancer studies, the main outcome under assessment is the time to an event of interest. The generic name for the time is survival time , although it may be applied to the time \u2018survived\u2019 from complete remission to relapse or progression as equally as to the time from diagnosis to death. If the event occurred in all individuals, many methods of analysis would be applicable . However, it is usual that at the end of follow-up some of the individuals have not had the event of interest , and thus their true time to event is unknown. Further, survival data are rarely Normally distributed , but are skewed and comprise typically of many early events and relatively few late ones. It is these features of the data that make the special methods called survival analysis necessary. Nonparametric methods ( PROC LIFETEST ) provide simple and quick looks at the survival experience and the Cox proportional hazards regression model ( PROC PHREG ) remains the dominant analysis method. Endpoints in Clinical Trials Related to Survival Analysis Link Check these websites Efficacy Endpoints in Oncology Clinical Trials Overall Survival Link OS is the gold standard for demonstrating clinical benefit . Defined as the time from randomization to death , this endpoint is unambiguous and is not subject to investigator interpretation. Survival is a direct clinical benefit to patients, and assessment can be calculated to the day. Patient benefit can be described as superior survival or noninferior survival after consideration of toxicity and the magnitude of benefit. A noninferiority analysis ensures that a survival advantage associated with an approved drug will not be lost with a new agent. Survival analysis requires a large sample size and may require long follow-up . Survival analysis may be confounded because of subsequent therapies administered after a study drug is discontinued. OS should be evaluated in randomized, controlled trials. Time to Tumor Progression and Progression-Free Survival Link Time to tumor progression (TTP) , is defined as the time from randomization to time of progressive disease (including deaths due to PD) . The progression-free survival (PFS) duration is defined as the time from randomization to objective tumor progression or death . Compared with TTP, PFS may be a preferred regulatory endpoint because it includes death and may correlate better with OS. In TTP analysis, deaths are censored either at the time of death or at an earlier visit representing informative censoring (nonrandom pattern of loss from the study). PFS assumes patient deaths are randomly related to tumor progression. However, in situations where the majority of deaths are unrelated to cancer, TTP can be an acceptable endpoint. Assessment of either PFS or TTP needs to be conducted in randomized trials. Because of the subjectivity that may be introduced in endpoint assessment, blinding of trials or the use of an external blinded review committee is recommended. In assessing TTP or PFS, patients must be evaluated on a regular basis in all treatment arms, and an assessment of all disease sites should be performed. To reduce bias, the same assessment technique should be used at each follow-up, and the same evaluation schedule should be consistently used. Time to Treatment Failure Link Time to treatment failure (TTF) is defined as the time from randomization to treatment discontinuation for any reason , including disease progression, treatment toxicity, patient preference, or death. From a regulatory point of view, TTF is generally not accepted as a valid endpoint. TTF is a composite endpoint influenced by factors unrelated to efficacy. Discontinuation may be a result of toxicity, patient preference, or a physician's reluctance to continue therapy. These factors are not a direct assessment of the effectiveness of a drug. Duration of Response Link Duration of response (DoR) is usually measured as the time from documentation of tumor response to disease progression or death. Patients whose first documented response assessment is disease progression will be exluded from the DoR analysis. It can be assessed in single-arm trials, requires a smaller population and can be assessed earlier (compared to survival trials) and its effect is attributable directly to the drug, not the natural history of the disease. However, is not a comprehensive measure of drug activity. Understanding the Basis of Survival Analysis Link Survival data are generally described and modelled in terms of two related probabilities, namely survival and hazard . The survival probability (which is also called the survivor function) $S(t)$ is the probability that an individual survives from the time origin to a specified future time $t$. It is fundamental to a survival analysis because survival probabilities for different values of $t$ provide crucial summary information from time to event data. These values describe directly the survival experience of a study cohort. The hazard is usually denoted by $h(t)$ or $l(t)$ and is the probability that an individual who is under observation at a time $t$ has an event at that time. Put another way, it represents the instantaneous event rate for an individual who has already survived to time $t$. Note that, in contrast to the survivor function, which focuses on not having an event, the hazard function focuses on the event occurring. It is of interest because it provides insight into the conditional failure rates and provides a vehicle for specifying a survival model. In summary, the hazard relates to the incident (current) event rate, while survival reflects the cumulative non-occurrence. Understanding the mechanics behind survival analysis is aided by facility with the distributions used, which can be derived from the probability density function and cumulative density functions of survival times. The Probability Density Function Link Imagine we have a random variable, $Time$, which records survival times. The function that describes likelihood of observing $Time$ at time $t$ relative to all other survival times is known as the probability density function ( pdf ), or $f(t)$. Integrating the pdf over a range of survival times gives the probability of observing a survival time within that interval. For example, if the survival times were known to be exponentially distributed, then the probability of observing a survival time within the interval $\\left [ a,b \\right ]$ is $Pr\\left ( a \\leq Time \\leq b \\right )=\\int_{a}^{b}f(t)dt=\\int_{a}^{b} \\lambda e^{-\\lambda t}dt$, where $\\lambda$ is the rate parameter of the exponential distribution and is equal to the reciprocal of the mean survival time. Most of the time we will not know a priori the distribution generating our observed survival times, but we can get and idea of what it looks like using nonparametric methods in SAS with PROC UNIVARIATE . 1 2 3 4 PROC UNIVARIATE DATA = SAS - data - set ( WHERE = ( censoring - variable = 1 )); VAR survival - time - variable ; HISTOGRAM survival - time - variable / KERNEL ; RUN ; In the graph above we see the correspondence between pdfs and histograms. Density functions are essentially histograms comprised of bins of vanishingly small widths. Technically, because there are no times less than $0$, there should be no graph to the left $Time=0$. The Cumulative Distribution Function Link The cumulative distribution function ( cdf ), $F(t)$, describes the probability of observing $Time$ less than or equal to some time $t$, or $Pr(Time \\le t)$. Above we described that integrating the pdf over some range yields the probability of observing $Time$ in that range. Thus, we define the cumulative distribution function as: $F(t)=\\int_{0}^{t}f(t)dt$ The above relationship between the cdf and pdf also implies: $f(t)=\\frac{dF(t)}{dt}$ In SAS, we can graph an estimate of the cdf using PROC UNIVARIATE . 1 2 3 4 PROC UNIVARIATE DATA = SAS - data - set ( WHERE = ( censoring - variable = 1 )); VAR survival - time - variable ; CDFPLOT survival - time - variable ; RUN ; In the graph produced with the code above we can check the probability of surviving a number of days. In intervals where event times are more probable, the cdf will increase faster. The Survival Function Link A simple transformation of the cumulative distribution function produces the survival function, $S(t)$: $S(t) = 1 - F(T)$. The survivor function, $S(t)$, describes the probability of surviving past time $t$, or $Pr(Time > t)$. If we were to plot the estimate of $S(t)$, we would see that it is a reflection of $F(t)$ (about $y=0$ and shifted up by $1$). We can use PROC LIFETEST to graph $S(t)$. 1 2 3 PROC LIFETEST DATA = SAS - data - set ( WHERE = ( censoring - variable = 1 )) PLOTS = SURVIVAL ( ATRISK ); TIME survival - time - variable * censoring - variable ( 0 ); RUN ; The probability of surviving beyond a number of days according to this survival plot can be confirmed by the cdf produced above, where the probability of surviving a number of days or fewer is equivalent. The Hazard Function Link The primary focus of survival analysis is typically to model the hazard rate , which has the following relationship with the $f(t)$ and $S(t)$: $h(t)=\\frac{f(t)}{S(t)}$ The hazard function, then, describes the relative likelihood of the event occurring at time $t(f(t))$, conditional on the subject\u2019s survival up to that time $t(S(t))$. The hazard rate thus describes the instantaneous rate of failure at time $t$ and ignores the accumulation of hazard up to time $t$ (unlike $F(t)$ and $S(t)$). We can estimate the hazard function is SAS as well using PROC LIFETEST : 1 2 3 PROC LIFETEST DATA = SAS - data - set ( WHERE = ( censoring - variable = 1 )) PLOTS = HAZARD ( BW = 200 ); /* BW = Bandwidth*/ TIME survival - time - variable * censoring - variable ( 0 ); RUN ; The plot shows the Estimated Hazard Rate which is the expected number of failures per time unit (per day in our example). The Cumulative Hazard Function Link Also useful to understand is the cumulative hazard function, which as the name implies, cumulates hazards over time. It is calculated by integrating the hazard function over an interval of time: $H(t)=\\int_{0}^{t}h(u)du$ Let us again think of the hazard function, $h(t)$, as the rate at which failures occur at time $t$. Let us further suppose, for illustrative purposes, that the hazard rate stays constant at $\\frac{x}{t}$ ($x$ number of failures per unit time $t$) over the interval $[0,t]$. Summing over the entire interval, then, we would expect to observe $x$ failures, as $\\frac{x}{t} t = x$, (assuming repeated failures are possible, such that failing does not remove one from observation). One interpretation of the cumulative hazard function is thus the expected number of failures over time interval $[0,t]$. It is not at all necessary that the hazard function stay constant for the above interpretation of the cumulative hazard function to hold, but for illustrative purposes it is easier to calculate the expected number of failures since integration is not needed. Expressing the above relationship as $\\frac{d}{dt}H(t)=h(t)$, we see that the hazard function describes the rate at which hazards are accumulated over time. Using the equations, $h(t)=\\frac{f(t)}{S(t)}$ and $f(t)=\u2212\\frac{dS}{dt}$, we can derive the following relationships between the cumulative hazard function and the other survival functions: $S(t)=exp(-H(t))$ $F(t)=1-exp(-H(t))$ $f(t)=h(t) \\cdot exp(-H(t))$ From these equations we can see that the cumulative hazard function $H(t)$ and the survival function $S(t)$ have a simple monotonic relationship, such that when the Survival function is at its maximum at the beginning of analysis time, the cumulative hazard function is at its minimum. As time progresses, the Survival function proceeds towards it minimum, while the cumulative hazard function proceeds to its maximum. From these equations we can also see that we would expect the pdf, $f(t)$, to be high when $h(t)$ the hazard rate is high (its location depends on the study) and when the cumulative hazard $H(t)$ is low (the beginning, for all studies). In other words, we would expect to find a lot of failure times in a given time interval if the hazard rate is high and there are still a lot of subjects at-risk. We can estimate the cumulative hazard function using PROC LIFETEST , the results of which we send to PROC SGPLOT for plotting. 1 2 3 4 5 6 7 8 9 ODS OUTPUT ProductLimitEstimates = PLE ; PROC LIFETEST DATA = SAS - data - set whas500 ( WHERE = ( censoring - variable = 1 )) NELSON OUTS = output - name ; TIME survival - time - variable * censoring - variable ( 0 ); RUN ; PROC SGPLOT DATA = PLE ; SERIES x = survival - time - variable y = CumHaz ; RUN ; Data Preparation and Exploration Link Structure of the Data Link To work with PROC LIFETEST and PROC PHREG for survival analysis, data can be structured in one of these 2 ways: One row of data per subject , with one outcome variable representing the time to event, one variable that codes for whether the event occurred or not (censored), and explanatory variables of interest, each with fixed values across follow up time. Both PROC LIFETEST and PROC PHREG will accept data structured this way. Multiple rows of data per subject (only accepted by PROC PHREG ) following the \"counting process\" style of input. For each subject, the whole follow up period is partitioned into intervals, each defined by a \"start\" and \"stop\" time. Covariates are permitted to change value between intervals. Additionally, another variable counts the number of events occurring in each interval (either 0 or 1 in Cox regression, same as the censoring variable). This structuring allows the modeling of time-varying covariates, or explanatory variables whose values change across follow-up time. Data that are structured in the first, single-row way can be modified to be structured like the second, multi-row way, but the reverse is typically not true. Data Exploration with PROC UNIVARIATE and PROC CORR Link Any serious endeavor into data analysis should begin with data exploration, in which the researcher becomes familiar with the distributions and typical values of each variable individually, as well as relationships between pairs or sets of variables. Within SAS, PROC UNIVARIATE provides easy, quick looks into the distributions of each variable, whereas PROC CORR can be used to examine bivariate relationships. Nonparametric Methods ( PROC LIFETEST ) Link 1 2 3 4 5 6 7 8 oms = ( lastcontact - starttreatment + 1 ) / 30 . 45 if exdate = . then censor = 1 ; else censor = 0 ; PROC LIFETEST DATA = SAS - data - set plots = ( s ) ; TIME osm * censor ( 1 ) ; STRATA alg /* aleatorization group, (= . they didn't get to randomization) */ RUN ; In the TIME statement, only patients that haven't been censored are analysed The STRATA statement includes only non-missing data points (no WHERE filtering is needed) Tip If you are performing a survival analysis which only applies to part of your population but you need the probabilities to be referred to the total population just define the excluded subpopulation as having event at time = 0. This way your plot will not start at 1/0. This applies, for example, to locoregional control time plots on which patients without complete response are excluded (event at time = 0) and P-value Calculation Link We select only 2 groups from the test data set (High and Low risk): 1 2 3 4 DATA bmt_small ; SET SASHELP . bmt ; WHERE group IN ( 2 , 3 ); RUN ; We generate the OUTSURV data set: 1 2 3 4 PROC LIFETEST DATA = bmt_small PLOTS = SURVIVAL ( CL CB = HW STRATA = PANEL ) METHOD = LT INTERVALS = ( 0 to 800 by 100 ) OUTSURV = bmt_param STDERR ; TIME t * status ( 0 ); STRATA group / ORDER = INTERNAL ; run ; We calculate the p-values from this data: 1 2 3 4 5 6 7 PROC SQL ; SELECT t , range ( survival ) AS RangeSurvival , sqrt ( sum ( sdf_stderr ** 2 )) AS Squares , range ( survival ) / sqrt ( sum ( sdf_stderr ** 2 )) AS z , probnorm ( abs ( range ( survival ) / sqrt ( sum ( sdf_stderr ** 2 )))) AS pz , 2 * ( 1 - probnorm ( abs ( range ( survival ) / sqrt ( sum ( sdf_stderr ** 2 ))))) AS pvalue FROM btm_param WHERE t > 0 GROUP BY t ; QUIT ; Other method: 1 2 3 4 5 6 7 8 9 10 11 DATA bmt600 ; brazo = 'A' ; valor = 1 ; contador = 32 ; OUTPUT ; brazo = 'A' ; valor = 2 ; contador = 13 ; OUTPUT ; brazo = 'B' ; valor = 1 ; contador = 18 ; OUTPUT ; brazo = 'B' ; valor = 2 ; contador = 36 ; OUTPUT ; RUN ; PROC FREQ DATA = bmt600 ; TABLES brazo * valor / CHISQ MEASURES RISKDIFF PLOTS = ( FREQPLOT ( TWOWAY = GROUPVERTICAL SCALE = PERCENT )); WEIGHT contador ; RUN ; Estimating the survival curve using the Kaplan\u2013Meier method Link As it's been said, in analyzing survival data, two functions that are dependent on time are of particular interest: the survival function and the hazard function. The survival function $S(t)$ is defined as the probability of surviving at least to time $t$. The hazard function $h(t)$ is the conditional probability of dying at time $t$ having survived to that time. The graph of $S(t)$ against t is called the survival curve. The Kaplan\u2013Meier method can be used to estimate this curve from the observed survival times without the assumption of an underlying probability distribution. The method is based on the basic idea that the probability of surviving $k$ or more periods from entering the study is a product of the $k$ observed survival rates for each period (i.e. the cumulative proportion surviving), given by the following: $S(k)=p_1 \\times p_2 \\times p_3 \\times ... \\times p_k$ Comparing survival curves of two groups using the log rank test Link Comparison of two survival curves can be done using a statistical hypothesis test called the log rank test. It is used to test the null hypothesis that there is no difference between the population survival curves . The test statistic is compared with a $\\chi^2$ distribution with 1 degree of freedom. An assumption for the log rank test is that of proportional hazards. Small departures from this assumption, however, do not invalidate the test . Cox's Proportional Hazards Regression Model ( PROC PHREG ) Link The log rank test is used to test whether there is a difference between the survival times of different groups but it does not allow other explanatory variables to be taken into account. Cox's proportional hazards model is analogous to a multiple regression model and enables the difference between survival times of particular groups of patients to be tested while allowing for other factors . In this model, the response (dependent) variable is the 'hazard'. The hazard is the probability of dying (or experiencing the event in question) given that patients have survived up to a given point in time, or the risk for death at that moment. In Cox's model no assumption is made about the probability distribution of the hazard. However, it is assumed that the hazard ratio does not depend on time . Censoring Link Right-censoring : for some subjects we do not know when they died after the issue, but we do know at least how many days they survived. Informative Censoring Link Check these papers Censoring in survival analysis: Potential for bias Impact of Informative Censoring on the Kaplan-Meier Estimate of Progression-Free Survival in Phase II Clinical Trials","title":"Survival Analysis"},{"location":"other-analysis/survival-analysis/#what-is-survival-analysis","text":"Check these websites An\u00e1lisis de supervivencia Introduction to survival analysis in SAS A SAS Macro to Generate Information Rich Kaplan-Meier Plots Kaplan-Meier Survival Plotting Macro %NEWSURV Application of Survival Analysis in Multiple Events Using SAS In many cancer studies, the main outcome under assessment is the time to an event of interest. The generic name for the time is survival time , although it may be applied to the time \u2018survived\u2019 from complete remission to relapse or progression as equally as to the time from diagnosis to death. If the event occurred in all individuals, many methods of analysis would be applicable . However, it is usual that at the end of follow-up some of the individuals have not had the event of interest , and thus their true time to event is unknown. Further, survival data are rarely Normally distributed , but are skewed and comprise typically of many early events and relatively few late ones. It is these features of the data that make the special methods called survival analysis necessary. Nonparametric methods ( PROC LIFETEST ) provide simple and quick looks at the survival experience and the Cox proportional hazards regression model ( PROC PHREG ) remains the dominant analysis method.","title":"What Is Survival Analysis?"},{"location":"other-analysis/survival-analysis/#endpoints-in-clinical-trials-related-to-survival-analysis","text":"Check these websites Efficacy Endpoints in Oncology Clinical Trials","title":"Endpoints in Clinical Trials Related to Survival Analysis"},{"location":"other-analysis/survival-analysis/#overall-survival","text":"OS is the gold standard for demonstrating clinical benefit . Defined as the time from randomization to death , this endpoint is unambiguous and is not subject to investigator interpretation. Survival is a direct clinical benefit to patients, and assessment can be calculated to the day. Patient benefit can be described as superior survival or noninferior survival after consideration of toxicity and the magnitude of benefit. A noninferiority analysis ensures that a survival advantage associated with an approved drug will not be lost with a new agent. Survival analysis requires a large sample size and may require long follow-up . Survival analysis may be confounded because of subsequent therapies administered after a study drug is discontinued. OS should be evaluated in randomized, controlled trials.","title":"Overall Survival"},{"location":"other-analysis/survival-analysis/#time-to-tumor-progression-and-progression-free-survival","text":"Time to tumor progression (TTP) , is defined as the time from randomization to time of progressive disease (including deaths due to PD) . The progression-free survival (PFS) duration is defined as the time from randomization to objective tumor progression or death . Compared with TTP, PFS may be a preferred regulatory endpoint because it includes death and may correlate better with OS. In TTP analysis, deaths are censored either at the time of death or at an earlier visit representing informative censoring (nonrandom pattern of loss from the study). PFS assumes patient deaths are randomly related to tumor progression. However, in situations where the majority of deaths are unrelated to cancer, TTP can be an acceptable endpoint. Assessment of either PFS or TTP needs to be conducted in randomized trials. Because of the subjectivity that may be introduced in endpoint assessment, blinding of trials or the use of an external blinded review committee is recommended. In assessing TTP or PFS, patients must be evaluated on a regular basis in all treatment arms, and an assessment of all disease sites should be performed. To reduce bias, the same assessment technique should be used at each follow-up, and the same evaluation schedule should be consistently used.","title":"Time to Tumor Progression and Progression-Free Survival"},{"location":"other-analysis/survival-analysis/#time-to-treatment-failure","text":"Time to treatment failure (TTF) is defined as the time from randomization to treatment discontinuation for any reason , including disease progression, treatment toxicity, patient preference, or death. From a regulatory point of view, TTF is generally not accepted as a valid endpoint. TTF is a composite endpoint influenced by factors unrelated to efficacy. Discontinuation may be a result of toxicity, patient preference, or a physician's reluctance to continue therapy. These factors are not a direct assessment of the effectiveness of a drug.","title":"Time to Treatment Failure"},{"location":"other-analysis/survival-analysis/#duration-of-response","text":"Duration of response (DoR) is usually measured as the time from documentation of tumor response to disease progression or death. Patients whose first documented response assessment is disease progression will be exluded from the DoR analysis. It can be assessed in single-arm trials, requires a smaller population and can be assessed earlier (compared to survival trials) and its effect is attributable directly to the drug, not the natural history of the disease. However, is not a comprehensive measure of drug activity.","title":"Duration of Response"},{"location":"other-analysis/survival-analysis/#understanding-the-basis-of-survival-analysis","text":"Survival data are generally described and modelled in terms of two related probabilities, namely survival and hazard . The survival probability (which is also called the survivor function) $S(t)$ is the probability that an individual survives from the time origin to a specified future time $t$. It is fundamental to a survival analysis because survival probabilities for different values of $t$ provide crucial summary information from time to event data. These values describe directly the survival experience of a study cohort. The hazard is usually denoted by $h(t)$ or $l(t)$ and is the probability that an individual who is under observation at a time $t$ has an event at that time. Put another way, it represents the instantaneous event rate for an individual who has already survived to time $t$. Note that, in contrast to the survivor function, which focuses on not having an event, the hazard function focuses on the event occurring. It is of interest because it provides insight into the conditional failure rates and provides a vehicle for specifying a survival model. In summary, the hazard relates to the incident (current) event rate, while survival reflects the cumulative non-occurrence. Understanding the mechanics behind survival analysis is aided by facility with the distributions used, which can be derived from the probability density function and cumulative density functions of survival times.","title":"Understanding the Basis of Survival Analysis"},{"location":"other-analysis/survival-analysis/#the-probability-density-function","text":"Imagine we have a random variable, $Time$, which records survival times. The function that describes likelihood of observing $Time$ at time $t$ relative to all other survival times is known as the probability density function ( pdf ), or $f(t)$. Integrating the pdf over a range of survival times gives the probability of observing a survival time within that interval. For example, if the survival times were known to be exponentially distributed, then the probability of observing a survival time within the interval $\\left [ a,b \\right ]$ is $Pr\\left ( a \\leq Time \\leq b \\right )=\\int_{a}^{b}f(t)dt=\\int_{a}^{b} \\lambda e^{-\\lambda t}dt$, where $\\lambda$ is the rate parameter of the exponential distribution and is equal to the reciprocal of the mean survival time. Most of the time we will not know a priori the distribution generating our observed survival times, but we can get and idea of what it looks like using nonparametric methods in SAS with PROC UNIVARIATE . 1 2 3 4 PROC UNIVARIATE DATA = SAS - data - set ( WHERE = ( censoring - variable = 1 )); VAR survival - time - variable ; HISTOGRAM survival - time - variable / KERNEL ; RUN ; In the graph above we see the correspondence between pdfs and histograms. Density functions are essentially histograms comprised of bins of vanishingly small widths. Technically, because there are no times less than $0$, there should be no graph to the left $Time=0$.","title":"The Probability Density Function"},{"location":"other-analysis/survival-analysis/#the-cumulative-distribution-function","text":"The cumulative distribution function ( cdf ), $F(t)$, describes the probability of observing $Time$ less than or equal to some time $t$, or $Pr(Time \\le t)$. Above we described that integrating the pdf over some range yields the probability of observing $Time$ in that range. Thus, we define the cumulative distribution function as: $F(t)=\\int_{0}^{t}f(t)dt$ The above relationship between the cdf and pdf also implies: $f(t)=\\frac{dF(t)}{dt}$ In SAS, we can graph an estimate of the cdf using PROC UNIVARIATE . 1 2 3 4 PROC UNIVARIATE DATA = SAS - data - set ( WHERE = ( censoring - variable = 1 )); VAR survival - time - variable ; CDFPLOT survival - time - variable ; RUN ; In the graph produced with the code above we can check the probability of surviving a number of days. In intervals where event times are more probable, the cdf will increase faster.","title":"The Cumulative Distribution Function"},{"location":"other-analysis/survival-analysis/#the-survival-function","text":"A simple transformation of the cumulative distribution function produces the survival function, $S(t)$: $S(t) = 1 - F(T)$. The survivor function, $S(t)$, describes the probability of surviving past time $t$, or $Pr(Time > t)$. If we were to plot the estimate of $S(t)$, we would see that it is a reflection of $F(t)$ (about $y=0$ and shifted up by $1$). We can use PROC LIFETEST to graph $S(t)$. 1 2 3 PROC LIFETEST DATA = SAS - data - set ( WHERE = ( censoring - variable = 1 )) PLOTS = SURVIVAL ( ATRISK ); TIME survival - time - variable * censoring - variable ( 0 ); RUN ; The probability of surviving beyond a number of days according to this survival plot can be confirmed by the cdf produced above, where the probability of surviving a number of days or fewer is equivalent.","title":"The Survival Function"},{"location":"other-analysis/survival-analysis/#the-hazard-function","text":"The primary focus of survival analysis is typically to model the hazard rate , which has the following relationship with the $f(t)$ and $S(t)$: $h(t)=\\frac{f(t)}{S(t)}$ The hazard function, then, describes the relative likelihood of the event occurring at time $t(f(t))$, conditional on the subject\u2019s survival up to that time $t(S(t))$. The hazard rate thus describes the instantaneous rate of failure at time $t$ and ignores the accumulation of hazard up to time $t$ (unlike $F(t)$ and $S(t)$). We can estimate the hazard function is SAS as well using PROC LIFETEST : 1 2 3 PROC LIFETEST DATA = SAS - data - set ( WHERE = ( censoring - variable = 1 )) PLOTS = HAZARD ( BW = 200 ); /* BW = Bandwidth*/ TIME survival - time - variable * censoring - variable ( 0 ); RUN ; The plot shows the Estimated Hazard Rate which is the expected number of failures per time unit (per day in our example).","title":"The Hazard Function"},{"location":"other-analysis/survival-analysis/#the-cumulative-hazard-function","text":"Also useful to understand is the cumulative hazard function, which as the name implies, cumulates hazards over time. It is calculated by integrating the hazard function over an interval of time: $H(t)=\\int_{0}^{t}h(u)du$ Let us again think of the hazard function, $h(t)$, as the rate at which failures occur at time $t$. Let us further suppose, for illustrative purposes, that the hazard rate stays constant at $\\frac{x}{t}$ ($x$ number of failures per unit time $t$) over the interval $[0,t]$. Summing over the entire interval, then, we would expect to observe $x$ failures, as $\\frac{x}{t} t = x$, (assuming repeated failures are possible, such that failing does not remove one from observation). One interpretation of the cumulative hazard function is thus the expected number of failures over time interval $[0,t]$. It is not at all necessary that the hazard function stay constant for the above interpretation of the cumulative hazard function to hold, but for illustrative purposes it is easier to calculate the expected number of failures since integration is not needed. Expressing the above relationship as $\\frac{d}{dt}H(t)=h(t)$, we see that the hazard function describes the rate at which hazards are accumulated over time. Using the equations, $h(t)=\\frac{f(t)}{S(t)}$ and $f(t)=\u2212\\frac{dS}{dt}$, we can derive the following relationships between the cumulative hazard function and the other survival functions: $S(t)=exp(-H(t))$ $F(t)=1-exp(-H(t))$ $f(t)=h(t) \\cdot exp(-H(t))$ From these equations we can see that the cumulative hazard function $H(t)$ and the survival function $S(t)$ have a simple monotonic relationship, such that when the Survival function is at its maximum at the beginning of analysis time, the cumulative hazard function is at its minimum. As time progresses, the Survival function proceeds towards it minimum, while the cumulative hazard function proceeds to its maximum. From these equations we can also see that we would expect the pdf, $f(t)$, to be high when $h(t)$ the hazard rate is high (its location depends on the study) and when the cumulative hazard $H(t)$ is low (the beginning, for all studies). In other words, we would expect to find a lot of failure times in a given time interval if the hazard rate is high and there are still a lot of subjects at-risk. We can estimate the cumulative hazard function using PROC LIFETEST , the results of which we send to PROC SGPLOT for plotting. 1 2 3 4 5 6 7 8 9 ODS OUTPUT ProductLimitEstimates = PLE ; PROC LIFETEST DATA = SAS - data - set whas500 ( WHERE = ( censoring - variable = 1 )) NELSON OUTS = output - name ; TIME survival - time - variable * censoring - variable ( 0 ); RUN ; PROC SGPLOT DATA = PLE ; SERIES x = survival - time - variable y = CumHaz ; RUN ;","title":"The Cumulative Hazard Function"},{"location":"other-analysis/survival-analysis/#data-preparation-and-exploration","text":"","title":"Data Preparation and Exploration"},{"location":"other-analysis/survival-analysis/#structure-of-the-data","text":"To work with PROC LIFETEST and PROC PHREG for survival analysis, data can be structured in one of these 2 ways: One row of data per subject , with one outcome variable representing the time to event, one variable that codes for whether the event occurred or not (censored), and explanatory variables of interest, each with fixed values across follow up time. Both PROC LIFETEST and PROC PHREG will accept data structured this way. Multiple rows of data per subject (only accepted by PROC PHREG ) following the \"counting process\" style of input. For each subject, the whole follow up period is partitioned into intervals, each defined by a \"start\" and \"stop\" time. Covariates are permitted to change value between intervals. Additionally, another variable counts the number of events occurring in each interval (either 0 or 1 in Cox regression, same as the censoring variable). This structuring allows the modeling of time-varying covariates, or explanatory variables whose values change across follow-up time. Data that are structured in the first, single-row way can be modified to be structured like the second, multi-row way, but the reverse is typically not true.","title":"Structure of the Data"},{"location":"other-analysis/survival-analysis/#data-exploration-with-proc-univariate-and-proc-corr","text":"Any serious endeavor into data analysis should begin with data exploration, in which the researcher becomes familiar with the distributions and typical values of each variable individually, as well as relationships between pairs or sets of variables. Within SAS, PROC UNIVARIATE provides easy, quick looks into the distributions of each variable, whereas PROC CORR can be used to examine bivariate relationships.","title":"Data Exploration with PROC UNIVARIATE and PROC CORR"},{"location":"other-analysis/survival-analysis/#nonparametric-methods-proc-lifetest","text":"1 2 3 4 5 6 7 8 oms = ( lastcontact - starttreatment + 1 ) / 30 . 45 if exdate = . then censor = 1 ; else censor = 0 ; PROC LIFETEST DATA = SAS - data - set plots = ( s ) ; TIME osm * censor ( 1 ) ; STRATA alg /* aleatorization group, (= . they didn't get to randomization) */ RUN ; In the TIME statement, only patients that haven't been censored are analysed The STRATA statement includes only non-missing data points (no WHERE filtering is needed) Tip If you are performing a survival analysis which only applies to part of your population but you need the probabilities to be referred to the total population just define the excluded subpopulation as having event at time = 0. This way your plot will not start at 1/0. This applies, for example, to locoregional control time plots on which patients without complete response are excluded (event at time = 0) and","title":"Nonparametric Methods (PROC LIFETEST)"},{"location":"other-analysis/survival-analysis/#p-value-calculation","text":"We select only 2 groups from the test data set (High and Low risk): 1 2 3 4 DATA bmt_small ; SET SASHELP . bmt ; WHERE group IN ( 2 , 3 ); RUN ; We generate the OUTSURV data set: 1 2 3 4 PROC LIFETEST DATA = bmt_small PLOTS = SURVIVAL ( CL CB = HW STRATA = PANEL ) METHOD = LT INTERVALS = ( 0 to 800 by 100 ) OUTSURV = bmt_param STDERR ; TIME t * status ( 0 ); STRATA group / ORDER = INTERNAL ; run ; We calculate the p-values from this data: 1 2 3 4 5 6 7 PROC SQL ; SELECT t , range ( survival ) AS RangeSurvival , sqrt ( sum ( sdf_stderr ** 2 )) AS Squares , range ( survival ) / sqrt ( sum ( sdf_stderr ** 2 )) AS z , probnorm ( abs ( range ( survival ) / sqrt ( sum ( sdf_stderr ** 2 )))) AS pz , 2 * ( 1 - probnorm ( abs ( range ( survival ) / sqrt ( sum ( sdf_stderr ** 2 ))))) AS pvalue FROM btm_param WHERE t > 0 GROUP BY t ; QUIT ; Other method: 1 2 3 4 5 6 7 8 9 10 11 DATA bmt600 ; brazo = 'A' ; valor = 1 ; contador = 32 ; OUTPUT ; brazo = 'A' ; valor = 2 ; contador = 13 ; OUTPUT ; brazo = 'B' ; valor = 1 ; contador = 18 ; OUTPUT ; brazo = 'B' ; valor = 2 ; contador = 36 ; OUTPUT ; RUN ; PROC FREQ DATA = bmt600 ; TABLES brazo * valor / CHISQ MEASURES RISKDIFF PLOTS = ( FREQPLOT ( TWOWAY = GROUPVERTICAL SCALE = PERCENT )); WEIGHT contador ; RUN ;","title":"P-value Calculation"},{"location":"other-analysis/survival-analysis/#estimating-the-survival-curve-using-the-kaplanmeier-method","text":"As it's been said, in analyzing survival data, two functions that are dependent on time are of particular interest: the survival function and the hazard function. The survival function $S(t)$ is defined as the probability of surviving at least to time $t$. The hazard function $h(t)$ is the conditional probability of dying at time $t$ having survived to that time. The graph of $S(t)$ against t is called the survival curve. The Kaplan\u2013Meier method can be used to estimate this curve from the observed survival times without the assumption of an underlying probability distribution. The method is based on the basic idea that the probability of surviving $k$ or more periods from entering the study is a product of the $k$ observed survival rates for each period (i.e. the cumulative proportion surviving), given by the following: $S(k)=p_1 \\times p_2 \\times p_3 \\times ... \\times p_k$","title":"Estimating the survival curve using the Kaplan\u2013Meier method"},{"location":"other-analysis/survival-analysis/#comparing-survival-curves-of-two-groups-using-the-log-rank-test","text":"Comparison of two survival curves can be done using a statistical hypothesis test called the log rank test. It is used to test the null hypothesis that there is no difference between the population survival curves . The test statistic is compared with a $\\chi^2$ distribution with 1 degree of freedom. An assumption for the log rank test is that of proportional hazards. Small departures from this assumption, however, do not invalidate the test .","title":"Comparing survival curves of two groups using the log rank test"},{"location":"other-analysis/survival-analysis/#coxs-proportional-hazards-regression-model-proc-phreg","text":"The log rank test is used to test whether there is a difference between the survival times of different groups but it does not allow other explanatory variables to be taken into account. Cox's proportional hazards model is analogous to a multiple regression model and enables the difference between survival times of particular groups of patients to be tested while allowing for other factors . In this model, the response (dependent) variable is the 'hazard'. The hazard is the probability of dying (or experiencing the event in question) given that patients have survived up to a given point in time, or the risk for death at that moment. In Cox's model no assumption is made about the probability distribution of the hazard. However, it is assumed that the hazard ratio does not depend on time .","title":"Cox's Proportional Hazards Regression Model (PROC PHREG)"},{"location":"other-analysis/survival-analysis/#censoring","text":"Right-censoring : for some subjects we do not know when they died after the issue, but we do know at least how many days they survived.","title":"Censoring"},{"location":"other-analysis/survival-analysis/#informative-censoring","text":"Check these papers Censoring in survival analysis: Potential for bias Impact of Informative Censoring on the Kaplan-Meier Estimate of Progression-Free Survival in Phase II Clinical Trials","title":"Informative Censoring"},{"location":"procedures/glimmix/","text":"Modelling a binary reponse variable: 1 2 3 4 5 6 PROC GLIMMIX DATA = SAS - data - set ; CLASS categorical1 categorical2 ; MODEL response = continuous2 continuous2 categorical1 / DIST = BINARY LINK = LOGIT ODDSRATIO SOLUTION CL ; RANDOM intercept / SUBJECT = categorical2 SOLUTION CL ; COVTEST / WALD ; RUN ; Modelling a multinomial reponse variable: 1 2 3 4 5 6 PROC GLIMMIX DATA = SAS - data - set ; CLASS categorical1 categorical2 ; MODEL response = continuous2 continuous2 categorical1 / DIST = MULTINOMIAL LINK = CLOGIT ODDSRATIO SOLUTION CL ; RANDOM intercept / SUBJECT = categorical2 SOLUTION CL ; COVTEST / WALD ; RUN ; LINK specifies the link function in the generalized linear mixed model ( link functions available ) LINK=LOGIT to use the logit link LINK=GENLOGIT | GLOGIT to use the generalized logit link LINK=CUMLOGIT | CLOGIT to use the cumulative logit link DIST specifies the built-in (conditional) probability distribution of the data ( distrubutions available and their corresponfing default link functions ) Estimating an Odds Ratio for a Variable Involved in an Interaction Link In models with LINK=LOGIT | GLOGIT | CLOGIT , you can obtain estimates of odds ratios through the ODDSRATIO options in the PROC GLIMMIX , LSMEANS , and MODEL statements. Note that for these link functions the EXP option in the ESTIMATE and LSMESTIMATE statements also produces odds or odds ratios. The ODDSRATIO option on the PROC GLIMMIX statement, requests odds ratio calculations for main effects. EXP requests exponentiation of the estimate ( ESTIMATE statement) or least squares means estimate ( LSMESTIMATE statement). If you specify the CL or ALPHA= option, the (adjusted) confidence bounds are also exponentiated. By default LSMEANS produces estimates on the logit scale . The ILINK option on the LSMEANS statement requests that the estimates be transformed back to the scale of the original data. The LSMEANS output will include estimates of the probability of each combination of the predictors interactions included in your model. The CL option requests confidence intervals for the estimates. For non-normal data, the EXP and ILINK options give you a way to obtain the quantity of interest on the scale of the mean (inverse link). Results presented in this fashion can be much easier to interpret than data on the link scale. Is it correct to assume, if you're using a different link function than LINK=LOGIT | GLOGIT | CLOGIT , that the exponentiated estimate can still be interpreted as the Odds Ratio? No, that is not correct. The odds ratio only make sense when you are comparing the predicted PROBABILITIES for two or more level of classification variables. If you use DIST=GAUSSIAN and LINK=IDENTITY , you are merely fitting a linear model to a response that has values 0 and 1. Check this dicussion for more information. 1 2 3 4 5 6 7 8 9 10 11 12 treatarm = { 1 , 2 } visit = { 1 , 2 , 3 , 4 , 5 } PROC GLIMMIX DATA = SAS - data - set ; CLASS categorical - variable ( s ) ; MODEL response = predictor ( s ) / DIST = BINARY LINK = LOGIT ODDSRATIO SOLUTION CL ; RANDOM intercept / SUBJECT = repeated - variable SOLUTION CL ; COVTEST / WALD ; LSMEANS treatarm * visit / SLICEDIFF = visit ODDSRATIO ILINK CL ; LSMESTIMATES treatarm * visit \" OR Visit 1 \" 1 0 0 0 0 - 1 0 0 0 0 , treatarm * visit \" OR Visit 2 \" 0 1 0 0 0 0 - 1 0 0 0 / EXP ILINK CL ; RUN ; Check these websites For more details check the SAS documentation An example different procedures ( PROC LOGISTIC and PROC GLIMMIX ) can be found here Some other options are also discussed here and here","title":"PROC GLIMMIX"},{"location":"procedures/glimmix/#estimating-an-odds-ratio-for-a-variable-involved-in-an-interaction","text":"In models with LINK=LOGIT | GLOGIT | CLOGIT , you can obtain estimates of odds ratios through the ODDSRATIO options in the PROC GLIMMIX , LSMEANS , and MODEL statements. Note that for these link functions the EXP option in the ESTIMATE and LSMESTIMATE statements also produces odds or odds ratios. The ODDSRATIO option on the PROC GLIMMIX statement, requests odds ratio calculations for main effects. EXP requests exponentiation of the estimate ( ESTIMATE statement) or least squares means estimate ( LSMESTIMATE statement). If you specify the CL or ALPHA= option, the (adjusted) confidence bounds are also exponentiated. By default LSMEANS produces estimates on the logit scale . The ILINK option on the LSMEANS statement requests that the estimates be transformed back to the scale of the original data. The LSMEANS output will include estimates of the probability of each combination of the predictors interactions included in your model. The CL option requests confidence intervals for the estimates. For non-normal data, the EXP and ILINK options give you a way to obtain the quantity of interest on the scale of the mean (inverse link). Results presented in this fashion can be much easier to interpret than data on the link scale. Is it correct to assume, if you're using a different link function than LINK=LOGIT | GLOGIT | CLOGIT , that the exponentiated estimate can still be interpreted as the Odds Ratio? No, that is not correct. The odds ratio only make sense when you are comparing the predicted PROBABILITIES for two or more level of classification variables. If you use DIST=GAUSSIAN and LINK=IDENTITY , you are merely fitting a linear model to a response that has values 0 and 1. Check this dicussion for more information. 1 2 3 4 5 6 7 8 9 10 11 12 treatarm = { 1 , 2 } visit = { 1 , 2 , 3 , 4 , 5 } PROC GLIMMIX DATA = SAS - data - set ; CLASS categorical - variable ( s ) ; MODEL response = predictor ( s ) / DIST = BINARY LINK = LOGIT ODDSRATIO SOLUTION CL ; RANDOM intercept / SUBJECT = repeated - variable SOLUTION CL ; COVTEST / WALD ; LSMEANS treatarm * visit / SLICEDIFF = visit ODDSRATIO ILINK CL ; LSMESTIMATES treatarm * visit \" OR Visit 1 \" 1 0 0 0 0 - 1 0 0 0 0 , treatarm * visit \" OR Visit 2 \" 0 1 0 0 0 0 - 1 0 0 0 / EXP ILINK CL ; RUN ; Check these websites For more details check the SAS documentation An example different procedures ( PROC LOGISTIC and PROC GLIMMIX ) can be found here Some other options are also discussed here and here","title":"Estimating an Odds Ratio for a Variable Involved in an Interaction"},{"location":"procedures/mixed/","text":"Linear Mixed Model General Concepts Link The linear mixed model is an extension of the general linear model, in which factors and covariates are assumed to have a linear relationship to the dependent variable. Factors. Categorical predictors should be selected as factors in the model. Each level of a factor can have a different linear effect on the value of the dependent variable. Fixed-effects factors are generally thought of as variables whose values of interest are all represented in the data file. Random-effects factors are variables whose values in the data file can be considered a random sample from a larger population of values. They are useful for explaining excess variability in the dependent variable. Practical Example A grocery store chain is interested in the effects of five different types of coupons on customer spending. At several store locations, these coupons are handed out to customers who frequent that location; one coupon selected at random is distributed to each customer. The type of coupon is a fixed effect because the company is interested in those particular coupons. The store location is a random effect because the locations used are a sample from the larger population of interest, and while there is likely to be store-to-store variation in customer spending, the company is not directly interested in that variation in the context of this problem. Covariates. Scale predictors should be selected as covariates in the model. Within combinations of factor levels (or cells), values of covariates are assumed to be linearly correlated with values of the dependent variables. Interactions. The Linear Mixed Models procedure allows you to specify factorial interactions, which means that each combination of factor levels can have a different linear effect on the dependent variable. Additionally, you may specify factor-covariate interactions, if you believe that the linear relationship between a covariate and the dependent variable changes for different levels of a factor. Random effects covariance structure. The Linear Mixed Models procedure allows you to specify the relationship between the levels of random effects. By default, levels of random effects are uncorrelated and have the same variance. Repeated effects. Factors and covariates are features of the general linear model. In the Linear Mixed Models procedure, repeated effects variables are added, allowing you to relax the assumption of independence of the error terms. In order to model the covariance structure of the error terms, you need to specify the following: Repeated effects variables are variables whose values in the data file can be considered as markers of multiple observations of a single subject. Subject variables define the individual subjects of the repeated measurements. The error terms for each individual are independent of those of other individuals. The covariance structure specifies the relationship between the levels of the repeated effects. The types of covariance structures available allow for residual terms with a wide variety of variances and covariances. Practical Example If the grocery store recorded the purchasing habits of their customers for four consecutive weeks, then the variable Week would be a repeated effects variable . Specifying a subject variable denoting the Customer ID differentiates the repeated observations of separate customers. Specifying a first-order autoregressive covariance structure reflects your belief that a higher-than-average volume of purchases in one week will correspond to a higher (or lower)-than-average volume in the following week. SAS Formulation Link 1 2 3 4 5 6 7 PROC MIXED DATA = SAS - data - set ; CLASS categorical1 categorical2 ; MODEL response = continuous1 categorical1 continuous1 * categorical1 / solution ; RANDOM categorical2 ; LSMEANS continuous1 * categorical1 / CL PDIFF DIFFS E ; RUN ; QUIT ;","title":"PROC MIXED"},{"location":"procedures/mixed/#linear-mixed-model-general-concepts","text":"The linear mixed model is an extension of the general linear model, in which factors and covariates are assumed to have a linear relationship to the dependent variable. Factors. Categorical predictors should be selected as factors in the model. Each level of a factor can have a different linear effect on the value of the dependent variable. Fixed-effects factors are generally thought of as variables whose values of interest are all represented in the data file. Random-effects factors are variables whose values in the data file can be considered a random sample from a larger population of values. They are useful for explaining excess variability in the dependent variable. Practical Example A grocery store chain is interested in the effects of five different types of coupons on customer spending. At several store locations, these coupons are handed out to customers who frequent that location; one coupon selected at random is distributed to each customer. The type of coupon is a fixed effect because the company is interested in those particular coupons. The store location is a random effect because the locations used are a sample from the larger population of interest, and while there is likely to be store-to-store variation in customer spending, the company is not directly interested in that variation in the context of this problem. Covariates. Scale predictors should be selected as covariates in the model. Within combinations of factor levels (or cells), values of covariates are assumed to be linearly correlated with values of the dependent variables. Interactions. The Linear Mixed Models procedure allows you to specify factorial interactions, which means that each combination of factor levels can have a different linear effect on the dependent variable. Additionally, you may specify factor-covariate interactions, if you believe that the linear relationship between a covariate and the dependent variable changes for different levels of a factor. Random effects covariance structure. The Linear Mixed Models procedure allows you to specify the relationship between the levels of random effects. By default, levels of random effects are uncorrelated and have the same variance. Repeated effects. Factors and covariates are features of the general linear model. In the Linear Mixed Models procedure, repeated effects variables are added, allowing you to relax the assumption of independence of the error terms. In order to model the covariance structure of the error terms, you need to specify the following: Repeated effects variables are variables whose values in the data file can be considered as markers of multiple observations of a single subject. Subject variables define the individual subjects of the repeated measurements. The error terms for each individual are independent of those of other individuals. The covariance structure specifies the relationship between the levels of the repeated effects. The types of covariance structures available allow for residual terms with a wide variety of variances and covariances. Practical Example If the grocery store recorded the purchasing habits of their customers for four consecutive weeks, then the variable Week would be a repeated effects variable . Specifying a subject variable denoting the Customer ID differentiates the repeated observations of separate customers. Specifying a first-order autoregressive covariance structure reflects your belief that a higher-than-average volume of purchases in one week will correspond to a higher (or lower)-than-average volume in the following week.","title":"Linear Mixed Model General Concepts"},{"location":"procedures/mixed/#sas-formulation","text":"1 2 3 4 5 6 7 PROC MIXED DATA = SAS - data - set ; CLASS categorical1 categorical2 ; MODEL response = continuous1 categorical1 continuous1 * categorical1 / solution ; RANDOM categorical2 ; LSMEANS continuous1 * categorical1 / CL PDIFF DIFFS E ; RUN ; QUIT ;","title":"SAS Formulation"},{"location":"procedures/regression-models/","text":"Interpreting the results of the SOLUTION option in the MODEL for categorical variables Link In procedures that use the GLM parameterization for CLASS variables such as PROC GLM , PROC MIXED and PROC GLIMMIX , a predictor variable specified in the CLASS statement is represented in the model by a set of design variables created using GLM parameterization. This is a less than full-rank parameterization in which a CLASS variable with k levels is represented in the design matrix by a set of k 0,1-coded indicator (or dummy ) variables. If the SOLUTION option in the MODEL statement is also specified, the following note is included in the displayed results below the parameter estimates table: 1 NOTE : The X 'X matrix has been found to be singular, and a generalized inverse was used to solve the normal equations. Terms whose estimates are followed by the letter ' B ' are not uniquely estimable . Note that there are many possible parameterizations, each of which imposes a different interpretation on the model parameters . Seealso Check this for a full explanation of the interpretation of the results. Read more about the GLM parametrization.","title":"General Notes on Regression Procedures"},{"location":"procedures/regression-models/#interpreting-the-results-of-the-solution-option-in-the-model-for-categorical-variables","text":"In procedures that use the GLM parameterization for CLASS variables such as PROC GLM , PROC MIXED and PROC GLIMMIX , a predictor variable specified in the CLASS statement is represented in the model by a set of design variables created using GLM parameterization. This is a less than full-rank parameterization in which a CLASS variable with k levels is represented in the design matrix by a set of k 0,1-coded indicator (or dummy ) variables. If the SOLUTION option in the MODEL statement is also specified, the following note is included in the displayed results below the parameter estimates table: 1 NOTE : The X 'X matrix has been found to be singular, and a generalized inverse was used to solve the normal equations. Terms whose estimates are followed by the letter ' B ' are not uniquely estimable . Note that there are many possible parameterizations, each of which imposes a different interpretation on the model parameters . Seealso Check this for a full explanation of the interpretation of the results. Read more about the GLM parametrization.","title":"Interpreting the results of the SOLUTION option in the MODEL for categorical variables"},{"location":"sas-outputs/excel/","text":"Export Data to Excel With Format Link 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 %macro ExportExcelWithFormat ( libname = , dataname = , select = , outputname = , sheetname = ); proc sql noprint ; create table tmp_vars as select name , format , label from dictionary . columns where libname = upcase ( \"&libname.\" ) and memname = upcase ( \"&dataname.\" ); quit ; data tmp_vars ; set tmp_vars end = last ; length formatcode $400 .; if format ^= \"\" then formatcode = catx ( \" \" , cats ( \"put\" , \"(\" , name , \",\" , format , \")\" ), \"as\" , name , \" label\" , \"'\" , label , \"'\" , \",\" ); else formatcode = cats ( name , \",\" ); if last then formatcode = substr ( formatcode , 1 , length ( formatcode ) - 1 ); run ; %let formatcodes = ; data _null_ ; set tmp_vars ; call symput ( ' formatcodes ' , trim ( resolve ( ' & formatcodes . ' ) || ' ' || trim ( formatcode ))); run ; %put & formatcodes .; proc sql ; create view tmp_view as select & formatcodes . from & libname .. & dataname .; quit ; data _datain ; retain & select ; set tmp_view ( keep =& select ); run ; %let formatcodes = %str (); PROC EXPORT DATA = _datain DBMS = xlsx REPLACE label OUTFILE = \"&outputname.\" ; SHEET = \"&sheetname.\" ; RUN ; proc sql ; drop table tmp_vars , _datain ; drop view tmp_view ; quit ; %mend ; %export ExcelWithFormat ( libname = SAS - libname , dataname = SAS - data - set , select = variable1 variable2 variable3 , outputname = %str ( & path \\ exceldata \\ File name ( & sysdate ). xlsx ), sheetname = Sheet name );","title":"Export to Excel file"},{"location":"sas-outputs/excel/#export-data-to-excel-with-format","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 %macro ExportExcelWithFormat ( libname = , dataname = , select = , outputname = , sheetname = ); proc sql noprint ; create table tmp_vars as select name , format , label from dictionary . columns where libname = upcase ( \"&libname.\" ) and memname = upcase ( \"&dataname.\" ); quit ; data tmp_vars ; set tmp_vars end = last ; length formatcode $400 .; if format ^= \"\" then formatcode = catx ( \" \" , cats ( \"put\" , \"(\" , name , \",\" , format , \")\" ), \"as\" , name , \" label\" , \"'\" , label , \"'\" , \",\" ); else formatcode = cats ( name , \",\" ); if last then formatcode = substr ( formatcode , 1 , length ( formatcode ) - 1 ); run ; %let formatcodes = ; data _null_ ; set tmp_vars ; call symput ( ' formatcodes ' , trim ( resolve ( ' & formatcodes . ' ) || ' ' || trim ( formatcode ))); run ; %put & formatcodes .; proc sql ; create view tmp_view as select & formatcodes . from & libname .. & dataname .; quit ; data _datain ; retain & select ; set tmp_view ( keep =& select ); run ; %let formatcodes = %str (); PROC EXPORT DATA = _datain DBMS = xlsx REPLACE label OUTFILE = \"&outputname.\" ; SHEET = \"&sheetname.\" ; RUN ; proc sql ; drop table tmp_vars , _datain ; drop view tmp_view ; quit ; %mend ; %export ExcelWithFormat ( libname = SAS - libname , dataname = SAS - data - set , select = variable1 variable2 variable3 , outputname = %str ( & path \\ exceldata \\ File name ( & sysdate ). xlsx ), sheetname = Sheet name );","title":"Export Data to Excel With Format"},{"location":"sas-outputs/graphs/","text":"Check these websites Here are some examples of complex graphs. Here there are instructions to play with the axis' attributes. Graphically speaking blog with useful tips for graphics. Welcome to the Three Ring %CIRCOS: An Example of Creating a Circular Graph without a Polar Axis Basic ODS Options Link You need to add this command to get the plots displayed in the output: 1 2 3 ODS GRAPHICS ON ; [ your code here ] ODS GRAPHICS OFF ; When you add the ODS TRACE statement, SAS writes a trace record to the log that includes information about each output object (name, label, template, path): 1 2 3 ODS TRACE ON ; [ your code here ] ODS TRACE OFF ; You produce a list of the possible output elements in the log that you may specify in the ODS SELECT/EXCLUDE statement: 1 2 3 ODS SELECT output - name1 output - name2 output - name3 ; [ your code here ] ODS SELECT ALL ; /* Reset this option to the default */ Yo can keeps some of the outputs in SAS-data-sets: 1 ODS OUTPUT output - name1 = generated - data - set1 output - name1 = generated - data - set2 output - name1 = generated - data - set3 ; Remove date and pagination from the automatic output header: 1 OPTIONS NODATE NONUMBER ; Remove graph's external borders: 1 ODS GRAPHICS / NOBORDER ; Plot Procedures Link GPLOT Link Reference lines: 1 2 3 4 5 6 SYMBOL1 COLOR = blue INTERPOL = join ; AXIS1 LABEL = ( 'X axis label' ) order = ( 0 to 15 by 1 ) reflabel = ( j = c h = 9 pt 'Reference line label 1' 'Reference line label 2' 'Reference line label 3' ); AXIS2 LABEL = ( 'Y axis label' j = c ); PROC GPLOT DATA = SAS - data - set ; PLOT variabley * variablex / HAXIS = AXIS1 VAXIS = AXIS2 HREF = 6 9 13 /*location of ref lines*/ ; RUN ; SGPLOT Link Highlight a certain boxplot and get the plot narrower: 1 2 3 4 5 6 7 8 PROC SGPLOT DATA = sashelp . heart ; /* The order matters: first thing defined goes to the back */ REFLINE 'Coronary Heart Disease' / AXIS = x LINEATTRS = ( THICKNESS = 70 COLOR = yellow ) TRANSPARENCY = 0 . 5 ; VBOX cholesterol / CATEGORY = deathcause ; XAXIS OFFSETMIN = 0 . 25 OFFSETMAX = 0 . 25 DISCRETEORDER = data ; YAXIS GRID ; RUN ; Specify the colors/point styles of groups in SAS statistical graphics Type of Plots Link Check these websites Clinical Graphs using SAS Graphical Results in Clinical Oncology Studies Annotate Dictionary Using the SG Annotation Macros Swimmer plot Link A swimmer plot is a graphical tool involving horizontal bars that can be used to show multiple pieces of information about a given data set in one glance. In this example a swimmer plot is used to tell \u201ca story\u201d about the effects of a study treatment on tumor response for individual subjects in an oncology study. Through the use of a swimmer plot we are able to look at our data on an individual subject level rather than an aggregate level that is often done for \u201ctime to response\u201d analysis using Kaplan-Meier methods. Check these websites Swimmer Plot Swimmer Plot: Tell a Graphical Story of Your Time to Response Data Using PROC SGPLOT Waterfall plot Link A waterfall chart is commonly used in the Oncology domain to track the change in tumor size for subjects in a study by treatment. The graph displays the change in tumor size for each subject in the study by descending percent change from baseline. A bar is displayed for each subject in decreasing order. Each bar is classified by the treatment. The response category is displayed at the end of the bar. Reference lines are drawn at RECIST threshold of -30% and at 20%. Check these websites Clinical graphs: Waterfall plot ++ Create a waterfall plot in SAS Waterfall plot: two different approaches, one beautiful graph Waterfall Charts in Oncology Trials - Ride the Wave A 3D waterfall chart Spaghetti or spider plot Link A spaghetti plot is a method of viewing data to visualize possible flows through systems. Flows depicted in this manner appear like noodles, hence the coining of this term. This method of statistics was first used to track routing through factories. Visualizing flow in this manner can reduce inefficiency within the flow of a system. Within medicine, they can illustrate the effects of drugs on patients during clinical trials. It can be used as another way of presenting the change from baseline for tumors for each subject in a study by week. The plot can be classified by response and stage. Check these websites Clinical Graphs: Spider plot SAS code examples Example 1 Example 2 Box-and-whiskers plot or boxplot Link This is a method for graphically depicting groups of numerical data through their quartiles. These plots may also have lines extending from the boxes (whiskers) indicating variability outside the upper and lower quartiles. Outliers may be plotted as individual points. Boxplots are non-parametric: they display variation in samples of a statistical population without making any assumptions of the underlying statistical distribution (though Tukey's boxplot assumes symmetry for the whiskers and normality for their length). The spacings between the different parts of the box indicate the degree of dispersion (spread) and skewness in the data, and show outliers. A boxplot is a standardized way of displaying the dataset based on a five-number summary: Minimum : the lowest data point excluding any outliers Maximum : the largest data point excluding any outliers Median (Q2 / 50th Percentile) : the middle value of the dataset First quartile (Q1 / 25th Percentile) : is also known as the lower quartile and is the middle value between the smallest number (not the minimum) and the median of the dataset Third quartile (Q3 / 75th Percentile) : is also known as the upper quartile and is the middle value between the largest number (not the maximum) and the median of the dataset Some boxplots include an additional character to represent the mean of the data (such as the diamond symbol in SAS procedure). SAS code examples Example 1 Stacked bar plot Link One of the most popular and useful graph types is the Bar Chart. The SGPLOT procedure supports many types of bar charts, each suitable for some specific use case. In this case the stacked bar plot is very useful to easily represent this kind of information about AEs. Check these websites Getting started with SGPLOT - VBAR SAS code examples Example 1 Miscellanea Link Available Colors at the SAS Registry Link You can check the list of SAS predefined colors and even list it using the SAS registry: 1 2 PROC REGISTRY LIST STARTAT = '\\COLORNAMES\\HTML' ; RUN ; Check this website Using the SAS Registry to Control Color","title":"Graphs and Plots"},{"location":"sas-outputs/graphs/#basic-ods-options","text":"You need to add this command to get the plots displayed in the output: 1 2 3 ODS GRAPHICS ON ; [ your code here ] ODS GRAPHICS OFF ; When you add the ODS TRACE statement, SAS writes a trace record to the log that includes information about each output object (name, label, template, path): 1 2 3 ODS TRACE ON ; [ your code here ] ODS TRACE OFF ; You produce a list of the possible output elements in the log that you may specify in the ODS SELECT/EXCLUDE statement: 1 2 3 ODS SELECT output - name1 output - name2 output - name3 ; [ your code here ] ODS SELECT ALL ; /* Reset this option to the default */ Yo can keeps some of the outputs in SAS-data-sets: 1 ODS OUTPUT output - name1 = generated - data - set1 output - name1 = generated - data - set2 output - name1 = generated - data - set3 ; Remove date and pagination from the automatic output header: 1 OPTIONS NODATE NONUMBER ; Remove graph's external borders: 1 ODS GRAPHICS / NOBORDER ;","title":"Basic ODS Options"},{"location":"sas-outputs/graphs/#plot-procedures","text":"","title":"Plot Procedures"},{"location":"sas-outputs/graphs/#gplot","text":"Reference lines: 1 2 3 4 5 6 SYMBOL1 COLOR = blue INTERPOL = join ; AXIS1 LABEL = ( 'X axis label' ) order = ( 0 to 15 by 1 ) reflabel = ( j = c h = 9 pt 'Reference line label 1' 'Reference line label 2' 'Reference line label 3' ); AXIS2 LABEL = ( 'Y axis label' j = c ); PROC GPLOT DATA = SAS - data - set ; PLOT variabley * variablex / HAXIS = AXIS1 VAXIS = AXIS2 HREF = 6 9 13 /*location of ref lines*/ ; RUN ;","title":"GPLOT"},{"location":"sas-outputs/graphs/#sgplot","text":"Highlight a certain boxplot and get the plot narrower: 1 2 3 4 5 6 7 8 PROC SGPLOT DATA = sashelp . heart ; /* The order matters: first thing defined goes to the back */ REFLINE 'Coronary Heart Disease' / AXIS = x LINEATTRS = ( THICKNESS = 70 COLOR = yellow ) TRANSPARENCY = 0 . 5 ; VBOX cholesterol / CATEGORY = deathcause ; XAXIS OFFSETMIN = 0 . 25 OFFSETMAX = 0 . 25 DISCRETEORDER = data ; YAXIS GRID ; RUN ; Specify the colors/point styles of groups in SAS statistical graphics","title":"SGPLOT"},{"location":"sas-outputs/graphs/#type-of-plots","text":"Check these websites Clinical Graphs using SAS Graphical Results in Clinical Oncology Studies Annotate Dictionary Using the SG Annotation Macros","title":"Type of Plots"},{"location":"sas-outputs/graphs/#swimmer-plot","text":"A swimmer plot is a graphical tool involving horizontal bars that can be used to show multiple pieces of information about a given data set in one glance. In this example a swimmer plot is used to tell \u201ca story\u201d about the effects of a study treatment on tumor response for individual subjects in an oncology study. Through the use of a swimmer plot we are able to look at our data on an individual subject level rather than an aggregate level that is often done for \u201ctime to response\u201d analysis using Kaplan-Meier methods. Check these websites Swimmer Plot Swimmer Plot: Tell a Graphical Story of Your Time to Response Data Using PROC SGPLOT","title":"Swimmer plot"},{"location":"sas-outputs/graphs/#waterfall-plot","text":"A waterfall chart is commonly used in the Oncology domain to track the change in tumor size for subjects in a study by treatment. The graph displays the change in tumor size for each subject in the study by descending percent change from baseline. A bar is displayed for each subject in decreasing order. Each bar is classified by the treatment. The response category is displayed at the end of the bar. Reference lines are drawn at RECIST threshold of -30% and at 20%. Check these websites Clinical graphs: Waterfall plot ++ Create a waterfall plot in SAS Waterfall plot: two different approaches, one beautiful graph Waterfall Charts in Oncology Trials - Ride the Wave A 3D waterfall chart","title":"Waterfall plot"},{"location":"sas-outputs/graphs/#spaghetti-or-spider-plot","text":"A spaghetti plot is a method of viewing data to visualize possible flows through systems. Flows depicted in this manner appear like noodles, hence the coining of this term. This method of statistics was first used to track routing through factories. Visualizing flow in this manner can reduce inefficiency within the flow of a system. Within medicine, they can illustrate the effects of drugs on patients during clinical trials. It can be used as another way of presenting the change from baseline for tumors for each subject in a study by week. The plot can be classified by response and stage. Check these websites Clinical Graphs: Spider plot SAS code examples Example 1 Example 2","title":"Spaghetti or spider plot"},{"location":"sas-outputs/graphs/#box-and-whiskers-plot-or-boxplot","text":"This is a method for graphically depicting groups of numerical data through their quartiles. These plots may also have lines extending from the boxes (whiskers) indicating variability outside the upper and lower quartiles. Outliers may be plotted as individual points. Boxplots are non-parametric: they display variation in samples of a statistical population without making any assumptions of the underlying statistical distribution (though Tukey's boxplot assumes symmetry for the whiskers and normality for their length). The spacings between the different parts of the box indicate the degree of dispersion (spread) and skewness in the data, and show outliers. A boxplot is a standardized way of displaying the dataset based on a five-number summary: Minimum : the lowest data point excluding any outliers Maximum : the largest data point excluding any outliers Median (Q2 / 50th Percentile) : the middle value of the dataset First quartile (Q1 / 25th Percentile) : is also known as the lower quartile and is the middle value between the smallest number (not the minimum) and the median of the dataset Third quartile (Q3 / 75th Percentile) : is also known as the upper quartile and is the middle value between the largest number (not the maximum) and the median of the dataset Some boxplots include an additional character to represent the mean of the data (such as the diamond symbol in SAS procedure). SAS code examples Example 1","title":"Box-and-whiskers plot or boxplot"},{"location":"sas-outputs/graphs/#stacked-bar-plot","text":"One of the most popular and useful graph types is the Bar Chart. The SGPLOT procedure supports many types of bar charts, each suitable for some specific use case. In this case the stacked bar plot is very useful to easily represent this kind of information about AEs. Check these websites Getting started with SGPLOT - VBAR SAS code examples Example 1","title":"Stacked bar plot"},{"location":"sas-outputs/graphs/#miscellanea","text":"","title":"Miscellanea"},{"location":"sas-outputs/graphs/#available-colors-at-the-sas-registry","text":"You can check the list of SAS predefined colors and even list it using the SAS registry: 1 2 PROC REGISTRY LIST STARTAT = '\\COLORNAMES\\HTML' ; RUN ; Check this website Using the SAS Registry to Control Color","title":"Available Colors at the SAS Registry"},{"location":"sas-outputs/ods/","text":"Setting the SAS System Options Link 1 OPTIONS NODATE PAGENO = 1 LINESIZE = 80 PAGESIZE = 40 ; The NODATE option suppresses the display of the date and time in the output PAGENO= specifies the starting page number LINESIZE= specifies the output line length PAGESIZE= specifies the number of lines on an output page NONUMBER removes pagination from the automatic output header ODS Link In order to produce outputs from SAS, the three more common ODS techniques, that produces different output files, are HTML, RTF, and PDF. Each ODS statement uses options that are specific to that destination. The ODS options (other than the FILE= option) used in the program are shown in the table below. RTF PDF HTML BODYTITLE STARTPAGE=NO KEEPN NOTOC_DATA / TOC_DATA CONTENTS COLUMNS= TEXT= BOOKMARKGEN=NO STARTPAGE=NO COMPRESS=9 TEXT= STYLE=SASWEB RS=NONE For an explanation of the options, refer to this page or to the ODS User's Guide . Remove graph's external borders: 1 ODS GRAPHICS / NOBORDER ; Basic ODS Options Link You need to add this command to get the plots displayed in the output: 1 2 3 ODS GRAPHICS ON ; [ your code here ] ODS GRAPHICS OFF ; When you add the ODS TRACE statement, SAS writes a trace record to the log that includes information about each output object (name, label, template, path): 1 2 3 ODS TRACE ON ; [ your code here ] ODS TRACE OFF ; You produce a list of the possible output elements in the log that you may specify in the ODS SELECT/EXCLUDE statement: 1 2 3 ODS SELECT output - name1 output - name2 output - name3 ; [ your code here ] ODS SELECT ALL ; /* Reset this option to the default */ Yo can keeps some of the outputs in SAS-data-sets: 1 ODS OUTPUT output - name1 = generated - data - set1 output - name1 = generated - data - set2 output - name1 = generated - data - set3 ; Control the output via ODS EXCLUDE Link Some interesting articles on this topic by Rick Wicklin : Turn off ODS when running simulations in SAS What is the best way to suppress ODS output in SAS? Five reasons to use ODS EXCLUDE to suppress SAS output Example on combining ODS EXCLUDE with ODS OUTPUT to control the obtained output: 1 2 3 4 5 6 ODS EXCLUDE ALL ; PROC FREQ DATA = SAS - data - set ; TABLE variable1 * variable2 ; ODS OUTPUT CROSSTABFREQS = custom - SAS - data - set ; RUN ; ODS EXCLUDE NONE ;","title":"Output Delivery System"},{"location":"sas-outputs/ods/#setting-the-sas-system-options","text":"1 OPTIONS NODATE PAGENO = 1 LINESIZE = 80 PAGESIZE = 40 ; The NODATE option suppresses the display of the date and time in the output PAGENO= specifies the starting page number LINESIZE= specifies the output line length PAGESIZE= specifies the number of lines on an output page NONUMBER removes pagination from the automatic output header","title":"Setting the SAS System Options"},{"location":"sas-outputs/ods/#ods","text":"In order to produce outputs from SAS, the three more common ODS techniques, that produces different output files, are HTML, RTF, and PDF. Each ODS statement uses options that are specific to that destination. The ODS options (other than the FILE= option) used in the program are shown in the table below. RTF PDF HTML BODYTITLE STARTPAGE=NO KEEPN NOTOC_DATA / TOC_DATA CONTENTS COLUMNS= TEXT= BOOKMARKGEN=NO STARTPAGE=NO COMPRESS=9 TEXT= STYLE=SASWEB RS=NONE For an explanation of the options, refer to this page or to the ODS User's Guide . Remove graph's external borders: 1 ODS GRAPHICS / NOBORDER ;","title":"ODS"},{"location":"sas-outputs/ods/#basic-ods-options","text":"You need to add this command to get the plots displayed in the output: 1 2 3 ODS GRAPHICS ON ; [ your code here ] ODS GRAPHICS OFF ; When you add the ODS TRACE statement, SAS writes a trace record to the log that includes information about each output object (name, label, template, path): 1 2 3 ODS TRACE ON ; [ your code here ] ODS TRACE OFF ; You produce a list of the possible output elements in the log that you may specify in the ODS SELECT/EXCLUDE statement: 1 2 3 ODS SELECT output - name1 output - name2 output - name3 ; [ your code here ] ODS SELECT ALL ; /* Reset this option to the default */ Yo can keeps some of the outputs in SAS-data-sets: 1 ODS OUTPUT output - name1 = generated - data - set1 output - name1 = generated - data - set2 output - name1 = generated - data - set3 ;","title":"Basic ODS Options"},{"location":"sas-outputs/ods/#control-the-output-via-ods-exclude","text":"Some interesting articles on this topic by Rick Wicklin : Turn off ODS when running simulations in SAS What is the best way to suppress ODS output in SAS? Five reasons to use ODS EXCLUDE to suppress SAS output Example on combining ODS EXCLUDE with ODS OUTPUT to control the obtained output: 1 2 3 4 5 6 ODS EXCLUDE ALL ; PROC FREQ DATA = SAS - data - set ; TABLE variable1 * variable2 ; ODS OUTPUT CROSSTABFREQS = custom - SAS - data - set ; RUN ; ODS EXCLUDE NONE ;","title":"Control the output via ODS EXCLUDE"},{"location":"sas-outputs/pdf/","text":"ODS definition for *.pdf format Link 1 2 3 ODS PDF DPI = 700 STYLE = customstyle FILE = \"path\\file (&sysdate).pdf\" PDFTOC = 1 ; (...) ODS PDF CLOSE ; DPI needs to be increased to show the preimage logo with good definition The FILE definition must contain the output path too, ODS PDF is not compatible with the PATH= option PDFTOC fixed the number of TOC levels displayed by default when openning the file (even if there are more defined the tree will appear contracted) TOC customization Link First level: ods proclabel=\"Name\"; Second level (remove): contents = \"\" inside PROC REPORT statement Third level (remove): define an auxiliary variable count = 1 in your data set and include it in the PROC REPORT 1 2 3 column ( \" Title \" count ... ) ; define count / group noprint ; break before count / contents = \"\" page ;","title":"Write to pdf file"},{"location":"sas-outputs/pdf/#ods-definition-for-pdf-format","text":"1 2 3 ODS PDF DPI = 700 STYLE = customstyle FILE = \"path\\file (&sysdate).pdf\" PDFTOC = 1 ; (...) ODS PDF CLOSE ; DPI needs to be increased to show the preimage logo with good definition The FILE definition must contain the output path too, ODS PDF is not compatible with the PATH= option PDFTOC fixed the number of TOC levels displayed by default when openning the file (even if there are more defined the tree will appear contracted)","title":"ODS definition for *.pdf format"},{"location":"sas-outputs/pdf/#toc-customization","text":"First level: ods proclabel=\"Name\"; Second level (remove): contents = \"\" inside PROC REPORT statement Third level (remove): define an auxiliary variable count = 1 in your data set and include it in the PROC REPORT 1 2 3 column ( \" Title \" count ... ) ; define count / group noprint ; break before count / contents = \"\" page ;","title":"TOC customization"},{"location":"sas-outputs/print/","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 PROC FORMAT ; VALUE $ nfmt 'Alfred' = 'verylightgreen' 'Henry' = 'verylightblue' 'Barbara' = 'verylightyellow' 'Carol' = 'verylightpurple' ; VALUE wfmt LOW - 85 . 99 = 'red' 86 - 99 . 99 = 'orange' 100 - HIGH = 'green' ; RUN ; PROC SORT DATA = sashelp . class OUT = classtest ; BY sex ; RUN ; PROC PRINT DATA = classtest ( FIRSTOBS = 2 OBS = 16 ) N STYLE ( TABLE ) = { BACKGROUND = yellow RULES = rows FRAME = void CELLSPACING = 0 BORDERCOLOR = red BORDERWIDTH = 5 } STYLE ( HEADER ) = { BACKGROUND = purple FONT_SIZE = 12 pt } STYLE ( GRANDTOTAL ) = { BACKGROUND = pink FONT_SIZE = 14 pt } STYLE ( OBS ) = { BACKGROUND = pink COLOR = BLACK FONT_SIZE = 12 pt } STYLE ( OBSHEADER ) = { BACKGROUND = green FONT_SIZE = 12 pt } ; TITLE 'PROC PRINT example' ; VAR name / STYLE ( HEADER ) = HEADER { FONT_SIZE = 12 pt } STYLE ( DATA ) = HEADER { FONT_SIZE = 12 pt BACKGROUND = $ nfmt . FOREGROUND = black } ; VAR age / STYLE ( HEADER ) = HEADER { FONT_SIZE = 12 pt } STYLE ( DATA ) = HEADER { FONT_SIZE = 12 pt } ; VAR height / STYLE ( HEADER ) = HEADER { FONT_SIZE = 12 pt } STYLE ( DATA ) = { FONT_SIZE = 12 pt } ; VAR weight / STYLE ( HEADER ) = HEADER { FONT_SIZE = 12 pt } STYLE ( DATA ) = { FONT_SIZE = 12 pt FOREGROUND = wfmt . FONT_WEIGHT = bold } ; BY sex ; SUM height weight ; RUN ; FIRSTOBS is the first observation printed OBS is the last observation printed N prints the number of observations in the data set BY is to generate a table for each BY variable level (the data is expected to be ordered) PAGEBY is for creating page breaks using a variable SUM produces totaling variables (you will get a subtotal for each BY group in which there is more than one observation) You can print only certain variables or changing their order using VAR FRAME= defines the borders of the cells: VOID means no borders while BOX means all borders","title":"PROC PRINT"},{"location":"sas-outputs/report/","text":"Examples Link How to Write a Header/Footer in your Tables Link 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ODS ESCAPECHAR = '^' ; PROC REPORT DATA = sashelp . cars ; WHERE Make = 'Jaguar' ; COLUMN ( '1) Label 1' model Invoice ) ( '2) Label 2' Horsepower Weight Length ); COMPUTE BEFORE _PAGE_ / STYLE = HEADER { JUST = L FONTWEIGHT = BOLD COLOR = PURPLE } ; LINE 'Test of custom header' ; ENDCOMP ; COMPUTE AFTER / STYLE = { TEXTDECORATION = UNDERLINE JUST = C COLOR = RED } ; LINE 'Test of a custom footer' ; LINE '^S={color=green} Test of a custom footer with a different style' ; ENDCOMP ; RUN ; Specify the STYLE of Your Global Header Link 1 2 3 4 5 PROC REPORT DATA = SAS - data - set HEADSKIP HEADLINE NOWINDOWS STYLE ( header ) = { ASIS = on BACKGROUND = very light grey FONTWEIGHT = BOLD } ; COLUMN ( \"Style of this global header\" var1 var2 ); DEFINE var1 / DISPLAY 'Parameters' LEFT STYLE = [ FONTWEIGHT = BOLD ]; DEFINE var2 / DISPLAY 'Values' CENTER ; RUN ; Check these websites Beyond the Basics: Advanced PROC REPORT Tips and Tricks Creating a Plan for Your Reports and Avoiding Common Pitfalls in REPORT Procedure Coding Turn Your Plain Report into a Painted Report Using ODS Styles Specify the STYLE of a Cell Based on Other Cell's Value Link 1 2 3 4 5 6 7 8 9 10 PROC REPORT DATA = SAS - data - set NOWD ; COLUMN timeinterval date1 date2 ; DEFINE timeinterval / DISPLAY NOPRINT ; DEFINE date1 / DISPLAY ; DEFINE date2 / DISPLAY ; COMPUTE date2 ; IF timeinterval lt 0 and timeinterval ne . then call define ( _col_ , \" style \" , \" style={foreground=red font_weight=bold} \" ) ; ELSE call define ( _col_ , \" style \" , \" style={foreground=green font_weight=bold} \" ) ; ENDCOMP ; RUN ; DEFINE the variables involved in your conditional structure before the variable to which you want to apply the new format DEFINE your variables as DISPLAY NOPRINT if you want to use them for the conditional structure but you don't want them to appear in your table Tip Remember, PROC REPORT builds each row from left to right, so the value used as a condition to define the style must be to the left of the values whose style/format you want to change. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 data test ; input flag pt $ firstdate $ lastdate $ ; datalines ; 0 X1 XX - XX - XXXX XX - XX - XXXX 1 X2 XX - XX - XXXX XX - XX - XXXX 0 X3 XX - XX - XXXX XX - XX - XXXX 0 X4 XX - XX - XXXX XX - XX - XXXX 0 X5 XX - XX - XXXX XX - XX - XXXX 2 X6 XX - XX - XXXX XX - XX - XXXX ; run ; proc report data = test nowindows headline style ( header ) = { background = very light grey } missing split = ' * ' ; column ( \" First and Last Dates in the Study \" ( ' Patient ' pt ) ( ' First Study Date ' firstdate ) ( ' Last Study Date ' lastdate ) flag color1 color2 ) ; define pt / '' display order = internal descending ; define firstdate / '' display ; define lastdate / '' display ; define flag / display noprint ; define color1 / computed noprint ; define color2 / computed noprint ; compute color1 ; if flag eq 1 then call define ( ' firstdate ' , \" style \" , \" style={background=yellow} \" ) ; endcomp ; compute color2 ; if flag eq 2 then call define ( ' lastdate ' , \" style \" , \" style={background=yellow} \" ) ; endcomp ; run ; Control Output Table Width Link If the margins of the page are defined 1 options topmargin = . 98 in bottommargin = . 98 in leftmargin = 1 . 18 in rightmargin = . 98 in ; and you 1 2 3 4 5 6 7 8 proc report data = LEDDall2report_ago split = '*' nowd colwidth = 10 headline headskip missing spanrows style ( report )= { width=100% } ; column ( \"Dosis equivalente de levodopa diaria y n\u00famero de f\u00e1rmacos de agonistas dopamin\u00e9rgicos: descripci\u00f3n del cambio respecto a basal\" ind texto param grupo1 grupo2 grupo3 grupo4 pvalor flag flag2 ); define ind / order noprint ; define texto / '' style ( column )= [ width = 7 % ] ; define param / '' style ( column )= [ width = 5 % ] ; define grupo1 / \"Visita 1 * (N=59)\" style ( column )= [ width = 5 % ] ; (...) run ; Introducing Line Breaks Link To introduce line breaks in the title or labels you can use the SPLIT= option. If you want to introduce a line break on the variable's value you need to define a ODS ESCAPECHAR= and use the n on your string as in the following example: Example ``` DATA addy; INFILE DATALINES DLM='|'; INPUT name ~$15. address ~$15. city $ state $; DATALINES; Debby Jones|1234 Johnny St|Chicago|IL Joe Smith|2345 Bobby Dr|New York|NY Ron Lee|3456 Suzy Ln|Miami|FL ; RUN; ODS ESCAPECHAR='^'; PROC REPORT DATA=addy SPLIT='~'; COLUMN state city address name addblock; DEFINE state / DISPLAY NOPRINT ORDER=INTERNAL; DEFINE city / DISPLAY NOPRINT ORDER=INTERNAL; DEFINE address / DISPLAY NOPRINT ORDER=INTERNAL; DEFINE name / DISPLAY NOPRINT ORDER=INTERNAL; DEFINE addblock / COMPUTED 'Mailing~Address' FLOW WIDTH=30; 1 2 3 COMPUTE addblock / CHAR LENGTH = 40 ; addblock = CATX ( '^n' , name , address , catx ( ', ' , city , state )); ENDCOMP ; RUN; ``` You can also force line breaks in the resulting tables to better visualize the data. 1 2 3 4 5 6 7 8 9 10 11 12 13 PROC REPORT DATA = SAS - data - set NOWINDOWS HEADLINE STYLE ( HEADER ) = { BACKGROUND = VERY LIGHT GREY } MISSING SPLIT = ' * ' ; COLUMN ( \" Sample report \" var1 var2 var3 ) ; DEFINE var1 / ' Label 1 ' GROUP ORDER = INTERNAL ; DEFINE var2 / ' Label 2 ' DISPLAY ; DEFINE var3 / ' Label 3 ' DISPLAY ; * Introduce some line separations between var1 values ; BREAK BEFORE var1 / SUMMARIZE STYLE = [ BACKGROUND = VERY LIGHT GREY ] ; * Avoid repeated labels ; COMPUTE var2 ; IF MISSING ( _BREAK_ ) THEN var1 = ' ' ; ENDCOMP ; RUN ; Tip If the variable is numeric a . will apear in the break row. You need to create a format to assign ' ' to . . If you describe the new value of . as ' ' your numbers will be truncated. Working with ACROSS Link Simple example: 1 2 3 4 5 6 7 8 9 PROC REPORT DATA = _AUX3 NOWINDOWS HEADLINE STYLE ( HEADER ) = { BACKGROUND = VERY LIGHT GREY } MISSING SPLIT = ' * ' ; COLUMN ( \" &&VAR&I (&&UNIT&I) \" & TIMEVAR . ( ' STATISTICS ' _LABEL_ ) & STRATAVAR ., COL1 ) ; DEFINE _LABEL_ / '' GROUP ORDER = DATA ; DEFINE & STRATAVAR . / '' ACROSS NOZERO ORDER = INTERNAL ; /* NOZERO = SINCE ALL PRODUCT CATEGORIES WILL NOT BE REPRESENTED FOR EACH PRODUCT LINE IN THE TABLE */ DEFINE & TIMEVAR . / '' F =& TIMEFMT . GROUP ORDER = INTERNAL ; DEFINE COL1 / '' GROUP ; RUN ; Complex example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 proc report data = _data2report nowindows headline style ( header ) = { background = very light grey } missing split = ' * ' ; column ( \" Lab tests (Hematology): normal, high abnormal and low abnormal results n(%) \" lbtest trtgrpnum ( ' Value at * screening ' clinsigSCR ) ( ' Day 6 ' clinsig60 , npctn60 _dummy ) ( ' Early Termination Day ' clinsigET , npctnET _dummy )) ; define lbtest / '' group order = internal ; define trtgrpnum / '' group order = internal ; define clinsigSCR / '' group order = internal ; define clinsig60 / '' across nozero order = internal ; define clinsigET / '' across nozero order = internal ; * nozdero = since all product categories will not be represented for each product line in the table ; define npctn60 / group '' ; define npctnET / group '' ; define _dummy / computed noprint ; /* This variable is created to avoid an error message */ compute after / style = [ just = L foreground = black FONT_SIZE = 9 pt ] ; line \" Table footer line 1 \" ; line \" Table footer line 2 \" ; endcomp ; * Introduce some line separations between arms of treatment ; break after trtgrpnum / skip ; * Introduce some line separations between tests ; break before lbtest / summarize style = [ background = very light grey FONT_WEIGHT = BOLD ] ; * Avoid repeated labels ; compute npctn60 ; if missing ( _break_ ) then lbtest = ' ' ; endcomp ; run ; Check these websites Sailing Over the ACROSS Hurdle in PROC REPORT Defining your own variables Link Check these websites Compute Block Basics (Part I) Compute Block Basics (Part II) There are two basic types of compute blocks; those that are associated with a location (the option BEFORE or AFTER follows the COMPUTE keyword), and those associated only with a report item . While the structure and execution of these two types of compute blocks is similar, how they are used and the timing of their execution can be quite different. The compute block starts with the COMPUTE statement and terminates with the ENDCOMP statement. Usually the compute block is placed in the REPORT step after the DEFINE statements. The syntax of the compute block looks something like: 1 2 3 compute <location> <report_item> </ options> ; one or more SAS language elements endcomp; The components of the COMPUTE statement include: location ( BEFORE | AFTER ): Specifies when the compute block is to execute and ultimately what is to be done with the result of the compute block. When a location is specified without also specifying a report_item, the location will be at the start ( BEFORE ) or at the end ( AFTER ) of the report. report_item : When the result of the compute block is associated with a variable or report item, its name is supplied here. This report_item variable can be any variable on the COLUMN statement. When report_item is a variable that either groups or orders rows (usage of GROUP or ORDER ) you may also use BEFORE and AFTER to apply the result at the start or end of each group. options : Several options are available that can be used to determine the appearance and location of the result of the compute block. SAS language elements : Any number of SAS language elements can be used within the compute block. These include the use of executable statements, logical processing ( IF-THEN / ELSE ), and most of the functions available in the DATA step. The compute block can be placed anywhere within the REPORT step, however generally compute blocks are grouped following the DEFINE statements. Using the LINE Statement Link It can be used to insert a blank line 1 2 3 COMPUTE AFTER variable1 ; LINE ' ' ; ENDCOMP ; It can be used to add lines of text 1 2 3 4 COMPUTE AFTER ; LINE @20 ' Some custom text ' ; LINE @20 ' More custom text ' ; ENDCOMP ; Note In these LINE statements the @ is used, as it is in the DATA step PUT statement, to designate the column number. If a specific column is not specified with the @ , and no justification options are specified, text will be centered. When writing to ODS destinations other than LISTING , proportional fonts may make exact placement of values difficult, and may require you to use a trial-and-error approach, and to make things more interesting some destinations ignore the @ altogether. It can be used to write formatted values 1 2 3 COMPUTE BEFORE variable1 ; line @3 variable1 $ formatname .; ENDCOMP ; Note The format could have also been used in the DEFINE statement, however in this case we wanted to show the unformatted value as well as the formatted group header. Creating and Modifying Columns Link 1 2 3 4 5 6 DEFINE obs / COMPUTED ; COMPUTE obs ; dsobs + 1 ; obs = dsobs ; ENDCOMPUTE ;","title":"PROC REPORT"},{"location":"sas-outputs/report/#examples","text":"","title":"Examples"},{"location":"sas-outputs/report/#how-to-write-a-headerfooter-in-your-tables","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 ODS ESCAPECHAR = '^' ; PROC REPORT DATA = sashelp . cars ; WHERE Make = 'Jaguar' ; COLUMN ( '1) Label 1' model Invoice ) ( '2) Label 2' Horsepower Weight Length ); COMPUTE BEFORE _PAGE_ / STYLE = HEADER { JUST = L FONTWEIGHT = BOLD COLOR = PURPLE } ; LINE 'Test of custom header' ; ENDCOMP ; COMPUTE AFTER / STYLE = { TEXTDECORATION = UNDERLINE JUST = C COLOR = RED } ; LINE 'Test of a custom footer' ; LINE '^S={color=green} Test of a custom footer with a different style' ; ENDCOMP ; RUN ;","title":"How to Write a Header/Footer in your Tables"},{"location":"sas-outputs/report/#specify-the-style-of-your-global-header","text":"1 2 3 4 5 PROC REPORT DATA = SAS - data - set HEADSKIP HEADLINE NOWINDOWS STYLE ( header ) = { ASIS = on BACKGROUND = very light grey FONTWEIGHT = BOLD } ; COLUMN ( \"Style of this global header\" var1 var2 ); DEFINE var1 / DISPLAY 'Parameters' LEFT STYLE = [ FONTWEIGHT = BOLD ]; DEFINE var2 / DISPLAY 'Values' CENTER ; RUN ; Check these websites Beyond the Basics: Advanced PROC REPORT Tips and Tricks Creating a Plan for Your Reports and Avoiding Common Pitfalls in REPORT Procedure Coding Turn Your Plain Report into a Painted Report Using ODS Styles","title":"Specify the STYLE of Your Global Header"},{"location":"sas-outputs/report/#specify-the-style-of-a-cell-based-on-other-cells-value","text":"1 2 3 4 5 6 7 8 9 10 PROC REPORT DATA = SAS - data - set NOWD ; COLUMN timeinterval date1 date2 ; DEFINE timeinterval / DISPLAY NOPRINT ; DEFINE date1 / DISPLAY ; DEFINE date2 / DISPLAY ; COMPUTE date2 ; IF timeinterval lt 0 and timeinterval ne . then call define ( _col_ , \" style \" , \" style={foreground=red font_weight=bold} \" ) ; ELSE call define ( _col_ , \" style \" , \" style={foreground=green font_weight=bold} \" ) ; ENDCOMP ; RUN ; DEFINE the variables involved in your conditional structure before the variable to which you want to apply the new format DEFINE your variables as DISPLAY NOPRINT if you want to use them for the conditional structure but you don't want them to appear in your table Tip Remember, PROC REPORT builds each row from left to right, so the value used as a condition to define the style must be to the left of the values whose style/format you want to change. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 data test ; input flag pt $ firstdate $ lastdate $ ; datalines ; 0 X1 XX - XX - XXXX XX - XX - XXXX 1 X2 XX - XX - XXXX XX - XX - XXXX 0 X3 XX - XX - XXXX XX - XX - XXXX 0 X4 XX - XX - XXXX XX - XX - XXXX 0 X5 XX - XX - XXXX XX - XX - XXXX 2 X6 XX - XX - XXXX XX - XX - XXXX ; run ; proc report data = test nowindows headline style ( header ) = { background = very light grey } missing split = ' * ' ; column ( \" First and Last Dates in the Study \" ( ' Patient ' pt ) ( ' First Study Date ' firstdate ) ( ' Last Study Date ' lastdate ) flag color1 color2 ) ; define pt / '' display order = internal descending ; define firstdate / '' display ; define lastdate / '' display ; define flag / display noprint ; define color1 / computed noprint ; define color2 / computed noprint ; compute color1 ; if flag eq 1 then call define ( ' firstdate ' , \" style \" , \" style={background=yellow} \" ) ; endcomp ; compute color2 ; if flag eq 2 then call define ( ' lastdate ' , \" style \" , \" style={background=yellow} \" ) ; endcomp ; run ;","title":"Specify the STYLE of a Cell Based on Other Cell's Value"},{"location":"sas-outputs/report/#control-output-table-width","text":"If the margins of the page are defined 1 options topmargin = . 98 in bottommargin = . 98 in leftmargin = 1 . 18 in rightmargin = . 98 in ; and you 1 2 3 4 5 6 7 8 proc report data = LEDDall2report_ago split = '*' nowd colwidth = 10 headline headskip missing spanrows style ( report )= { width=100% } ; column ( \"Dosis equivalente de levodopa diaria y n\u00famero de f\u00e1rmacos de agonistas dopamin\u00e9rgicos: descripci\u00f3n del cambio respecto a basal\" ind texto param grupo1 grupo2 grupo3 grupo4 pvalor flag flag2 ); define ind / order noprint ; define texto / '' style ( column )= [ width = 7 % ] ; define param / '' style ( column )= [ width = 5 % ] ; define grupo1 / \"Visita 1 * (N=59)\" style ( column )= [ width = 5 % ] ; (...) run ;","title":"Control Output Table Width"},{"location":"sas-outputs/report/#introducing-line-breaks","text":"To introduce line breaks in the title or labels you can use the SPLIT= option. If you want to introduce a line break on the variable's value you need to define a ODS ESCAPECHAR= and use the n on your string as in the following example: Example ``` DATA addy; INFILE DATALINES DLM='|'; INPUT name ~$15. address ~$15. city $ state $; DATALINES; Debby Jones|1234 Johnny St|Chicago|IL Joe Smith|2345 Bobby Dr|New York|NY Ron Lee|3456 Suzy Ln|Miami|FL ; RUN; ODS ESCAPECHAR='^'; PROC REPORT DATA=addy SPLIT='~'; COLUMN state city address name addblock; DEFINE state / DISPLAY NOPRINT ORDER=INTERNAL; DEFINE city / DISPLAY NOPRINT ORDER=INTERNAL; DEFINE address / DISPLAY NOPRINT ORDER=INTERNAL; DEFINE name / DISPLAY NOPRINT ORDER=INTERNAL; DEFINE addblock / COMPUTED 'Mailing~Address' FLOW WIDTH=30; 1 2 3 COMPUTE addblock / CHAR LENGTH = 40 ; addblock = CATX ( '^n' , name , address , catx ( ', ' , city , state )); ENDCOMP ; RUN; ``` You can also force line breaks in the resulting tables to better visualize the data. 1 2 3 4 5 6 7 8 9 10 11 12 13 PROC REPORT DATA = SAS - data - set NOWINDOWS HEADLINE STYLE ( HEADER ) = { BACKGROUND = VERY LIGHT GREY } MISSING SPLIT = ' * ' ; COLUMN ( \" Sample report \" var1 var2 var3 ) ; DEFINE var1 / ' Label 1 ' GROUP ORDER = INTERNAL ; DEFINE var2 / ' Label 2 ' DISPLAY ; DEFINE var3 / ' Label 3 ' DISPLAY ; * Introduce some line separations between var1 values ; BREAK BEFORE var1 / SUMMARIZE STYLE = [ BACKGROUND = VERY LIGHT GREY ] ; * Avoid repeated labels ; COMPUTE var2 ; IF MISSING ( _BREAK_ ) THEN var1 = ' ' ; ENDCOMP ; RUN ; Tip If the variable is numeric a . will apear in the break row. You need to create a format to assign ' ' to . . If you describe the new value of . as ' ' your numbers will be truncated.","title":"Introducing Line Breaks"},{"location":"sas-outputs/report/#working-with-across","text":"Simple example: 1 2 3 4 5 6 7 8 9 PROC REPORT DATA = _AUX3 NOWINDOWS HEADLINE STYLE ( HEADER ) = { BACKGROUND = VERY LIGHT GREY } MISSING SPLIT = ' * ' ; COLUMN ( \" &&VAR&I (&&UNIT&I) \" & TIMEVAR . ( ' STATISTICS ' _LABEL_ ) & STRATAVAR ., COL1 ) ; DEFINE _LABEL_ / '' GROUP ORDER = DATA ; DEFINE & STRATAVAR . / '' ACROSS NOZERO ORDER = INTERNAL ; /* NOZERO = SINCE ALL PRODUCT CATEGORIES WILL NOT BE REPRESENTED FOR EACH PRODUCT LINE IN THE TABLE */ DEFINE & TIMEVAR . / '' F =& TIMEFMT . GROUP ORDER = INTERNAL ; DEFINE COL1 / '' GROUP ; RUN ; Complex example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 proc report data = _data2report nowindows headline style ( header ) = { background = very light grey } missing split = ' * ' ; column ( \" Lab tests (Hematology): normal, high abnormal and low abnormal results n(%) \" lbtest trtgrpnum ( ' Value at * screening ' clinsigSCR ) ( ' Day 6 ' clinsig60 , npctn60 _dummy ) ( ' Early Termination Day ' clinsigET , npctnET _dummy )) ; define lbtest / '' group order = internal ; define trtgrpnum / '' group order = internal ; define clinsigSCR / '' group order = internal ; define clinsig60 / '' across nozero order = internal ; define clinsigET / '' across nozero order = internal ; * nozdero = since all product categories will not be represented for each product line in the table ; define npctn60 / group '' ; define npctnET / group '' ; define _dummy / computed noprint ; /* This variable is created to avoid an error message */ compute after / style = [ just = L foreground = black FONT_SIZE = 9 pt ] ; line \" Table footer line 1 \" ; line \" Table footer line 2 \" ; endcomp ; * Introduce some line separations between arms of treatment ; break after trtgrpnum / skip ; * Introduce some line separations between tests ; break before lbtest / summarize style = [ background = very light grey FONT_WEIGHT = BOLD ] ; * Avoid repeated labels ; compute npctn60 ; if missing ( _break_ ) then lbtest = ' ' ; endcomp ; run ; Check these websites Sailing Over the ACROSS Hurdle in PROC REPORT","title":"Working with ACROSS"},{"location":"sas-outputs/report/#defining-your-own-variables","text":"Check these websites Compute Block Basics (Part I) Compute Block Basics (Part II) There are two basic types of compute blocks; those that are associated with a location (the option BEFORE or AFTER follows the COMPUTE keyword), and those associated only with a report item . While the structure and execution of these two types of compute blocks is similar, how they are used and the timing of their execution can be quite different. The compute block starts with the COMPUTE statement and terminates with the ENDCOMP statement. Usually the compute block is placed in the REPORT step after the DEFINE statements. The syntax of the compute block looks something like: 1 2 3 compute <location> <report_item> </ options> ; one or more SAS language elements endcomp; The components of the COMPUTE statement include: location ( BEFORE | AFTER ): Specifies when the compute block is to execute and ultimately what is to be done with the result of the compute block. When a location is specified without also specifying a report_item, the location will be at the start ( BEFORE ) or at the end ( AFTER ) of the report. report_item : When the result of the compute block is associated with a variable or report item, its name is supplied here. This report_item variable can be any variable on the COLUMN statement. When report_item is a variable that either groups or orders rows (usage of GROUP or ORDER ) you may also use BEFORE and AFTER to apply the result at the start or end of each group. options : Several options are available that can be used to determine the appearance and location of the result of the compute block. SAS language elements : Any number of SAS language elements can be used within the compute block. These include the use of executable statements, logical processing ( IF-THEN / ELSE ), and most of the functions available in the DATA step. The compute block can be placed anywhere within the REPORT step, however generally compute blocks are grouped following the DEFINE statements.","title":"Defining your own variables"},{"location":"sas-outputs/report/#using-the-line-statement","text":"It can be used to insert a blank line 1 2 3 COMPUTE AFTER variable1 ; LINE ' ' ; ENDCOMP ; It can be used to add lines of text 1 2 3 4 COMPUTE AFTER ; LINE @20 ' Some custom text ' ; LINE @20 ' More custom text ' ; ENDCOMP ; Note In these LINE statements the @ is used, as it is in the DATA step PUT statement, to designate the column number. If a specific column is not specified with the @ , and no justification options are specified, text will be centered. When writing to ODS destinations other than LISTING , proportional fonts may make exact placement of values difficult, and may require you to use a trial-and-error approach, and to make things more interesting some destinations ignore the @ altogether. It can be used to write formatted values 1 2 3 COMPUTE BEFORE variable1 ; line @3 variable1 $ formatname .; ENDCOMP ; Note The format could have also been used in the DEFINE statement, however in this case we wanted to show the unformatted value as well as the formatted group header.","title":"Using the LINE Statement"},{"location":"sas-outputs/report/#creating-and-modifying-columns","text":"1 2 3 4 5 6 DEFINE obs / COMPUTED ; COMPUTE obs ; dsobs + 1 ; obs = dsobs ; ENDCOMPUTE ;","title":"Creating and Modifying Columns"},{"location":"sas-outputs/tabulate/","text":"PROC TABULATE is a procedure that displays descriptive statistics in tabular format. It computes many statistics that other procedures compute, such as MEANS , FREQ , and REPORT and displays these statistics in a table format. TABULATE will produce tables in up to three dimensions and allows, within each dimension, multiple variables to be reported one after another hierarchically. There are also some very nice mechanisms that can be used to label and format the results. 1 2 3 4 5 6 PROC TABULATE <options> ; CLASS variables </ options> ; VAR variables </ options> ; TABLE <page> , <row> , column </ options> ; ... other statements...; RUN; VAR is used to list the variables you intend to use to create summary statistics on. They must be numeric . CLASS variables allow you to get statistics by category. You will get one column/row for each value of the classification variable. You can also specify the universal CLASS variable ALL which allows you to get totals . They can be either numeric or character and you can only request counts and percents as statistics. This is almost like using a BY statement within the TABLE . TABLE consists of up to three dimension expressions and the table options. You can have multiple table statements in one PROC TABULATE . This will generate one table for each statement. A comma specifies to add a new dimension . The order of the dimensions is page, row and column. If you only specify one dimension, then it is assumed to be column. If two are specified, row, then column. The asterisk is used to produce a cross tabulation of one variable with another (within the same dimension however, different from PROC FREQ ). A blank is used to represent concatenation (i.e. place this output element after the preceding variable listed). Parenthesis will group elements and associate an operator with each element in the group. Angle brackets specify a denominator definition for use in percentage calculations (e.g. pctn<variable> ). Check these websites PROC TABULATE and the Neat Things You Can Do With It VARDEF=divisor specifies the divisor to be used in the calculation of the variances. If divisor is DF (default), the degrees of freedom (N-1) are used as the divisor. ORDER= specifies the order of appearance in the table of the CLASS variable levels FORMATTED : ordered by the formatted values DATA : the order that the observations are read from the data set FREQ : order the values so the one that occurs most frequently in the data set appears first INTERNAL : ordered by the SORT procedure (defaults) /CONDENSE prints multiple logical pages on a single physical page /PRINTMISS species that row and column headings are the same for all logical pages of the table /ROW = spacing specifies whether all title elements in a row crossing are allotted space even when they are blank. When ROW=CONSTANT (or CONST ), the default, all row title elements have space allotted to them; when ROW=FLOAT , the row title space is divided equally among the nonblank title elements in the crossing Statistics that Are Available in PROC TABULATE Link If you do not provide a statistic name, the default statistic produced will be N for the CLASS variables and SUM for the VAR variables. Use the following keywords to request statistics in the TABLE statement or to specify statistic keywords in the KEYWORD or KEYLABEL statement. Tip If a variable name (class or analysis) and a statistic name are the same, then enclose the statistic name in single quotation marks (for example, 'MAX' ). Descriptive statistic keywords: Percentages: PCTN , COLPCTN , ROWPCTN , REPPCTN , PAGEPCTN Additions: SUM , SUMWGT , PCTSUM , COLPCTSUM , ROWPCTSUM , REPPCTSUM , PAGEPCTSUM Elements: N , NMISS Basic statistics: MEAN , STDDEV | STD , STDERR , MIN , MAX , RANGE , MODE , LCLM , UCLM , KURTOSIS | KURT , SKEWNESS | SKEW Quantile statistics: P1 , P5 , P10 , Q1 | P25 , MEDIAN | P50 , Q3 | P75 , P90 , P95 , P99 , QRANGE Hypothesis testing: PROBT | PRT , T Others: CSS , CV , USS , VAR To compute standard error of the mean ( STDERR ) or Student's t-test, you must use the default value of the VARDEF= option, which is DF . The VARDEF= option is specified in the PROC TABULATE statement. To compute weighted quantiles, you must use QMETHOD=OS in the PROC TABULATE statement. Use both LCLM and UCLM to compute a two-sided confidence limit for the mean. Use only LCLM or UCLM to compute a one-sided confidence limit. Use the ALPHA= option in the PROC TABULATE statement to specify a confidence level. Single Dimensional Table Link 1 2 3 4 5 PROC TABULATE DATA = sashelp . class ; CLASS Sex ; VAR Height Weight ; TABLE Height * ( N MEAN ) Height * MEAN * Sex Weight * MEAN * Sex ; RUN ; Two Dimensional Table Link You can get very different table structures by changing where the statistic definitions are placed. They can be attached to either the VAR or the CLASS variable, but the numbers in the cells will always be calculated using the VAR variable(s). The statistic specification can be attached to the columns , 1 2 3 4 5 PROC TABULATE data = sashelp . class ; CLASS Sex ; VAR Height Weight ; TABLE Sex , Height * ( N MEAN MAX ) Weight * ( N MEAN MAX ) ; RUN ; or they can be attached to the rows . 1 2 3 4 5 PROC TABULATE data = sashelp . class ; CLASS Sex ; VAR Height Weight ; TABLE Sex * ( N MEAN MAX ), Height Weight ; RUN ; You can specify multiple classification variables . They can be used in any of the dimensions and can be nested. When you have multiple CLASS variables, it is recommended to use the option MISSING to keep the observations that have any missing values and consider them as valid levels for the CLASS variable(s) instead of dropping those observations from all the tables. 1 2 3 4 5 PROC TABULATE DATA = sashelp . cars ; CLASS DriveTrain Origin Type / MISSING ; VAR Weight Length ; TABLE Origin * Type , DriveTrain * Weight * MEAN DriveTrain * Length * MEAN ; RUN ; In order to get marginal statistics in your table, you use the ALL keyword. You can use the keyword in multiple places and, depending on where you put the keyword, there will be different subtotals produced. 1 2 3 4 PROC TABULATE DATA = sashelp . cars ; CLASS DriveTrain Origin Type ; TABLE ( Origin All = 'Total' ) * ( DriveTrain ALL = 'Subtotal DriveTrain' ), ( Type ALL = 'Subtotal Type' * N ); RUN ; Three Dimensional Table Link Three dimensional tables have a nice way to fill in the upper left area. Instead of the label of the page dimension appearing above the table, you can use the BOX=_page_ option to place that label inside the big white box. 1 2 3 4 5 PROC TABULATE data = sashelp . cars ; CLASS DriveTrain Origin Type ; VAR Weight Length ; TABLE Origin = 'Made in' , Type = 'Category' , DriveTrain * Weight * MEAN / BOX = _PAGE_ ; RUN ; Formatting tables Link There are two ways to add labels for your variables : Add the text after the variable name: variable =\u2018label\u2019 . This will work for both variables and statistics. Add a LABEL statement for variables and/or a KEYLABEL statement for statistics to your code: 1 2 LABEL var = \u2018 label \u2019 ; KEYLABEL stat = \u2018 label \u2019 ; In order to hide variable or statistic labels, you leave the label specification blank ( variable =\u2018 \u2019 ). To change the font color of each variable you can define their style as in the following example: 1 2 3 4 5 6 7 8 PROC TABULATE DATA = sashelp . class ; CLASS Age Sex ; TABLES Age , Sex = 'Gender' * ( N * { STYLE = [ COLOR = Black ] } PCTN = 'Percent' * { STYLE = [ COLOR = Green ] } ROWPCTN = 'Row Percent' * { STYLE = [ COLOR = Purple ] } COLPCTN = 'Column Percent' * { STYLE = [ COLOR = Red ] } ); RUN ; You can also specify formats for numbers in the cells of the table using the variable-or-statistic*F=fmt. expression. The CLASSLEV statement is used to assign some style attributes to the variable values only (not to the column header) The NOSEPS option removes the horizontal dividers between the row values from your table 1 PROC TABULATE DATA = SAS - data - set NOSEPS ; INDENT= is used for subsetting row subheaders RTS= specifies how wide you want the row header field to be Use the BOX= option to fill in the big white box in the upper left 1 TABLE ( ... ) / BOX = { LABEL = ' Custom label for upper left box ' } INDENT = 3 RTS = 12 ; Depending on where you place the style options, many different results can be achieved. If you place the style options on the PROC TABULATE statement, for example, you will affect all the table cells. Note that for the CLASS , CLASSLEV , VAR , and KEYWORD statements, the style options can also be specified in the dimension expression in the Table statement. See below for a list of some of the different places where you can put the style options and what portion of the table they will affect. 1 2 3 4 5 6 7 8 PROC TABULATE DATA = SAS - data - set F = 10 . 2 S = [ custom style attributes ]; CLASS variable1 / S = [ custom style attributes ]; CLASSLEV variable1 / S = [ custom style attributes ]; VAR variable2 ; TABLE variable1 = '' all = { label = 'Total' S = [ custom style attributes ], MEAN = { S = [ custom style attributes ] } * variable2 / BOX = { LABEL = 'custom label' S = [ custom style attributes ] } ; RUN ; Possible style attributes Background color: BACKGROUND=yellow Foreground color: FOREGROUND=black Font color: COLOR=red Change font characteristics: FONTt_WEIGHT | FONT_FACE | FONT_SIZE Vertical justification: VJUST=B|C|T Horizontal justification: JUST=R|C|L Specify thickness of borders: BORDERWIDTH= Change size of table cells: CELLWIDTH=200 | CELLHEIGHT=50 Specify vertical and horizontal rule dividers: RULES=none (removes all ines from the table) Specify white space around cell: CELLSPACING=0 Specify thickness of spacing around cell: CELLPADDING=10 Specify width of table: OUTPUTWIDTH= Dealing with Missing Values Link Force Missing Rows and Columns with CLASSDATA Link When creating the same report over time, it is important that the format stays the same to ensure consistency. With the TABULATE procedure, levels of a class variable, which do not have any observations, will not appear in the report . To create consistent reports, a solution is needed that will ensure a standard report format over time. In order to account for changes that occur over time, all levels of class variables will need to be included in the standard report format. Creating a SAS data set with all levels of every class variable, along with declaring the CLASSDATA option in PROC TABULATE , will solve this problem. This paper shows how to set up the SAS data set to be used in conjunction with the CLASSDATA option, and how to avoid common errors that will cause PROC TABULATE to error out. The result is a report format that will not change over time when the data changes over time . The basic syntax for of a PROC TABULATE that uses the CLASSDATA option is as follows: 1 2 3 4 5 PROC TABULATE DATA=SAS-data-set CLASSDATA=SAS-data-set <options> ; CLASS variables </options> ; VAR variables </options> ; TABLE page, row, column </options> ; RUN; Read this article for more information. How to Force Missing Values to Appear Link Provided that the specific category is present (non-missing) at least in one of the variables in the TABLE statement, you can force the missing values to appear. 1 2 3 4 PROC TABULATE DATA = SAS - data - set ORDER = FREQ OUT = Output - SAS - data - set MISSING ; CLASS var1 var2 crossvar ; TABLE var1 var2 , crossvar * ( N COLPCTN ) / PRINTMISS MISSTEXT = '0' ; RUN ; Examples Link 1 2 3 4 5 6 7 PROC TABULATE DATA = SAS - data - set ORDER = FREQ ; VAR var1 var2 ; CLASS AEencoding ; CLASS grade / ORDER = FORMATTED ; CLASS treatment / ORDER = FORMATTED ; TABLE AEencoding = '' , treatment = 'Treatment/Grade' * grade = '' * ( N = 'N' var1 = '%' * SUM = '' ) ALL = 'Total (N=# cases)' * ( N = 'N' var2 = '%' * SUM = '' ) / BOX = \"Preferred MeDDRA Term\" ; RUN ;","title":"PROC TABULATE"},{"location":"sas-outputs/tabulate/#statistics-that-are-available-in-proc-tabulate","text":"If you do not provide a statistic name, the default statistic produced will be N for the CLASS variables and SUM for the VAR variables. Use the following keywords to request statistics in the TABLE statement or to specify statistic keywords in the KEYWORD or KEYLABEL statement. Tip If a variable name (class or analysis) and a statistic name are the same, then enclose the statistic name in single quotation marks (for example, 'MAX' ). Descriptive statistic keywords: Percentages: PCTN , COLPCTN , ROWPCTN , REPPCTN , PAGEPCTN Additions: SUM , SUMWGT , PCTSUM , COLPCTSUM , ROWPCTSUM , REPPCTSUM , PAGEPCTSUM Elements: N , NMISS Basic statistics: MEAN , STDDEV | STD , STDERR , MIN , MAX , RANGE , MODE , LCLM , UCLM , KURTOSIS | KURT , SKEWNESS | SKEW Quantile statistics: P1 , P5 , P10 , Q1 | P25 , MEDIAN | P50 , Q3 | P75 , P90 , P95 , P99 , QRANGE Hypothesis testing: PROBT | PRT , T Others: CSS , CV , USS , VAR To compute standard error of the mean ( STDERR ) or Student's t-test, you must use the default value of the VARDEF= option, which is DF . The VARDEF= option is specified in the PROC TABULATE statement. To compute weighted quantiles, you must use QMETHOD=OS in the PROC TABULATE statement. Use both LCLM and UCLM to compute a two-sided confidence limit for the mean. Use only LCLM or UCLM to compute a one-sided confidence limit. Use the ALPHA= option in the PROC TABULATE statement to specify a confidence level.","title":"Statistics that Are Available in PROC TABULATE"},{"location":"sas-outputs/tabulate/#single-dimensional-table","text":"1 2 3 4 5 PROC TABULATE DATA = sashelp . class ; CLASS Sex ; VAR Height Weight ; TABLE Height * ( N MEAN ) Height * MEAN * Sex Weight * MEAN * Sex ; RUN ;","title":"Single Dimensional Table"},{"location":"sas-outputs/tabulate/#two-dimensional-table","text":"You can get very different table structures by changing where the statistic definitions are placed. They can be attached to either the VAR or the CLASS variable, but the numbers in the cells will always be calculated using the VAR variable(s). The statistic specification can be attached to the columns , 1 2 3 4 5 PROC TABULATE data = sashelp . class ; CLASS Sex ; VAR Height Weight ; TABLE Sex , Height * ( N MEAN MAX ) Weight * ( N MEAN MAX ) ; RUN ; or they can be attached to the rows . 1 2 3 4 5 PROC TABULATE data = sashelp . class ; CLASS Sex ; VAR Height Weight ; TABLE Sex * ( N MEAN MAX ), Height Weight ; RUN ; You can specify multiple classification variables . They can be used in any of the dimensions and can be nested. When you have multiple CLASS variables, it is recommended to use the option MISSING to keep the observations that have any missing values and consider them as valid levels for the CLASS variable(s) instead of dropping those observations from all the tables. 1 2 3 4 5 PROC TABULATE DATA = sashelp . cars ; CLASS DriveTrain Origin Type / MISSING ; VAR Weight Length ; TABLE Origin * Type , DriveTrain * Weight * MEAN DriveTrain * Length * MEAN ; RUN ; In order to get marginal statistics in your table, you use the ALL keyword. You can use the keyword in multiple places and, depending on where you put the keyword, there will be different subtotals produced. 1 2 3 4 PROC TABULATE DATA = sashelp . cars ; CLASS DriveTrain Origin Type ; TABLE ( Origin All = 'Total' ) * ( DriveTrain ALL = 'Subtotal DriveTrain' ), ( Type ALL = 'Subtotal Type' * N ); RUN ;","title":"Two Dimensional Table"},{"location":"sas-outputs/tabulate/#three-dimensional-table","text":"Three dimensional tables have a nice way to fill in the upper left area. Instead of the label of the page dimension appearing above the table, you can use the BOX=_page_ option to place that label inside the big white box. 1 2 3 4 5 PROC TABULATE data = sashelp . cars ; CLASS DriveTrain Origin Type ; VAR Weight Length ; TABLE Origin = 'Made in' , Type = 'Category' , DriveTrain * Weight * MEAN / BOX = _PAGE_ ; RUN ;","title":"Three Dimensional Table"},{"location":"sas-outputs/tabulate/#formatting-tables","text":"There are two ways to add labels for your variables : Add the text after the variable name: variable =\u2018label\u2019 . This will work for both variables and statistics. Add a LABEL statement for variables and/or a KEYLABEL statement for statistics to your code: 1 2 LABEL var = \u2018 label \u2019 ; KEYLABEL stat = \u2018 label \u2019 ; In order to hide variable or statistic labels, you leave the label specification blank ( variable =\u2018 \u2019 ). To change the font color of each variable you can define their style as in the following example: 1 2 3 4 5 6 7 8 PROC TABULATE DATA = sashelp . class ; CLASS Age Sex ; TABLES Age , Sex = 'Gender' * ( N * { STYLE = [ COLOR = Black ] } PCTN = 'Percent' * { STYLE = [ COLOR = Green ] } ROWPCTN = 'Row Percent' * { STYLE = [ COLOR = Purple ] } COLPCTN = 'Column Percent' * { STYLE = [ COLOR = Red ] } ); RUN ; You can also specify formats for numbers in the cells of the table using the variable-or-statistic*F=fmt. expression. The CLASSLEV statement is used to assign some style attributes to the variable values only (not to the column header) The NOSEPS option removes the horizontal dividers between the row values from your table 1 PROC TABULATE DATA = SAS - data - set NOSEPS ; INDENT= is used for subsetting row subheaders RTS= specifies how wide you want the row header field to be Use the BOX= option to fill in the big white box in the upper left 1 TABLE ( ... ) / BOX = { LABEL = ' Custom label for upper left box ' } INDENT = 3 RTS = 12 ; Depending on where you place the style options, many different results can be achieved. If you place the style options on the PROC TABULATE statement, for example, you will affect all the table cells. Note that for the CLASS , CLASSLEV , VAR , and KEYWORD statements, the style options can also be specified in the dimension expression in the Table statement. See below for a list of some of the different places where you can put the style options and what portion of the table they will affect. 1 2 3 4 5 6 7 8 PROC TABULATE DATA = SAS - data - set F = 10 . 2 S = [ custom style attributes ]; CLASS variable1 / S = [ custom style attributes ]; CLASSLEV variable1 / S = [ custom style attributes ]; VAR variable2 ; TABLE variable1 = '' all = { label = 'Total' S = [ custom style attributes ], MEAN = { S = [ custom style attributes ] } * variable2 / BOX = { LABEL = 'custom label' S = [ custom style attributes ] } ; RUN ; Possible style attributes Background color: BACKGROUND=yellow Foreground color: FOREGROUND=black Font color: COLOR=red Change font characteristics: FONTt_WEIGHT | FONT_FACE | FONT_SIZE Vertical justification: VJUST=B|C|T Horizontal justification: JUST=R|C|L Specify thickness of borders: BORDERWIDTH= Change size of table cells: CELLWIDTH=200 | CELLHEIGHT=50 Specify vertical and horizontal rule dividers: RULES=none (removes all ines from the table) Specify white space around cell: CELLSPACING=0 Specify thickness of spacing around cell: CELLPADDING=10 Specify width of table: OUTPUTWIDTH=","title":"Formatting tables"},{"location":"sas-outputs/tabulate/#dealing-with-missing-values","text":"","title":"Dealing with Missing Values"},{"location":"sas-outputs/tabulate/#force-missing-rows-and-columns-with-classdata","text":"When creating the same report over time, it is important that the format stays the same to ensure consistency. With the TABULATE procedure, levels of a class variable, which do not have any observations, will not appear in the report . To create consistent reports, a solution is needed that will ensure a standard report format over time. In order to account for changes that occur over time, all levels of class variables will need to be included in the standard report format. Creating a SAS data set with all levels of every class variable, along with declaring the CLASSDATA option in PROC TABULATE , will solve this problem. This paper shows how to set up the SAS data set to be used in conjunction with the CLASSDATA option, and how to avoid common errors that will cause PROC TABULATE to error out. The result is a report format that will not change over time when the data changes over time . The basic syntax for of a PROC TABULATE that uses the CLASSDATA option is as follows: 1 2 3 4 5 PROC TABULATE DATA=SAS-data-set CLASSDATA=SAS-data-set <options> ; CLASS variables </options> ; VAR variables </options> ; TABLE page, row, column </options> ; RUN; Read this article for more information.","title":"Force Missing Rows and Columns with CLASSDATA"},{"location":"sas-outputs/tabulate/#how-to-force-missing-values-to-appear","text":"Provided that the specific category is present (non-missing) at least in one of the variables in the TABLE statement, you can force the missing values to appear. 1 2 3 4 PROC TABULATE DATA = SAS - data - set ORDER = FREQ OUT = Output - SAS - data - set MISSING ; CLASS var1 var2 crossvar ; TABLE var1 var2 , crossvar * ( N COLPCTN ) / PRINTMISS MISSTEXT = '0' ; RUN ;","title":"How to Force Missing Values to Appear"},{"location":"sas-outputs/tabulate/#examples","text":"1 2 3 4 5 6 7 PROC TABULATE DATA = SAS - data - set ORDER = FREQ ; VAR var1 var2 ; CLASS AEencoding ; CLASS grade / ORDER = FORMATTED ; CLASS treatment / ORDER = FORMATTED ; TABLE AEencoding = '' , treatment = 'Treatment/Grade' * grade = '' * ( N = 'N' var1 = '%' * SUM = '' ) ALL = 'Total (N=# cases)' * ( N = 'N' var2 = '%' * SUM = '' ) / BOX = \"Preferred MeDDRA Term\" ; RUN ;","title":"Examples"},{"location":"sas-outputs/template/","text":"Check these websites Here you can find some notes on Graph Template Language (categories of statements) Bob Rodr\u00edguez is has written a lot about templates, check his papers for more information Here you can find the official documentation on ODS Graphics Template Modification PROC TEMPLATE style tips Style Templates vs Graph Templates Link Modifying Style Templates Link Obtain the source code 1 2 3 PROC TEMPLATE ; SOURCE styles . default ; RUN ; Modify the code 1 2 3 4 5 6 PROC TEMPLATE ; DEFINE STYLE MyListingStyle ; PARENT = styles . listing ; [ make desired changes in code ] END ; RUN ; Generate the plot 1 2 ODS LISTING STYLE = mylistingstyle ; [ SGPLOT Statements ] Modifying Graph Templates Link Obtain the source code PROC TEMPLATE; SOURCE Stat.Lifetest.Graphics.ProductLimitSurvival; RUN; Modify the code PROC TEMPLATE; DEFINE Stat.Lifetest.Graphics.ProductLimitSurvival; SOURCE Stat.Lifetest.Graphics.ProductLimitSurvival; [make desired changes in code] END; RUN; Generate the plot PROC LIFETEST DATA=db PLOTS=S; [statements] RUN; Revert to default template PROC TEMPLATE; DELETE Stat.Lifetest.Graphics.ProductLimitSurvival; RUN; Basic Graph Template Functionalities Link Obtaining the Default Templates Link First you need to know the name of the template. For this you can either look for its name listing all the available default templates that are kept in sashelp.tmplmst ... 1 2 3 4 PROC TEMPLATE ; PATH sashelp . tmplmst ; LIST Base . Freq / SORT = path DESCENDING ; RUN ; ... or use the ODS TRACE ON to obtain the name of an specific template. 1 2 3 4 5 6 7 8 ODS TRACE ON ; ODS GRAPHICS ON ; PROC FREQ DATA = sashelp . baseball ; TABLE League * Division / AGREE NOCOL NOROW ; TEST KAPPA ; RUN ; ODS GRAPHICS OFF ; ODS TRACE OFF ; You will obtain the following log output for the Agreement Plot where you can obtain the name of the template you are interested in: 1 2 3 4 5 6 7 Output Added: ------------- Name: AgreePlot Label: Agreement Plot Template: Base.Freq.Graphics.AgreePlot Path: Freq.Table1.AgreePlot ------------- Then you use the SOURCE option from the TEMPLATE procedure to show in the log the full object template. 1 2 3 4 %let path = C : \\ your - path - here ; PROC TEMPLATE ; SOURCE Base . Freq . Graphics . AgreePlot / file = \"&path. \\a greeplot.sas\" ; RUN ; Note Remember that you must add a PROC TEMPLATE; statement before the generated source statements and optionally a RUN; statement after the END; statement before you submit your modified definition. Editing Templates Link Graph definitions are self-contained and do not support inheritance (via the PARENT= option) as do table definitions. Consequently, the EDIT statement in PROC TEMPLATE is not supported for graph definitions. Here are some important points about what you can and cannot change in a template: Do not change the template name . A statistical procedure can access only a predefined list of templates. If you change the name, the procedure cannot find your template. You must make sure that it is in a template store that is read before Sashelp.Tmplmst through the ODS PATH statement. Do not change the names of columns . The underlying data object contains predefined column names that you must use. Be very careful if you change how a column is used in a template. Usually, columns are not interchangeable. Do not change the names of DYNAMIC variables . Changing dynamic variable names can lead to runtime errors. Do not add dynamic variables, because the procedure cannot set their values. Do not change the names of statements (for example, from a SCATTERPLOT to a NEEDLEPLOT or other type of plot). You can change any of the following: You can add macro variables that behave like dynamic variables . They are resolved at the time that the statistical procedure is run, and not at the time that the template is compiled. They are defined with an MVAR or NMVAR statement at the beginning the template. You can also move a variable from a DYNAMIC statement to an MVAR or NMVAR statement if you want to set it yourself rather than letting the procedure set it. You can change the graph size . You can change graph titles, footnotes, axis labels, and any other text that appears in the graph . You can change which plot features are displayed . You can change axis features, such as grid lines, offsets, view ports, tick value formatting, and so on . You can change the content and arrangement of insets (small tables of statistics embedded in some graphs). You can change the legend location, contents, border, background, title, and so on . Using Customized Templates Link The ODS PATH statement specifies the template stores to search, as well as the order in which to search them. You can change the default template search path by using the ODS PATH statement. 1 ODS PATH work . mystore ( update ) sashelp . tmplmst ( read ); You can display the current template search path with the following statement: 1 ODS PATH SHOW ; The log messages for the default template search path are as follows: 1 2 3 4 Current ODS PATH list is : 1 . WORK . MYSTORE ( UPDATE ) 2 . SASHELP . TMPLMST ( READ ) When you are done, you can reset the default template search path as follows: 1 ODS PATH RESET ; 1 2 3 4 Current ODS PATH list is : 1 . SASUSER . TEMPLAT ( UPDATE ) 2 . SASHELP . TMPLMST ( READ ) Reverting to the Default Templates Link The following statements delete the modified template from SASUSER.TEMPLAT and revert to the default template in SASHELP.TMPLMST , which is where the SAS templates are stored. 1 2 3 PROC TEMPLATE ; DELETE Base . Freq . Graphics . AgreePlot ; RUN ; The following note is printed in the SAS log: 1 NOTE : 'Base.Freq.Graphics.AgreePlot' has been deleted from : SASUSER . TEMPLAT You can run the following step to delete the entire SASUSER.TEMPLAT store of customized templates: 1 2 3 4 5 ODS PATH sashelp . tmplmst ( read ); PROC DATASETS LIBRARY = sasuser NOLIST ; DELETE TEMPLAT ( MEMTYPE = ITEMSTOR ); RUN ; ODS PATH sasuser . templat ( update ) sashelp . tmplmst ( read ); PROC TEMPLATE Features Link Include an Image as the Header Link 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 PROC TEMPLATE ; DEFINE STYLE template_header_image ; PARENT = styles . default ; ( ... ) STYLE SYSTEMTITLE / TEXTALIGN = l VERTICALALIGN = t PREIMAGE = \" c:\\path-to-your-file\\header-image.png \" FOREGROUND = # ffffff ; END ; RUN ; ODS PDF DPI = 700 STYLE = template_header_image FILE = \" your-path\\your-file (&sysdate).pdf \" ; TITLE \"\" ; ( ... ) ODS PDF CLOSE ; DPI needs to be increased to show the PREIMAGE logo with good definition You need to specify TITLE \"\"; for the PREIMAGE to appear Cell Values Alignment Link 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 PROC TEMPLATE ; DEFINE STYLE template_header_image ; PARENT = styles . default ; ( ... ) CLASS CELL / PADDINGRIGHT = 15 /* Controls the horizontal spacing between one cell and the next one */ TEXTALIGN = left ; CLASS HEADER / /* Centers all headers (including rowheaders) */ TEXTALIGN = center ; CLASS ROWHEADER / /* Takes rowheaders back to left alignment */ TEXTALIGN = left ; CONTEXT ' .header ' / /* Activates the bottom border of the header fields */ BORDERBOTTOMSTYLE = solid BORDERBOTTOMCOLOR = black BORDERBOTTOMWIDTH = 1 px ; END ; RUN ; Other Related Topics Link Solve the error \" unable to write to the template store \" : 1 2 3 4 ERROR : Template ' xxxxx ' was unable to write to the template store ! ODS PATH SHOW ; ODS PATH ( PREPEND ) work . templat ( UPDATE ) ;","title":"PROC TEMPLATE"},{"location":"sas-outputs/template/#style-templates-vs-graph-templates","text":"","title":"Style Templates vs Graph Templates"},{"location":"sas-outputs/template/#modifying-style-templates","text":"Obtain the source code 1 2 3 PROC TEMPLATE ; SOURCE styles . default ; RUN ; Modify the code 1 2 3 4 5 6 PROC TEMPLATE ; DEFINE STYLE MyListingStyle ; PARENT = styles . listing ; [ make desired changes in code ] END ; RUN ; Generate the plot 1 2 ODS LISTING STYLE = mylistingstyle ; [ SGPLOT Statements ]","title":"Modifying Style Templates"},{"location":"sas-outputs/template/#modifying-graph-templates","text":"Obtain the source code PROC TEMPLATE; SOURCE Stat.Lifetest.Graphics.ProductLimitSurvival; RUN; Modify the code PROC TEMPLATE; DEFINE Stat.Lifetest.Graphics.ProductLimitSurvival; SOURCE Stat.Lifetest.Graphics.ProductLimitSurvival; [make desired changes in code] END; RUN; Generate the plot PROC LIFETEST DATA=db PLOTS=S; [statements] RUN; Revert to default template PROC TEMPLATE; DELETE Stat.Lifetest.Graphics.ProductLimitSurvival; RUN;","title":"Modifying Graph Templates"},{"location":"sas-outputs/template/#basic-graph-template-functionalities","text":"","title":"Basic Graph Template Functionalities"},{"location":"sas-outputs/template/#obtaining-the-default-templates","text":"First you need to know the name of the template. For this you can either look for its name listing all the available default templates that are kept in sashelp.tmplmst ... 1 2 3 4 PROC TEMPLATE ; PATH sashelp . tmplmst ; LIST Base . Freq / SORT = path DESCENDING ; RUN ; ... or use the ODS TRACE ON to obtain the name of an specific template. 1 2 3 4 5 6 7 8 ODS TRACE ON ; ODS GRAPHICS ON ; PROC FREQ DATA = sashelp . baseball ; TABLE League * Division / AGREE NOCOL NOROW ; TEST KAPPA ; RUN ; ODS GRAPHICS OFF ; ODS TRACE OFF ; You will obtain the following log output for the Agreement Plot where you can obtain the name of the template you are interested in: 1 2 3 4 5 6 7 Output Added: ------------- Name: AgreePlot Label: Agreement Plot Template: Base.Freq.Graphics.AgreePlot Path: Freq.Table1.AgreePlot ------------- Then you use the SOURCE option from the TEMPLATE procedure to show in the log the full object template. 1 2 3 4 %let path = C : \\ your - path - here ; PROC TEMPLATE ; SOURCE Base . Freq . Graphics . AgreePlot / file = \"&path. \\a greeplot.sas\" ; RUN ; Note Remember that you must add a PROC TEMPLATE; statement before the generated source statements and optionally a RUN; statement after the END; statement before you submit your modified definition.","title":"Obtaining the Default Templates"},{"location":"sas-outputs/template/#editing-templates","text":"Graph definitions are self-contained and do not support inheritance (via the PARENT= option) as do table definitions. Consequently, the EDIT statement in PROC TEMPLATE is not supported for graph definitions. Here are some important points about what you can and cannot change in a template: Do not change the template name . A statistical procedure can access only a predefined list of templates. If you change the name, the procedure cannot find your template. You must make sure that it is in a template store that is read before Sashelp.Tmplmst through the ODS PATH statement. Do not change the names of columns . The underlying data object contains predefined column names that you must use. Be very careful if you change how a column is used in a template. Usually, columns are not interchangeable. Do not change the names of DYNAMIC variables . Changing dynamic variable names can lead to runtime errors. Do not add dynamic variables, because the procedure cannot set their values. Do not change the names of statements (for example, from a SCATTERPLOT to a NEEDLEPLOT or other type of plot). You can change any of the following: You can add macro variables that behave like dynamic variables . They are resolved at the time that the statistical procedure is run, and not at the time that the template is compiled. They are defined with an MVAR or NMVAR statement at the beginning the template. You can also move a variable from a DYNAMIC statement to an MVAR or NMVAR statement if you want to set it yourself rather than letting the procedure set it. You can change the graph size . You can change graph titles, footnotes, axis labels, and any other text that appears in the graph . You can change which plot features are displayed . You can change axis features, such as grid lines, offsets, view ports, tick value formatting, and so on . You can change the content and arrangement of insets (small tables of statistics embedded in some graphs). You can change the legend location, contents, border, background, title, and so on .","title":"Editing Templates"},{"location":"sas-outputs/template/#using-customized-templates","text":"The ODS PATH statement specifies the template stores to search, as well as the order in which to search them. You can change the default template search path by using the ODS PATH statement. 1 ODS PATH work . mystore ( update ) sashelp . tmplmst ( read ); You can display the current template search path with the following statement: 1 ODS PATH SHOW ; The log messages for the default template search path are as follows: 1 2 3 4 Current ODS PATH list is : 1 . WORK . MYSTORE ( UPDATE ) 2 . SASHELP . TMPLMST ( READ ) When you are done, you can reset the default template search path as follows: 1 ODS PATH RESET ; 1 2 3 4 Current ODS PATH list is : 1 . SASUSER . TEMPLAT ( UPDATE ) 2 . SASHELP . TMPLMST ( READ )","title":"Using Customized Templates"},{"location":"sas-outputs/template/#reverting-to-the-default-templates","text":"The following statements delete the modified template from SASUSER.TEMPLAT and revert to the default template in SASHELP.TMPLMST , which is where the SAS templates are stored. 1 2 3 PROC TEMPLATE ; DELETE Base . Freq . Graphics . AgreePlot ; RUN ; The following note is printed in the SAS log: 1 NOTE : 'Base.Freq.Graphics.AgreePlot' has been deleted from : SASUSER . TEMPLAT You can run the following step to delete the entire SASUSER.TEMPLAT store of customized templates: 1 2 3 4 5 ODS PATH sashelp . tmplmst ( read ); PROC DATASETS LIBRARY = sasuser NOLIST ; DELETE TEMPLAT ( MEMTYPE = ITEMSTOR ); RUN ; ODS PATH sasuser . templat ( update ) sashelp . tmplmst ( read );","title":"Reverting to the Default Templates"},{"location":"sas-outputs/template/#proc-template-features","text":"","title":"PROC TEMPLATE Features"},{"location":"sas-outputs/template/#include-an-image-as-the-header","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 PROC TEMPLATE ; DEFINE STYLE template_header_image ; PARENT = styles . default ; ( ... ) STYLE SYSTEMTITLE / TEXTALIGN = l VERTICALALIGN = t PREIMAGE = \" c:\\path-to-your-file\\header-image.png \" FOREGROUND = # ffffff ; END ; RUN ; ODS PDF DPI = 700 STYLE = template_header_image FILE = \" your-path\\your-file (&sysdate).pdf \" ; TITLE \"\" ; ( ... ) ODS PDF CLOSE ; DPI needs to be increased to show the PREIMAGE logo with good definition You need to specify TITLE \"\"; for the PREIMAGE to appear","title":"Include an Image as the Header"},{"location":"sas-outputs/template/#cell-values-alignment","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 PROC TEMPLATE ; DEFINE STYLE template_header_image ; PARENT = styles . default ; ( ... ) CLASS CELL / PADDINGRIGHT = 15 /* Controls the horizontal spacing between one cell and the next one */ TEXTALIGN = left ; CLASS HEADER / /* Centers all headers (including rowheaders) */ TEXTALIGN = center ; CLASS ROWHEADER / /* Takes rowheaders back to left alignment */ TEXTALIGN = left ; CONTEXT ' .header ' / /* Activates the bottom border of the header fields */ BORDERBOTTOMSTYLE = solid BORDERBOTTOMCOLOR = black BORDERBOTTOMWIDTH = 1 px ; END ; RUN ;","title":"Cell Values Alignment"},{"location":"sas-outputs/template/#other-related-topics","text":"Solve the error \" unable to write to the template store \" : 1 2 3 4 ERROR : Template ' xxxxx ' was unable to write to the template store ! ODS PATH SHOW ; ODS PATH ( PREPEND ) work . templat ( UPDATE ) ;","title":"Other Related Topics"},{"location":"sasvsr/bridge/","text":"Under construction","title":"A Bridge Between SAS and R"},{"location":"sasvsr/iml/","text":"Under construction","title":"Interactive Matrix Language (IML)"},{"location":"sasvsr/importing-reporting/","text":"Under construction","title":"Importing and Reporting Data"},{"location":"sasvsr/inferential/","text":"Under construction","title":"Analyzing the Data via Inferential Procedures"},{"location":"sasvsr/introduction/","text":"Check these websites Choosing the correct statistical test in SAS, STATA, SPSS and R Import, Export, and Convert Data Files Really useful R package: sas7bdat SAS to R Migration A Saga Of Migrating From SAS To R 5 Ways to Convert SAS Data to R","title":"Introduction"},{"location":"sasvsr/proc-ods-macros/","text":"Under construction","title":"Descriptive Procedures, Output Delivery System and Macros"},{"location":"sasvsr/random-plotting/","text":"Under construction","title":"Random Number Generation and Plotting"},{"location":"sasvsr/variables-functions/","text":"Under construction","title":"Creating New Variables, Functions and Data Tables"},{"location":"sql/advanced/","text":"Querying Dictionary Information and Views Link Exploring Dictionary Tables Link At initialization, SAS creates special read-only dictionary tables and views that contain information about each SAS session or batch job. Dictionary tables contain data or metadata about the SAS session. SAS assigns the special reserved libref Dictionary to the dictionary tables . Within the dictionary tables, SAS stores library and table names in uppercase. But, SAS stores column names in the dictionary tables in the same case in which they were defined when created. So, the column names can be all lowercase, all uppercase, or mixed case. When you query a dictionary table, SAS gathers information that is pertinent to that table. The dictionary tables are only accessible with PROC SQL . Because dictionary tables are read-only, you cannot insert rows or columns, alter column attributes, or add integrity constraints to them. SAS provides PROC SQL views, based on the dictionary tables, that can be used in other SAS programming steps such as the DATA or PROC step. These views are stored in the SASHelp library and are commonly called SASHELP views. Querying Dictionary Information Link To prepare for writing a specific query, you can use a DESCRIBE statement to explore the structure of dictionary tables. 1 DESCRIBE TABLE table - 1 < , ... table - n > ; The output from the DESCRIBE TABLE statement is a CREATE TABLE statement written to the log that contains the column names and attributes. Because the libref dictionary is automatically assigned, you don't need to use a LIBNAME statement to run this code. Commonly used dictionary tables: Table Contents Dictionary.Dictionaries Data about the dictionary tables that are created by SAS Dictionary.Tables Data about the contents of all the tables available in this SAS session Dictionary.Columns Information such as name, type, length and format about all columns in all tables that are known to the current SAS session SAS libraries and the contents of libraries: Table Contents Dictionary.Members General information about the members of a SAS library Dictionary.Views Detailed information about all views available in this SAS session Dictionary.Catalogs Information about catalog entries Indexes and integrity constraints: Table Contents Dictionary.Indexes Information about indexes defined for all tables available in this SAS session Dictionary.Table_Constraints Information about the integrity constraints in all tables available in this SAS session Dictionary.Check_Constraints Information aabout check constraints in all tables available in this SAS session Dictionary.Referential_Constraints Information about the referential constraints in all tables available in this SAS session Dictionary.Constraint_Column_Usage Information about the columns that are referenced by integrity constraints Dictionary.Constraint_Table_Usage Information about tables that use integrity constraints Global macro variables, SAS system options, and more: Table Contents Dictionary.Macros Information about macro variables\u2019 names and values Dictionary.Options Information about the current settings of SAS system options Dictionary.Titles Information about the text currently assigned to titles and footnotes Dictionary.Extfiles Information about currently assigned filerefs Example: 1 2 3 proc sql ; describe table dictionary . tables ; quit ; Displaying Specific Metadata Link 1 2 SELECT object - item < , ... object - item > FROM table - 1 < , ... table - n > ; If you only want to select information about specific columns, you indicate the specific columns in the SELECT clause. If you only want to display results from specific tables, not all tables currently available in this SAS session, you subset the tables using a \u00b4WHERE\u00b4 clause in the query. 1 2 3 SELECT object - item < , ... object - item > FROM table - 1 < , ... table - n > ; WHERE libname = 'LIB-NAME' ; When you query dictionary tables, you supply values to the WHERE clause in the appropriate case. Remember that library names are stored in dictionary tables in uppercase. Because different dictionary tables might store similar data using different cases, you might be tempted to use SAS functions, such as UPCASE or LOWCASE . But, the WHERE clause won't process most function calls, such as UPCASE . The functions prevent the WHERE clause from optimizing the condition, which could degrade performance. Example: 1 2 3 4 5 6 7 8 title 'Tables in the ORION Library' ; proc sql ; select memname 'Table Name' , nobs , nvar , crdate from dictionary . tables where libname = 'ORION' ; quit ; title ; Using Dictionary Tables in Other SAS Code Link In SAS procedures and the DATA step, you must refer to sashelp instead of dictionary. Remember that PROC SQL views based on the dictionary tables are stored in the sashelp library. In addition, in PROC and DATA steps, the libref cannot exceed eight characters . Most of the sashelp library dictionary view names are similar to dictionary table names, but they are shortened to eight characters or fewer. They begin with the letter v , and do not end in s . So to run correctly, you change dictionary.tables to sashelp.vtable . Example: 1 2 3 4 5 6 title 'Tables in the ORION Library' ; proc print data = sashelp . vtable label ; var memname nobs nvar ; where libname = 'ORION' ; run ; title ; Using Dictionary Views Link To use a dictionary table in a DATA or PROC step, you can reference the views of the dictionary tables, which are available in the SASHelp library. You can browse the library to determine the name or use the name of the dictionary table to extrapolate the view name. The names of the views in SASHelp are similar to the dictionary table names, start with the letter v, are eight characters or less, and generally don't have an s on the end. Example: 1 2 3 4 5 6 7 8 9 title 'SAS Objects by Library' ; proc tabulate data = sashelp . vmember format = 8 .; class libname memtype ; keylabel N = ' ' ; table libname , memtype / rts = 10 misstext = 'None' ; where libname in ( 'ORION' , 'SASUSER' , 'SASHELP' ); run ; title ; Using SQL Procedure Options Link PROC SQL options give you finer control over your SQL processes by providing features that allow you to control your processing and your output. SQL Options: Controlling Processing Link Option Effect OUTOBS=n Restricts the number of rows that a query outputs INOBS=n Sets a limit of n rows from each source table that contributes to a query NOEXEC Checks syntax for all SQL statements without executing them SQL Options: Controlling Display Link Option Effect PRINT | NOPRINT Controls whether the results of a SELECT statement are displayed as a report NONUMBER | NUMBER Controls whether the row number is displayed as the first column in the query output NOSTIMER | STIMER Controls whether PROC SQL writes resource utilization statistics to the SAS log NODOUBLE | DOUBLE Controls whether the report is double-spaced NOFLOW | FLOW Controls text wrapping in character columns SQL Options: Limiting the Number of Rows That SAS Writes or Reads Link 1 2 OUTOBS = n INOBS = n To limit the number of output rows , you can use the PROC SQL option OUTOBS= . The value n is an integer that specifies the maximum number of rows that PROC SQL writes to output. To limit the number of rows that PROC SQL reads as input , you can use the INOBS= option in the PROC SQL statement. n is an integer that specifies the maximum number of rows that PROC SQL reads from each source table. The INOBS= option is generally more efficient than the OUTOBS= option. However, the INOBS= option might not always produce the desired results. If you use an inner join to combine large tables, it's most efficient to use the INOBS= option, if possible. If the join produces no output, try increasing the value of n. If you use an inner join to combine small tables, using the OUTOBS= option to limit the output rows ensures that you'll get output when matches exist. Example: Limiting the Number of Rows That SAS Writes 1 2 3 4 5 6 7 8 9 10 11 12 proc sql outobs = 10 ; title \"10 Most Profitable Customers\" ; select Customer_ID , sum ( Unit_Sales_Price - Unit_Cost_Price ) as Profit_2011 format = comma8 . 2 from orion . price_list as p , orion . order_fact as o where p . Product_ID = o . Product_id and year ( Order_date ) = 2011 group by Customer_ID order by Profit_2011 desc ; quit ; title ; Example: Limiting the Number of Rows That SAS Reads 1 2 3 4 5 6 7 8 9 10 proc sql inobs = 10 ; title \"orion.price_list - INOBS=10\" ; select Product_ID , Unit_Cost_price format = comma8 . 2 , Unit_Sales_Price format = comma8 . 2 , Unit_Sales_Price - Unit_Cost_Price as Margin format = comma8 . 2 from orion . price_list ; quit ; title ; Using the Macro Language with PROC SQL Link The Macro Facility Link The SAS macro facility consists of the macro processor and the SAS macro language. It is a text processing tool that enables you to automate and customize SAS code. The SAS macro facility enables you to assign a name to character strings or groups of SAS programming statements. Then, you can work with the names rather than the text itself. SAS inserts the new value of the macro variable throughout your PROC SQL statements automatically. After all macro variables are resolved, the PROC SQL statement executes with the values from those macro variables. The macro facility is a programming tool that you can use to substitute text in your SAS programs to automatically generate and execute code and write SAS programs that are dynamic. PROC SQL and Macro Variables Link In PROC SQL , you can use macro variables to store values returned by a query. You can then reference these macro variables in other PROC SQL statements and steps. Using macro variables in your program enables quick and easy updates, because you need to make the change in only one place. Macro variables are sometimes referred to as symbolic variables because SAS programs can reference macro variables as symbols for SAS code. Macro variables can supply a variety of information, including operating system information, SAS session information, or user-defined values. The information is stored as a text string value that remains constant until you change it. The text can include complete or partial SAS steps, as well as complete or partial SAS statements. There are two types of macro variables. SAS provides automatic macro variables. You create and define user-defined macro variables. The name and value of a macro variable are stored in an area of memory called a global symbol table, which is created by SAS at initialization. %PUT statement Link You can use the %PUT statement to write your own messages, including macro variable values, to the SAS log. 1 %PUT text; The macro processor processes %PUT . Remember that the macro processor does not require text to be quoted. You can follow the keyword %PUT with optional text, and then the reference to the macro variable. %PUT statements are valid anywhere in a SAS program. The %PUT statement writes only to the SAS log and always writes to a new log line, starting in column one. You can follow the keyword, %PUT , with optional text, and then the reference to the macro variable. !!! note\" %put _all_ statement \" To display all macro variables in the global symbol table, use the statement %put _all_; . This report will be written to the SAS log. To create a report of just the automatic macro variables, use the statement %put _automatic_; . Creating User-Defined Macro Variables Link You use the %LET statement to create a user-def %LET statements are global statements and are valid anywhere in a SAS program. 1 %LET variable=value; After the keyword %LET , you specify the name of the macro variable, an equal sign, and then the value of the macro variable. Macro variable names must start with letters or underscores . The rest of the name can be letters, digits, or underscores. You don't need to enclose the value of the macro variable in quotation marks. SAS considers everything that appears between the equal sign and the semicolon to be part of the macro variable value. You can assign any valid SAS variable name to a macro variable as long as the name isn't a reserved word. If you assign a macro variable name that isn't valid, SAS issues an error message in the log. Value can be any string of 0 to 65,534 characters. Value can also be a macro variable reference. SAS stores all macro variable values as text strings, even if they contain only numbers. SAS doesn't evaluate mathematical expressions in macro variable values. SAS stores the value in the case that is specified in the %LET statement. SAS stores quotation marks as part of the macro variable value. The %LET statement removes leading and trailing blanks from the macro variable value before storing it. The %LET statement doesn't remove blanks within the macro variable value. SAS stores the user-defined macro variable's name and value in the global symbol table until the end of your SAS session, when they are deleted from memory. Unlike column names in a data table, SAS stores the macro variable names in uppercase, regardless of how the name was created. SAS stores the values as mixed-case text depending on how they were created. But, remember that the values are all character types, even when the characters are digits. Example: 1 2 3 %let DataSetName = employee_payroll ; %let BigSalary = 100000 ; %let Libname = ' orion ' ; Resolving User-Defined Macro Variables Link To reference the macro variable, you precede the name of the macro variable with an ampersand. 1 & macro - variable - name Although macro variables are stored in uppercase, references to macro variable names aren't case sensitive. You can reference a macro variable anywhere in a SAS program. If you need to reference a macro variable within quotation marks, such as in a title, you must use double quotation marks. The macro processor won't resolve macro variable references that appear within single quotation marks. Instead, SAS interprets the macro variable reference as part of the text string. After the program code is submitted and before the program executes, the macro processor searches for macro triggers such as &. The macro processor finds the stored value for the named macro variable in the global symbol table. Then the macro processor substitutes that value into the program in place of the reference. Finally, the program executes and creates the report. Example: 1 2 3 4 5 proc sql ; select Employee_ID , Salary from orion . & DataSetName where Salary >& BigSalary ; quit ; Displaying Macro Variable Values Link %PUT statement Link You can use the %PUT statement to write your own messages, including macro variable values, to the SAS log. The macro processor processes %PUT . Remember that the macro processor does not require text to be enclosed in quotation marks. 1 %PUT text; %PUT statements are valid anywhere in a SAS program. The %PUT statement writes only to the SAS log and always writes to a new log line, starting in column one. You can follow the keyword, %PUT , with optional text, and then the reference to the macro variable. SYMBOLGEN global system option Link The SYMBOLGEN global system option enables SAS to display the value of the macro variable in the SAS log as it is resolved. 1 OPTIONS SYMBOLGEN ; The default setting for this system option is NOSYMBOLGEN . Because SYMBOLGEN is a system option, its setting remains in effect until you modify it or until you end your SAS session. Example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 %put The value of BigSalary is & BigSalary ; %let DataSetName = Employee_Payroll ; %let BigSalary = 100000 ; options symbolgen ; proc sql ; title \"Salaries > &bigsalary\" ; select Employee_ID , Salary from orion . & DataSetName where Salary > & BigSalary ; quit ; title ; %let DataSetName = Employee_Payroll ; %let BigSalary = 100000 ; proc sql feedback ; title \"Salaries > &bigsalary\" ; select Employee_ID , Salary from orion . & DataSetName where Salary > & BigSalary ; quit ; title ; FEEDBACK option Link You can use the FEEDBACK option to display the query in the SAS log after it has expanded references, such as macro variables. 1 FEEDBACK ; The default option is NOFEEDBACK . FEEDBACK PROC SQL option writes the query with the substituted macro variable values to the SAS log. When you activate the FEEDBACK PROC SQL option, SAS writes a message to the log displaying the query with the references expanded. Using a Query to Generate Macro Values Link You can use a PROC SQL query to generate values that are sent to the symbol table for later use. 1 2 3 4 SELECT column - 1 < , ... column - n > INTO : macro - variable - 1 < , ... : macro - variable - n > FROM table | view < additional clauses > ; PROC SQL creates macro variables or updates existing macro variables using an INTO clause. The INTO clause is located between the SELECT clause and the FROM clause. It cannot be used in a CREATE TABLE or CREATE VIEW statement. You list the names of the macro variables to be created in the INTO clause. Each macro variable name is preceded with a colon. PROC SQL generates the values that are assigned to these variables by executing the query. Creating a Single Macro Variable Link This syntax of the INTO clause places values from the first row returned by an SQL query into a macro variable. 1 2 3 4 SELECT column - 1 < , ... column - n > INTO : macro - variable - 1 < , ... : macro - variable - n > FROM table | view < additional clauses > ; The value from the first column in the SELECT list is placed in the first macro variable listed in the INTO clause. The second column in the SELECT list is placed in the second macro, and so on. When storing a single value into a macro variable, PROC SQL preserves leading or trailing blanks. If the macro variable already exists, the INTO clause replaces the existing value with a new value from the SELECT clause. Data from additional rows returned by the query is ignored. This method is used most often with queries that return only one row. Example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 %let Dept = Sales ; proc sql noprint ; select avg ( Salary ) into : MeanSalary from orion . employee_payroll as p , orion . employee_organization as o where p . Employee_ID = o . Employee_ID and Department = propcase ( \"&Dept\" ); reset print number ; title \"&Dept Department Employees Earning\" ; title2 \"More Than The Department Average \" \"Of &MeanSalary\" ; select p . Employee_ID , Salary from orion . employee_payroll as p , orion . employee_organization as o where p . Employee_ID = o . Employee_ID and Department = Propcase ( \"&Dept\" ) and Salary > & MeanSalary ; quit ; title ; Creating Multiple Macro Variables Link You can use the same syntax of the INTO clause to create multiple macro variables from a single row. To do this, you separate each name with a comma in the INTO clause. 1 2 3 4 SELECT column - 1 INTO : macro - variable - 1 < , ... : macro - variable - n > FROM table | view < additional clauses > ; Remember that macro variables can hold only character values. So, numeric values are converted to character values by using the BEST8. format and are right aligned. Example: 1 2 3 4 5 6 7 proc sql noprint ; select avg ( Salary ), min ( Salary ), max ( Salary ) into : MeanSalary , : MinSalary , : MaxSalary from orion . employee_payroll ; %put Mean : & MeanSalary Min : & MinSalary Max : & MaxSalary ; quit ; Second Syntax form Link 1 2 3 4 5 SELECT column - a < , column - b , ... > INTO : macro - variable - a_1 - : macro - variable - a_n < , : macro - variable - b_1 - : macro - variable - b_n > FROM table - 1 | view - 1 < table - x | view - x > < additional clauses > ; Third Syntax form Link 1 2 3 4 5 SELECT column - 1 < , column - 2 , ... > INTO : macro - variable - 1 SEPARATED BY 'delimiter' < , : macro - variable - 2 SEPARATED BY 'delimiter' > FROM table - 1 | view - 1 < , ... table - x | view - x > < additional clauses > ;","title":"Using Advanced PROC SQL Features"},{"location":"sql/advanced/#querying-dictionary-information-and-views","text":"","title":"Querying Dictionary Information and Views"},{"location":"sql/advanced/#exploring-dictionary-tables","text":"At initialization, SAS creates special read-only dictionary tables and views that contain information about each SAS session or batch job. Dictionary tables contain data or metadata about the SAS session. SAS assigns the special reserved libref Dictionary to the dictionary tables . Within the dictionary tables, SAS stores library and table names in uppercase. But, SAS stores column names in the dictionary tables in the same case in which they were defined when created. So, the column names can be all lowercase, all uppercase, or mixed case. When you query a dictionary table, SAS gathers information that is pertinent to that table. The dictionary tables are only accessible with PROC SQL . Because dictionary tables are read-only, you cannot insert rows or columns, alter column attributes, or add integrity constraints to them. SAS provides PROC SQL views, based on the dictionary tables, that can be used in other SAS programming steps such as the DATA or PROC step. These views are stored in the SASHelp library and are commonly called SASHELP views.","title":"Exploring Dictionary Tables"},{"location":"sql/advanced/#querying-dictionary-information","text":"To prepare for writing a specific query, you can use a DESCRIBE statement to explore the structure of dictionary tables. 1 DESCRIBE TABLE table - 1 < , ... table - n > ; The output from the DESCRIBE TABLE statement is a CREATE TABLE statement written to the log that contains the column names and attributes. Because the libref dictionary is automatically assigned, you don't need to use a LIBNAME statement to run this code. Commonly used dictionary tables: Table Contents Dictionary.Dictionaries Data about the dictionary tables that are created by SAS Dictionary.Tables Data about the contents of all the tables available in this SAS session Dictionary.Columns Information such as name, type, length and format about all columns in all tables that are known to the current SAS session SAS libraries and the contents of libraries: Table Contents Dictionary.Members General information about the members of a SAS library Dictionary.Views Detailed information about all views available in this SAS session Dictionary.Catalogs Information about catalog entries Indexes and integrity constraints: Table Contents Dictionary.Indexes Information about indexes defined for all tables available in this SAS session Dictionary.Table_Constraints Information about the integrity constraints in all tables available in this SAS session Dictionary.Check_Constraints Information aabout check constraints in all tables available in this SAS session Dictionary.Referential_Constraints Information about the referential constraints in all tables available in this SAS session Dictionary.Constraint_Column_Usage Information about the columns that are referenced by integrity constraints Dictionary.Constraint_Table_Usage Information about tables that use integrity constraints Global macro variables, SAS system options, and more: Table Contents Dictionary.Macros Information about macro variables\u2019 names and values Dictionary.Options Information about the current settings of SAS system options Dictionary.Titles Information about the text currently assigned to titles and footnotes Dictionary.Extfiles Information about currently assigned filerefs Example: 1 2 3 proc sql ; describe table dictionary . tables ; quit ;","title":"Querying Dictionary Information"},{"location":"sql/advanced/#displaying-specific-metadata","text":"1 2 SELECT object - item < , ... object - item > FROM table - 1 < , ... table - n > ; If you only want to select information about specific columns, you indicate the specific columns in the SELECT clause. If you only want to display results from specific tables, not all tables currently available in this SAS session, you subset the tables using a \u00b4WHERE\u00b4 clause in the query. 1 2 3 SELECT object - item < , ... object - item > FROM table - 1 < , ... table - n > ; WHERE libname = 'LIB-NAME' ; When you query dictionary tables, you supply values to the WHERE clause in the appropriate case. Remember that library names are stored in dictionary tables in uppercase. Because different dictionary tables might store similar data using different cases, you might be tempted to use SAS functions, such as UPCASE or LOWCASE . But, the WHERE clause won't process most function calls, such as UPCASE . The functions prevent the WHERE clause from optimizing the condition, which could degrade performance. Example: 1 2 3 4 5 6 7 8 title 'Tables in the ORION Library' ; proc sql ; select memname 'Table Name' , nobs , nvar , crdate from dictionary . tables where libname = 'ORION' ; quit ; title ;","title":"Displaying Specific Metadata"},{"location":"sql/advanced/#using-dictionary-tables-in-other-sas-code","text":"In SAS procedures and the DATA step, you must refer to sashelp instead of dictionary. Remember that PROC SQL views based on the dictionary tables are stored in the sashelp library. In addition, in PROC and DATA steps, the libref cannot exceed eight characters . Most of the sashelp library dictionary view names are similar to dictionary table names, but they are shortened to eight characters or fewer. They begin with the letter v , and do not end in s . So to run correctly, you change dictionary.tables to sashelp.vtable . Example: 1 2 3 4 5 6 title 'Tables in the ORION Library' ; proc print data = sashelp . vtable label ; var memname nobs nvar ; where libname = 'ORION' ; run ; title ;","title":"Using Dictionary Tables in Other SAS Code"},{"location":"sql/advanced/#using-dictionary-views","text":"To use a dictionary table in a DATA or PROC step, you can reference the views of the dictionary tables, which are available in the SASHelp library. You can browse the library to determine the name or use the name of the dictionary table to extrapolate the view name. The names of the views in SASHelp are similar to the dictionary table names, start with the letter v, are eight characters or less, and generally don't have an s on the end. Example: 1 2 3 4 5 6 7 8 9 title 'SAS Objects by Library' ; proc tabulate data = sashelp . vmember format = 8 .; class libname memtype ; keylabel N = ' ' ; table libname , memtype / rts = 10 misstext = 'None' ; where libname in ( 'ORION' , 'SASUSER' , 'SASHELP' ); run ; title ;","title":"Using Dictionary Views"},{"location":"sql/advanced/#using-sql-procedure-options","text":"PROC SQL options give you finer control over your SQL processes by providing features that allow you to control your processing and your output.","title":"Using SQL Procedure Options"},{"location":"sql/advanced/#sql-options-controlling-processing","text":"Option Effect OUTOBS=n Restricts the number of rows that a query outputs INOBS=n Sets a limit of n rows from each source table that contributes to a query NOEXEC Checks syntax for all SQL statements without executing them","title":"SQL Options: Controlling Processing"},{"location":"sql/advanced/#sql-options-controlling-display","text":"Option Effect PRINT | NOPRINT Controls whether the results of a SELECT statement are displayed as a report NONUMBER | NUMBER Controls whether the row number is displayed as the first column in the query output NOSTIMER | STIMER Controls whether PROC SQL writes resource utilization statistics to the SAS log NODOUBLE | DOUBLE Controls whether the report is double-spaced NOFLOW | FLOW Controls text wrapping in character columns","title":"SQL Options: Controlling Display"},{"location":"sql/advanced/#sql-options-limiting-the-number-of-rows-that-sas-writes-or-reads","text":"1 2 OUTOBS = n INOBS = n To limit the number of output rows , you can use the PROC SQL option OUTOBS= . The value n is an integer that specifies the maximum number of rows that PROC SQL writes to output. To limit the number of rows that PROC SQL reads as input , you can use the INOBS= option in the PROC SQL statement. n is an integer that specifies the maximum number of rows that PROC SQL reads from each source table. The INOBS= option is generally more efficient than the OUTOBS= option. However, the INOBS= option might not always produce the desired results. If you use an inner join to combine large tables, it's most efficient to use the INOBS= option, if possible. If the join produces no output, try increasing the value of n. If you use an inner join to combine small tables, using the OUTOBS= option to limit the output rows ensures that you'll get output when matches exist. Example: Limiting the Number of Rows That SAS Writes 1 2 3 4 5 6 7 8 9 10 11 12 proc sql outobs = 10 ; title \"10 Most Profitable Customers\" ; select Customer_ID , sum ( Unit_Sales_Price - Unit_Cost_Price ) as Profit_2011 format = comma8 . 2 from orion . price_list as p , orion . order_fact as o where p . Product_ID = o . Product_id and year ( Order_date ) = 2011 group by Customer_ID order by Profit_2011 desc ; quit ; title ; Example: Limiting the Number of Rows That SAS Reads 1 2 3 4 5 6 7 8 9 10 proc sql inobs = 10 ; title \"orion.price_list - INOBS=10\" ; select Product_ID , Unit_Cost_price format = comma8 . 2 , Unit_Sales_Price format = comma8 . 2 , Unit_Sales_Price - Unit_Cost_Price as Margin format = comma8 . 2 from orion . price_list ; quit ; title ;","title":"SQL Options: Limiting the Number of Rows That SAS Writes or Reads"},{"location":"sql/advanced/#using-the-macro-language-with-proc-sql","text":"","title":"Using the Macro Language with PROC SQL"},{"location":"sql/advanced/#the-macro-facility","text":"The SAS macro facility consists of the macro processor and the SAS macro language. It is a text processing tool that enables you to automate and customize SAS code. The SAS macro facility enables you to assign a name to character strings or groups of SAS programming statements. Then, you can work with the names rather than the text itself. SAS inserts the new value of the macro variable throughout your PROC SQL statements automatically. After all macro variables are resolved, the PROC SQL statement executes with the values from those macro variables. The macro facility is a programming tool that you can use to substitute text in your SAS programs to automatically generate and execute code and write SAS programs that are dynamic.","title":"The Macro Facility"},{"location":"sql/advanced/#proc-sql-and-macro-variables","text":"In PROC SQL , you can use macro variables to store values returned by a query. You can then reference these macro variables in other PROC SQL statements and steps. Using macro variables in your program enables quick and easy updates, because you need to make the change in only one place. Macro variables are sometimes referred to as symbolic variables because SAS programs can reference macro variables as symbols for SAS code. Macro variables can supply a variety of information, including operating system information, SAS session information, or user-defined values. The information is stored as a text string value that remains constant until you change it. The text can include complete or partial SAS steps, as well as complete or partial SAS statements. There are two types of macro variables. SAS provides automatic macro variables. You create and define user-defined macro variables. The name and value of a macro variable are stored in an area of memory called a global symbol table, which is created by SAS at initialization.","title":"PROC SQL and Macro Variables"},{"location":"sql/advanced/#put-statement","text":"You can use the %PUT statement to write your own messages, including macro variable values, to the SAS log. 1 %PUT text; The macro processor processes %PUT . Remember that the macro processor does not require text to be quoted. You can follow the keyword %PUT with optional text, and then the reference to the macro variable. %PUT statements are valid anywhere in a SAS program. The %PUT statement writes only to the SAS log and always writes to a new log line, starting in column one. You can follow the keyword, %PUT , with optional text, and then the reference to the macro variable. !!! note\" %put _all_ statement \" To display all macro variables in the global symbol table, use the statement %put _all_; . This report will be written to the SAS log. To create a report of just the automatic macro variables, use the statement %put _automatic_; .","title":"%PUT statement"},{"location":"sql/advanced/#creating-user-defined-macro-variables","text":"You use the %LET statement to create a user-def %LET statements are global statements and are valid anywhere in a SAS program. 1 %LET variable=value; After the keyword %LET , you specify the name of the macro variable, an equal sign, and then the value of the macro variable. Macro variable names must start with letters or underscores . The rest of the name can be letters, digits, or underscores. You don't need to enclose the value of the macro variable in quotation marks. SAS considers everything that appears between the equal sign and the semicolon to be part of the macro variable value. You can assign any valid SAS variable name to a macro variable as long as the name isn't a reserved word. If you assign a macro variable name that isn't valid, SAS issues an error message in the log. Value can be any string of 0 to 65,534 characters. Value can also be a macro variable reference. SAS stores all macro variable values as text strings, even if they contain only numbers. SAS doesn't evaluate mathematical expressions in macro variable values. SAS stores the value in the case that is specified in the %LET statement. SAS stores quotation marks as part of the macro variable value. The %LET statement removes leading and trailing blanks from the macro variable value before storing it. The %LET statement doesn't remove blanks within the macro variable value. SAS stores the user-defined macro variable's name and value in the global symbol table until the end of your SAS session, when they are deleted from memory. Unlike column names in a data table, SAS stores the macro variable names in uppercase, regardless of how the name was created. SAS stores the values as mixed-case text depending on how they were created. But, remember that the values are all character types, even when the characters are digits. Example: 1 2 3 %let DataSetName = employee_payroll ; %let BigSalary = 100000 ; %let Libname = ' orion ' ;","title":"Creating User-Defined Macro Variables"},{"location":"sql/advanced/#resolving-user-defined-macro-variables","text":"To reference the macro variable, you precede the name of the macro variable with an ampersand. 1 & macro - variable - name Although macro variables are stored in uppercase, references to macro variable names aren't case sensitive. You can reference a macro variable anywhere in a SAS program. If you need to reference a macro variable within quotation marks, such as in a title, you must use double quotation marks. The macro processor won't resolve macro variable references that appear within single quotation marks. Instead, SAS interprets the macro variable reference as part of the text string. After the program code is submitted and before the program executes, the macro processor searches for macro triggers such as &. The macro processor finds the stored value for the named macro variable in the global symbol table. Then the macro processor substitutes that value into the program in place of the reference. Finally, the program executes and creates the report. Example: 1 2 3 4 5 proc sql ; select Employee_ID , Salary from orion . & DataSetName where Salary >& BigSalary ; quit ;","title":"Resolving User-Defined Macro Variables"},{"location":"sql/advanced/#displaying-macro-variable-values","text":"","title":"Displaying Macro Variable Values"},{"location":"sql/advanced/#put-statement_1","text":"You can use the %PUT statement to write your own messages, including macro variable values, to the SAS log. The macro processor processes %PUT . Remember that the macro processor does not require text to be enclosed in quotation marks. 1 %PUT text; %PUT statements are valid anywhere in a SAS program. The %PUT statement writes only to the SAS log and always writes to a new log line, starting in column one. You can follow the keyword, %PUT , with optional text, and then the reference to the macro variable.","title":"%PUT statement"},{"location":"sql/advanced/#symbolgen-global-system-option","text":"The SYMBOLGEN global system option enables SAS to display the value of the macro variable in the SAS log as it is resolved. 1 OPTIONS SYMBOLGEN ; The default setting for this system option is NOSYMBOLGEN . Because SYMBOLGEN is a system option, its setting remains in effect until you modify it or until you end your SAS session. Example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 %put The value of BigSalary is & BigSalary ; %let DataSetName = Employee_Payroll ; %let BigSalary = 100000 ; options symbolgen ; proc sql ; title \"Salaries > &bigsalary\" ; select Employee_ID , Salary from orion . & DataSetName where Salary > & BigSalary ; quit ; title ; %let DataSetName = Employee_Payroll ; %let BigSalary = 100000 ; proc sql feedback ; title \"Salaries > &bigsalary\" ; select Employee_ID , Salary from orion . & DataSetName where Salary > & BigSalary ; quit ; title ;","title":"SYMBOLGEN global system option"},{"location":"sql/advanced/#feedback-option","text":"You can use the FEEDBACK option to display the query in the SAS log after it has expanded references, such as macro variables. 1 FEEDBACK ; The default option is NOFEEDBACK . FEEDBACK PROC SQL option writes the query with the substituted macro variable values to the SAS log. When you activate the FEEDBACK PROC SQL option, SAS writes a message to the log displaying the query with the references expanded.","title":"FEEDBACK option"},{"location":"sql/advanced/#using-a-query-to-generate-macro-values","text":"You can use a PROC SQL query to generate values that are sent to the symbol table for later use. 1 2 3 4 SELECT column - 1 < , ... column - n > INTO : macro - variable - 1 < , ... : macro - variable - n > FROM table | view < additional clauses > ; PROC SQL creates macro variables or updates existing macro variables using an INTO clause. The INTO clause is located between the SELECT clause and the FROM clause. It cannot be used in a CREATE TABLE or CREATE VIEW statement. You list the names of the macro variables to be created in the INTO clause. Each macro variable name is preceded with a colon. PROC SQL generates the values that are assigned to these variables by executing the query.","title":"Using a Query to Generate Macro Values"},{"location":"sql/advanced/#creating-a-single-macro-variable","text":"This syntax of the INTO clause places values from the first row returned by an SQL query into a macro variable. 1 2 3 4 SELECT column - 1 < , ... column - n > INTO : macro - variable - 1 < , ... : macro - variable - n > FROM table | view < additional clauses > ; The value from the first column in the SELECT list is placed in the first macro variable listed in the INTO clause. The second column in the SELECT list is placed in the second macro, and so on. When storing a single value into a macro variable, PROC SQL preserves leading or trailing blanks. If the macro variable already exists, the INTO clause replaces the existing value with a new value from the SELECT clause. Data from additional rows returned by the query is ignored. This method is used most often with queries that return only one row. Example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 %let Dept = Sales ; proc sql noprint ; select avg ( Salary ) into : MeanSalary from orion . employee_payroll as p , orion . employee_organization as o where p . Employee_ID = o . Employee_ID and Department = propcase ( \"&Dept\" ); reset print number ; title \"&Dept Department Employees Earning\" ; title2 \"More Than The Department Average \" \"Of &MeanSalary\" ; select p . Employee_ID , Salary from orion . employee_payroll as p , orion . employee_organization as o where p . Employee_ID = o . Employee_ID and Department = Propcase ( \"&Dept\" ) and Salary > & MeanSalary ; quit ; title ;","title":"Creating a Single Macro Variable"},{"location":"sql/advanced/#creating-multiple-macro-variables","text":"You can use the same syntax of the INTO clause to create multiple macro variables from a single row. To do this, you separate each name with a comma in the INTO clause. 1 2 3 4 SELECT column - 1 INTO : macro - variable - 1 < , ... : macro - variable - n > FROM table | view < additional clauses > ; Remember that macro variables can hold only character values. So, numeric values are converted to character values by using the BEST8. format and are right aligned. Example: 1 2 3 4 5 6 7 proc sql noprint ; select avg ( Salary ), min ( Salary ), max ( Salary ) into : MeanSalary , : MinSalary , : MaxSalary from orion . employee_payroll ; %put Mean : & MeanSalary Min : & MinSalary Max : & MaxSalary ; quit ;","title":"Creating Multiple Macro Variables"},{"location":"sql/advanced/#second-syntax-form","text":"1 2 3 4 5 SELECT column - a < , column - b , ... > INTO : macro - variable - a_1 - : macro - variable - a_n < , : macro - variable - b_1 - : macro - variable - b_n > FROM table - 1 | view - 1 < table - x | view - x > < additional clauses > ;","title":"Second Syntax form"},{"location":"sql/advanced/#third-syntax-form","text":"1 2 3 4 5 SELECT column - 1 < , column - 2 , ... > INTO : macro - variable - 1 SEPARATED BY 'delimiter' < , : macro - variable - 2 SEPARATED BY 'delimiter' > FROM table - 1 | view - 1 < , ... table - x | view - x > < additional clauses > ;","title":"Third Syntax form"},{"location":"sql/displaying-results/","text":"Presenting Data Link Ordering Rows Link 1 2 3 4 5 SELECT object - item < , ... object - item > FROM from - list < WHERE sql - expression > < ORDER BY order - by - item < DESC > < ,... order - by - item < DESC >> ; The SQL processor determines the order in which PROC SQL writes the rows to the output. The processor might not output the rows in the same order as they are encountered in the table. To guarantee the order of the rows in the output, you must use an ORDER BY clause. When you use an ORDER BY clause, you change the order of the output but not the order of the rows that are stored in the table. By default, PROC SQL sorts in ascending order from the lowest value to the highest. To sort in descending order, you specify the keyword DESC following the column name that you want to sort in descending order. The ORDER BY clause treats missing values as the smallest possible value, regardless of whether the values are character or numeric. So, when you sort in ascending order, missing values appear first in query results. When you specify multiple columns in the ORDER BY clause, PROC SQL sorts the rows by the values of the first column. The values of the following columns represent secondary sorts and PROC SQL sorts the rows that have the same value for the primary sort by using the secondary value. Example: 1 2 3 4 5 6 7 proc sql ; select Employee_ID , max ( Qtr1 , Qtr2 , Qtr3 , Qtr4 ) from orion . employee_donations where Paid_By = \"Cash or Check\" order by 2 desc , Employee_ID ; quit ; Specifying Labels and Formats Link 1 'label' By default, PROC SQL formats output by using column attributes that are already saved in the table or, if there are no permanent attributes, by using the default attributes. However, you can use the ANSI standard column modifier to create a label that is displayed as the heading for the column in the output. The label can be stored permanently when creating or altering a table. The text can be up to 256 characters and must be enclosed in quotation marks. 1 LABEL = 'label' You can take advantage of SAS enhancements, such as the LABEL= column modifier. You can specify the LABEL= column modifier after any column name or expression that is specified in the SELECT clause. You can specify up to 256 characters for the text. Generally, using the SAS method makes your code easier to read and follow. 1 FORMAT = formatw . d To make data values easier to read in output, you can use the FORMAT= column modifier to associate formats with column values. The FORMAT= column modifier is also a SAS enhancement. A format is an instruction that SAS uses to write data values. Formats specified in the SELECT clause affect only how the data values appear in the output, not how the actual data values are stored in the table. Example: 1 2 3 4 5 6 7 8 proc sql ; select Employee_ID 'Employee ID' , max ( Qtr1 , Qtr2 , Qtr3 , Qtr4 ) label = 'Maximum' format = dollar5 . from orion . employee_donations where Paid_By = \"Cash or Check\" order by 2 desc , Employee_ID ; quit ; Adding Titles, Footnotes, and Constant Text Link 1 2 TITLE < n > 'text' ; FOOTNOTE < n > 'text' ; Titles appear at the top and footnotes appear at the bottom of each page of SAS output. Titles and footnotes appear in both HTML output and listing output. In HTML output, titles appear at the top and footnotes appear at the bottom of the output no matter how long the output is. Listing output is divided into pages of a specified size, and your titles and footnotes appear at the top and bottom of each page, respectively. TITLE and FOOTNOTE statements remain in effect until they are redefined or cancelled. Redefining a title or footnote line with the same or smaller number cancels any highernumbered lines. To cancel all previous titles or footnotes, specify a null TITLE or FOOTNOTE statement (no line number and no text following the keyword). In listing and HTML output, any lines for which you do not specify a title appear blank. While the TITLE statement with the largest number appears on the last title line, footnote lines are \"pushed up\" from the bottom. The FOOTNOTE statement with the smallest number appears on top and the footnote statement with the largest number appears on the bottom line. 1 2 3 SELECT object - item < , ... object - item > 'constant text' < AS alias > < 'column label' > FROM from - list ; Remember that a constant, sometimes called a literal, is a number or character string that indicates a fixed value. The SELECT clause can use constants as expressions. Example: 1 2 3 4 5 6 7 8 9 10 proc sql ; title ' Annual Bonuses for Active Employees ' ; select Employee_ID label = ' Employee Number ' , ' Bonus is: ' , Salary * . 05 format = comma12 . 2 from orion . employee_payroll where Employee_Term_Date is missing order by Salary desc ; quit ; title ; Controlling PROC SQL Output Link 1 PROC SQL < option ( s ) > ; Options that are included in the PROC SQL statement are temporary and apply only to that PROC SQL step. If multiple queries are specified, PROC SQL applies the options to all of the statements in the step. After you specify an option, it remains in effect until SAS encounters the beginning of another PROC SQL step or or you change the option. 1 RESET < option ( s ) > ; The RESET statement enables you to add, drop, or change the options in the PROC SQL step without restarting the procedure. The RESET statement is useful to change options and add additional options not previously listed in the PROC SQL statement. 1 OUTOBS = n The OUTOBS= option restricts the number of rows that a query outputs to a report or writes to a table. 1 NONUMBER | NUMBER The NUMBER or NONUMBER option controls whether the SELECT statement includes a column named ROW , which displays row numbers as the first column of query output.This option has no effect on the underlying table. The NONUMBER option is the default setting. 1 NODOUBLE | DOUBLE The DOUBLE or NODOUBLE option defines the spacing between lines for an output destination that has a physical page limitation. The DOUBLE option double-spaces the report, which places a blank line between the rows. The NODOUBLE option single-spaces the report and is the default setting. The DOUBLE|NODOUBLE option primarily affects the LISTING destination and has no effect on other output destinations such as PDF , RTF , and HTML . 1 NOFLOW | FLOW <= n < m >> The FLOW or NOFLOW option defines whether character columns with long values wrap, or flow, within the column or the row wraps around to additional lines of output to display all columns. The FLOW option causes text to wrap or flow within column limitations rather than wrapping an entire row. The NOFLOW option is the default setting. The FLOW option affects output destinations with a physical page limitation. Example: 1 2 3 4 5 6 proc sql flow = 5 20 double number outobs = 10 ; title \"Sample Report\" ; select * from orion . employee_organization ; quit ; title ; Producing Summary Statistics Link Understanding Summary Functions Link Using PROC SQL , you can summarize data in a variety of ways: across rows, down a column across an entire table, or down a column by groups of rows. To summarize data, your query must create one or more summary columns that appear in the output. To calculate summary columns in your query output, you add summary functions to expressions in the SELECT clause. Summary functions are also called aggregate functions. Summary functions reduce all the values in each row or column in a table to one summarizing or aggregate value. Some of the most commonly used summary functions are shown below: ANSI SAS Returned Value AVG MEAN Mean (average) value COUNT , FREQ N Number of nonmissing values MAX MAX Largest value MIN MIN Smallest nonmissing value SUM SUM Sum of nonmissing values NMISS Number of missing values STD Standard deviation VAR Variance SAS and ANSI summary functions do not work exactly the same way. The main difference is in the number of arguments that each can accept. An argument of a summary function is often a column name, although there are other types of arguments as well. 1 2 SUM ( argument ) SUM ( argument - 1 < , argument - n > ) Both ANSI functions and SAS summary functions can accept a single argument. If a summary function specifies only one column as an argument, the summary function calculates a single summary value for that column, using the values from one or more rows. SAS summary functions can accept multiple arguments, but ANSI summary functions cannot. If a SAS summary function specifies multiple columns as arguments, the summary function calculates the statistic across each row, using the values in the listed columns. ANSI summary functions cannot summarize across rows. When you specify one/multiple argument/s for a summary function that has the same ANSI and SAS name, PROC SQL executes the ANSI summary function. If you specify multiple arguments for an ANSI summary function, PROC SQL does the following: If a SAS function has the specified name, PROC SQL runs it. If no SAS function exists, PROC SQL generates an error. Example: Summarizing Across a Row 1 2 3 4 5 6 7 proc sql ; select Employee_ID label = 'Employee ID' , Qtr1 , Qtr2 , Qtr3 , Qtr4 , sum ( Qtr1 , Qtr2 , Qtr3 , Qtr4 ) from orion . employee_donations ; quit ; Example: Summarizing Down a Column 1 2 3 4 5 proc sql ; select sum ( Qtr1 ) 'Total Quarter 1 Donations' from orion . employee_donations ; quit ; Example: Calculating Multiple Summary Columns 1 2 3 4 5 6 7 proc sql ; select sum ( Qtr1 ) 'Total Quarter 1 Donations' , sum ( Qtr2 ) 'Total Quarter 2 Donations' from orion . employee_donations ; quit ; Example: Counting the Number of Rows That Have a Missing Value 1 2 3 4 5 proc sql ; select Employee_ID from orion . employee_information where Employee_Term_Date is missing ; quit ; Example: Combining Summarized and Non-Summarized Columns 1 2 3 4 5 proq sql ; select Employee_Gender as Gender , avg ( Salary ) as Average from orion . employee_information where Employee_Term_Date is missing ; quit ; Producing Summary Statistics Link 1 2 3 COUNT ( argument ) FREQ ( argument ) N ( argument ) To count the number of rows in a table or in a subset, you can use the ANSI COUNT function, the ANSI FREQ function, or the SAS N function. The COUNT function argument can be either a column name or an asterisk: If you specify a column name, the COUNT function counts the number of rows in the table or in a subset of rows that have a nonmissing value for that column If you specify an asterisk, the COUNT function returns the total number of rows in a table or in a group of rows The FREQ and N functions cannot accept an asterisk as an argument Calculating Summary Statistics for Groups of Data Link PROC SQL processes non-summarized columns and summarized columns differently: For non-summarized columns, PROC SQL generates one row of output for each row that the query processes For summarized columns, PROC SQL reduces multiple input rows to a single row of output When the SELECT clause list contains at least one column that a summary function creates and a column that is not summarized, and the query has no GROUP BY clause to group the output data by the non-summarized column, PROC SQL remerges the data by default. Remerging data requires two passes through the table, which takes additional processing time. Here is a simplified description of the process of remerging: In the first pass through the data, PROC SQL calculates the value of any summary functions for the subset of rows specified in the WHERE clause, and then returns a single value for each summary column. In the second pass, PROC SQL selects any non-summarized column values from the rows specified in the WHERE clause. PROC SQL then appends the summary column values to each row to create the output. There is one type of non-summarized column that you can combine with a summarized column in the SELECT clause to generate one row of output: a column that contains a constant value. 1 2 OPTIONS SQLREMERGE | NOSQLREMERGE ; PROC SQL REMERGE | NOREMERGE ; Although SAS remerges summary statistics by default , most database management systems do not. Instead, these other systems generate an error. To prevent SAS from remerging summary statistics in every PROC SQL step in the current session, you can specify the SAS system option NOSQLREMERGE in the OPTIONS statement. Remember that SAS system options remain in effect until you change them or until the end of your session. If you want to prevent SAS from remerging summary statistics in individual queries or individual PROC SQL steps, you can specify the NOREMERGE option in the PROC SQL statement instead of using the SAS system option. Remember that PROC SQL options remain in effect until you change or reset them, or until PROC SQL reaches a step boundary, such as a QUIT statement. Example: Using Remerged Summary Statistics 1 2 3 4 5 6 7 8 9 10 11 proc sql ; title \"Male Employee Salaries\" ; select Employee_ID , Salary format = comma12 ., Salary / sum ( Salary ) 'PCT of Total' format = percent6 . 2 from orion . employee_payroll where Employee_Gender = 'M' and Employee_Term_Date is missing order by 3 desc ; quit ; title ; Grouping Data Link 1 2 3 4 5 6 SELECT object - item < , ... object - item > FROM from - list < WHERE sql - expression > < GROUP BY object - item < , ... object - item >> < ORDER BY order - by - item < DESC > < ,... order - by - item < DESC >> ; To summarize data by groups, you use a GROUP BY clause. In the GROUP BY clause, you specify one or more columns that you want to use to categorize the data for summarizing. As in the ORDER BY clause, you can specify a column name, a column alias, or an expression. You separate multiple columns with commas. In the GROUP BY clause, you can specify columns that are calculated in the SELECT clause, except for columns that are created by a summary function . Example: Grouping Data by Using the GROUP BY Clause 1 2 3 4 5 6 7 8 9 proc sql ; select Employee_Gender as Gender , Marital_Status as M_Status , avg ( Salary ) as Average from orion . employee_information where Employee_Term_Date is missing group by Employee_Gender , Marital_Status ; quit ; To select a set of rows before the query processes them, you use a WHERE clause. You cannot use a WHERE clause to subset grouped rows by referring to a summary column that is calculated in the SELECT clause. In a WHERE clause, you cannot use summary functions that specify a single argument. 1 2 3 4 5 6 7 SELECT object - item < , ... object - item > FROM from - list < WHERE sql - expression > < GROUP BY object - item < , ... object - item >> < HAVING sql - expression > < ORDER BY order - by - item < DESC > < ,... order - by - item < DESC >> ; To select groups to appear in the output, you use the HAVING clause. PROC SQL processes the HAVING clause after grouping rows. Following the keyword HAVING is an expression that PROC SQL uses to subset the grouped rows that appear in the output. Expressions in the HAVING clause follow most of the same rules as expressions in the WHERE clause. Unlike the WHERE clause, the HAVING clause can refer to the following: the column alias of a calculated column without using the keyword CALCULATED the column alias of a column that was created by a summary function with a single argument Unlike the GROUP BY clause, the HAVING clause can use an expression that contains a summary function or that references a column that a summary function created. Example: Selecting Groups with the HAVING Clause 1 2 3 4 5 6 proc sql ; select Department , count ( * ) as Count from orion . employee_information group by Department having Count ge 25 ; quit ; 1 FIND ( string , substring < , modifier ( s ) > < , startpos > ) The FIND function searches for a specific substring of characters within a character string and then performs one of the following actions: * If the FIND function finds all of the characters in the specified substring, the function returns an integer that represents the starting position of the substring within the string. * If the FIND function does not find the substring, the function returns a value of 0 . The FIND function has four arguments, which are separated by commas. The first two arguments \u2013 string and substring \u2013 are required , and the other two are optional. The arguments are described below: string is what the function searches to locate the first occurrence of the substring. string can be a constant, a variable, or an expression that resolves to a character string. substring is what you're looking for. Like the string , the substring can be a constant, a variable, or an expression that resolves to a character string. There are two modifiers , and you can specify one or both of them: The modifier i tells the FIND function to ignore case when searching for a substring. The modifier t tells the FIND function to trim any trailing blanks from both the string and the substring before searching. If you specify both modifiers, you enclose them in a single set of quotation marks. The modifiers are not case sensitive. startpos is an integer that specifies both a starting position for the search and the direction of the search. A positive integer causes the FIND function to search from left to right. A negative integer causes the FIND function to search from right to left. By default, if you do not specify startpos , the FIND function starts at the first position in the string and searches from left to right. Example: Identifying Rows That Meet a Criterion by Using the FIND Function 1 2 3 4 5 6 proc sql ; select Department , Job_Title , find ( Job_Title , \"manager\" , \"i\" ) as Position from orion . employee_organization ; quit ; Tip Boolean expressions are expressions that evaluate to one of two values: * TRUE (or 1) * FALSE (or 0) Example: Counting Rows by Using Boolean Expressions 1 2 3 4 5 6 7 8 9 proc sql ; select Department , sum ( find ( Job_Title , \"manager\" , \"i\" ) > 0 ) as Managers , sum ( find ( Job_Title , \"manager\" , \"i\" ) = 0 ) as Employees from orion . employee_information group by Department ; quit ;","title":"Displaying Query Results"},{"location":"sql/displaying-results/#presenting-data","text":"","title":"Presenting Data"},{"location":"sql/displaying-results/#ordering-rows","text":"1 2 3 4 5 SELECT object - item < , ... object - item > FROM from - list < WHERE sql - expression > < ORDER BY order - by - item < DESC > < ,... order - by - item < DESC >> ; The SQL processor determines the order in which PROC SQL writes the rows to the output. The processor might not output the rows in the same order as they are encountered in the table. To guarantee the order of the rows in the output, you must use an ORDER BY clause. When you use an ORDER BY clause, you change the order of the output but not the order of the rows that are stored in the table. By default, PROC SQL sorts in ascending order from the lowest value to the highest. To sort in descending order, you specify the keyword DESC following the column name that you want to sort in descending order. The ORDER BY clause treats missing values as the smallest possible value, regardless of whether the values are character or numeric. So, when you sort in ascending order, missing values appear first in query results. When you specify multiple columns in the ORDER BY clause, PROC SQL sorts the rows by the values of the first column. The values of the following columns represent secondary sorts and PROC SQL sorts the rows that have the same value for the primary sort by using the secondary value. Example: 1 2 3 4 5 6 7 proc sql ; select Employee_ID , max ( Qtr1 , Qtr2 , Qtr3 , Qtr4 ) from orion . employee_donations where Paid_By = \"Cash or Check\" order by 2 desc , Employee_ID ; quit ;","title":"Ordering Rows"},{"location":"sql/displaying-results/#specifying-labels-and-formats","text":"1 'label' By default, PROC SQL formats output by using column attributes that are already saved in the table or, if there are no permanent attributes, by using the default attributes. However, you can use the ANSI standard column modifier to create a label that is displayed as the heading for the column in the output. The label can be stored permanently when creating or altering a table. The text can be up to 256 characters and must be enclosed in quotation marks. 1 LABEL = 'label' You can take advantage of SAS enhancements, such as the LABEL= column modifier. You can specify the LABEL= column modifier after any column name or expression that is specified in the SELECT clause. You can specify up to 256 characters for the text. Generally, using the SAS method makes your code easier to read and follow. 1 FORMAT = formatw . d To make data values easier to read in output, you can use the FORMAT= column modifier to associate formats with column values. The FORMAT= column modifier is also a SAS enhancement. A format is an instruction that SAS uses to write data values. Formats specified in the SELECT clause affect only how the data values appear in the output, not how the actual data values are stored in the table. Example: 1 2 3 4 5 6 7 8 proc sql ; select Employee_ID 'Employee ID' , max ( Qtr1 , Qtr2 , Qtr3 , Qtr4 ) label = 'Maximum' format = dollar5 . from orion . employee_donations where Paid_By = \"Cash or Check\" order by 2 desc , Employee_ID ; quit ;","title":"Specifying Labels and Formats"},{"location":"sql/displaying-results/#adding-titles-footnotes-and-constant-text","text":"1 2 TITLE < n > 'text' ; FOOTNOTE < n > 'text' ; Titles appear at the top and footnotes appear at the bottom of each page of SAS output. Titles and footnotes appear in both HTML output and listing output. In HTML output, titles appear at the top and footnotes appear at the bottom of the output no matter how long the output is. Listing output is divided into pages of a specified size, and your titles and footnotes appear at the top and bottom of each page, respectively. TITLE and FOOTNOTE statements remain in effect until they are redefined or cancelled. Redefining a title or footnote line with the same or smaller number cancels any highernumbered lines. To cancel all previous titles or footnotes, specify a null TITLE or FOOTNOTE statement (no line number and no text following the keyword). In listing and HTML output, any lines for which you do not specify a title appear blank. While the TITLE statement with the largest number appears on the last title line, footnote lines are \"pushed up\" from the bottom. The FOOTNOTE statement with the smallest number appears on top and the footnote statement with the largest number appears on the bottom line. 1 2 3 SELECT object - item < , ... object - item > 'constant text' < AS alias > < 'column label' > FROM from - list ; Remember that a constant, sometimes called a literal, is a number or character string that indicates a fixed value. The SELECT clause can use constants as expressions. Example: 1 2 3 4 5 6 7 8 9 10 proc sql ; title ' Annual Bonuses for Active Employees ' ; select Employee_ID label = ' Employee Number ' , ' Bonus is: ' , Salary * . 05 format = comma12 . 2 from orion . employee_payroll where Employee_Term_Date is missing order by Salary desc ; quit ; title ;","title":"Adding Titles, Footnotes, and Constant Text"},{"location":"sql/displaying-results/#controlling-proc-sql-output","text":"1 PROC SQL < option ( s ) > ; Options that are included in the PROC SQL statement are temporary and apply only to that PROC SQL step. If multiple queries are specified, PROC SQL applies the options to all of the statements in the step. After you specify an option, it remains in effect until SAS encounters the beginning of another PROC SQL step or or you change the option. 1 RESET < option ( s ) > ; The RESET statement enables you to add, drop, or change the options in the PROC SQL step without restarting the procedure. The RESET statement is useful to change options and add additional options not previously listed in the PROC SQL statement. 1 OUTOBS = n The OUTOBS= option restricts the number of rows that a query outputs to a report or writes to a table. 1 NONUMBER | NUMBER The NUMBER or NONUMBER option controls whether the SELECT statement includes a column named ROW , which displays row numbers as the first column of query output.This option has no effect on the underlying table. The NONUMBER option is the default setting. 1 NODOUBLE | DOUBLE The DOUBLE or NODOUBLE option defines the spacing between lines for an output destination that has a physical page limitation. The DOUBLE option double-spaces the report, which places a blank line between the rows. The NODOUBLE option single-spaces the report and is the default setting. The DOUBLE|NODOUBLE option primarily affects the LISTING destination and has no effect on other output destinations such as PDF , RTF , and HTML . 1 NOFLOW | FLOW <= n < m >> The FLOW or NOFLOW option defines whether character columns with long values wrap, or flow, within the column or the row wraps around to additional lines of output to display all columns. The FLOW option causes text to wrap or flow within column limitations rather than wrapping an entire row. The NOFLOW option is the default setting. The FLOW option affects output destinations with a physical page limitation. Example: 1 2 3 4 5 6 proc sql flow = 5 20 double number outobs = 10 ; title \"Sample Report\" ; select * from orion . employee_organization ; quit ; title ;","title":"Controlling PROC SQL Output"},{"location":"sql/displaying-results/#producing-summary-statistics","text":"","title":"Producing Summary Statistics"},{"location":"sql/displaying-results/#understanding-summary-functions","text":"Using PROC SQL , you can summarize data in a variety of ways: across rows, down a column across an entire table, or down a column by groups of rows. To summarize data, your query must create one or more summary columns that appear in the output. To calculate summary columns in your query output, you add summary functions to expressions in the SELECT clause. Summary functions are also called aggregate functions. Summary functions reduce all the values in each row or column in a table to one summarizing or aggregate value. Some of the most commonly used summary functions are shown below: ANSI SAS Returned Value AVG MEAN Mean (average) value COUNT , FREQ N Number of nonmissing values MAX MAX Largest value MIN MIN Smallest nonmissing value SUM SUM Sum of nonmissing values NMISS Number of missing values STD Standard deviation VAR Variance SAS and ANSI summary functions do not work exactly the same way. The main difference is in the number of arguments that each can accept. An argument of a summary function is often a column name, although there are other types of arguments as well. 1 2 SUM ( argument ) SUM ( argument - 1 < , argument - n > ) Both ANSI functions and SAS summary functions can accept a single argument. If a summary function specifies only one column as an argument, the summary function calculates a single summary value for that column, using the values from one or more rows. SAS summary functions can accept multiple arguments, but ANSI summary functions cannot. If a SAS summary function specifies multiple columns as arguments, the summary function calculates the statistic across each row, using the values in the listed columns. ANSI summary functions cannot summarize across rows. When you specify one/multiple argument/s for a summary function that has the same ANSI and SAS name, PROC SQL executes the ANSI summary function. If you specify multiple arguments for an ANSI summary function, PROC SQL does the following: If a SAS function has the specified name, PROC SQL runs it. If no SAS function exists, PROC SQL generates an error. Example: Summarizing Across a Row 1 2 3 4 5 6 7 proc sql ; select Employee_ID label = 'Employee ID' , Qtr1 , Qtr2 , Qtr3 , Qtr4 , sum ( Qtr1 , Qtr2 , Qtr3 , Qtr4 ) from orion . employee_donations ; quit ; Example: Summarizing Down a Column 1 2 3 4 5 proc sql ; select sum ( Qtr1 ) 'Total Quarter 1 Donations' from orion . employee_donations ; quit ; Example: Calculating Multiple Summary Columns 1 2 3 4 5 6 7 proc sql ; select sum ( Qtr1 ) 'Total Quarter 1 Donations' , sum ( Qtr2 ) 'Total Quarter 2 Donations' from orion . employee_donations ; quit ; Example: Counting the Number of Rows That Have a Missing Value 1 2 3 4 5 proc sql ; select Employee_ID from orion . employee_information where Employee_Term_Date is missing ; quit ; Example: Combining Summarized and Non-Summarized Columns 1 2 3 4 5 proq sql ; select Employee_Gender as Gender , avg ( Salary ) as Average from orion . employee_information where Employee_Term_Date is missing ; quit ;","title":"Understanding Summary Functions"},{"location":"sql/displaying-results/#producing-summary-statistics_1","text":"1 2 3 COUNT ( argument ) FREQ ( argument ) N ( argument ) To count the number of rows in a table or in a subset, you can use the ANSI COUNT function, the ANSI FREQ function, or the SAS N function. The COUNT function argument can be either a column name or an asterisk: If you specify a column name, the COUNT function counts the number of rows in the table or in a subset of rows that have a nonmissing value for that column If you specify an asterisk, the COUNT function returns the total number of rows in a table or in a group of rows The FREQ and N functions cannot accept an asterisk as an argument","title":"Producing Summary Statistics"},{"location":"sql/displaying-results/#calculating-summary-statistics-for-groups-of-data","text":"PROC SQL processes non-summarized columns and summarized columns differently: For non-summarized columns, PROC SQL generates one row of output for each row that the query processes For summarized columns, PROC SQL reduces multiple input rows to a single row of output When the SELECT clause list contains at least one column that a summary function creates and a column that is not summarized, and the query has no GROUP BY clause to group the output data by the non-summarized column, PROC SQL remerges the data by default. Remerging data requires two passes through the table, which takes additional processing time. Here is a simplified description of the process of remerging: In the first pass through the data, PROC SQL calculates the value of any summary functions for the subset of rows specified in the WHERE clause, and then returns a single value for each summary column. In the second pass, PROC SQL selects any non-summarized column values from the rows specified in the WHERE clause. PROC SQL then appends the summary column values to each row to create the output. There is one type of non-summarized column that you can combine with a summarized column in the SELECT clause to generate one row of output: a column that contains a constant value. 1 2 OPTIONS SQLREMERGE | NOSQLREMERGE ; PROC SQL REMERGE | NOREMERGE ; Although SAS remerges summary statistics by default , most database management systems do not. Instead, these other systems generate an error. To prevent SAS from remerging summary statistics in every PROC SQL step in the current session, you can specify the SAS system option NOSQLREMERGE in the OPTIONS statement. Remember that SAS system options remain in effect until you change them or until the end of your session. If you want to prevent SAS from remerging summary statistics in individual queries or individual PROC SQL steps, you can specify the NOREMERGE option in the PROC SQL statement instead of using the SAS system option. Remember that PROC SQL options remain in effect until you change or reset them, or until PROC SQL reaches a step boundary, such as a QUIT statement. Example: Using Remerged Summary Statistics 1 2 3 4 5 6 7 8 9 10 11 proc sql ; title \"Male Employee Salaries\" ; select Employee_ID , Salary format = comma12 ., Salary / sum ( Salary ) 'PCT of Total' format = percent6 . 2 from orion . employee_payroll where Employee_Gender = 'M' and Employee_Term_Date is missing order by 3 desc ; quit ; title ;","title":"Calculating Summary Statistics for Groups of Data"},{"location":"sql/displaying-results/#grouping-data","text":"1 2 3 4 5 6 SELECT object - item < , ... object - item > FROM from - list < WHERE sql - expression > < GROUP BY object - item < , ... object - item >> < ORDER BY order - by - item < DESC > < ,... order - by - item < DESC >> ; To summarize data by groups, you use a GROUP BY clause. In the GROUP BY clause, you specify one or more columns that you want to use to categorize the data for summarizing. As in the ORDER BY clause, you can specify a column name, a column alias, or an expression. You separate multiple columns with commas. In the GROUP BY clause, you can specify columns that are calculated in the SELECT clause, except for columns that are created by a summary function . Example: Grouping Data by Using the GROUP BY Clause 1 2 3 4 5 6 7 8 9 proc sql ; select Employee_Gender as Gender , Marital_Status as M_Status , avg ( Salary ) as Average from orion . employee_information where Employee_Term_Date is missing group by Employee_Gender , Marital_Status ; quit ; To select a set of rows before the query processes them, you use a WHERE clause. You cannot use a WHERE clause to subset grouped rows by referring to a summary column that is calculated in the SELECT clause. In a WHERE clause, you cannot use summary functions that specify a single argument. 1 2 3 4 5 6 7 SELECT object - item < , ... object - item > FROM from - list < WHERE sql - expression > < GROUP BY object - item < , ... object - item >> < HAVING sql - expression > < ORDER BY order - by - item < DESC > < ,... order - by - item < DESC >> ; To select groups to appear in the output, you use the HAVING clause. PROC SQL processes the HAVING clause after grouping rows. Following the keyword HAVING is an expression that PROC SQL uses to subset the grouped rows that appear in the output. Expressions in the HAVING clause follow most of the same rules as expressions in the WHERE clause. Unlike the WHERE clause, the HAVING clause can refer to the following: the column alias of a calculated column without using the keyword CALCULATED the column alias of a column that was created by a summary function with a single argument Unlike the GROUP BY clause, the HAVING clause can use an expression that contains a summary function or that references a column that a summary function created. Example: Selecting Groups with the HAVING Clause 1 2 3 4 5 6 proc sql ; select Department , count ( * ) as Count from orion . employee_information group by Department having Count ge 25 ; quit ; 1 FIND ( string , substring < , modifier ( s ) > < , startpos > ) The FIND function searches for a specific substring of characters within a character string and then performs one of the following actions: * If the FIND function finds all of the characters in the specified substring, the function returns an integer that represents the starting position of the substring within the string. * If the FIND function does not find the substring, the function returns a value of 0 . The FIND function has four arguments, which are separated by commas. The first two arguments \u2013 string and substring \u2013 are required , and the other two are optional. The arguments are described below: string is what the function searches to locate the first occurrence of the substring. string can be a constant, a variable, or an expression that resolves to a character string. substring is what you're looking for. Like the string , the substring can be a constant, a variable, or an expression that resolves to a character string. There are two modifiers , and you can specify one or both of them: The modifier i tells the FIND function to ignore case when searching for a substring. The modifier t tells the FIND function to trim any trailing blanks from both the string and the substring before searching. If you specify both modifiers, you enclose them in a single set of quotation marks. The modifiers are not case sensitive. startpos is an integer that specifies both a starting position for the search and the direction of the search. A positive integer causes the FIND function to search from left to right. A negative integer causes the FIND function to search from right to left. By default, if you do not specify startpos , the FIND function starts at the first position in the string and searches from left to right. Example: Identifying Rows That Meet a Criterion by Using the FIND Function 1 2 3 4 5 6 proc sql ; select Department , Job_Title , find ( Job_Title , \"manager\" , \"i\" ) as Position from orion . employee_organization ; quit ; Tip Boolean expressions are expressions that evaluate to one of two values: * TRUE (or 1) * FALSE (or 0) Example: Counting Rows by Using Boolean Expressions 1 2 3 4 5 6 7 8 9 proc sql ; select Department , sum ( find ( Job_Title , \"manager\" , \"i\" ) > 0 ) as Managers , sum ( find ( Job_Title , \"manager\" , \"i\" ) = 0 ) as Employees from orion . employee_information group by Department ; quit ;","title":"Grouping Data"},{"location":"sql/efficiencies/","text":"Overview of Computing Resources Link You should consider various programming techniques based on their ability to conserve specific resources, such as CPU time, I/O, and memory. In some cases, data storage space or network bandwidth might be an issue. The following table shows the resources that SAS programs require: Term Definition CPU The amount of time that the central processing unit, or CPU, uses to perform tasks such as calculations, reading and writing data, conditional logic, and iterative logic I/O A measurement of the Read and Write operations that the computer performs as it copies data and programs from a storage device to memory (input and output) memory The size of the work area in volatile memory the computer requires for holding executable program modules, data, and buffers data storage space Physical space on mass storage devices network bandwidth The available throughput for data communications To determine which efficiency trade-offs to make, you'll have to consider your SAS programs, the data that you read and create, and the characteristics of your site. Examples of trade-offs include the following: Saving disk space by compressing data requires more CPU time to uncompress data the next time it's read Reducing I/O by increasing buffer size requires more memory to hold the buffers Assessing Your Site's Efficiency Needs Link No single set of programming techniques is most efficient in all situations. Before you can select the most efficient programming techniques, you need to understand your site's technical environment and resource constraints and then decide for yourself what your critical resources are. Here are some factors to consider: Hardware : In terms of hardware, you can analyze the amount of available memory, the number of CPUs, the number and type of peripheral devices, the communications hardware, network bandwidth, storage capacity, I/O bandwidth, and the capacity to upgrade. System Load : To determine system load, you can look at the number of users or jobs sharing system resources, the expected network traffic, and the predicted increase in load over time. Operating Environment : You should also understand how your operating environment allocates resources, schedules jobs, and performs I/O. SAS Environment : You should know which SAS software products are installed, the number of CPUs and amount of memory allocated for SAS programs, and which methods are available for running SAS programs at your site. It is important to understand your programs and data. The number of times the program is executed affects whether saving more resources is worth the time and effort. As either programs or data increase in size, your potential for savings improves. You should focus on improving the efficiency of large programs, programs that read large amounts of data or programs that read many data sets. Knowing whether your data is mostly character or numeric, whether it contains missing values, and whether and how it is sorted will help you choose programming techniques. Using Benchmarks to Compare Techniques Link To decide which technique is most efficient for a given task, you can benchmark, or measure and compare, the resource usage of each technique. Here are some guidelines for benchmarking SAS programs: Run your SAS programs against the actual data to determine which technique is the most efficient Run your tests under the conditions that your final program will use Here is the benchmarking process and a few more guidelines: Turn on the appropriate SAS options to report resource usage. Test only one technique or change at a time, with as little additional code as possible. You should start at the beginning and make one change at a time to pinpoint resource usage. Run each program three to five times and base your conclusions on averages, not on a single execution. Averaging is particularly important when you benchmark elapsed time. Test each technique in a separate SAS session. SAS can look ahead to the next step; for instance, if two steps are the same type, SAS can determine not to unload and reload the same SAS procedure. When you finish testing, turn off the options that report resource usage, because they consume resources. When you analyze your results, you should exclude outliers. Data from outliers might lead you to tune your program to run less efficiently than it should. Using SAS System Options to Track Resources Link The following options control the resource usage statistics that SAS writes to the log for each SAS step: STIMER : On by default in all operating environments. This option prints the familiar statistics for real time and CPU time in the SAS log after each step. FULLSTIMER (called FULLSTATS in z/OS): More useful than STIMER for benchmarking; writes all available system performance statistics to the SAS log. The statistics shown vary by SAS version and operating environment. STATS and MEMRPT (z/OS only): Used to control the statistics that are printed. STATS controls whether any statistics are listed, and MEMRPT specifies whether memory usage statistics are written to the SAS log. Here's the syntax for specifying options to track resource usage in different operating environments: Windows and UNIX 1 2 OPTIONS STIMER | NOSTIMER ; OPTIONS FULLSTIMER | NOFULLSTIMER ; z/OS 1 2 3 4 OPTIONS FULLSTATS | NOFULLSTATS ; OPTIONS STATS | NOSTATS ; OPTIONS MEMRPT | NOMEMRPT ; STIMER | NOSTIMER ( invocation only )","title":"Measuring Efficiencies"},{"location":"sql/efficiencies/#overview-of-computing-resources","text":"You should consider various programming techniques based on their ability to conserve specific resources, such as CPU time, I/O, and memory. In some cases, data storage space or network bandwidth might be an issue. The following table shows the resources that SAS programs require: Term Definition CPU The amount of time that the central processing unit, or CPU, uses to perform tasks such as calculations, reading and writing data, conditional logic, and iterative logic I/O A measurement of the Read and Write operations that the computer performs as it copies data and programs from a storage device to memory (input and output) memory The size of the work area in volatile memory the computer requires for holding executable program modules, data, and buffers data storage space Physical space on mass storage devices network bandwidth The available throughput for data communications To determine which efficiency trade-offs to make, you'll have to consider your SAS programs, the data that you read and create, and the characteristics of your site. Examples of trade-offs include the following: Saving disk space by compressing data requires more CPU time to uncompress data the next time it's read Reducing I/O by increasing buffer size requires more memory to hold the buffers","title":"Overview of Computing Resources"},{"location":"sql/efficiencies/#assessing-your-sites-efficiency-needs","text":"No single set of programming techniques is most efficient in all situations. Before you can select the most efficient programming techniques, you need to understand your site's technical environment and resource constraints and then decide for yourself what your critical resources are. Here are some factors to consider: Hardware : In terms of hardware, you can analyze the amount of available memory, the number of CPUs, the number and type of peripheral devices, the communications hardware, network bandwidth, storage capacity, I/O bandwidth, and the capacity to upgrade. System Load : To determine system load, you can look at the number of users or jobs sharing system resources, the expected network traffic, and the predicted increase in load over time. Operating Environment : You should also understand how your operating environment allocates resources, schedules jobs, and performs I/O. SAS Environment : You should know which SAS software products are installed, the number of CPUs and amount of memory allocated for SAS programs, and which methods are available for running SAS programs at your site. It is important to understand your programs and data. The number of times the program is executed affects whether saving more resources is worth the time and effort. As either programs or data increase in size, your potential for savings improves. You should focus on improving the efficiency of large programs, programs that read large amounts of data or programs that read many data sets. Knowing whether your data is mostly character or numeric, whether it contains missing values, and whether and how it is sorted will help you choose programming techniques.","title":"Assessing Your Site's Efficiency Needs"},{"location":"sql/efficiencies/#using-benchmarks-to-compare-techniques","text":"To decide which technique is most efficient for a given task, you can benchmark, or measure and compare, the resource usage of each technique. Here are some guidelines for benchmarking SAS programs: Run your SAS programs against the actual data to determine which technique is the most efficient Run your tests under the conditions that your final program will use Here is the benchmarking process and a few more guidelines: Turn on the appropriate SAS options to report resource usage. Test only one technique or change at a time, with as little additional code as possible. You should start at the beginning and make one change at a time to pinpoint resource usage. Run each program three to five times and base your conclusions on averages, not on a single execution. Averaging is particularly important when you benchmark elapsed time. Test each technique in a separate SAS session. SAS can look ahead to the next step; for instance, if two steps are the same type, SAS can determine not to unload and reload the same SAS procedure. When you finish testing, turn off the options that report resource usage, because they consume resources. When you analyze your results, you should exclude outliers. Data from outliers might lead you to tune your program to run less efficiently than it should.","title":"Using Benchmarks to Compare Techniques"},{"location":"sql/efficiencies/#using-sas-system-options-to-track-resources","text":"The following options control the resource usage statistics that SAS writes to the log for each SAS step: STIMER : On by default in all operating environments. This option prints the familiar statistics for real time and CPU time in the SAS log after each step. FULLSTIMER (called FULLSTATS in z/OS): More useful than STIMER for benchmarking; writes all available system performance statistics to the SAS log. The statistics shown vary by SAS version and operating environment. STATS and MEMRPT (z/OS only): Used to control the statistics that are printed. STATS controls whether any statistics are listed, and MEMRPT specifies whether memory usage statistics are written to the SAS log. Here's the syntax for specifying options to track resource usage in different operating environments: Windows and UNIX 1 2 OPTIONS STIMER | NOSTIMER ; OPTIONS FULLSTIMER | NOFULLSTIMER ; z/OS 1 2 3 4 OPTIONS FULLSTATS | NOFULLSTATS ; OPTIONS STATS | NOSTATS ; OPTIONS MEMRPT | NOMEMRPT ; STIMER | NOSTIMER ( invocation only )","title":"Using SAS System Options to Track Resources"},{"location":"sql/getting-started/","text":"Understanding the Basics about PROC SQL Link PROC SQL is a procedure that implements Structured Query Language on which you can take advantage of SAS enhancements such as formats, labels, and functions. It is a tool for retrieving, manipulating and managing data. For example, you can retrieve data by selecting the variables and observations that you want to analyze ( query the data ). PROC SQL complements but does not replace the DATA step. PROC SQL can read two types of SAS data sets \u2013 SAS data files and SAS data views \u2013 as well as database management system (DBMS) tables . SQL and SAS use different terminology, as shown below: SAS Data Processing SQL data set file table observation record row variable field column When you use PROC SQL in your SAS programs, you follow the same basic process that you use to create any SAS program: Define the business need Plan the output Identify the data Write the program Run the program Review the results Debug or modify the program Understanding PROC SQL Syntax Link 1 2 3 PROC SQL < options > ; < additional statement ( s ); > QUIT ; There are some differences between the syntax of a PROC SQL step and the syntax of other PROC steps. To start the SQL procedure, you specify the PROC SQL statement. Following the PROC SQL statement are one or more other statements that perform tasks such as querying data. When SAS executes the PROC SQL statement, the SQL procedure starts to run. SAS executes each statement in the PROC SQL step immediately, so no RUN statement is needed. PROC SQL continues to run until it encounters a step boundary and stops running. At the end of a PROC SQL step, you can specify a QUIT statement as an explicit step boundary. The beginning of another step \u2013 a PROC step or a DATA step \u2013 is also a step boundary. The SELECT statement, also called a query, retrieves data from one or more tables and creates a report that displays the data. It can contain a combination of two to six clauses, which must appear in the order shown below . The first two clauses \u2013 SELECT and FROM \u2013 are the only required clauses. 1 2 3 4 5 6 7 SELECT object - item < , ... object - item > FROM from - list < WHERE sql - expression > < GROUP BY object - item < , ... object - item >> < HAVING sql - expression > < ORDER BY order - by - item < DESC > < , ... order - by - item >> ; The SELECT clause specifies the columns that you want to appear in the output and indicates the order in which you want them to appear. The FROM clause specifies one or more tables that contain the data you need. The WHERE clause selects a subset of rows to be processed. The GROUP BY clause classifies the data into groups. The HAVING clause subsets groups of data. The ORDER BY clause sorts rows by the values of one or more columns. The SELECT statement ends in a semicolon (there is only one semicolon in an entire SELECT statement). When you submit a PROC SQL query, the SQL processor analyzes your query and determines the most efficient way to process it. By default, a report is generated. Checking Query Syntax without Executing the Program Link To check your program syntax efficiently, you can tell SAS to compile a PROC SQL statement or step without executing it. To check PROC SQL syntax without executing a statement or step, you can use the VALIDATE statement or the NOEXEC option. Checking Query Syntax by Using the VALIDATE Statement Link The VALIDATE statement must appear just before a SELECT statement, followed immediately by a query-expression. If your PROC SQL program contains multiple queries, you must specify the VALIDATE statement before each query that you want to validate. 1 VALIDATE query - expression ; Example: 1 2 3 4 5 6 7 8 9 10 proc sql ; validate select Employee_ID , Job_Title from orion . staff ; validate select Employee_ID , Employee_Name , Postal_Code from orion . employee_addresses where Postal_Code contains '33' order by Postal_Code ; quit ; Checking PROC SQL Syntax by Using the NOEXEC Option Link To check the syntax of all statements in your PROC SQL program without executing the program, you can specify the NOEXEC option in the PROC SQL statement. 1 PROC SQL NOEXEC ; Like all PROC SQL options, the NOEXEC option stays in effect until PROC SQL encounters a step boundary or until you issue a RESET statement within the PROC SQL step. Example: 1 2 3 4 5 6 7 proc sql noexec ; select Order_ID , Product_ID from orion . order_fact where Order_Type = 1 ; select Product_ID , Product_Name from orion . product_dim ; quit ;","title":"Getting Started with the SQL Procedure"},{"location":"sql/getting-started/#understanding-the-basics-about-proc-sql","text":"PROC SQL is a procedure that implements Structured Query Language on which you can take advantage of SAS enhancements such as formats, labels, and functions. It is a tool for retrieving, manipulating and managing data. For example, you can retrieve data by selecting the variables and observations that you want to analyze ( query the data ). PROC SQL complements but does not replace the DATA step. PROC SQL can read two types of SAS data sets \u2013 SAS data files and SAS data views \u2013 as well as database management system (DBMS) tables . SQL and SAS use different terminology, as shown below: SAS Data Processing SQL data set file table observation record row variable field column When you use PROC SQL in your SAS programs, you follow the same basic process that you use to create any SAS program: Define the business need Plan the output Identify the data Write the program Run the program Review the results Debug or modify the program","title":"Understanding the Basics about PROC SQL"},{"location":"sql/getting-started/#understanding-proc-sql-syntax","text":"1 2 3 PROC SQL < options > ; < additional statement ( s ); > QUIT ; There are some differences between the syntax of a PROC SQL step and the syntax of other PROC steps. To start the SQL procedure, you specify the PROC SQL statement. Following the PROC SQL statement are one or more other statements that perform tasks such as querying data. When SAS executes the PROC SQL statement, the SQL procedure starts to run. SAS executes each statement in the PROC SQL step immediately, so no RUN statement is needed. PROC SQL continues to run until it encounters a step boundary and stops running. At the end of a PROC SQL step, you can specify a QUIT statement as an explicit step boundary. The beginning of another step \u2013 a PROC step or a DATA step \u2013 is also a step boundary. The SELECT statement, also called a query, retrieves data from one or more tables and creates a report that displays the data. It can contain a combination of two to six clauses, which must appear in the order shown below . The first two clauses \u2013 SELECT and FROM \u2013 are the only required clauses. 1 2 3 4 5 6 7 SELECT object - item < , ... object - item > FROM from - list < WHERE sql - expression > < GROUP BY object - item < , ... object - item >> < HAVING sql - expression > < ORDER BY order - by - item < DESC > < , ... order - by - item >> ; The SELECT clause specifies the columns that you want to appear in the output and indicates the order in which you want them to appear. The FROM clause specifies one or more tables that contain the data you need. The WHERE clause selects a subset of rows to be processed. The GROUP BY clause classifies the data into groups. The HAVING clause subsets groups of data. The ORDER BY clause sorts rows by the values of one or more columns. The SELECT statement ends in a semicolon (there is only one semicolon in an entire SELECT statement). When you submit a PROC SQL query, the SQL processor analyzes your query and determines the most efficient way to process it. By default, a report is generated.","title":"Understanding PROC SQL Syntax"},{"location":"sql/getting-started/#checking-query-syntax-without-executing-the-program","text":"To check your program syntax efficiently, you can tell SAS to compile a PROC SQL statement or step without executing it. To check PROC SQL syntax without executing a statement or step, you can use the VALIDATE statement or the NOEXEC option.","title":"Checking Query Syntax without Executing the Program"},{"location":"sql/getting-started/#checking-query-syntax-by-using-the-validate-statement","text":"The VALIDATE statement must appear just before a SELECT statement, followed immediately by a query-expression. If your PROC SQL program contains multiple queries, you must specify the VALIDATE statement before each query that you want to validate. 1 VALIDATE query - expression ; Example: 1 2 3 4 5 6 7 8 9 10 proc sql ; validate select Employee_ID , Job_Title from orion . staff ; validate select Employee_ID , Employee_Name , Postal_Code from orion . employee_addresses where Postal_Code contains '33' order by Postal_Code ; quit ;","title":"Checking Query Syntax by Using the VALIDATE Statement"},{"location":"sql/getting-started/#checking-proc-sql-syntax-by-using-the-noexec-option","text":"To check the syntax of all statements in your PROC SQL program without executing the program, you can specify the NOEXEC option in the PROC SQL statement. 1 PROC SQL NOEXEC ; Like all PROC SQL options, the NOEXEC option stays in effect until PROC SQL encounters a step boundary or until you issue a RESET statement within the PROC SQL step. Example: 1 2 3 4 5 6 7 proc sql noexec ; select Order_ID , Product_ID from orion . order_fact where Order_Type = 1 ; select Product_ID , Product_Name from orion . product_dim ; quit ;","title":"Checking PROC SQL Syntax by Using the NOEXEC Option"},{"location":"sql/joins/","text":"Understanding SQL Joins Link If you're working with tables that have different columns, you can combine tables horizontally to combine the columns that you want in your output. 1 2 3 SELECT object - item < , ... object - item > FROM from - list < additional clauses > ; To combine tables horizontally, you can use either the SELECT statement in PROC SQL or the MERGE statement in the DATA step. The results of an SQL join and a DATA step merge are similar but not always identical. Whatever method you use, you must also specify criteria for matching and combining the rows across the tables. Inner joins and Outer joins Link PROC SQL supports two types of joins: Inner joins: return a result table that contains all of the matching rows in the input tables. The result of an inner join does not include any nonmatching rows. Outer joins: return a result table that contains all of the matching rows in the input tables and some or all of the nonmatching rows from one or both tables. A full outer join returns all of the matching rows plus the nonmatching rows from both tables. A left outer join returns all of the matching rows plus the nonmatching rows from the first, or left, table. A right outer join returns all of the matching rows plus the nonmatching rows from the second, or right, table. When performing an SQL join, the SQL processor first generates all possible combinations of all of the rows in the tables to be combined. An intermediate result table that contains all possible combinations of rows in the input tables is called a Cartesian product . From the rows in the Cartesian product, the SQL processor can then select the appropriate rows for an inner join or an outer join. Cartesian product Link The number of rows in a Cartesian product is the product of the number of rows in the contributing tables. 1 2 3 SELECT object - item < , ... object - item > FROM table - name , table - name < ,... table - name > ; A Cartesian product is rarely the result that you want when you join tables. To create a Cartesian product, you use the SELECT statement without specifying any conditions for matching rows. Example: Creating a Cartesian Product 1 2 3 4 proc sql ; select * from customers , transactions , inventory ; quit ; Cross Join (Cartesian product) Link You can also use alternative syntax, called cross join syntax, to produce the Cartesian product of two tables. 1 2 SELECT object - item < , ... object - item > FROM table - name - 1 CROSS JOIN table - name - 2 ; In the FROM clause, the table names are separated by the CROSS JOIN keywords instead of by a comma. Example: 1 2 3 4 5 6 PROC SQL ; CREATE TABLE dataset - new AS SELECT * FROM dataset1 CROSS JOIN dataset2 WHERE dataset1 . reference - variable = dataset2 . reference - variable ; quit ; Working with Inner Joins Link Performing an Inner Join Link Inner join returns only the matching rows. Another way to say this is that an inner join returns rows that meet certain join conditions that you specify. 1 2 3 4 5 SELECT object - item < , ... object - item > FROM table - name , table - name WHERE join - condition ( s ) < AND sql - expression > < additional clauses > ; The syntax for an inner join requires a WHERE clause in addition to the SELECT and FROM clauses. In the WHERE clause, one or more join conditions specify the combined rows that the query should return. Example: 1 2 3 4 5 6 7 8 9 10 11 proc sql ; select * from customers , transactions where customers . ID = transactions . ID ; quit ; proc sql ; select c . ID , Name , Action , Amount from customers as c , transactions as t where c . ID = t . ID ; quit ; The join conditions are specified as expressions. The WHERE clause can also contain one or more additional expressions that subset the rows in additional ways. The column or columns that are specified in join conditions are referred to as join keys. Using PROC SQL , you can also specify join keys that have different names. However, by default, a DATA step merge requires the join keys to have the same name in both tables. PROC SQL can join tables that are not sorted on the join keys, unlike a DATA step merge. When you refer to columns in multiple tables that have the same name, you must qualify the column names to specify the location of each column. Qualified column names consist of the table name (a table qualifier), a period, and then the column name. A join on an equal condition is called an equijoin . Using PROC SQL , you can also join tables on an inequality. However, DATA step merges can only join tables on an equality. Conceptually, when PROC SQL processes an inner join, the SQL processor performs two main steps: Builds the Cartesian product of all tables listed in the FROM clause Eliminates the rows that don't meet the join conditions A DATA step merge does not create a Cartesian product. Unlike DATA step merges, SQL joins do not overlay columns that have the same name. By default, the result set contains all of the columns that have the same name. To display columns that are the same as, or derived from, columns in multiple tables, you must use a join instead of a subquery. By default, a DATA step merge builds a result set in the order of the join key values. PROC SQL does not order the rows in query results by default. To guarantee the order of the rows, you must use an ORDER BY clause. 1 2 3 4 5 SELECT object - item < , ... object - item > FROM table - name < AS > alias - 1 table - name - 2 < AS > alias - 2 WHERE join - condition ( s ) < additional clauses > ; You can simplify the syntax of your SQL joins by assigning aliases to tables in the FROM clause. Example: 1 2 3 SELECT c . ID , Name , Action , Amount FROM customers as c , transactions as t WHERE c . ID = t . ID ; After the table name, you can optionally specify the keyword AS and then the alias. The alias must follow the standard rules for SAS names. In other clauses, you can reference each table by its alias instead of by its full name. Using Alternative Syntax for an Inner Join Link You can also perform an inner join by using the alternative syntax shown here. 1 2 3 4 5 6 7 SELECT object - item < , ... object - item > FROM table - name - 1 << AS > alias - 1 > INNER JOIN table - name - 2 << AS > alias - 2 > ON join - condition ( s ) WHERE sql - expression < additional clauses > ; In the alternate syntax for an inner join, the FROM clause uses the INNER JOIN keywords to join two tables. The alternate syntax uses the ON clause to specify the join conditions. The alternate syntax allows additional clauses, so you can still use the WHERE clause to specify any additional subsetting conditions. Example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 proc sql ; select c . ID , Name , Action , Amount from customers as c inner join transactions as t on c . ID = t . ID ; quit ; proc sql ; select c . ID , Name , Action , Amount from customers as c inner join transactions as t on c . ID = t . ID ; quit ; Working with Outer Joins Link 1 2 3 4 5 6 SELECT object - item < , ... object - item > FROM table - name << AS > alias > LEFT | RIGHT | FULL JOIN table - name << AS > alias > ON join - condition ( s ) < additional clauses > ; The syntax for outer joins is similar to the alternate syntax for inner joins. In an outer join, the FROM clause lists two table names with keywords that indicate the type of join in between: LEFT JOIN , RIGHT JOIN , or FULL JOIN . The left table is the first table listed, and the right table is the second table. The ON clause specifies the join conditions. As with inner joins, you can also add optional clauses, such as a WHERE clause, to subset the rows. Performing a Left Outer Join Link Use a left outer join to select all the rows in the customers table and only the matching rows in the transactions table . Example: 1 2 3 4 5 6 7 proc sql ; SELECT * FROM customers as c LEFT JOIN transactions as t ON c . ID = t . ID ; quit ; Performing a Right Outer Join Link Use a right outer join to select all the rows in the transactions table and only the matching rows in the customers table . Example: 1 2 3 4 5 6 proc sql ; SELECT * FROM customers as c RIGHT JOIN transactions as t ON c . ID = t . ID ; quit ; Performing a Full Outer Join Link We want the query to combine and return all matching rows and also to return the rows from both tables that do not match . Example: 1 2 3 4 5 6 proc sql ; SELECT * FROM customers as c FULL JOIN transactions as t ON c . ID = t . ID ; quit ; In a full outer join, the order of the tables affects the order of the columns in the result set . The columns from the left table appear before the columns from the right table. Outer joins can only process two tables at a time. However, you can stack multiple outer joins in a single query. 1 COALESCE ( argument - 1 , argument - 2 < , argument - n > ) To overlay columns in SQL joins, you can use the COALESCE function, which is a SAS function. The COALESCE function returns the value of the first nonmissing argument from two or more arguments that you specify. An argument can be a constant, an expression, or a column name. All arguments must be of the same type. Example: Using the COALESCE Function to Overlay Columns 1 2 3 4 5 6 proc sql ; select coalesce ( c . ID , t . ID ) as ID , Name , Action , Amount from customers c full join transactions t on c . ID = t . ID ; quit ; Differences between a PROC SQL join and a DATA STEP merge Link The table below shows the differences between a PROC SQL join and a DATA STEP merge. Key Points SQL Join DATA Step Merge Explicit sorting of data before join/merge Not required Required Same-name columns in join/merge expressions Not required Required Equality in join or merge expressions Not required Required The table below shows the differences between inner and outer joins. Key Points Inner Join Outer Join Table Limit 256 256 Join Behavior Returns matching rows only Returns matching and non-matching rows Join Options Matching rows only LEFT , FULL , RIGHT Syntax Changes Multiple tables in the FROM clause (separated by commas); WHERE clause that specifies join criteria ON clause that specifies join criteria Working with Complex SQL Joins Link Even if the information you need for your report is located in only two tables, you may need to read the same table twice, or even more times. 1 2 FROM table - name - 1 < AS > alias - 1 , table - name - 1 < AS > alias - 2 , In order to read from the same table twice, it must be listed in the FROM clause twice. A different table alias is required for each listing to distinguish the different uses. This is called a self-join , or a reflexive join. Example: Performing a Complex Join 1 2 3 4 5 6 7 8 9 10 11 12 13 14 proc sql ; select e . Employee_ID \"Employee ID\" , e . Employee_Name \"Employee Name\" , m . Employee_ID \"Manager ID\" , m . Employee_Name \"Manager Name\" , e . Country from orion . employee_addresses as e , orion . employee_addresses as m , orion . employee_organization as o where e . Employee_ID = o . Employee_ID and o . Manager_ID = m . Employee_ID and Department contains 'Sales' order by Country , 4 , 1 ; quit ;","title":"Working with SQL Joins"},{"location":"sql/joins/#understanding-sql-joins","text":"If you're working with tables that have different columns, you can combine tables horizontally to combine the columns that you want in your output. 1 2 3 SELECT object - item < , ... object - item > FROM from - list < additional clauses > ; To combine tables horizontally, you can use either the SELECT statement in PROC SQL or the MERGE statement in the DATA step. The results of an SQL join and a DATA step merge are similar but not always identical. Whatever method you use, you must also specify criteria for matching and combining the rows across the tables.","title":"Understanding SQL Joins"},{"location":"sql/joins/#inner-joins-and-outer-joins","text":"PROC SQL supports two types of joins: Inner joins: return a result table that contains all of the matching rows in the input tables. The result of an inner join does not include any nonmatching rows. Outer joins: return a result table that contains all of the matching rows in the input tables and some or all of the nonmatching rows from one or both tables. A full outer join returns all of the matching rows plus the nonmatching rows from both tables. A left outer join returns all of the matching rows plus the nonmatching rows from the first, or left, table. A right outer join returns all of the matching rows plus the nonmatching rows from the second, or right, table. When performing an SQL join, the SQL processor first generates all possible combinations of all of the rows in the tables to be combined. An intermediate result table that contains all possible combinations of rows in the input tables is called a Cartesian product . From the rows in the Cartesian product, the SQL processor can then select the appropriate rows for an inner join or an outer join.","title":"Inner joins and Outer joins"},{"location":"sql/joins/#cartesian-product","text":"The number of rows in a Cartesian product is the product of the number of rows in the contributing tables. 1 2 3 SELECT object - item < , ... object - item > FROM table - name , table - name < ,... table - name > ; A Cartesian product is rarely the result that you want when you join tables. To create a Cartesian product, you use the SELECT statement without specifying any conditions for matching rows. Example: Creating a Cartesian Product 1 2 3 4 proc sql ; select * from customers , transactions , inventory ; quit ;","title":"Cartesian product"},{"location":"sql/joins/#cross-join-cartesian-product","text":"You can also use alternative syntax, called cross join syntax, to produce the Cartesian product of two tables. 1 2 SELECT object - item < , ... object - item > FROM table - name - 1 CROSS JOIN table - name - 2 ; In the FROM clause, the table names are separated by the CROSS JOIN keywords instead of by a comma. Example: 1 2 3 4 5 6 PROC SQL ; CREATE TABLE dataset - new AS SELECT * FROM dataset1 CROSS JOIN dataset2 WHERE dataset1 . reference - variable = dataset2 . reference - variable ; quit ;","title":"Cross Join (Cartesian product)"},{"location":"sql/joins/#working-with-inner-joins","text":"","title":"Working with Inner Joins"},{"location":"sql/joins/#performing-an-inner-join","text":"Inner join returns only the matching rows. Another way to say this is that an inner join returns rows that meet certain join conditions that you specify. 1 2 3 4 5 SELECT object - item < , ... object - item > FROM table - name , table - name WHERE join - condition ( s ) < AND sql - expression > < additional clauses > ; The syntax for an inner join requires a WHERE clause in addition to the SELECT and FROM clauses. In the WHERE clause, one or more join conditions specify the combined rows that the query should return. Example: 1 2 3 4 5 6 7 8 9 10 11 proc sql ; select * from customers , transactions where customers . ID = transactions . ID ; quit ; proc sql ; select c . ID , Name , Action , Amount from customers as c , transactions as t where c . ID = t . ID ; quit ; The join conditions are specified as expressions. The WHERE clause can also contain one or more additional expressions that subset the rows in additional ways. The column or columns that are specified in join conditions are referred to as join keys. Using PROC SQL , you can also specify join keys that have different names. However, by default, a DATA step merge requires the join keys to have the same name in both tables. PROC SQL can join tables that are not sorted on the join keys, unlike a DATA step merge. When you refer to columns in multiple tables that have the same name, you must qualify the column names to specify the location of each column. Qualified column names consist of the table name (a table qualifier), a period, and then the column name. A join on an equal condition is called an equijoin . Using PROC SQL , you can also join tables on an inequality. However, DATA step merges can only join tables on an equality. Conceptually, when PROC SQL processes an inner join, the SQL processor performs two main steps: Builds the Cartesian product of all tables listed in the FROM clause Eliminates the rows that don't meet the join conditions A DATA step merge does not create a Cartesian product. Unlike DATA step merges, SQL joins do not overlay columns that have the same name. By default, the result set contains all of the columns that have the same name. To display columns that are the same as, or derived from, columns in multiple tables, you must use a join instead of a subquery. By default, a DATA step merge builds a result set in the order of the join key values. PROC SQL does not order the rows in query results by default. To guarantee the order of the rows, you must use an ORDER BY clause. 1 2 3 4 5 SELECT object - item < , ... object - item > FROM table - name < AS > alias - 1 table - name - 2 < AS > alias - 2 WHERE join - condition ( s ) < additional clauses > ; You can simplify the syntax of your SQL joins by assigning aliases to tables in the FROM clause. Example: 1 2 3 SELECT c . ID , Name , Action , Amount FROM customers as c , transactions as t WHERE c . ID = t . ID ; After the table name, you can optionally specify the keyword AS and then the alias. The alias must follow the standard rules for SAS names. In other clauses, you can reference each table by its alias instead of by its full name.","title":"Performing an Inner Join"},{"location":"sql/joins/#using-alternative-syntax-for-an-inner-join","text":"You can also perform an inner join by using the alternative syntax shown here. 1 2 3 4 5 6 7 SELECT object - item < , ... object - item > FROM table - name - 1 << AS > alias - 1 > INNER JOIN table - name - 2 << AS > alias - 2 > ON join - condition ( s ) WHERE sql - expression < additional clauses > ; In the alternate syntax for an inner join, the FROM clause uses the INNER JOIN keywords to join two tables. The alternate syntax uses the ON clause to specify the join conditions. The alternate syntax allows additional clauses, so you can still use the WHERE clause to specify any additional subsetting conditions. Example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 proc sql ; select c . ID , Name , Action , Amount from customers as c inner join transactions as t on c . ID = t . ID ; quit ; proc sql ; select c . ID , Name , Action , Amount from customers as c inner join transactions as t on c . ID = t . ID ; quit ;","title":"Using Alternative Syntax for an Inner Join"},{"location":"sql/joins/#working-with-outer-joins","text":"1 2 3 4 5 6 SELECT object - item < , ... object - item > FROM table - name << AS > alias > LEFT | RIGHT | FULL JOIN table - name << AS > alias > ON join - condition ( s ) < additional clauses > ; The syntax for outer joins is similar to the alternate syntax for inner joins. In an outer join, the FROM clause lists two table names with keywords that indicate the type of join in between: LEFT JOIN , RIGHT JOIN , or FULL JOIN . The left table is the first table listed, and the right table is the second table. The ON clause specifies the join conditions. As with inner joins, you can also add optional clauses, such as a WHERE clause, to subset the rows.","title":"Working with Outer Joins"},{"location":"sql/joins/#performing-a-left-outer-join","text":"Use a left outer join to select all the rows in the customers table and only the matching rows in the transactions table . Example: 1 2 3 4 5 6 7 proc sql ; SELECT * FROM customers as c LEFT JOIN transactions as t ON c . ID = t . ID ; quit ;","title":"Performing a Left Outer Join"},{"location":"sql/joins/#performing-a-right-outer-join","text":"Use a right outer join to select all the rows in the transactions table and only the matching rows in the customers table . Example: 1 2 3 4 5 6 proc sql ; SELECT * FROM customers as c RIGHT JOIN transactions as t ON c . ID = t . ID ; quit ;","title":"Performing a Right Outer Join"},{"location":"sql/joins/#performing-a-full-outer-join","text":"We want the query to combine and return all matching rows and also to return the rows from both tables that do not match . Example: 1 2 3 4 5 6 proc sql ; SELECT * FROM customers as c FULL JOIN transactions as t ON c . ID = t . ID ; quit ; In a full outer join, the order of the tables affects the order of the columns in the result set . The columns from the left table appear before the columns from the right table. Outer joins can only process two tables at a time. However, you can stack multiple outer joins in a single query. 1 COALESCE ( argument - 1 , argument - 2 < , argument - n > ) To overlay columns in SQL joins, you can use the COALESCE function, which is a SAS function. The COALESCE function returns the value of the first nonmissing argument from two or more arguments that you specify. An argument can be a constant, an expression, or a column name. All arguments must be of the same type. Example: Using the COALESCE Function to Overlay Columns 1 2 3 4 5 6 proc sql ; select coalesce ( c . ID , t . ID ) as ID , Name , Action , Amount from customers c full join transactions t on c . ID = t . ID ; quit ;","title":"Performing a Full Outer Join"},{"location":"sql/joins/#differences-between-a-proc-sql-join-and-a-data-step-merge","text":"The table below shows the differences between a PROC SQL join and a DATA STEP merge. Key Points SQL Join DATA Step Merge Explicit sorting of data before join/merge Not required Required Same-name columns in join/merge expressions Not required Required Equality in join or merge expressions Not required Required The table below shows the differences between inner and outer joins. Key Points Inner Join Outer Join Table Limit 256 256 Join Behavior Returns matching rows only Returns matching and non-matching rows Join Options Matching rows only LEFT , FULL , RIGHT Syntax Changes Multiple tables in the FROM clause (separated by commas); WHERE clause that specifies join criteria ON clause that specifies join criteria","title":"Differences between a PROC SQL join and a DATA STEP merge"},{"location":"sql/joins/#working-with-complex-sql-joins","text":"Even if the information you need for your report is located in only two tables, you may need to read the same table twice, or even more times. 1 2 FROM table - name - 1 < AS > alias - 1 , table - name - 1 < AS > alias - 2 , In order to read from the same table twice, it must be listed in the FROM clause twice. A different table alias is required for each listing to distinguish the different uses. This is called a self-join , or a reflexive join. Example: Performing a Complex Join 1 2 3 4 5 6 7 8 9 10 11 12 13 14 proc sql ; select e . Employee_ID \"Employee ID\" , e . Employee_Name \"Employee Name\" , m . Employee_ID \"Manager ID\" , m . Employee_Name \"Manager Name\" , e . Country from orion . employee_addresses as e , orion . employee_addresses as m , orion . employee_organization as o where e . Employee_ID = o . Employee_ID and o . Manager_ID = m . Employee_ID and Department contains 'Sales' order by Country , 4 , 1 ; quit ;","title":"Working with Complex SQL Joins"},{"location":"sql/miscellanea/","text":"Introduction to PROC SQL Link Check these websites SQL syntax available in SAS Code Examples Link Creating Tables Filtering Specific Data Link 1 2 3 4 5 6 PROC SQL ; CREATE TABLE table - name AS SELECT DISTINCT variable1 FROM origin - data - set WHERE variable2 NE '' AND variable3 IN ( 'value1' 'value2' ) ; QUIT ; Macrovariable Creation Link Check more information in this section . In this code a macrovariable is created containing a list of a variable distinct values ( list ). Another macrovariable is also created whose value is the number of elements in the list ( nelements ). 1 2 3 4 5 6 7 8 9 10 11 12 13 * Count distinct elements ; PROC SQL NOPRINT ; SELECT COUNT ( DISTINCT variable2check ) INTO : nelements FROM original - data - set ; QUIT ; * Load these distinct elements in a macrovariable list ; PROC SQL NOPRINT ; SELECT DISTINCT variable2check . INTO : list SEPARATED BY '$' FROM original - data - set ; QUIT ; Is there any way to do these two operations in just one PROC SQL ? Two different INTO: are not compatible. General PROC SQL options: The NOPRINT option avoids any output printing SELECT statement elements: The COUNT(*) option counts the number of elements of the table designated in the FROM statement The DISTINCT option selects only different variable values INTO: name creates a name macrovariable containing the result of that specific query SEPARATED BY ' ' defines a the separator between elements Selecting Distinct Values of a Variable to Create a Data Set with them Link 1 2 3 4 PROC SQL NOPRINT ; CREATE TABLE new - SAS - data - set AS SELECT DISTINCT analyzed - variable FROM original - SAS - data - set ; QUIT ; Cartesian Product of Data Sets (All Possible Combinations) Link 1 2 3 PROC SQL ; CREATE TABLE new - SAS - data - set AS SELECT variable1 , variable2 , variablE3 FROM original - SAS - data - set1 AS f1 CROSS JOIN original - SAS - data - set2 AS f2 CROSS JOIN original - SAS - data - set3 AS f3 ; QUIT ; Selecting the Maximum Value Link If you need only the maximum value of a certain variable for equal registers except this value, you will use the MAX() function. If you want to keep in the selection any other variable that you do not need to group you will need to apply the MAX() function to it as well. 1 2 3 4 5 6 PROC SQL ; CREATE TABLE new - SAS - data - set AS SELECT grouped - variable1 , grouped - variable2 , MAX ( ungrouped - variable ) AS alias - ungrouped , MAX ( maximized - variable ) AS alias - maximized FROM original - SAS - data - set GROUP BY grouped - variable1 , grouped - variable2 ; QUIT ; Counting Grouped Elements Link A simple example first. 1 2 3 4 5 PROC SQL NOPRINT ; SELECT COUNTt ( * ) INTO : nlabs FROM SAS - data - set ; QUIT ; In this first PROC SQL a number of count variables are created: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 PROC SQL ; CREATE TABLE AESWPP AS SELECT COHORT , AEBODSY , AEDECOD , PT , MAX ( AETOXGRN ) AS GRADE , COUNT ( * ) AS NEVENTS , NTOTPATSCOHORT . NPATS AS NTOTPATSCOHORT , 100 / NTOTPATSCOHORT . NPATS AS PCTPATC , NTOTEVENTSCOHORT . NEVENTS AS NTOTEVENTSCOHORT , 100 / NTOTEVENTSCOHORT . NEVENTS AS PCTEVENTSC , NTOTPATS . NPATS AS NTOTPATS , 100 / NTOTPATS . NPATS AS PCTPATTOT , NTOTEVENTS . NEVENTS AS NTOTEVENTS , 100 / NTOTEVENTS . NEVENTS AS PCTOTEVENTS FROM ADS . TEAE AS TEAE , ( SELECT COUNT ( DISTINCT PT ) AS NPATS , COHORT AS COHORTP FROM ADS . TEAE GROUP BY COHORT ) NTOTPATSCOHORT , ( SELECT COUNT ( * ) AS NEVENTS , COHORT AS COHORTE FROM ADS . TEAE GROUP BY COHORT ) NTOTEVENTSCOHORT , ( SELECT COUNT ( DISTINCT PT ) AS NPATS FROM ADS . TEAE ) NTOTPATS , ( SELECT COUNT ( * ) AS NEVENTS FROM ADS . TEAE ) NTOTEVENTS WHERE NTOTPATSCOHORT . COHORTP EQ TEAE . COHORT AND NTOTEVENTSCOHORT . COHORTE EQ TEAE . COHORT GROUP BY COHORT , AEBODSY , AEDECOD , PT ; QUIT ; In this second PROC SQL the number of certain ocurrences are counted. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 PROC SQL ; CREATE TABLE AESWPP_1 AS SELECT * , CASE WHEN GRADE EQ 1 THEN COUNT ( 1 ) ELSE 0 END AS GRADE1 , CASE WHEN GRADE EQ 2 THEN COUNT ( 1 ) ELSE 0 END AS GRADE2 , CASE WHEN GRADE EQ 3 THEN COUNT ( 1 ) ELSE 0 END AS GRADE3 , CASE WHEN GRADE EQ 4 THEN COUNT ( 1 ) ELSE 0 END AS GRADE4 , CASE WHEN GRADE EQ 5 THEN COUNT ( 1 ) ELSE 0 END AS GRADE5 FROM AESWPP GROUP BY COHORT , AEBODSY , AEDECOD , GRADE ORDER BY COHORT , AEBODSY , AEDECOD ; QUIT ; Selecting First and Last Dates Related to a Patient Link 1 2 3 4 5 6 proc sql ; create table firstlastdates as select min ( STARTDATE ) as date1 , max ( ENDDATE ) as date2 , pt , visit from origin - SAS - data - set group by pt , visit ; quit ;","title":"Miscellanea"},{"location":"sql/miscellanea/#introduction-to-proc-sql","text":"Check these websites SQL syntax available in SAS","title":"Introduction to PROC SQL"},{"location":"sql/miscellanea/#code-examples","text":"","title":"Code Examples"},{"location":"sql/miscellanea/#creating-tables-filtering-specific-data","text":"1 2 3 4 5 6 PROC SQL ; CREATE TABLE table - name AS SELECT DISTINCT variable1 FROM origin - data - set WHERE variable2 NE '' AND variable3 IN ( 'value1' 'value2' ) ; QUIT ;","title":"Creating Tables Filtering Specific Data"},{"location":"sql/miscellanea/#macrovariable-creation","text":"Check more information in this section . In this code a macrovariable is created containing a list of a variable distinct values ( list ). Another macrovariable is also created whose value is the number of elements in the list ( nelements ). 1 2 3 4 5 6 7 8 9 10 11 12 13 * Count distinct elements ; PROC SQL NOPRINT ; SELECT COUNT ( DISTINCT variable2check ) INTO : nelements FROM original - data - set ; QUIT ; * Load these distinct elements in a macrovariable list ; PROC SQL NOPRINT ; SELECT DISTINCT variable2check . INTO : list SEPARATED BY '$' FROM original - data - set ; QUIT ; Is there any way to do these two operations in just one PROC SQL ? Two different INTO: are not compatible. General PROC SQL options: The NOPRINT option avoids any output printing SELECT statement elements: The COUNT(*) option counts the number of elements of the table designated in the FROM statement The DISTINCT option selects only different variable values INTO: name creates a name macrovariable containing the result of that specific query SEPARATED BY ' ' defines a the separator between elements","title":"Macrovariable Creation"},{"location":"sql/miscellanea/#selecting-distinct-values-of-a-variable-to-create-a-data-set-with-them","text":"1 2 3 4 PROC SQL NOPRINT ; CREATE TABLE new - SAS - data - set AS SELECT DISTINCT analyzed - variable FROM original - SAS - data - set ; QUIT ;","title":"Selecting Distinct Values of a Variable to Create a Data Set with them"},{"location":"sql/miscellanea/#cartesian-product-of-data-sets-all-possible-combinations","text":"1 2 3 PROC SQL ; CREATE TABLE new - SAS - data - set AS SELECT variable1 , variable2 , variablE3 FROM original - SAS - data - set1 AS f1 CROSS JOIN original - SAS - data - set2 AS f2 CROSS JOIN original - SAS - data - set3 AS f3 ; QUIT ;","title":"Cartesian Product of Data Sets (All Possible Combinations)"},{"location":"sql/miscellanea/#selecting-the-maximum-value","text":"If you need only the maximum value of a certain variable for equal registers except this value, you will use the MAX() function. If you want to keep in the selection any other variable that you do not need to group you will need to apply the MAX() function to it as well. 1 2 3 4 5 6 PROC SQL ; CREATE TABLE new - SAS - data - set AS SELECT grouped - variable1 , grouped - variable2 , MAX ( ungrouped - variable ) AS alias - ungrouped , MAX ( maximized - variable ) AS alias - maximized FROM original - SAS - data - set GROUP BY grouped - variable1 , grouped - variable2 ; QUIT ;","title":"Selecting the Maximum Value"},{"location":"sql/miscellanea/#counting-grouped-elements","text":"A simple example first. 1 2 3 4 5 PROC SQL NOPRINT ; SELECT COUNTt ( * ) INTO : nlabs FROM SAS - data - set ; QUIT ; In this first PROC SQL a number of count variables are created: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 PROC SQL ; CREATE TABLE AESWPP AS SELECT COHORT , AEBODSY , AEDECOD , PT , MAX ( AETOXGRN ) AS GRADE , COUNT ( * ) AS NEVENTS , NTOTPATSCOHORT . NPATS AS NTOTPATSCOHORT , 100 / NTOTPATSCOHORT . NPATS AS PCTPATC , NTOTEVENTSCOHORT . NEVENTS AS NTOTEVENTSCOHORT , 100 / NTOTEVENTSCOHORT . NEVENTS AS PCTEVENTSC , NTOTPATS . NPATS AS NTOTPATS , 100 / NTOTPATS . NPATS AS PCTPATTOT , NTOTEVENTS . NEVENTS AS NTOTEVENTS , 100 / NTOTEVENTS . NEVENTS AS PCTOTEVENTS FROM ADS . TEAE AS TEAE , ( SELECT COUNT ( DISTINCT PT ) AS NPATS , COHORT AS COHORTP FROM ADS . TEAE GROUP BY COHORT ) NTOTPATSCOHORT , ( SELECT COUNT ( * ) AS NEVENTS , COHORT AS COHORTE FROM ADS . TEAE GROUP BY COHORT ) NTOTEVENTSCOHORT , ( SELECT COUNT ( DISTINCT PT ) AS NPATS FROM ADS . TEAE ) NTOTPATS , ( SELECT COUNT ( * ) AS NEVENTS FROM ADS . TEAE ) NTOTEVENTS WHERE NTOTPATSCOHORT . COHORTP EQ TEAE . COHORT AND NTOTEVENTSCOHORT . COHORTE EQ TEAE . COHORT GROUP BY COHORT , AEBODSY , AEDECOD , PT ; QUIT ; In this second PROC SQL the number of certain ocurrences are counted. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 PROC SQL ; CREATE TABLE AESWPP_1 AS SELECT * , CASE WHEN GRADE EQ 1 THEN COUNT ( 1 ) ELSE 0 END AS GRADE1 , CASE WHEN GRADE EQ 2 THEN COUNT ( 1 ) ELSE 0 END AS GRADE2 , CASE WHEN GRADE EQ 3 THEN COUNT ( 1 ) ELSE 0 END AS GRADE3 , CASE WHEN GRADE EQ 4 THEN COUNT ( 1 ) ELSE 0 END AS GRADE4 , CASE WHEN GRADE EQ 5 THEN COUNT ( 1 ) ELSE 0 END AS GRADE5 FROM AESWPP GROUP BY COHORT , AEBODSY , AEDECOD , GRADE ORDER BY COHORT , AEBODSY , AEDECOD ; QUIT ;","title":"Counting Grouped Elements"},{"location":"sql/miscellanea/#selecting-first-and-last-dates-related-to-a-patient","text":"1 2 3 4 5 6 proc sql ; create table firstlastdates as select min ( STARTDATE ) as date1 , max ( ENDDATE ) as date2 , pt , visit from origin - SAS - data - set group by pt , visit ; quit ;","title":"Selecting First and Last Dates Related to a Patient"},{"location":"sql/nested-queries/","text":"Understanding Subqueries Link A subquery is a query that resides inside an outer query, in a WHERE clause or a HAVING clause. The subquery returns values to be used in the outer query's clause. A subquery can return only a single column , but it can return one value or multiple values from that column. PROC SQL evaluates nested queries from the inside out , starting with the subquery, and processing the outer query last. Subqueries can be nested so that the innermost subquery returns a value or values to be used by the next outer query. 1 2 3 4 5 6 7 8 9 SELECT ... FROM ... < WHERE ... > ( SELECT only - a - single - column FROM ... < WHERE ... > ) /* do not use ';' here in () */ < GROUP BY ... > < HAVING ... > < ORDER BY ... > ; Example: Multiple Levels of Subquery Nesting 1 2 3 4 5 6 7 8 9 10 11 12 13 14 proc sql ; title 'Home Phone Numbers of Sales Employees' ; title2 'Who Made Donations' ; select Employee_ID , Phone_Number as Home_Phone from orion . employee_phones where Phone_Type = 'Home' and Employee_ID in ( select Employee_ID from orion . employee_donations where Employee_ID in ( select Employee_ID from orion . sales )); quit ; title ; Types of Subqueries Link Noncorrelated Subquery Link A noncorrelated subquery is a self-contained subquery . It executes independently of the outer query before it passes one or more values back to the outer query. Example: 1 2 3 4 5 6 proc sql ; select Job_Title , avg ( Salary ) as MeanSalary from orion . staff group by Job_Title having avg ( Salary ) > ( select avg ( Salary ) as CompanyMeanSalary from orion . staff ); /* noncorrelated subquery */ quit ; Using a noncorrelated subquery enables you to build and test your code in pieces. Correlated Subquery Link A correlated subquery is dependent on the outer query . A correlated subquery requires one or more values to be passed to it by the outer query before the subquery can be resolved. Example: 1 2 3 4 5 6 7 8 9 10 proc sql ; select Employee_ID , Employee_Name as Manager_Name length = 25 from orion . Employee_Addresses where 'AU' = ( SELECT Country FROM work . supervisors WHERE employee_addresses . Employee_ID = supervisors . Employee_ID ); quit ; This PROC SQL step creates a report that lists the employee ID and name of all Orion Star managers in Australia. The outer query selects the Employee_ID and Employee_Name columns from the employee_addresses table, which contains information about all Orion Star employees. However, employee_addresses has no column that indicates which employees are managers. Instead, the outer query's WHERE clause uses a subquery to identify the managers. The subquery references the work.supervisors table, which contains information about only the managers. Notice that the subquery's WHERE clause matches the rows in the two tables by the values of Employee_ID . To do this, the outer query must pass each Employee_ID value to the subquery before the subquery can return a value to outer query. This subquery is dependent on the outer query, so it cannot stand alone. It is a correlated subquery. Operators that Accept Multiple Values Link The operator that precedes a subquery ... (SELECT ...) indicates whether the outer query will accept only a single value or multiple values. Example: 1 2 3 4 5 6 7 8 9 proc sql ; select Employee_Name , City , Country from orion . Employee_Addresses where Employee_ID IN /*use IN operator for a list of values*/ ( SELECT Employee_ID FROM orion . Employee_Payroll WHERE month ( Birth_Date ) = 2 ) order by Employee_Name ; quit ; If a subquery returns multiple values, you must use the IN operator, or a comparison operator with either the ANY or ALL keyword. The EQUAL operator does not accept multiple values. So, when a subquery returns multiple values and the EQUAL operator is used, an error message appears in the log and the query cannot be processed. Example: Using a Subquery That Returns a Single Value 1 2 3 4 5 6 7 proc sql ; select Job_Title , avg ( Salary ) as MeanSalary from orion . staff group by Job_Title having avg ( Salary ) > ( 38041 . 51 ); quit ; Example: Using a Subquery That Returns Multiple Values 1 2 3 4 5 6 7 8 9 proc sql ; select Employee_Name , City , Country from orion . employee_addresses where Employee_ID in ( select Employee_ID from orion . employee_payroll where month ( Birth_Date ) = 2 ) order by Employee_Name ; quit ; Example: Using a Subquery That Returns Multiple Values 1 2 3 4 5 6 7 8 9 10 11 12 13 14 proc sql ; title 'Level IV Sales Reps Who Earn Less Than' ; title2 'Any Lower Level Sales Reps' ; select Employee_ID , Salary from orion . staff where Job_Title = 'Sales Rep. IV' and Salary < any ( select Salary from orion . staff where Job_Title in ( 'Sales Rep. I' , 'Sales Rep. II' , 'Sales Rep. III' )); quit ; title ; Use of the ANY Keyword with a Comparison Operator Link The ANY keyword specifies that at least one of a set of values obtained from a subquery must satisfy a given condition in order for the expression to be true. Suppose you're working with a subquery that returns the three values 20, 30, and 40. WHERE column-name =ANY (SELECT ...) : this is true if the value of the specified column is equal to any one of the values that the subquery returns: 20, 30, or 40 WHERE column-name >ANY (SELECT ...) : this is true when the value of the specified column is greater than any value returned by the subquery: 20, 30, or 40 (i.e. greater than the smallest value returned by the subquery) WHERE column-name <ANY (SELECT ...) : this is true when the value of the specified column is less than any value returned by the subquery: 20, 30, or 40 (i.e. less than the largest value returned by the subquery) Use of the ALL Keyword with a Comparison Operator Link The ALL keyword specifies that all of the values from a subquery must satisfy a given condition in order for the expression to be true. WHERE column-name >ALL (SELECT ...) : this is true when the value of the specified column is greater than all of the values returned by the subquery: 20, 30, or 40 (i.e. greater than the highest value that the subquery returns) WHERE column-name <ALL (SELECT ...) : this is true when the value of the specified column is less than all of the values returned by the subquery: 20, 30, or 40 (i.e. less than the smallest value that the subquery returns) Understanding In-Line Views Link An in-line view is a query that is nested in the FROM clause of another query . 1 2 3 4 5 6 7 8 9 SELECT ... FROM ... ( SELECT ... FROM ... < WHERE ... > ) < WHERE ... > < GROUP BY ... > < HAVING ... > < ORDER BY ... > ; It acts as a virtual table , which the query uses instead of a physical table. An in-line view can contain any of the clauses in a SELECT statement except for the ORDER BY clause . Unlike a subquery, an in-line view can return a single column or multiple columns to the outer query . An in-line view must be enclosed in parentheses and can be referenced only in the query in which it is defined. Example: Using an In-Line View 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 proc sql ; title ' Employees with Salaries less than ' ; title2 ' 95% of the Average for their Job ' ; select Employee_Name , emp . Job_Title , Salary format = comma7 ., Job_Avg format = comma7 . from ( select Job_Title , avg ( Salary ) as Job_Avg format = comma7 . from orion . employee_payroll as p , orion . employee_organization as o where p . Employee_ID = o . Employee_ID and Employee_Term_Date is missing and Department = \" Sales \" group by Job_Title ) as job , orion . salesstaff as emp where emp . Job_Title = job . Job_Title and Salary < Job_Avg * . 95 and Emp_Term_Date is missing order by Job_Title , Employee_Name ; quit ; title ; Example: Using an In-Line View and a Subquery 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 proc sql ; select Employee_Name format = $ 25 . as Name , City from orion . employee_addresses where Employee_ID in ( select Manager_ID from orion . employee_organization as o , ( select distinct Employee_ID from orion . order_fact as of , orion . product_dim as p where of . Product_ID = p . Product_ID and year ( Order_Date ) = 2011 and Product_Name contains 'Expedition Zero' and Employee_ID ne 99999999 ) as ID where o . Employee_ID = ID . Employee_ID );","title":"Working with Nested Queries"},{"location":"sql/nested-queries/#understanding-subqueries","text":"A subquery is a query that resides inside an outer query, in a WHERE clause or a HAVING clause. The subquery returns values to be used in the outer query's clause. A subquery can return only a single column , but it can return one value or multiple values from that column. PROC SQL evaluates nested queries from the inside out , starting with the subquery, and processing the outer query last. Subqueries can be nested so that the innermost subquery returns a value or values to be used by the next outer query. 1 2 3 4 5 6 7 8 9 SELECT ... FROM ... < WHERE ... > ( SELECT only - a - single - column FROM ... < WHERE ... > ) /* do not use ';' here in () */ < GROUP BY ... > < HAVING ... > < ORDER BY ... > ; Example: Multiple Levels of Subquery Nesting 1 2 3 4 5 6 7 8 9 10 11 12 13 14 proc sql ; title 'Home Phone Numbers of Sales Employees' ; title2 'Who Made Donations' ; select Employee_ID , Phone_Number as Home_Phone from orion . employee_phones where Phone_Type = 'Home' and Employee_ID in ( select Employee_ID from orion . employee_donations where Employee_ID in ( select Employee_ID from orion . sales )); quit ; title ;","title":"Understanding Subqueries"},{"location":"sql/nested-queries/#types-of-subqueries","text":"","title":"Types of Subqueries"},{"location":"sql/nested-queries/#noncorrelated-subquery","text":"A noncorrelated subquery is a self-contained subquery . It executes independently of the outer query before it passes one or more values back to the outer query. Example: 1 2 3 4 5 6 proc sql ; select Job_Title , avg ( Salary ) as MeanSalary from orion . staff group by Job_Title having avg ( Salary ) > ( select avg ( Salary ) as CompanyMeanSalary from orion . staff ); /* noncorrelated subquery */ quit ; Using a noncorrelated subquery enables you to build and test your code in pieces.","title":"Noncorrelated Subquery"},{"location":"sql/nested-queries/#correlated-subquery","text":"A correlated subquery is dependent on the outer query . A correlated subquery requires one or more values to be passed to it by the outer query before the subquery can be resolved. Example: 1 2 3 4 5 6 7 8 9 10 proc sql ; select Employee_ID , Employee_Name as Manager_Name length = 25 from orion . Employee_Addresses where 'AU' = ( SELECT Country FROM work . supervisors WHERE employee_addresses . Employee_ID = supervisors . Employee_ID ); quit ; This PROC SQL step creates a report that lists the employee ID and name of all Orion Star managers in Australia. The outer query selects the Employee_ID and Employee_Name columns from the employee_addresses table, which contains information about all Orion Star employees. However, employee_addresses has no column that indicates which employees are managers. Instead, the outer query's WHERE clause uses a subquery to identify the managers. The subquery references the work.supervisors table, which contains information about only the managers. Notice that the subquery's WHERE clause matches the rows in the two tables by the values of Employee_ID . To do this, the outer query must pass each Employee_ID value to the subquery before the subquery can return a value to outer query. This subquery is dependent on the outer query, so it cannot stand alone. It is a correlated subquery.","title":"Correlated Subquery"},{"location":"sql/nested-queries/#operators-that-accept-multiple-values","text":"The operator that precedes a subquery ... (SELECT ...) indicates whether the outer query will accept only a single value or multiple values. Example: 1 2 3 4 5 6 7 8 9 proc sql ; select Employee_Name , City , Country from orion . Employee_Addresses where Employee_ID IN /*use IN operator for a list of values*/ ( SELECT Employee_ID FROM orion . Employee_Payroll WHERE month ( Birth_Date ) = 2 ) order by Employee_Name ; quit ; If a subquery returns multiple values, you must use the IN operator, or a comparison operator with either the ANY or ALL keyword. The EQUAL operator does not accept multiple values. So, when a subquery returns multiple values and the EQUAL operator is used, an error message appears in the log and the query cannot be processed. Example: Using a Subquery That Returns a Single Value 1 2 3 4 5 6 7 proc sql ; select Job_Title , avg ( Salary ) as MeanSalary from orion . staff group by Job_Title having avg ( Salary ) > ( 38041 . 51 ); quit ; Example: Using a Subquery That Returns Multiple Values 1 2 3 4 5 6 7 8 9 proc sql ; select Employee_Name , City , Country from orion . employee_addresses where Employee_ID in ( select Employee_ID from orion . employee_payroll where month ( Birth_Date ) = 2 ) order by Employee_Name ; quit ; Example: Using a Subquery That Returns Multiple Values 1 2 3 4 5 6 7 8 9 10 11 12 13 14 proc sql ; title 'Level IV Sales Reps Who Earn Less Than' ; title2 'Any Lower Level Sales Reps' ; select Employee_ID , Salary from orion . staff where Job_Title = 'Sales Rep. IV' and Salary < any ( select Salary from orion . staff where Job_Title in ( 'Sales Rep. I' , 'Sales Rep. II' , 'Sales Rep. III' )); quit ; title ;","title":"Operators that Accept Multiple Values"},{"location":"sql/nested-queries/#use-of-the-any-keyword-with-a-comparison-operator","text":"The ANY keyword specifies that at least one of a set of values obtained from a subquery must satisfy a given condition in order for the expression to be true. Suppose you're working with a subquery that returns the three values 20, 30, and 40. WHERE column-name =ANY (SELECT ...) : this is true if the value of the specified column is equal to any one of the values that the subquery returns: 20, 30, or 40 WHERE column-name >ANY (SELECT ...) : this is true when the value of the specified column is greater than any value returned by the subquery: 20, 30, or 40 (i.e. greater than the smallest value returned by the subquery) WHERE column-name <ANY (SELECT ...) : this is true when the value of the specified column is less than any value returned by the subquery: 20, 30, or 40 (i.e. less than the largest value returned by the subquery)","title":"Use of the ANY Keyword with a Comparison Operator"},{"location":"sql/nested-queries/#use-of-the-all-keyword-with-a-comparison-operator","text":"The ALL keyword specifies that all of the values from a subquery must satisfy a given condition in order for the expression to be true. WHERE column-name >ALL (SELECT ...) : this is true when the value of the specified column is greater than all of the values returned by the subquery: 20, 30, or 40 (i.e. greater than the highest value that the subquery returns) WHERE column-name <ALL (SELECT ...) : this is true when the value of the specified column is less than all of the values returned by the subquery: 20, 30, or 40 (i.e. less than the smallest value that the subquery returns)","title":"Use of the ALL Keyword with a Comparison Operator"},{"location":"sql/nested-queries/#understanding-in-line-views","text":"An in-line view is a query that is nested in the FROM clause of another query . 1 2 3 4 5 6 7 8 9 SELECT ... FROM ... ( SELECT ... FROM ... < WHERE ... > ) < WHERE ... > < GROUP BY ... > < HAVING ... > < ORDER BY ... > ; It acts as a virtual table , which the query uses instead of a physical table. An in-line view can contain any of the clauses in a SELECT statement except for the ORDER BY clause . Unlike a subquery, an in-line view can return a single column or multiple columns to the outer query . An in-line view must be enclosed in parentheses and can be referenced only in the query in which it is defined. Example: Using an In-Line View 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 proc sql ; title ' Employees with Salaries less than ' ; title2 ' 95% of the Average for their Job ' ; select Employee_Name , emp . Job_Title , Salary format = comma7 ., Job_Avg format = comma7 . from ( select Job_Title , avg ( Salary ) as Job_Avg format = comma7 . from orion . employee_payroll as p , orion . employee_organization as o where p . Employee_ID = o . Employee_ID and Employee_Term_Date is missing and Department = \" Sales \" group by Job_Title ) as job , orion . salesstaff as emp where emp . Job_Title = job . Job_Title and Salary < Job_Avg * . 95 and Emp_Term_Date is missing order by Job_Title , Employee_Name ; quit ; title ; Example: Using an In-Line View and a Subquery 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 proc sql ; select Employee_Name format = $ 25 . as Name , City from orion . employee_addresses where Employee_ID in ( select Manager_ID from orion . employee_organization as o , ( select distinct Employee_ID from orion . order_fact as of , orion . product_dim as p where of . Product_ID = p . Product_ID and year ( Order_Date ) = 2011 and Product_Name contains 'Expedition Zero' and Employee_ID ne 99999999 ) as ID where o . Employee_ID = ID . Employee_ID );","title":"Understanding In-Line Views"},{"location":"sql/set-operators/","text":"Introducing Set Operators Link What Are Set Operators? Link To vertically combine the results of two queries , you can use one of four set operators: UNION (ANSI standard for SQL) OUTER UNION (SAS enhancement) EXCEPT (ANSI standard for SQL) INTERSECT (ANSI standard for SQL) To modify the default behavior of a set operator, you can also use one or both of the modifiers ALL and CORR . A set operator vertically combines the intermediate result sets from two queries to produce a final result set. Set operators act on the intermediate result sets, not directly on the input tables. Example: Using a Set Operator in an In-Line View 1 2 3 4 5 6 7 8 9 10 title 'Number of Employees Who Completed' ; title2 'Training A, But Not Training B' ; proc sql ; select count ( ID ) as Count from ( select ID , Name from orion . train_a except select ID , Name from orion . train_b where EDate is not missing ); quit ; title ; How Set Operators Combine Rows and Columns by Default? Link By default, the set operators handle the rows in the intermediate result sets as follows: The UNION operator produces ALL UNIQUE rows from BOTH result sets The OUTER UNION operator produces ALL rows from BOTH result sets The EXCEPT operator produces UNIQUE rows from the FIRST result set that are NOT in the SECOND The INTERSECT operator produces UNIQUE rows from the FIRST result set that are in the SECOND By default, the set operators handle the columns in the intermediate result sets as follows: The UNION , EXCEPT , and INTERSECT operators align columns by their POSITION in both intermediate result sets The OUTER UNION operator produces all columns from both result sets Modifying the Default Behavior of the Set Operators Link To change the default behavior of the set operators, you can use one or both of the modifiers ALL and CORR (or CORRESPONDING ) along with each set operator. ALL Modifies the Default Behavior for Rows Link The UNION , EXCEPT , and INTERSECT set operators produce only the unique rows by default. To produce only the unique rows, PROC SQL must make a second pass through the data to eliminate the duplicate rows . When used with these three operators, the ALL modifier prevents this second pass , which means it does not suppress the duplicate rows. Warning ALL cannot be used with the OUTER UNION operator. CORR Modifies the Default Behavior for Columns Link With the UNION , EXCEPT , and INTERSECT operators, the CORR modifier aligns columns that have the same NAME (not the same position) in both intermediate result sets instead of aligning columns by position . With these three operators, CORR also suppresses any columns with names that do not appear in both intermediate result sets. With the OUTER UNION operator, CORR aligns columns by NAME , like the other operators, instead of producing all columns. However, the final result set includes any columns that do not appear in both result sets. Tip Remember that you can use ALL and CORR separately or together, except when you are using the OUTER UNION operator. Syntax of a Set Operation Link A set operation consists of two sets of query clauses that are combined by one of the four set operators and, optionally, one or both modifiers. 1 2 3 4 SELECT ... UNION | OUTER UNION | EXCEPT | INTERSECT < ALL > < CORR > SELECT ...; A SELECT statement can contain more than one set operation. 1 2 3 4 5 6 7 8 SELECT * FROM table - one UNION SELECT * FROM table - two UNION SELECT * FROM table - three ; No matter how many set operations a SELECT statement contains, it always produces one final result set. Order of Evaluation of Set Operators Link If your set operation contains multiple instances of the same set operator , PROC SQL processes the set operations in order, from top to bottom. If a single statement contains different set operators , PROC SQL follows specific rules to determine the order in which to evaluate the set operations. By default, within a single statement, PROC SQL evaluates the INTERSECT operator first. The other three operators ( UNION , OUTER UNION , and EXCEPT ) have the same order of precedence, so PROC SQL will evaluate them in the order in which they appear in the statement. To override the default order in which PROC SQL evaluates the set operators, you can add parentheses to your code. PROC SQL combines any queries that are enclosed in parentheses first. Example: Combining Set Operators 1 2 3 4 5 6 7 8 9 title \"Who on Bob's Team Has Not\" ; title2 'Started Any Training' ; proc sql ; select ID , Name from orion . team except ( select ID , Name from orion . train_a union select ID , Name from orion . train_b ); quit ; Using the UNION Operator Link Using the UNION Operator without Modifiers Link By default, to process a set operation with the UNION operator, the SQL processor performs two main steps: Vertically combines the rows from the two intermediate result sets (by default, the columns are aligned by position and to be aligned, the columns must have the same type) Identifies and removes any duplicate rows from the combined result set The final result set consists of the remaining rows. Using the UNION Operator with the CORR Modifier Link When the CORR modifier is used with the UNION operator, the SQL processor performs two main steps: Aligns columns by name and removes all columns that do not have a matching name in the other result set Removes the duplicate rows from the combined result set Example: Using the UNION Operator with the CORR Modifer 1 2 3 4 5 6 7 8 9 proc sql ; select * from orion . train_a union corr select * from orion . train_b where Edate is not missing ; quit ; Using the UNION Operator with the ALL Modifier Link In certain situations, you can increase the efficiency of your code by adding the ALL modifier to suppress the removal of duplicate rows. It makes sense to use the ALL modifier when the presence of duplicate rows in the final result set is not a problem, or duplicate rows are not possible (e.g. a column might have a unique or primary key constraint). Example: Combining Three Queries with the UNION Operator and the ALL Modifier 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 title ' Payroll Report for Level I, II, ' ; title2 ' and III Employees ' ; proc sql ; select ' Total Paid to ALL Level I Staff ' , sum ( Salary ) format = comma12 . from orion . staff where scan ( Job_Title , - 1 , ' ' ) = ' I ' union all select ' Total Paid to ALL Level II Staff ' , sum ( Salary ) format = comma12 . from orion . staff where scan ( Job_Title , - 1 , ' ' ) = ' II ' union all select ' Total Paid to ALL Level III Staff ' , sum ( Salary ) format = comma12 . from orion . staff where scan ( Job_Title , - 1 , ' ' ) = ' III ' ; quit ; Using the OUTER UNION Operator Link Using the OUTER UNION Operator Alone and with the CORR Modifier Link By default, the OUTER UNION operator vertically combines the rows from both intermediate result sets. However, unlike the UNION operator, OUTER UNION does not align common columns by default or remove duplicate rows. When CORR is used with OUTER UNION , it overlays common columns. However, with UNION , the CORR modifier does not suppress columns that have different names, as it does with the other set operators. Example: Using the OUTER UNION Operator with the CORR Modifer 1 2 3 4 5 6 7 8 9 proc sql ; select * from orion . train_a outer union corr select * from orion . train_b where EDate is not missing ; quit ; OUTER UNION Set Operation versus Traditional SAS Programming Link There are several methods of using SAS to vertically combine all of the rows and columns in two results sets. The most common alternative to the OUTER UNION set operator with the CORR modifier is the SET statement in the DATA step. Using the EXCEPT Operator Link Using the EXCEPT Operator without Modifiers Link By default, the EXCEPT operator vertically combines two intermediate result sets as follows: After producing the two intermediate result sets, PROC SQL removes any duplicate rows within the first result set (none of the rows in the second result set will appear in the final result, so there is no need to look for duplicates there) Aligns the columns by their position in the intermediate result sets (to be aligned, the columns must be the same type) Identifies and removes any rows in the first result set that match any rows in the second result set The final result set contains the rows and columns that remain in the first result set. Using the EXCEPT Operator with Modifiers Link When ALL is used with the EXCEPT operator, duplicate rows are not removed from the intermediate result sets. With CORR , the columns are aligned by name instead of by position in order to identify matching rows. Columns that do not have the same name in both intermediate result sets are eliminated from the final result set. Example : Using the EXCEPT Operator with the ALL and CORR Modifers 1 2 3 4 5 6 7 8 9 proc sql ; select * from orion . train_a except all corr select * from orion . train_b where EDate is not missing ; quit ; Using the INTERSECT Operator Link Using the INTERSECT Operator without Modifiers Link By default, the INTERSECT operator vertically combines two intermediate result sets as follows: After producing the two intermediate result sets, PROC SQL removes any duplicate rows within each of the intermediate result sets Aligns the columns by their position in the intermediate result sets (to be aligned, the columns must be the same type) Identifies any rows in the first result set that match any rows in the second result set The final result set contains the matching rows in the first result set. The column names in the final result set are determined by the first result set. Using the INTERSECT Operator with Modifiers Link When ALL is used with the INTERSECT operator, duplicate rows are not removed from the intermediate result sets. Example: Using the INTERSECT Operator with the ALL Modifier 1 2 3 4 5 6 7 8 9 proc sql ; select ID , Name from orion . train_a intersect all select ID , Name from orion . train_b where EDate is not missing ; quit ; With CORR , the columns are aligned by name instead of by position in order to identify matching rows. Columns that do not have the same name in both intermediate result sets are eliminated from the final result set.","title":"Working with Set Operators"},{"location":"sql/set-operators/#introducing-set-operators","text":"","title":"Introducing Set Operators"},{"location":"sql/set-operators/#what-are-set-operators","text":"To vertically combine the results of two queries , you can use one of four set operators: UNION (ANSI standard for SQL) OUTER UNION (SAS enhancement) EXCEPT (ANSI standard for SQL) INTERSECT (ANSI standard for SQL) To modify the default behavior of a set operator, you can also use one or both of the modifiers ALL and CORR . A set operator vertically combines the intermediate result sets from two queries to produce a final result set. Set operators act on the intermediate result sets, not directly on the input tables. Example: Using a Set Operator in an In-Line View 1 2 3 4 5 6 7 8 9 10 title 'Number of Employees Who Completed' ; title2 'Training A, But Not Training B' ; proc sql ; select count ( ID ) as Count from ( select ID , Name from orion . train_a except select ID , Name from orion . train_b where EDate is not missing ); quit ; title ;","title":"What Are Set Operators?"},{"location":"sql/set-operators/#how-set-operators-combine-rows-and-columns-by-default","text":"By default, the set operators handle the rows in the intermediate result sets as follows: The UNION operator produces ALL UNIQUE rows from BOTH result sets The OUTER UNION operator produces ALL rows from BOTH result sets The EXCEPT operator produces UNIQUE rows from the FIRST result set that are NOT in the SECOND The INTERSECT operator produces UNIQUE rows from the FIRST result set that are in the SECOND By default, the set operators handle the columns in the intermediate result sets as follows: The UNION , EXCEPT , and INTERSECT operators align columns by their POSITION in both intermediate result sets The OUTER UNION operator produces all columns from both result sets","title":"How Set Operators Combine Rows and Columns by Default?"},{"location":"sql/set-operators/#modifying-the-default-behavior-of-the-set-operators","text":"To change the default behavior of the set operators, you can use one or both of the modifiers ALL and CORR (or CORRESPONDING ) along with each set operator.","title":"Modifying the Default Behavior of the Set Operators"},{"location":"sql/set-operators/#all-modifies-the-default-behavior-for-rows","text":"The UNION , EXCEPT , and INTERSECT set operators produce only the unique rows by default. To produce only the unique rows, PROC SQL must make a second pass through the data to eliminate the duplicate rows . When used with these three operators, the ALL modifier prevents this second pass , which means it does not suppress the duplicate rows. Warning ALL cannot be used with the OUTER UNION operator.","title":"ALL Modifies the Default Behavior for Rows"},{"location":"sql/set-operators/#corr-modifies-the-default-behavior-for-columns","text":"With the UNION , EXCEPT , and INTERSECT operators, the CORR modifier aligns columns that have the same NAME (not the same position) in both intermediate result sets instead of aligning columns by position . With these three operators, CORR also suppresses any columns with names that do not appear in both intermediate result sets. With the OUTER UNION operator, CORR aligns columns by NAME , like the other operators, instead of producing all columns. However, the final result set includes any columns that do not appear in both result sets. Tip Remember that you can use ALL and CORR separately or together, except when you are using the OUTER UNION operator.","title":"CORR Modifies the Default Behavior for Columns"},{"location":"sql/set-operators/#syntax-of-a-set-operation","text":"A set operation consists of two sets of query clauses that are combined by one of the four set operators and, optionally, one or both modifiers. 1 2 3 4 SELECT ... UNION | OUTER UNION | EXCEPT | INTERSECT < ALL > < CORR > SELECT ...; A SELECT statement can contain more than one set operation. 1 2 3 4 5 6 7 8 SELECT * FROM table - one UNION SELECT * FROM table - two UNION SELECT * FROM table - three ; No matter how many set operations a SELECT statement contains, it always produces one final result set.","title":"Syntax of a Set Operation"},{"location":"sql/set-operators/#order-of-evaluation-of-set-operators","text":"If your set operation contains multiple instances of the same set operator , PROC SQL processes the set operations in order, from top to bottom. If a single statement contains different set operators , PROC SQL follows specific rules to determine the order in which to evaluate the set operations. By default, within a single statement, PROC SQL evaluates the INTERSECT operator first. The other three operators ( UNION , OUTER UNION , and EXCEPT ) have the same order of precedence, so PROC SQL will evaluate them in the order in which they appear in the statement. To override the default order in which PROC SQL evaluates the set operators, you can add parentheses to your code. PROC SQL combines any queries that are enclosed in parentheses first. Example: Combining Set Operators 1 2 3 4 5 6 7 8 9 title \"Who on Bob's Team Has Not\" ; title2 'Started Any Training' ; proc sql ; select ID , Name from orion . team except ( select ID , Name from orion . train_a union select ID , Name from orion . train_b ); quit ;","title":"Order of Evaluation of Set Operators"},{"location":"sql/set-operators/#using-the-union-operator","text":"","title":"Using the UNION Operator"},{"location":"sql/set-operators/#using-the-union-operator-without-modifiers","text":"By default, to process a set operation with the UNION operator, the SQL processor performs two main steps: Vertically combines the rows from the two intermediate result sets (by default, the columns are aligned by position and to be aligned, the columns must have the same type) Identifies and removes any duplicate rows from the combined result set The final result set consists of the remaining rows.","title":"Using the UNION Operator without Modifiers"},{"location":"sql/set-operators/#using-the-union-operator-with-the-corr-modifier","text":"When the CORR modifier is used with the UNION operator, the SQL processor performs two main steps: Aligns columns by name and removes all columns that do not have a matching name in the other result set Removes the duplicate rows from the combined result set Example: Using the UNION Operator with the CORR Modifer 1 2 3 4 5 6 7 8 9 proc sql ; select * from orion . train_a union corr select * from orion . train_b where Edate is not missing ; quit ;","title":"Using the UNION Operator with the CORR Modifier"},{"location":"sql/set-operators/#using-the-union-operator-with-the-all-modifier","text":"In certain situations, you can increase the efficiency of your code by adding the ALL modifier to suppress the removal of duplicate rows. It makes sense to use the ALL modifier when the presence of duplicate rows in the final result set is not a problem, or duplicate rows are not possible (e.g. a column might have a unique or primary key constraint). Example: Combining Three Queries with the UNION Operator and the ALL Modifier 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 title ' Payroll Report for Level I, II, ' ; title2 ' and III Employees ' ; proc sql ; select ' Total Paid to ALL Level I Staff ' , sum ( Salary ) format = comma12 . from orion . staff where scan ( Job_Title , - 1 , ' ' ) = ' I ' union all select ' Total Paid to ALL Level II Staff ' , sum ( Salary ) format = comma12 . from orion . staff where scan ( Job_Title , - 1 , ' ' ) = ' II ' union all select ' Total Paid to ALL Level III Staff ' , sum ( Salary ) format = comma12 . from orion . staff where scan ( Job_Title , - 1 , ' ' ) = ' III ' ; quit ;","title":"Using the UNION Operator with the ALL Modifier"},{"location":"sql/set-operators/#using-the-outer-union-operator","text":"","title":"Using the OUTER UNION Operator"},{"location":"sql/set-operators/#using-the-outer-union-operator-alone-and-with-the-corr-modifier","text":"By default, the OUTER UNION operator vertically combines the rows from both intermediate result sets. However, unlike the UNION operator, OUTER UNION does not align common columns by default or remove duplicate rows. When CORR is used with OUTER UNION , it overlays common columns. However, with UNION , the CORR modifier does not suppress columns that have different names, as it does with the other set operators. Example: Using the OUTER UNION Operator with the CORR Modifer 1 2 3 4 5 6 7 8 9 proc sql ; select * from orion . train_a outer union corr select * from orion . train_b where EDate is not missing ; quit ;","title":"Using the OUTER UNION Operator Alone and with the CORR Modifier"},{"location":"sql/set-operators/#outer-union-set-operation-versus-traditional-sas-programming","text":"There are several methods of using SAS to vertically combine all of the rows and columns in two results sets. The most common alternative to the OUTER UNION set operator with the CORR modifier is the SET statement in the DATA step.","title":"OUTER UNION Set Operation versus Traditional SAS Programming"},{"location":"sql/set-operators/#using-the-except-operator","text":"","title":"Using the EXCEPT Operator"},{"location":"sql/set-operators/#using-the-except-operator-without-modifiers","text":"By default, the EXCEPT operator vertically combines two intermediate result sets as follows: After producing the two intermediate result sets, PROC SQL removes any duplicate rows within the first result set (none of the rows in the second result set will appear in the final result, so there is no need to look for duplicates there) Aligns the columns by their position in the intermediate result sets (to be aligned, the columns must be the same type) Identifies and removes any rows in the first result set that match any rows in the second result set The final result set contains the rows and columns that remain in the first result set.","title":"Using the EXCEPT Operator without Modifiers"},{"location":"sql/set-operators/#using-the-except-operator-with-modifiers","text":"When ALL is used with the EXCEPT operator, duplicate rows are not removed from the intermediate result sets. With CORR , the columns are aligned by name instead of by position in order to identify matching rows. Columns that do not have the same name in both intermediate result sets are eliminated from the final result set. Example : Using the EXCEPT Operator with the ALL and CORR Modifers 1 2 3 4 5 6 7 8 9 proc sql ; select * from orion . train_a except all corr select * from orion . train_b where EDate is not missing ; quit ;","title":"Using the EXCEPT Operator with Modifiers"},{"location":"sql/set-operators/#using-the-intersect-operator","text":"","title":"Using the INTERSECT Operator"},{"location":"sql/set-operators/#using-the-intersect-operator-without-modifiers","text":"By default, the INTERSECT operator vertically combines two intermediate result sets as follows: After producing the two intermediate result sets, PROC SQL removes any duplicate rows within each of the intermediate result sets Aligns the columns by their position in the intermediate result sets (to be aligned, the columns must be the same type) Identifies any rows in the first result set that match any rows in the second result set The final result set contains the matching rows in the first result set. The column names in the final result set are determined by the first result set.","title":"Using the INTERSECT Operator without Modifiers"},{"location":"sql/set-operators/#using-the-intersect-operator-with-modifiers","text":"When ALL is used with the INTERSECT operator, duplicate rows are not removed from the intermediate result sets. Example: Using the INTERSECT Operator with the ALL Modifier 1 2 3 4 5 6 7 8 9 proc sql ; select ID , Name from orion . train_a intersect all select ID , Name from orion . train_b where EDate is not missing ; quit ; With CORR , the columns are aligned by name instead of by position in order to identify matching rows. Columns that do not have the same name in both intermediate result sets are eliminated from the final result set.","title":"Using the INTERSECT Operator with Modifiers"},{"location":"sql/tables-views/","text":"Creating Tables Link Methods of Creating Tables with the CREATE TABLE Statement Link PROC SQL provides three methods of creating new tables: If one or more existing tables have the data that you need, you can copy some or all of the existing columns and rows to a new table when you create it. If you start with an existing table, you can also copy only the column structure to create a new table that has no rows (i.e. an empty table). You'll need to add the data in a separate step. If no existing table has the column structure that you want, you can define new columns in your code to create an empty table. Each method of creating a table uses a different form of the CREATE TABLE statement. Each CREATE TABLE statement can create only one table. Using a Query in the CREATE TABLE Statement Link To create a table by copying columns and rows from one or more existing tables, you use a query in the CREATE TABLE statement. 1 2 CREATE TABLE table - name AS SELECT ...; Following the CREATE TABLE keywords and the name of the table that you want to create, you specify the keyword AS . Next, you specify a query that returns the columns and rows that you want from the existing table or tables. In addition to the required SELECT and FROM clauses, you can use any of the optional clauses from a SELECT statement. The entire statement contains only one semicolon, which appears at the end. When you specify a query within the CREATE TABLE statement, the report output is suppressed. The CREATE TABLE statement generates only a table. When you create a new table, it is a good idea to verify the contents of the table by using the DESCRIBE TABLE statement. Example: Creating a Table by Copying Columns and Rows, and Validating the Table 1 2 3 4 5 6 7 8 9 10 11 12 13 proc sql ; create table work . birthmonths as select Employee_Name as Name format = $ 25 ., City format = $ 25 ., month ( Birth_Date ) as BirthMonth 'Birth Month' format = 3 . from orion . employee_payroll as p , orion . employee_addresses as a where p . Employee_ID = a . Employee_ID and Employee_Term_Date is missing order by BirthMonth , City , Name ; describe table orion . birthmonths ; quit ; Using the LIKE Clause in the CREATE TABLE Statement Link To create a table by copying columns and rows from one or more existing tables, you use a query in the CREATE TABLE statement. 1 2 CREATE TABLE table - name - 2 LIKE table - name - 1 ; Following the keywords CREATE TABLE , you specify the name of the table that you want to create ( table-name-2 ). Table names must follow the rules for SAS names. The LIKE clause specifies the name of the existing table whose column structure you want to copy Example: Creating a Table by Copying the Column Structure 1 2 3 4 proc sql ; create table work . new_sales_staff like orion . sales ; quit ; Defining Columns in the CREATE TABLE Statement Link To create a new, empty table that is not based on an existing table, you can define the columns in the CREATE TABLE statement. 1 2 3 CREATE TABLE table - name ( column - name type < column - modifier ( s ) > < ,... column - name type < column - modifier ( s ) >> ); Following the CREATE TABLE keywords, you specify the name of the table you are creating. Then you specify a set of column definitions that, together, make up the table definition. The column definitions are separated by commas. The entire table definition is enclosed in parentheses. Each column definition consists of a column name (which must follow the rules for SAS names), a type (also referred to as a data type), and (optional) column modifiers such as informats, formats, and labels. In SAS data sets, a column's type can be either character or numeric. Columns are defined as follows: To define a character column , you specify the CHARACTER keyword and, optionally, a column length in parentheses. In SAS, the default column length for character columns is eight characters. To define a numeric column , you specify the NUMERIC keyword. In a SAS data set, PROC SQL creates all numeric columns with the maximum precision allowed by SAS, so there is no need to specify a length. For ANSI compliance, PROC SQL accepts additional data types in column definitions. However, all additional types are converted to either character or numeric in the SAS data set. For example, in a SAS data set, a date column is a numeric column that has a date informat or format. Example: Creating a Table by Defining the Columns 1 2 3 4 5 6 7 proc sql ; create table discounts ( Product_ID num format = z12 ., Start_Date date , End_Date date , Discount num format = percent .); quit ; Methods of Adding Data to a Table with the INSERT Statement Link PROC SQL provides the following methods of adding data to an existing table: Specify column name-value pairs. Specify an ordered list of values. Specify a query that returns one or more rows. If the table already contains rows, the new rows are appended. Each method uses a different form of the INSERT statement. The first part of this statement is always the same: the keywords INSERT INTO are followed by the name of the table into which you are adding data. The rest of the INSERT statement varies by method. Each INSERT statement can add data to only one table. Specifying Column Name-Value Pairs in the SET Clause Link In the INSERT statement, the SET clause specifies or alters the values of one or more columns in a row. 1 2 3 INSERT INTO table - name SET column - name = value , column - name = value ... ; The SET clause contains one or more pairs of column names and values. In each pair, the column name and the value are joined by an equal sign. Multiple pairs are separated by commas. The column names can be listed in any order. Character values are enclosed in quotation marks. To insert values into multiple rows, you can specify multiple SET clauses in the INSERT statement. Multiple SET clauses are not separated by commas. Multiple SET clauses do not need to list the same columns or list columns in the same order. Example: Using the SET Clause to Add Data to a Table 1 2 3 4 5 6 7 8 9 10 11 proc sql ; insert into discounts set Product_ID = 230100300006 , Start_Date = '01MAR2013' d , End_Date = '15MAR2013' d , Discount = . 33 set Product_ID = 230100600018 , Start_Date = '16MAR2013' d , End_Date = '31MAR2013' d , Discount = . 15 ; quit ; Specifying an Ordered List of Values in the VALUES Clause Link In the INSERT statement, the VALUES clause adds values to the columns in a single row. 1 2 3 INSERT INTO table - name < ( column < , ... column > ) > VALUES ( value , value , ... ); After the VALUES keyword, you specify a list of one or more values in parentheses. Within the list, you separate multiple values with commas. To add more than one row of values to the table, you specify additional VALUES clauses. Multiple VALUES clauses are not separated by commas. By default, in each VALUES clause, you specify values for all of the columns in the table, in the order that the columns appear in the table. If you want to specify values in an order that is different from the column order in the table, or if you want to specify values for only a subset of the columns in the table, you must add an optional column list in parentheses after the table name. The order of the columns in the column list is independent of the order of the columns in the table. When you specify a column list, the order of the values in each VALUES clause must match the order of the columns in the column list. Any columns that are in the table but do not appear in a column list are given missing values. Example: Using the VALUES Clause to Add Data to a Table 1 2 3 4 5 6 7 proc sql ; insert into discounts values ( 230100300006 , '01MAR2013' d , '15MAR2013' d ,. 33 ) values ( 230100600018 , '16MAR2013' d , '31MAR2013' d ,. 15 ); quit ; Specifying a Query in the INSERT Statement Link To add data from one existing table to another, you can specify a query in the INSERT statement. 1 2 3 4 5 INSERT INTO table - name < ( column < , ... column > ) > SELECT column ( s ) FROM table - name < additional clauses > ; By default, the SELECT clause specifies values for every column in the target table, and the order of the values must match the order of the columns in the target table. If you want to specify values in a different order or for only a subset of the columns in the target table, you can add an optional column list in parentheses after the table name in the INSERT statement. The order of the columns in the column list is independent of the order of the columns in the table. Example: Specifying a Query in the INSERT Statement 1 2 3 4 5 6 7 8 9 proc sql ; insert into discounts ( Product_ID , Discount , Start_Date , End_Date ) select distinct Product_ID ,. 35 , '01MAR2013' d , '31MAR2013' d from orion . product_dim where Supplier_Name contains 'Pro Sportswear Inc' ; quit ; Creating PROC SQL Views Link Understanding PROC SQL Views Link A SAS view that is created by PROC SQL is called a PROC SQL view. A PROC SQL view is a stored query that can be based on one or more tables or any kind of SAS view: PROC SQL views, DATA step views, or SAS/ACCESS views. So, a PROC SQL view contains query code but no actual data. Unlike an empty table, a view is not a physical table. Instead, a PROC SQL view is sometimes referred to as a virtual table because it can be referenced in queries and other SAS programs in the same manner as a physical table. Like a table, a PROC SQL view has a name. When a SAS program runs, each time a view is referenced, the stored query executes and extracts the most current data from the underlying data source. ANSI standards specify that a PROC SQL view must reside in the same library as the source table or tables. However, a PROC SQL view cannot have the same name as a table stored in the same library. Nor can you create a table that has the same name as an existing view in the same library. There is a SAS enhancement in PROC SQL that enables you to create a view that can reside in a different library than its data source. The main advantages and disadvantages of referencing PROC SQL views instead of tables in your SAS programs are listed below. Advantages: Avoid storing copies of large tables. Avoid a frequent refresh of table copies. When the underlying data changes, a view surfaces the most current data. Combine data from multiple database tables and multiple libraries or databases. Simplify complex queries. Prevent other users from inadvertently altering the query code. Prevent other users from viewing data that they should not be able to see. Disadvantages: Might produce different results each time they are accessed if the data in the underlying data sources changes. Can require significant resources each time that they execute. With a view, you save disk storage space at the cost of extra CPU and memory usage. To help you decide whether to use a view or a table, keep this guideline in mind: When accessing the same data several times in a program, use a table instead of a view. This ensures consistent results from one step to the next and can significantly reduce the use of certain resources. Understanding PROC SQL Views Link To create a PROC SQL view, you use the CREATE VIEW statement. Unlike the CREATE TABLE statement, the CREATE VIEW statement has only one form, which contains a query. When you submit the CREATE VIEW statement, the query's report output is suppressed. 1 2 CREATE VIEW proc - sql - view AS SELECT ... ; Following the CREATE VIEW keywords, you specify the name of the PROC SQL view that you want to create. View names must follow the rules for SAS names. You cannot specify the name of an existing table or view in the same SAS library. Then you specify the keyword AS , followed by the query clauses. According to ANSI standards, a view must reside in the same library as the contributing table or tables. In PROC SQL , the default libref for the table (or tables) in the FROM clause is the libref of the library that contains the view. When the view and data source are in the same location, you can specify a one-level name for the table (or tables) in the FROM clause. In this situation, the one-level name does not designate temporary tables in the SAS work library. Technically, you can use any of the optional query clauses in the CREATE VIEW statement. However, for the sake of efficiency, it is recommended that you avoid using the ORDER BY clause in a query that defines a view. Using the ORDER BY clause in a view definition forces PROC SQL to sort the data every time the view is referenced. Instead, you can use an ORDER BY clause in queries that reference the view. Creating and Validating a PROC SQL View Link The DESCRIBE VIEW statement prints a description of the view in the log. The syntax of the DESCRIBE VIEW statement is similar to the syntax of the DESCRIBE TABLE statement. Example: Creating, Validating, and Referencing a PROC SQL View 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 proc sql ; create view orion . tom_zhou as select Employee_Name as Name format = $ 25 . 0 , Job_Title as Title format = $ 15 . 0 , Salary 'Annual Salary' format = comma10 . 2 , int (( today () - Employee_Hire_Date ) / 365 . 25 ) as YOS 'Years of Service' from employee_addresses as a , employee_payroll as p , employee_organization as o where a . Employee_ID = p . Employee_ID and o . Employee_ID = p . Employee_ID and Manager_ID = 120102 ; describe view orion . tom_zhou ; quit ; proc sql ; title \"Tom Zhou's Direct Reports\" ; title2 \"By Title and Years of Service\" ; select * from orion . tom_zhou order by Title desc , YOS desc ; quit ; title \"Tom Zhou's Group - Salary Statistics\" ; proc means data = orion . tom_zhou min mean max ; var salary ; class title ; run ; title ; Making a View Portable with the USING Clause Link By using a SAS enhancement to the ANSI standard for SQL, you can create a usable view that is stored in a different physical location than its source tables. In other words, you can make the view portable. To create a portable view, you add a USING clause to the query in the CREATE VIEW statement. 1 2 3 CREATE VIEW proc - sql - view AS SELECT ... USING LIBNAME - clause < , ... LIBNAME - clause > ; In the USING clause, an embedded LIBNAME statement enables you to assign a libref that is used for the source tables. In the syntax, this is called a LIBNAME clause because it appears within another clause. To reference source tables in multiple libraries, you can specify multiple LIBNAME clauses. In the FROM clause, you use two-level names for the tables. The library definition in the USING clause is local to the view so it does not interfere with any other location that is assigned to that libref in the same SAS session. This means that the USING clause defines the library only while the view is executing. The USING clause libref is deassigned when the stored query finishes running. In general, when you create a permanent PROC SQL view based on data in one or more permanent tables, it is a good practice to make the view portable by adding the USING clause. Example: Making a View Portable with the USING Clause 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 libname orion 'file-path-1' ; proc sql ; create view orion . tom_zhou as select Employee_Name as Name format = $ 25 . 0 , Job_Title as Title format = $ 15 . 0 , Salary \"Annual Salary\" format = comma10 . 2 , int (( today () - Employee_Hire_Date ) / 365 . 25 ) as YOS 'Years of Service' from orion . employee_addresses as a , orion . employee_payroll as p , orion . employee_organization as o where a . Employee_ID = p . Employee_ID and o . Employee_ID = p . Employee_ID and Manager_ID = 120102 using libname orion 'file-path-2' ; quit ; Note: file-path-1 is the location in which you want to create the view, and file-path-2 is the physical location in which the source data is stored. You must have write access to the location specified as file-path-1 .","title":"Creating Tables and Views"},{"location":"sql/tables-views/#creating-tables","text":"","title":"Creating Tables"},{"location":"sql/tables-views/#methods-of-creating-tables-with-the-create-table-statement","text":"PROC SQL provides three methods of creating new tables: If one or more existing tables have the data that you need, you can copy some or all of the existing columns and rows to a new table when you create it. If you start with an existing table, you can also copy only the column structure to create a new table that has no rows (i.e. an empty table). You'll need to add the data in a separate step. If no existing table has the column structure that you want, you can define new columns in your code to create an empty table. Each method of creating a table uses a different form of the CREATE TABLE statement. Each CREATE TABLE statement can create only one table.","title":"Methods of Creating Tables with the CREATE TABLE Statement"},{"location":"sql/tables-views/#using-a-query-in-the-create-table-statement","text":"To create a table by copying columns and rows from one or more existing tables, you use a query in the CREATE TABLE statement. 1 2 CREATE TABLE table - name AS SELECT ...; Following the CREATE TABLE keywords and the name of the table that you want to create, you specify the keyword AS . Next, you specify a query that returns the columns and rows that you want from the existing table or tables. In addition to the required SELECT and FROM clauses, you can use any of the optional clauses from a SELECT statement. The entire statement contains only one semicolon, which appears at the end. When you specify a query within the CREATE TABLE statement, the report output is suppressed. The CREATE TABLE statement generates only a table. When you create a new table, it is a good idea to verify the contents of the table by using the DESCRIBE TABLE statement. Example: Creating a Table by Copying Columns and Rows, and Validating the Table 1 2 3 4 5 6 7 8 9 10 11 12 13 proc sql ; create table work . birthmonths as select Employee_Name as Name format = $ 25 ., City format = $ 25 ., month ( Birth_Date ) as BirthMonth 'Birth Month' format = 3 . from orion . employee_payroll as p , orion . employee_addresses as a where p . Employee_ID = a . Employee_ID and Employee_Term_Date is missing order by BirthMonth , City , Name ; describe table orion . birthmonths ; quit ;","title":"Using a Query in the CREATE TABLE Statement"},{"location":"sql/tables-views/#using-the-like-clause-in-the-create-table-statement","text":"To create a table by copying columns and rows from one or more existing tables, you use a query in the CREATE TABLE statement. 1 2 CREATE TABLE table - name - 2 LIKE table - name - 1 ; Following the keywords CREATE TABLE , you specify the name of the table that you want to create ( table-name-2 ). Table names must follow the rules for SAS names. The LIKE clause specifies the name of the existing table whose column structure you want to copy Example: Creating a Table by Copying the Column Structure 1 2 3 4 proc sql ; create table work . new_sales_staff like orion . sales ; quit ;","title":"Using the LIKE Clause in the CREATE TABLE Statement"},{"location":"sql/tables-views/#defining-columns-in-the-create-table-statement","text":"To create a new, empty table that is not based on an existing table, you can define the columns in the CREATE TABLE statement. 1 2 3 CREATE TABLE table - name ( column - name type < column - modifier ( s ) > < ,... column - name type < column - modifier ( s ) >> ); Following the CREATE TABLE keywords, you specify the name of the table you are creating. Then you specify a set of column definitions that, together, make up the table definition. The column definitions are separated by commas. The entire table definition is enclosed in parentheses. Each column definition consists of a column name (which must follow the rules for SAS names), a type (also referred to as a data type), and (optional) column modifiers such as informats, formats, and labels. In SAS data sets, a column's type can be either character or numeric. Columns are defined as follows: To define a character column , you specify the CHARACTER keyword and, optionally, a column length in parentheses. In SAS, the default column length for character columns is eight characters. To define a numeric column , you specify the NUMERIC keyword. In a SAS data set, PROC SQL creates all numeric columns with the maximum precision allowed by SAS, so there is no need to specify a length. For ANSI compliance, PROC SQL accepts additional data types in column definitions. However, all additional types are converted to either character or numeric in the SAS data set. For example, in a SAS data set, a date column is a numeric column that has a date informat or format. Example: Creating a Table by Defining the Columns 1 2 3 4 5 6 7 proc sql ; create table discounts ( Product_ID num format = z12 ., Start_Date date , End_Date date , Discount num format = percent .); quit ;","title":"Defining Columns in the CREATE TABLE Statement"},{"location":"sql/tables-views/#methods-of-adding-data-to-a-table-with-the-insert-statement","text":"PROC SQL provides the following methods of adding data to an existing table: Specify column name-value pairs. Specify an ordered list of values. Specify a query that returns one or more rows. If the table already contains rows, the new rows are appended. Each method uses a different form of the INSERT statement. The first part of this statement is always the same: the keywords INSERT INTO are followed by the name of the table into which you are adding data. The rest of the INSERT statement varies by method. Each INSERT statement can add data to only one table.","title":"Methods of Adding Data to a Table with the INSERT Statement"},{"location":"sql/tables-views/#specifying-column-name-value-pairs-in-the-set-clause","text":"In the INSERT statement, the SET clause specifies or alters the values of one or more columns in a row. 1 2 3 INSERT INTO table - name SET column - name = value , column - name = value ... ; The SET clause contains one or more pairs of column names and values. In each pair, the column name and the value are joined by an equal sign. Multiple pairs are separated by commas. The column names can be listed in any order. Character values are enclosed in quotation marks. To insert values into multiple rows, you can specify multiple SET clauses in the INSERT statement. Multiple SET clauses are not separated by commas. Multiple SET clauses do not need to list the same columns or list columns in the same order. Example: Using the SET Clause to Add Data to a Table 1 2 3 4 5 6 7 8 9 10 11 proc sql ; insert into discounts set Product_ID = 230100300006 , Start_Date = '01MAR2013' d , End_Date = '15MAR2013' d , Discount = . 33 set Product_ID = 230100600018 , Start_Date = '16MAR2013' d , End_Date = '31MAR2013' d , Discount = . 15 ; quit ;","title":"Specifying Column Name-Value Pairs in the SET Clause"},{"location":"sql/tables-views/#specifying-an-ordered-list-of-values-in-the-values-clause","text":"In the INSERT statement, the VALUES clause adds values to the columns in a single row. 1 2 3 INSERT INTO table - name < ( column < , ... column > ) > VALUES ( value , value , ... ); After the VALUES keyword, you specify a list of one or more values in parentheses. Within the list, you separate multiple values with commas. To add more than one row of values to the table, you specify additional VALUES clauses. Multiple VALUES clauses are not separated by commas. By default, in each VALUES clause, you specify values for all of the columns in the table, in the order that the columns appear in the table. If you want to specify values in an order that is different from the column order in the table, or if you want to specify values for only a subset of the columns in the table, you must add an optional column list in parentheses after the table name. The order of the columns in the column list is independent of the order of the columns in the table. When you specify a column list, the order of the values in each VALUES clause must match the order of the columns in the column list. Any columns that are in the table but do not appear in a column list are given missing values. Example: Using the VALUES Clause to Add Data to a Table 1 2 3 4 5 6 7 proc sql ; insert into discounts values ( 230100300006 , '01MAR2013' d , '15MAR2013' d ,. 33 ) values ( 230100600018 , '16MAR2013' d , '31MAR2013' d ,. 15 ); quit ;","title":"Specifying an Ordered List of Values in the VALUES Clause"},{"location":"sql/tables-views/#specifying-a-query-in-the-insert-statement","text":"To add data from one existing table to another, you can specify a query in the INSERT statement. 1 2 3 4 5 INSERT INTO table - name < ( column < , ... column > ) > SELECT column ( s ) FROM table - name < additional clauses > ; By default, the SELECT clause specifies values for every column in the target table, and the order of the values must match the order of the columns in the target table. If you want to specify values in a different order or for only a subset of the columns in the target table, you can add an optional column list in parentheses after the table name in the INSERT statement. The order of the columns in the column list is independent of the order of the columns in the table. Example: Specifying a Query in the INSERT Statement 1 2 3 4 5 6 7 8 9 proc sql ; insert into discounts ( Product_ID , Discount , Start_Date , End_Date ) select distinct Product_ID ,. 35 , '01MAR2013' d , '31MAR2013' d from orion . product_dim where Supplier_Name contains 'Pro Sportswear Inc' ; quit ;","title":"Specifying a Query in the INSERT Statement"},{"location":"sql/tables-views/#creating-proc-sql-views","text":"","title":"Creating PROC SQL Views"},{"location":"sql/tables-views/#understanding-proc-sql-views","text":"A SAS view that is created by PROC SQL is called a PROC SQL view. A PROC SQL view is a stored query that can be based on one or more tables or any kind of SAS view: PROC SQL views, DATA step views, or SAS/ACCESS views. So, a PROC SQL view contains query code but no actual data. Unlike an empty table, a view is not a physical table. Instead, a PROC SQL view is sometimes referred to as a virtual table because it can be referenced in queries and other SAS programs in the same manner as a physical table. Like a table, a PROC SQL view has a name. When a SAS program runs, each time a view is referenced, the stored query executes and extracts the most current data from the underlying data source. ANSI standards specify that a PROC SQL view must reside in the same library as the source table or tables. However, a PROC SQL view cannot have the same name as a table stored in the same library. Nor can you create a table that has the same name as an existing view in the same library. There is a SAS enhancement in PROC SQL that enables you to create a view that can reside in a different library than its data source. The main advantages and disadvantages of referencing PROC SQL views instead of tables in your SAS programs are listed below. Advantages: Avoid storing copies of large tables. Avoid a frequent refresh of table copies. When the underlying data changes, a view surfaces the most current data. Combine data from multiple database tables and multiple libraries or databases. Simplify complex queries. Prevent other users from inadvertently altering the query code. Prevent other users from viewing data that they should not be able to see. Disadvantages: Might produce different results each time they are accessed if the data in the underlying data sources changes. Can require significant resources each time that they execute. With a view, you save disk storage space at the cost of extra CPU and memory usage. To help you decide whether to use a view or a table, keep this guideline in mind: When accessing the same data several times in a program, use a table instead of a view. This ensures consistent results from one step to the next and can significantly reduce the use of certain resources.","title":"Understanding PROC SQL Views"},{"location":"sql/tables-views/#understanding-proc-sql-views_1","text":"To create a PROC SQL view, you use the CREATE VIEW statement. Unlike the CREATE TABLE statement, the CREATE VIEW statement has only one form, which contains a query. When you submit the CREATE VIEW statement, the query's report output is suppressed. 1 2 CREATE VIEW proc - sql - view AS SELECT ... ; Following the CREATE VIEW keywords, you specify the name of the PROC SQL view that you want to create. View names must follow the rules for SAS names. You cannot specify the name of an existing table or view in the same SAS library. Then you specify the keyword AS , followed by the query clauses. According to ANSI standards, a view must reside in the same library as the contributing table or tables. In PROC SQL , the default libref for the table (or tables) in the FROM clause is the libref of the library that contains the view. When the view and data source are in the same location, you can specify a one-level name for the table (or tables) in the FROM clause. In this situation, the one-level name does not designate temporary tables in the SAS work library. Technically, you can use any of the optional query clauses in the CREATE VIEW statement. However, for the sake of efficiency, it is recommended that you avoid using the ORDER BY clause in a query that defines a view. Using the ORDER BY clause in a view definition forces PROC SQL to sort the data every time the view is referenced. Instead, you can use an ORDER BY clause in queries that reference the view.","title":"Understanding PROC SQL Views"},{"location":"sql/tables-views/#creating-and-validating-a-proc-sql-view","text":"The DESCRIBE VIEW statement prints a description of the view in the log. The syntax of the DESCRIBE VIEW statement is similar to the syntax of the DESCRIBE TABLE statement. Example: Creating, Validating, and Referencing a PROC SQL View 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 proc sql ; create view orion . tom_zhou as select Employee_Name as Name format = $ 25 . 0 , Job_Title as Title format = $ 15 . 0 , Salary 'Annual Salary' format = comma10 . 2 , int (( today () - Employee_Hire_Date ) / 365 . 25 ) as YOS 'Years of Service' from employee_addresses as a , employee_payroll as p , employee_organization as o where a . Employee_ID = p . Employee_ID and o . Employee_ID = p . Employee_ID and Manager_ID = 120102 ; describe view orion . tom_zhou ; quit ; proc sql ; title \"Tom Zhou's Direct Reports\" ; title2 \"By Title and Years of Service\" ; select * from orion . tom_zhou order by Title desc , YOS desc ; quit ; title \"Tom Zhou's Group - Salary Statistics\" ; proc means data = orion . tom_zhou min mean max ; var salary ; class title ; run ; title ;","title":"Creating and Validating a PROC SQL View"},{"location":"sql/tables-views/#making-a-view-portable-with-the-using-clause","text":"By using a SAS enhancement to the ANSI standard for SQL, you can create a usable view that is stored in a different physical location than its source tables. In other words, you can make the view portable. To create a portable view, you add a USING clause to the query in the CREATE VIEW statement. 1 2 3 CREATE VIEW proc - sql - view AS SELECT ... USING LIBNAME - clause < , ... LIBNAME - clause > ; In the USING clause, an embedded LIBNAME statement enables you to assign a libref that is used for the source tables. In the syntax, this is called a LIBNAME clause because it appears within another clause. To reference source tables in multiple libraries, you can specify multiple LIBNAME clauses. In the FROM clause, you use two-level names for the tables. The library definition in the USING clause is local to the view so it does not interfere with any other location that is assigned to that libref in the same SAS session. This means that the USING clause defines the library only while the view is executing. The USING clause libref is deassigned when the stored query finishes running. In general, when you create a permanent PROC SQL view based on data in one or more permanent tables, it is a good practice to make the view portable by adding the USING clause. Example: Making a View Portable with the USING Clause 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 libname orion 'file-path-1' ; proc sql ; create view orion . tom_zhou as select Employee_Name as Name format = $ 25 . 0 , Job_Title as Title format = $ 15 . 0 , Salary \"Annual Salary\" format = comma10 . 2 , int (( today () - Employee_Hire_Date ) / 365 . 25 ) as YOS 'Years of Service' from orion . employee_addresses as a , orion . employee_payroll as p , orion . employee_organization as o where a . Employee_ID = p . Employee_ID and o . Employee_ID = p . Employee_ID and Manager_ID = 120102 using libname orion 'file-path-2' ; quit ; Note: file-path-1 is the location in which you want to create the view, and file-path-2 is the physical location in which the source data is stored. You must have write access to the location specified as file-path-1 .","title":"Making a View Portable with the USING Clause"},{"location":"statistics/anova/","text":"Graphical Analysis of Associations Link Before analyzing your data, you need to have a general idea of any associations between predictor variables and response variables An association exists between two variables when the expected value of one variable differs at different levels of the other variable One method for doing this is to conduct a graphical analysis of your data Associations between categorical predictor variable and a continuous response variable can be explored with SGPLOT to product box plots (box-and-whisker plots) ( X predictor variable vs Y response variable) If the regression line conecting the means of Y at each value of X is not horizontal there might be an association between them If the regression line is horizontal there is no association : knowing the value of X doesn't tell you anything about the value of Y PROC SGPLOT Link 1 2 3 4 PROC SGPLOT DATA = SAS - data - set ; VBOX response - variable / CATEGORY = predictor - variable CONNECT = MEAN DATALABEL = outlier - ID - variable ; RUN ; Two-Sample t-Tests Link You can use a one-sample t-test to determine if the mean of a population is equal to a particular value or not When you collect a random sample of independent observations from two different populations, you can perform a two-sample t-test When you compare the means of two populations using a two-sample t-test you make three assumptions: The data contains independent observations The distributions of the two populations are normal (check histograms and normal probability/Q-Q plots) The variances in these normal distributions are equal ( F-test is the formal way to verify this assumption) $F$ statistic: $F=max(s_1^2,s_2^2)/min(s_1^2,s_2^2) \\ge 1$ $H_0$: \u03c3$_1^2$ $=$ \u03c3$_2^2\\rightarrow F \\approx 1$ $H_a$: \u03c3$_1^2$ $\\ne$ \u03c3$_2^2\\rightarrow F\\gt 1$ The Pr>F value in the Equality of Variances table represents the p-value of the F-test for equal variances Two-sided Tests PROC TTEST Link PROC TTEST performs a two-sided two-sample t-test by default (confidence limits and ODS graphics included) It automatically test the assumption of equal variances and provides an exact two-sample t-test ( pooled ) when the assumptions are met and an approximate t-test ( scatterthwaite ) when it is not met The pooled and scatterthwaite t-tests are equal when the variances are equal 1 2 3 4 5 PROC TTEST DATA = SAS - data - set < options > plots ( shownull ) = interval ; \\ * shownull = vertical reference line at the mean value of H0 * \\ CLASS variable ; \\ * Classification variable * \\ VAR variable ( s ); \\ * Continuous response variables * \\ RUN ; One-sided Tests It can increase the power of a statistical test, meaning that if you are right about the direction of the true difference, you will more likely detect a significant difference with a one-sided test than with a tow-sided test The difference between the mean values for the null hypothesis will be defined by the alphabetical order of the classification variables (e.g.: female - male) 1 2 3 4 5 PROC TTEST DATA = SAS - data - set plots ( only shownull ) = interval H0 = 0 SIDES = u ; \\ * only = suppress the default plots ; u / l = upper / lower - tailed t - test * \\ CLASS variable ; \\ * Classification variable * \\ VAR variable ( s ); \\ * Continuous response variables * \\ RUN ; One-Way ANOVA Link When you want to determine whether there are significant differences between the means of two or more populations , you can use analysis of variance (ANOVA). You have a continuous dependent ( response ) variable and a categorical independent ( predictor ) variable You can have many levels of the predictor variable , but you can have only one predictor variable The squared value of the t statistic for a two-sample t-test is equal to the F statistic of a one-way ANOVA with two populations With ANOVA the $H_0$ is that all of the population means are equal and $H_a$ is that not all the population means are equal (at least one mean is different) To perform an ANOVA test you make three assumptions: You have a good, random, representative sample The error terms are normally distributed The residuals (each observation minus its group mean) are estimates of the error term in the model so you verify this assumption by examining diagnostic plots of the residuals (if they are approximately normal, the error terms will be too) If your sample sizes are reasonably large and approximately equal across groups, then only severe departures from normality are considered a problem Residuals always sum to 0, regardless of the number of observations. The error terms have equal variances across the predictor variable levels: you can conduct a formal test for equal variances and also plot the residuals vs predicted values as a way to graphically verify this assumption PROC GLM Link You can use PROC GLM to verify the ANOVA assumptions and perform the ANOVA test. It fits a general linear model of which ANOVA is a special case and also displays the sums of squares associated with each hypothesis it tests. 1 2 3 4 5 6 7 PROC GLM DATA=SAS-data-set PLOTS(ONLY)=DIAGNOSTICS(UNPACK); /* print each plot on a separated page */ CLASS variable(s); MODEL dependents=intependents </options> ; MEANS effects / HOVTEST </options> ; RUN; QUIT; HOVTEST : homogeneity of variance test option (Levene's test by default) + plot of residuals vs predicted values (means) If the between-group variability is significantly larger than the within-group variability , you reject the null that all the group means are equal You partition out the variability using sums of squares: Between-group variation: also called Model Sum of Squares (SSM): $\\sum n_i (\\overline Y_i- \\overline {\\overline Y})^2$ Within-group variation: also called Error Sum of Squares (SSE): $\\sum \\sum (Y_{ij}- \\overline Y_i)^2$ Total variation: also called the Total Sum of Squares (SST): $\\sum \\sum (Y_{ij}- \\overline {\\overline Y})^2$ SSM and SSE represent pieces of SST : the SSM is the variability explanied by the predictor variable levels and SSE the variability not explained by the predictor variable levels You want the larger piece of the total to be better represented by what you can explain (SSM) vs what you can't explain (SSE) ANOVA with Data from a Randomized Block Design Link In an observational study , you often examine what already occurred, and therefore have little control over factors contributing to the outcome. In a controlled experiment , you can manipulate the factors of interest and can more reasonably claim causation. The variation due to the nuisance factors (fundamental to the probabilistic model but are no longer of interest) is part of the random variation that the error sum of squares accounts for. Including a blocking variable in the model is in essence like adding a second predictor variable to the model in terms of the way you write it The way you set up your experiment and data collection is what defines it as a blocking factor Although you're not specifically interested in its effect, controlling the blocking variable makes it easier to detect an effect of the factor of interest In a model that does not include a blocking variable, its effects are lumped into the error term of the model (unaccounted for variation) When you include a blocking variable in your ANOVA model, any effects caused by the nuisance factors that are common within a sector are accounted for in the model sum of squares rather than the error sum of squares You make two more assumptions when you include a blocking factor in the model: Primary variable levels are randomly assigned within each block The effects of the primary variable are constant across the levels of the blocking factor (the effects don't depend on the block they are in, there are no interactions with the blocking variable) Note Levene's test for homogeneity is only available for one-way ANOVA models , so in this case, you have to use the Residuals by Predicted plot. PROC GLM Link 1 2 3 4 5 6 PROC GLM DATA = SAS - data - set PLOTS ( ONLY ) = DIAGNOSTICS ( UNPACK ); /* print each plot on a separated page */ CLASS variable ( s ) blocking - factor ( s ); MODEL dependents = intependents blocking - factor ( s ) </ options > ; RUN ; QUIT ; Rule of thumb : if the F-value is > 1 , then it helped to add the blocking factor in your model If you compare the MSE ( Mean Square in the table) without and with including the blocking variable in the model, there is a drop of its value meaning that you have been able to account for a bit more of the unexplained variability due to the nuisance factors helping o have more precise estimates of the effect of your primary variable It is also reflected in the R-Square value that is increased when a blocking factor is added to the model Thanks to adding a blocking variable to your model you can get your primary variable to be significant The Type III SS at the bottom of the output tests for the difference due to each variable, controlling for or adjusting for the other variable ANOVA Post Hoc Tests Link This test is used to determine which means differ from other means and control the error rate using multiple comparison method . Assuming the null hypothesis is true for your different comparisons, the probability that you conclude a difference exist at least one time when there really isn't a difference increases with the more tests you perform. So the chance that you make a Type I error increases each time you conduct a statistical test . The comparisonwise error rate (CER) is the probability of a Type I error on a single pairwise test (\u03b1) The experimentwise error rate (EER) is the probability of making at least one Type I error when performing the whole set of comparisons. It takes into consideration the number of pairwise comparisons you make, so it increases as the number of tests increase: EER=1-(1-\\alpha)^{comparisons} Tukey's Multiple Comparison Method This method, which is also known as the Honestly Significant Difference test, is a popular multiple comparison test that controls the EER This tests compares all possible pairs of means, so it can only be used when you make pairwise comparisons This method controls $EER=\\alpha$ when all possible pairwise comparisons are considered and controls $EER<\\alpha$ when fewer than all pairwise comparisons are considered Dunnett's Multiple Comparison Method This method is a specialised multiple comparison test that allows you to compare a single control group to all other groups It controls $EER \\le \\alpha$ when all groups are compared to the reference group (control) It accounts for the correlation that exists between the comparisons and you can conduct one-sided tests of hypothesis against the reference group PROC GLM Link 1 2 3 4 5 6 7 8 9 PROC GLM DATA=SAS-data-set; CLASS variable(s); MODEL dependents=intependents </options> ; LSMEANS effects </options-test-1> ; LSMEANS effects </options-test-2> ; [...] LSMEANS effects </options-test-n> ; RUN; QUIT; PDIFF=ALL requests p-values for the differences between ALL the means and a diffogram is produced automatically displaying all pairwise least square means differences and indicating which are significant It can be undestood as a least squares mean by least squares mean plot The point estimates for differences between the means for each pairwise comparison can be found at the intersections of the gray grid lines (intersection of appropriate indexes) The red/blue diagonal lines show the confidence intervals for the true differences of the means for each pairwise comparison The grey 45$^{\\circ}$ reference line represents equality of the means (if the confidence interval crosses over it, then there is no significant difference between the two groups and the diagonal line for the pair will be dashed and red ; if the difference is significant the line will be solid and blue ) The ADJUST= option specifies the adjustment method for multiple comparisons If you don't specify an option SAS uses the Tukey method by default , if you specify ADJUST=Dunnett the GLM procedure produces multiple comparisons using Dunnett's method and a control plot The control plot displays the least squares mean and confidence limits of each group compared to the reference group The middle horizontal line represents its least square mean value (you can see the arithmetic mean value un the upper right corner of the graph) The shaded area goes from the lower decision limit (LDL) to the upper decision limit (UDL) There is a vertical line for each group that you're comparing to the reference (control) group. If a vertical line extends past the shaded area , then the group represented by the line is significantly different (small p-value) than the reference group PDIFF=CONTROLU('value') specifies the control group for the Dunnett's case: the direction of the sign in Ha is the same as the direction you are testing, so this is a one-sided upper-tailed t-test If you specify ADJUST=T SAS will make no adjustments for multiple comparisons: is not recommended as there's a tendency to find more significant pairwise differences than might actually exist Two-Way ANOVA with Interactions Link When you have a continuous response variable and two categorical predictor variables , you use the two-way ANOVA model Effect : the magnitude of the expected change in the response variable presumably caused by the change in value of a predictor variable in the model In addition, the variables in a model can be referred to as effects or terms Main effect : is the effect of a single predictor variable Interaction effects : when the relationship of the response variable with a predictor changes with the changing of another predictor variable (the effect of one variable depends on the value of the other variable) When you consider an ANOVA with more than one predictor variable, it's called n-way ANOVA where n represents the number of predictor variables The analysis in a randomized block design is actually a special type of two-way ANOVA in which you have one factor of interest and one blocking factor When you analyze a two-way ANOVA with interactions, you first look at any tests for interactions among the factors If there is no interaction between the factors you can interpret the tests for the individual factor effects to determine their significance/non-significance If an interaction exists between any factors , the tests for the individual factor effects might be misleading due to masking of these effect by the interaction (this is specially true for unbalanced data with different number of observations for each combination of groups) When the interaction is not statistically significant you can analyze the main effect with the model in its current form (generally the method you use when you analyze designed experiments) Even when you analyze designed experiments, some statisticians might suggest that if the interaction is not significant, you can delete the interaction effect from your model, rerun the model and then just analyze the main effects increasing the power of the main effects test If the interaction term is significant , it is good practice to keep the main effect terms that make up the interaction in the model, whether they are significant or not (this preserves model hierarchy) You have to make the same three assumptions used in the ANOVA test The interaction terms are also called product terms or crossed effects PROC GLM Link 1 2 3 4 5 6 7 PROC GLM DATA = SAS - data - set ; CLASS independent1 independent2 ; MODEL dependent = independent1 independent2 independent1 * independent2 ; or MODEL dependent = independent1 | independent2 ; RUN ; QUIT ; This program is fitting to this model : Y_{ijk}=\\mu + \\alpha_i+\\beta_j+(\\alpha\\beta)_{ij}+\\epsilon_{ijk} dependent = overall mean + intependent1 + independent2 + interaction12 + unaccounted for variation In most situations you will want to use the Type III SS The Type I SS (sequential) are the sums of squares you obtain from fitting the effects in the order you specify in the model The Type III SS (marginal) are the sums of squares you obtain from fitting each effect after all the other terms in the model, that is the sums of squares for each effect corrected for the other terms in the model When examining these results you first have to look at the interaction term and if it's significant (p-value), the main effects don't tell you the whole story . It that is the case, you don't need to worry all that much about the significance of the main effects at this point for two reasons: You know that the effect of each variable1 level changes for the different variable2 levels You want to include the main effects in the model, whether they are significant or not, to preserve model hierarchy You can analyze the interaction between terms by looking at the interaction plot that SAS produces by default when you include an interaction term in the model To analyze and interpret the effect of one of the interacting variables you need to add the LSMEANS statement to your program 1 2 3 4 5 6 PROC GLM DATA = SAS - data - set ORDER = INTERNAL PLOTS ( ONLY ) = INTPLOT ; CLASS independent1 independent2 ; MODEL dependent = intependent1 independent2 independent1 * independent2 ; LSMEANS independent1 * independent2 / SLICE = independent1 ; RUN ; QUIT ; SAS creates two types of mean plots when you use the LSMEANS statement with an interaction term: The first plot displays the least squares mean (LS-Mean) for every effect level The second plot contains the same information rearranged so you can look a little closer at the combination levels STORE statement Link You can add a STORE statement to save your analysis results in an item store (a binary file format that cannot be modified). This allows you to run post-processing analysis on the stored results even if you no longer have access to the original data set. The STORE statement applies to the following SAS/STAT procedures: GENMOD , GLIMMIX , GLM , GLMSELECT , LOGISTIC , MIXED , ORTHOREG , PHREG , PROBIT , SURVEYLOGISTIC , SURVEYPHREG , and SURVEYREG . 1 2 STORE <OUT = > item-store-name < / LABEL='label'>; item-store-name is a usual one- or two-level SAS name, similar to the names that are used for SAS data sets label identifies the estimate on the output (is optional) PROC PLM Link To perform post-fitting statistical analysis and plotting for the contents of the store item, you use PROC PLM . The statements and options that are available vary depending upon which procedure you used to produce the item store. 1 2 3 4 5 6 7 8 9 10 PROC PLM RESTORE = item - store - specification < options > ; EFFECTPLOT INTERACTION ( SLICEBY = variable ) < plot - type < ( plot - definition options ) >> / CLM </ options > ; LSMEANS < model - effects > </ options > ; LSMESTIMATE model - effect < ' label ' > values < divisor = n >< ,... < ' label ' > values < divisor = n > </ options > ; SHOW options ; SLICE model - effect / SLICEBY = variable ADJUST = tukey </ options > ; WHERE expression ; RUN ; RESTORE specifies the source item store for processing EFFECTPLOT produces a display of the fitted model and provides options for changing and enhancing the displays LSMEANS computes and compares least squares means (LS-means) of fixed effects LSMESTIMATE provides custom hypothesis tests among least squares means SHOW uses ODS to display contents of the item store. This statement is useful for verifying that the contents of the item store apply to the analysis and for generating ODS tables. SLICE provides a general mechanism for performing a partitioned analysis of the LS-means for an interaction (analysis of simple effects) and it uses the same options as the LSMEANS statement WHERE is used in the PLM procedure when the item store contains BY-variable information and you want to apply the PROC PLM statements to only a subset of the BY groups","title":"Analysis of Variance (ANOVA)"},{"location":"statistics/anova/#graphical-analysis-of-associations","text":"Before analyzing your data, you need to have a general idea of any associations between predictor variables and response variables An association exists between two variables when the expected value of one variable differs at different levels of the other variable One method for doing this is to conduct a graphical analysis of your data Associations between categorical predictor variable and a continuous response variable can be explored with SGPLOT to product box plots (box-and-whisker plots) ( X predictor variable vs Y response variable) If the regression line conecting the means of Y at each value of X is not horizontal there might be an association between them If the regression line is horizontal there is no association : knowing the value of X doesn't tell you anything about the value of Y","title":"Graphical Analysis of Associations"},{"location":"statistics/anova/#proc-sgplot","text":"1 2 3 4 PROC SGPLOT DATA = SAS - data - set ; VBOX response - variable / CATEGORY = predictor - variable CONNECT = MEAN DATALABEL = outlier - ID - variable ; RUN ;","title":"PROC SGPLOT"},{"location":"statistics/anova/#two-sample-t-tests","text":"You can use a one-sample t-test to determine if the mean of a population is equal to a particular value or not When you collect a random sample of independent observations from two different populations, you can perform a two-sample t-test When you compare the means of two populations using a two-sample t-test you make three assumptions: The data contains independent observations The distributions of the two populations are normal (check histograms and normal probability/Q-Q plots) The variances in these normal distributions are equal ( F-test is the formal way to verify this assumption) $F$ statistic: $F=max(s_1^2,s_2^2)/min(s_1^2,s_2^2) \\ge 1$ $H_0$: \u03c3$_1^2$ $=$ \u03c3$_2^2\\rightarrow F \\approx 1$ $H_a$: \u03c3$_1^2$ $\\ne$ \u03c3$_2^2\\rightarrow F\\gt 1$ The Pr>F value in the Equality of Variances table represents the p-value of the F-test for equal variances Two-sided Tests","title":"Two-Sample t-Tests"},{"location":"statistics/anova/#proc-ttest","text":"PROC TTEST performs a two-sided two-sample t-test by default (confidence limits and ODS graphics included) It automatically test the assumption of equal variances and provides an exact two-sample t-test ( pooled ) when the assumptions are met and an approximate t-test ( scatterthwaite ) when it is not met The pooled and scatterthwaite t-tests are equal when the variances are equal 1 2 3 4 5 PROC TTEST DATA = SAS - data - set < options > plots ( shownull ) = interval ; \\ * shownull = vertical reference line at the mean value of H0 * \\ CLASS variable ; \\ * Classification variable * \\ VAR variable ( s ); \\ * Continuous response variables * \\ RUN ; One-sided Tests It can increase the power of a statistical test, meaning that if you are right about the direction of the true difference, you will more likely detect a significant difference with a one-sided test than with a tow-sided test The difference between the mean values for the null hypothesis will be defined by the alphabetical order of the classification variables (e.g.: female - male) 1 2 3 4 5 PROC TTEST DATA = SAS - data - set plots ( only shownull ) = interval H0 = 0 SIDES = u ; \\ * only = suppress the default plots ; u / l = upper / lower - tailed t - test * \\ CLASS variable ; \\ * Classification variable * \\ VAR variable ( s ); \\ * Continuous response variables * \\ RUN ;","title":"PROC TTEST"},{"location":"statistics/anova/#one-way-anova","text":"When you want to determine whether there are significant differences between the means of two or more populations , you can use analysis of variance (ANOVA). You have a continuous dependent ( response ) variable and a categorical independent ( predictor ) variable You can have many levels of the predictor variable , but you can have only one predictor variable The squared value of the t statistic for a two-sample t-test is equal to the F statistic of a one-way ANOVA with two populations With ANOVA the $H_0$ is that all of the population means are equal and $H_a$ is that not all the population means are equal (at least one mean is different) To perform an ANOVA test you make three assumptions: You have a good, random, representative sample The error terms are normally distributed The residuals (each observation minus its group mean) are estimates of the error term in the model so you verify this assumption by examining diagnostic plots of the residuals (if they are approximately normal, the error terms will be too) If your sample sizes are reasonably large and approximately equal across groups, then only severe departures from normality are considered a problem Residuals always sum to 0, regardless of the number of observations. The error terms have equal variances across the predictor variable levels: you can conduct a formal test for equal variances and also plot the residuals vs predicted values as a way to graphically verify this assumption","title":"One-Way ANOVA"},{"location":"statistics/anova/#proc-glm","text":"You can use PROC GLM to verify the ANOVA assumptions and perform the ANOVA test. It fits a general linear model of which ANOVA is a special case and also displays the sums of squares associated with each hypothesis it tests. 1 2 3 4 5 6 7 PROC GLM DATA=SAS-data-set PLOTS(ONLY)=DIAGNOSTICS(UNPACK); /* print each plot on a separated page */ CLASS variable(s); MODEL dependents=intependents </options> ; MEANS effects / HOVTEST </options> ; RUN; QUIT; HOVTEST : homogeneity of variance test option (Levene's test by default) + plot of residuals vs predicted values (means) If the between-group variability is significantly larger than the within-group variability , you reject the null that all the group means are equal You partition out the variability using sums of squares: Between-group variation: also called Model Sum of Squares (SSM): $\\sum n_i (\\overline Y_i- \\overline {\\overline Y})^2$ Within-group variation: also called Error Sum of Squares (SSE): $\\sum \\sum (Y_{ij}- \\overline Y_i)^2$ Total variation: also called the Total Sum of Squares (SST): $\\sum \\sum (Y_{ij}- \\overline {\\overline Y})^2$ SSM and SSE represent pieces of SST : the SSM is the variability explanied by the predictor variable levels and SSE the variability not explained by the predictor variable levels You want the larger piece of the total to be better represented by what you can explain (SSM) vs what you can't explain (SSE)","title":"PROC GLM"},{"location":"statistics/anova/#anova-with-data-from-a-randomized-block-design","text":"In an observational study , you often examine what already occurred, and therefore have little control over factors contributing to the outcome. In a controlled experiment , you can manipulate the factors of interest and can more reasonably claim causation. The variation due to the nuisance factors (fundamental to the probabilistic model but are no longer of interest) is part of the random variation that the error sum of squares accounts for. Including a blocking variable in the model is in essence like adding a second predictor variable to the model in terms of the way you write it The way you set up your experiment and data collection is what defines it as a blocking factor Although you're not specifically interested in its effect, controlling the blocking variable makes it easier to detect an effect of the factor of interest In a model that does not include a blocking variable, its effects are lumped into the error term of the model (unaccounted for variation) When you include a blocking variable in your ANOVA model, any effects caused by the nuisance factors that are common within a sector are accounted for in the model sum of squares rather than the error sum of squares You make two more assumptions when you include a blocking factor in the model: Primary variable levels are randomly assigned within each block The effects of the primary variable are constant across the levels of the blocking factor (the effects don't depend on the block they are in, there are no interactions with the blocking variable) Note Levene's test for homogeneity is only available for one-way ANOVA models , so in this case, you have to use the Residuals by Predicted plot.","title":"ANOVA with Data from a Randomized Block Design"},{"location":"statistics/anova/#proc-glm_1","text":"1 2 3 4 5 6 PROC GLM DATA = SAS - data - set PLOTS ( ONLY ) = DIAGNOSTICS ( UNPACK ); /* print each plot on a separated page */ CLASS variable ( s ) blocking - factor ( s ); MODEL dependents = intependents blocking - factor ( s ) </ options > ; RUN ; QUIT ; Rule of thumb : if the F-value is > 1 , then it helped to add the blocking factor in your model If you compare the MSE ( Mean Square in the table) without and with including the blocking variable in the model, there is a drop of its value meaning that you have been able to account for a bit more of the unexplained variability due to the nuisance factors helping o have more precise estimates of the effect of your primary variable It is also reflected in the R-Square value that is increased when a blocking factor is added to the model Thanks to adding a blocking variable to your model you can get your primary variable to be significant The Type III SS at the bottom of the output tests for the difference due to each variable, controlling for or adjusting for the other variable","title":"PROC GLM"},{"location":"statistics/anova/#anova-post-hoc-tests","text":"This test is used to determine which means differ from other means and control the error rate using multiple comparison method . Assuming the null hypothesis is true for your different comparisons, the probability that you conclude a difference exist at least one time when there really isn't a difference increases with the more tests you perform. So the chance that you make a Type I error increases each time you conduct a statistical test . The comparisonwise error rate (CER) is the probability of a Type I error on a single pairwise test (\u03b1) The experimentwise error rate (EER) is the probability of making at least one Type I error when performing the whole set of comparisons. It takes into consideration the number of pairwise comparisons you make, so it increases as the number of tests increase: EER=1-(1-\\alpha)^{comparisons} Tukey's Multiple Comparison Method This method, which is also known as the Honestly Significant Difference test, is a popular multiple comparison test that controls the EER This tests compares all possible pairs of means, so it can only be used when you make pairwise comparisons This method controls $EER=\\alpha$ when all possible pairwise comparisons are considered and controls $EER<\\alpha$ when fewer than all pairwise comparisons are considered Dunnett's Multiple Comparison Method This method is a specialised multiple comparison test that allows you to compare a single control group to all other groups It controls $EER \\le \\alpha$ when all groups are compared to the reference group (control) It accounts for the correlation that exists between the comparisons and you can conduct one-sided tests of hypothesis against the reference group","title":"ANOVA Post Hoc Tests"},{"location":"statistics/anova/#proc-glm_2","text":"1 2 3 4 5 6 7 8 9 PROC GLM DATA=SAS-data-set; CLASS variable(s); MODEL dependents=intependents </options> ; LSMEANS effects </options-test-1> ; LSMEANS effects </options-test-2> ; [...] LSMEANS effects </options-test-n> ; RUN; QUIT; PDIFF=ALL requests p-values for the differences between ALL the means and a diffogram is produced automatically displaying all pairwise least square means differences and indicating which are significant It can be undestood as a least squares mean by least squares mean plot The point estimates for differences between the means for each pairwise comparison can be found at the intersections of the gray grid lines (intersection of appropriate indexes) The red/blue diagonal lines show the confidence intervals for the true differences of the means for each pairwise comparison The grey 45$^{\\circ}$ reference line represents equality of the means (if the confidence interval crosses over it, then there is no significant difference between the two groups and the diagonal line for the pair will be dashed and red ; if the difference is significant the line will be solid and blue ) The ADJUST= option specifies the adjustment method for multiple comparisons If you don't specify an option SAS uses the Tukey method by default , if you specify ADJUST=Dunnett the GLM procedure produces multiple comparisons using Dunnett's method and a control plot The control plot displays the least squares mean and confidence limits of each group compared to the reference group The middle horizontal line represents its least square mean value (you can see the arithmetic mean value un the upper right corner of the graph) The shaded area goes from the lower decision limit (LDL) to the upper decision limit (UDL) There is a vertical line for each group that you're comparing to the reference (control) group. If a vertical line extends past the shaded area , then the group represented by the line is significantly different (small p-value) than the reference group PDIFF=CONTROLU('value') specifies the control group for the Dunnett's case: the direction of the sign in Ha is the same as the direction you are testing, so this is a one-sided upper-tailed t-test If you specify ADJUST=T SAS will make no adjustments for multiple comparisons: is not recommended as there's a tendency to find more significant pairwise differences than might actually exist","title":"PROC GLM"},{"location":"statistics/anova/#two-way-anova-with-interactions","text":"When you have a continuous response variable and two categorical predictor variables , you use the two-way ANOVA model Effect : the magnitude of the expected change in the response variable presumably caused by the change in value of a predictor variable in the model In addition, the variables in a model can be referred to as effects or terms Main effect : is the effect of a single predictor variable Interaction effects : when the relationship of the response variable with a predictor changes with the changing of another predictor variable (the effect of one variable depends on the value of the other variable) When you consider an ANOVA with more than one predictor variable, it's called n-way ANOVA where n represents the number of predictor variables The analysis in a randomized block design is actually a special type of two-way ANOVA in which you have one factor of interest and one blocking factor When you analyze a two-way ANOVA with interactions, you first look at any tests for interactions among the factors If there is no interaction between the factors you can interpret the tests for the individual factor effects to determine their significance/non-significance If an interaction exists between any factors , the tests for the individual factor effects might be misleading due to masking of these effect by the interaction (this is specially true for unbalanced data with different number of observations for each combination of groups) When the interaction is not statistically significant you can analyze the main effect with the model in its current form (generally the method you use when you analyze designed experiments) Even when you analyze designed experiments, some statisticians might suggest that if the interaction is not significant, you can delete the interaction effect from your model, rerun the model and then just analyze the main effects increasing the power of the main effects test If the interaction term is significant , it is good practice to keep the main effect terms that make up the interaction in the model, whether they are significant or not (this preserves model hierarchy) You have to make the same three assumptions used in the ANOVA test The interaction terms are also called product terms or crossed effects","title":"Two-Way ANOVA with Interactions"},{"location":"statistics/anova/#proc-glm_3","text":"1 2 3 4 5 6 7 PROC GLM DATA = SAS - data - set ; CLASS independent1 independent2 ; MODEL dependent = independent1 independent2 independent1 * independent2 ; or MODEL dependent = independent1 | independent2 ; RUN ; QUIT ; This program is fitting to this model : Y_{ijk}=\\mu + \\alpha_i+\\beta_j+(\\alpha\\beta)_{ij}+\\epsilon_{ijk} dependent = overall mean + intependent1 + independent2 + interaction12 + unaccounted for variation In most situations you will want to use the Type III SS The Type I SS (sequential) are the sums of squares you obtain from fitting the effects in the order you specify in the model The Type III SS (marginal) are the sums of squares you obtain from fitting each effect after all the other terms in the model, that is the sums of squares for each effect corrected for the other terms in the model When examining these results you first have to look at the interaction term and if it's significant (p-value), the main effects don't tell you the whole story . It that is the case, you don't need to worry all that much about the significance of the main effects at this point for two reasons: You know that the effect of each variable1 level changes for the different variable2 levels You want to include the main effects in the model, whether they are significant or not, to preserve model hierarchy You can analyze the interaction between terms by looking at the interaction plot that SAS produces by default when you include an interaction term in the model To analyze and interpret the effect of one of the interacting variables you need to add the LSMEANS statement to your program 1 2 3 4 5 6 PROC GLM DATA = SAS - data - set ORDER = INTERNAL PLOTS ( ONLY ) = INTPLOT ; CLASS independent1 independent2 ; MODEL dependent = intependent1 independent2 independent1 * independent2 ; LSMEANS independent1 * independent2 / SLICE = independent1 ; RUN ; QUIT ; SAS creates two types of mean plots when you use the LSMEANS statement with an interaction term: The first plot displays the least squares mean (LS-Mean) for every effect level The second plot contains the same information rearranged so you can look a little closer at the combination levels","title":"PROC GLM"},{"location":"statistics/anova/#store-statement","text":"You can add a STORE statement to save your analysis results in an item store (a binary file format that cannot be modified). This allows you to run post-processing analysis on the stored results even if you no longer have access to the original data set. The STORE statement applies to the following SAS/STAT procedures: GENMOD , GLIMMIX , GLM , GLMSELECT , LOGISTIC , MIXED , ORTHOREG , PHREG , PROBIT , SURVEYLOGISTIC , SURVEYPHREG , and SURVEYREG . 1 2 STORE <OUT = > item-store-name < / LABEL='label'>; item-store-name is a usual one- or two-level SAS name, similar to the names that are used for SAS data sets label identifies the estimate on the output (is optional)","title":"STORE statement"},{"location":"statistics/anova/#proc-plm","text":"To perform post-fitting statistical analysis and plotting for the contents of the store item, you use PROC PLM . The statements and options that are available vary depending upon which procedure you used to produce the item store. 1 2 3 4 5 6 7 8 9 10 PROC PLM RESTORE = item - store - specification < options > ; EFFECTPLOT INTERACTION ( SLICEBY = variable ) < plot - type < ( plot - definition options ) >> / CLM </ options > ; LSMEANS < model - effects > </ options > ; LSMESTIMATE model - effect < ' label ' > values < divisor = n >< ,... < ' label ' > values < divisor = n > </ options > ; SHOW options ; SLICE model - effect / SLICEBY = variable ADJUST = tukey </ options > ; WHERE expression ; RUN ; RESTORE specifies the source item store for processing EFFECTPLOT produces a display of the fitted model and provides options for changing and enhancing the displays LSMEANS computes and compares least squares means (LS-means) of fixed effects LSMESTIMATE provides custom hypothesis tests among least squares means SHOW uses ODS to display contents of the item store. This statement is useful for verifying that the contents of the item store apply to the analysis and for generating ODS tables. SLICE provides a general mechanism for performing a partitioned analysis of the LS-means for an interaction (analysis of simple effects) and it uses the same options as the LSMEANS statement WHERE is used in the PLM procedure when the item store contains BY-variable information and you want to apply the PROC PLM statements to only a subset of the BY groups","title":"PROC PLM"},{"location":"statistics/categorical-data/","text":"When you response variable is categorical, you need to use a different kind of regression analysis: logistic regression . Describing Categorical Data Link When you examine the distribution of a categorical variable , you want to know the values of the variable and the frequency or count of each value in the data ( one-way frequency able ). 1 2 3 4 PROC FREQ DATA = SAS - data - set ; TABLES variable1 variable2 variable3 </ options > ; < additional statements > RUN ; To look for a possible association between two or more categorical variables, you can create a crosstabulation / contingency table (when it displays statistics for two variables is also called two-way frequency able ). 1 2 3 4 PROC FREQ DATA = SAS - data - set ; TABLES variable - rows * variable - columns </ options > ; < additional statements > RUN ; Two distribution plots are associated with a frequency or crosstabulation table: a frequency plot , PLOTS=(FREQPLOT) , and a cumulative frequency plot . In PROC FREQ output, the default order for character values is alphaumeric . To reorder the values of an ordinal variable in your PROC FREQ output you can: Create a new variable in which the values are stored in logical order Apply a temporary format to the original variable How to replace the variable's name with the variable's label in PROC FREQ output 1 2 3 4 options validvarname = any ; PROC FREQ DATA = SAS - data - set ( RENAME = ( variable1 = \"Label variable 1\" n variable1 = \"Label variable 1\" n )); TABLES \"Label variable 1\" n ; RUN ; Count the distinct values of a variable : The question of how to count distinct values of a CLASS or BY variable using either PROC MEANS or PROC SUMMARY is asked frequently. While neither of these procedures has this ability, PROC SQL can count these values using the DISTINCT option and PROC FREQ using the NLEVELS option. Tests of Association Link Pearson Chi-square Test Link To perform a formal test of association between two categorical variables, you use the (Pearson) chi-square test which measures the difference between the observed cell frequencies and the cell frequencies that are expected if there is no association between variables ($H_0$ is true): $Expected=Row \\ total\\cdot Column\\ total/Total \\ sample \\ size$ If the sample size decreases , the chi-square value decreases and the p-value for the chi-square statistic increases Hypothesis testing: $H_0$ : no association; $H_a$ : association Cramer's V statistic Link It is one measure of strength of an association between two categorical variables: For two-by-two tables, Cramer's V is in the range of -1 to 1 For larger tables, Cramer's V is in the range of 0 to 1 Values farther away from 0 indicate a relatively strong association between the variables To measure the strength of the association between a binary predictor variable and a binary outcome variable, you can use an odds ratio : $Odds \\ Ratio=\\frac{Odds \\ of \\ Outcome \\ in \\ Group \\ B}{Odds \\ of \\ Outcome \\ in \\ Group \\ A}$; $Odds=p_{event}/(1-p_{event})$ The value of the odds ratio can range from 0 to $\\infty$; it cannot be negative When the odds ratio is 1 , there is no association between variables When the odds ratio >1/<1, the group in the numerator/denominator is more likely to have the outcome The odds ratio is approximately the same regardless of the sample size To estimate the true odds ratio while taking into account the variability of the sample statistic, you can calculate confidence intervals You can use an odds ratio to test for significance between two categorical variables Odds ratio expressed as percent difference: $(odd \\ ratio -1) \\cdot 100$ 1 2 3 4 PROC FREQ DATA = SAS - data - set ; TABLES variable - rows * variable - columns / CHISQ EXPECTED </ options > ; < additional statements > RUN ; CHISQ produces the Pearson chi-square test of association, the likelihood-ratio chi-square and the Mantel-Haenszel: $\\sum \\frac{(obs. \\ freq. - exp. \\ freq.)^2}{exp. \\ freq.}$ EXPECTED prints the expected cell frequencies CELLCHI2 prints each cell's contribution to the total chi-square statistic: $ \\frac{(obs. \\ freq. - exp. \\ freq.)^2}{exp. \\ freq.}$ NOCOL / NOROW suppresses the printing of the column/row percentages NOPERCENT supresses the printing of the cell percentages RELRISK (relative risk) prints a table that contains risk ratios (probability ratios) and odds ratios; PROC FREQ uses the classification in the first column of the crosstabulation table as the outcome of interest and the first/second row in the numerator/denominator Mantel-Haenszel chi-square test Link For ordinal associations , the Mantel-Haenszel chi-square test is a more powerful test. The levels must be in a logical order for the test results to be meaningful Hypothesis testing: $H_0$ : no ordinal association; $H_a$ : ordinal association Similarly to the Pearson case, the Mantel-Haenszel chi-square statistic/p-value indicate whether an association exists but not its magnitude and they depend on and reflect the sample size To measure the strength of the association between two ordinal variables you can use the Spearman correlation statistic. You should only use it if both variables are ordinal and are in logical order Is considered to be a rank correlation because it provides a degree of association between the ranks of the ordinal variables This statistic has a range between -1 and +1 : values close to -1/+1 indicate that there is a relatively high degree of negative/positive correlation and values close to 0 indicate a weak correlation It is not affected by the sample size 1 2 3 4 PROC FREQ DATA = SAS - data - set ; TABLES variable - rows * variable - columns / CHISQ EXPECTED </ options > ; < additional statements > RUN ; MEASURES produces the Spearman correlation statistic along with other measurement of association CL produces confidence bounds for the statistics that the MEASURES option requests The confidence bounds are valid only if the sample size is large (>25) The asymptotic standard error ( ASE ) is used for large samples and is used to calculate the confidence intervals for various measures of association (including the Spearman correlation coefficient) Introduction to Logistic Regression Link Logistic Regression is a generalized linear model (like Linear Regression or ANOVA) that you can use to predict a categorical response/outcome based on one or more continuous/categorical predictor variables. There are three models: Linear vs Logistic Link Linear Regression Model Link Assumes that the expected value of the response continuous variable ($Y$) has a linear relationship with the predictor variable ($X$) The conditional mean of the response hast the linear form $E(Y|X)=\\beta_0+\\beta_1X$ and it ranges $(-\\infty,+\\infty)$ Why not to use Linear Regression to model a binary response variable Following the Linear Regression Model scheme, the response variable is calculated as $Y_i=\\beta_0+\\beta_1\\cdot X_i+\\epsilon_i$, where $\\beta_0$ and $\\beta_1$ are obtained by the method of least squares. This model assumes that the data is continuous , which is not true for the case of binary data This model assumes that the mean of the response is $\\beta_0+\\beta_1\\cdot X$ , while for binary data the mean of the response is the probability of a success If the response variable has only two levels, you cannot assume the constant variance and normality that are required for linear regression Binary Logistic Regression Model Link The predictor variable ($X$) is used to estimate the probability of a specific outcome ($p$) for which you need to use a nonlinear function The mean of the response is a probability, which is between $(0, 1)$. The Inverse Logit Function binds the linear predictor between $0$ and $1$ is defined as $p_i=(1+e^{-(\\beta_0+\\beta_1 X_i)})^{-1}$ This model applies a Logit Transformation to the probabilities $logit(p_i)=ln\\left ( \\frac{p_i}{1-p_i} \\right ) = \\beta_0+\\beta_1X_i$, so that the transformed probabilities and predictor variables end up with a linear relationship The logit is the natural log of the odds (the probability of the event occurring divided by the probability of the event not occurring) We make the assumption that the logit transformation of the probabilities results in a linear relationship with the predictor variables (we can use a linear function $X$ to model the logit in order to indireclty model the probability) The logit of the probability transforms the probability into a linear function, which has no lower or upper bounds. So a logit has no lower or upper bounds . PROC LOGISTIC Link To model categorical data you use the LOGISTIC procedure. Some of the most common statements of this procedure are shown here: 1 2 3 4 5 6 PROC LOGISTIC DATA=SAS-data-set <options> ; CLASS variable < (variable_options)> ... </ options> ; MODEL response < (variable_options)> = predictors </ options> ; UNITS independent1=list... </ options> ; ODDSRATIO < 'label'> variable </ options> ; RUN; CLASS is used to define the classification (categorical) predictor variables (if any); this statement must precede the MODEL statement CLODDS = PL (profile likelihood) | WALD (default) | BOTH is an example of a general option that you can specify in the MODEL statement which computes confidence intervals for the odds ratios of all predictor variables and also enables the production of the odds ratio plot Example PROC LOGISTIC DATA=statdata.sales_inc PLOTS(ONLY)=(EFFECT ODDSRATIO); CLASS gender; MODEL purchase(EVENT='1')=gender / CLODDS=PL; RUN; Classification Variables Parametrization Link When the predictor variable is categorical, the assumption of linearity cannot be met. To get past the obstacle of nonlinearity, the CLASS statement creates a set of one or more design variables (also called dummy variables). PROC LOGISTIC uses these variables, and not the original ones, in model calculations. Different parametrization methods for the classification variables will produce the same results regarding the significance of the categorical predictors, but understanding the parametrization method helps to interpret the results accurately. Here we present two of the most common methods of parameterizing ( PARAM = ) the classification variables. For both of them: The default reference level is the level that has the highest ranked value (or the last value) when the levels are sorted in ascending alphanumeric order The number of design variables (or $\\beta$) that are created are the number of levels of the classification variable -1 Effect coding (default) Link Also called deviation from the mean coding , it compares the effect of each level of the variable to the average effect of all levels . Example Using this parametrization scheme the model will be described as follows $logit(p)=\\beta_0+\\beta_1\\cdot D_{Low \\ Income} + \\beta_2\\cdot D_{Medium \\ Income}$ $\\beta_0$ is the average value of the logit across all income levels $\\beta_1$ is the difference between the logit for income level 1 and $\\beta_0$ $\\beta_2$ is the difference between the logit for income level 2 and $\\beta_0$ Here's the Analysis of Maximum Likelihood Estimates table that PROC LOGISTIC generates for this example on which the parameter estimates ($\\beta_0$, $\\beta_1$, $\\beta_2$) and p-values reflect differences from the overall mean value over all levels. The p-values indicate whether each particular level is significant compared to the average effect of all levels. The p-values for $\\beta_1$ and $\\beta_2$ not significant meaning that the effect of those levels is not different than the average effect of low, medium and high income. Reference cell coding Link It compares the effect of each level of the predictor to the effect of another level that is the designated reference level . Example To use this scheme the classification variable has to be defined in the following way CLASS gender (PARAM=REF REF='Male'); Using this parametrization scheme the model will be described as follows $logit(p)=\\beta_0+\\beta_1\\cdot D_{Low \\ Income} + \\beta_2\\cdot D_{Medium \\ Income}$ $\\beta_0$ is the intercept, but not in terms of where you cross the $Y$ axis, instead is the value of the logit of the probability when income is high (or at the reference level) $\\beta_1$ is the difference between the logit of the probability for low and high income $\\beta_2$ is the difference between the logit of the probability for medium and high income Here's the Analysis of Maximum Likelihood Estimates table that PROC LOGISTIC generates for this example on which the parameter estimates ($\\beta_0$, $\\beta_1$, $\\beta_2$) and p-values reflect differences with respect to the reference level. The p-values indicate whether each particular level is significant compared to the reference level. The p-value for $\\beta_1<0.05$ is significant meaning that the effect of a low income is statistically different than the effect of a high income on the probability that people will spend at least $100\\$$. The same applies to $\\beta_2$. Fitting a Binary Logistic Regression Link 1 2 3 4 5 PROC LOGISTIC DATA = statdata . sales_inc PLOTS ( ONLY ) = ( EFFECT ); CLASS Gender ( PARAM = REF REF = 'Male' ); MODEL Purchase ( event = '1' ) = Gender ; title1 'LOGISTIC MODEL (1): Purchase = Gender' ; RUN ; We look at the first few tables to make sure that the model is set up the way we want The Model Information table describes the data set, the response variable, the number of response levels, the type of model, the algorithm used to obtain the parameter estimates, and the number of observations read and used. The Response Profile table shows the values of the response variable, listed according to their ordered value and frequency. By default, PROC LOGISTIC orders the values of the response variable alphanumercally and bases the logistic regression model on the probability of the lowest value. However, we set the EVENT=1 , the highest value, so this model is based on the probability that Purchase=1 . Below this table, we see the probability that PROC LOGISTIC is modeling, as shown in the log. The Class Level Information table displays the predictor variable in the CLASS statement Gender (in the model we fixed 'Male' as the reference level, so the design variable is 1 when Gender='Female' and 0 when Gender='Male' ). The Model Convergence Status simply indicates that the convergence criterion was met. There are a number of options to control the convergence criterion, but the default is the gradient convergence criterion with a default value of $10^{-8}$. The Model Fit Statistic table reports the resuls of three tests (for the model with the intercept only and the model with the intercept and the predictor variables): AIC, SC and -2$\\cdot$Log(likelihood). AIC and SC are goodness-of-fit measures that you can use to compare one model to another (lower values indicate more desirable model, although there is no standard for determining how much of a difference indicates an improvement) and are not dependent on the number of terms in the model. Akaike's Information Criterion (AIC) : it adjusts for the number of predictor valriables. It is the best statitstic to come up with the best explanatory model . Schwarz's Bayesian Criterion (SC) : it adjusts for the number of predictor variables and the number of observations. This test uses a bigger penalty for extra variables and therefore favors more parsimonious models. It is the best statitstic to come up with the best predictive model . The Testing Global Null Hypothesis: BETA=0 table provides three statistics to test $H_0$ that all the regression coefficients in the model are 0. The Likelihood Ratio is the most reliable test, specially for small sample sizes. It is similar to the overall F test in linear regression. The Type 3 Analysis of Effects table is generated when CLASS specifies a categorical predictor variable. It shows the results of testing whether each individual parameter estimate is statistically different from 0 ( Pr>ChiSq $<0.05$). The Wald Chi-Square statistic tests the listed effects. When there is only one predictor variable in the model, the value listed in the table will be identical to the Wald test in the Testing Global Null Hypothesis table. The Analysis of Maximum Likelihood Estimates table lists the estimated model parameters (the betas), their standard errors, Wald test statistics and corresponding p-values. The parameter estimates are the estimated coefficients of the fitted logistic regression model. We can use these estimates to construct the logistic regression equation $logit(\\beta)=\\beta_0+\\beta_1 \\cdot Categorical \\ predictor$. The Odds Ratio Estimates table ( ONLY for binary logistic regression ) shows the OR ratio for the modeled event. Notice that PROC LOGISTIC calculates Wald confidence limits by default. The Association of Predicted Probabilities and Observed Responses table lists several goodness-of-fit measures. The Odds Ratio Estimates and Profile-Likelihood Confidence Intervals table ( ONLY for multiple logistic regression ) contains the OR estimates and the profile-likelihood confidence limits that the CLODDS= option specified for the main effects. For multiple logistic regression, remember that PROC LOGISTIC calculates the adjusted odds ratios. For a continuous predictor, the odds ratio measures the change in odds associated with a one-unit difference of the predictor variable by default although it is specified otherwise in the UNITS statement. In the Odds Ratios plot , the dots represent the OR estimates and the horizontal lines represent the confidence intervals for those estimates. There is a reference line at one to check if the confidence intervals cross this value meaning that it is not statistically different from 1. The Effect plot shows the levels of the CLASS predictor variable vs the probability of the desired outcome. If you are performing a multiple logistic regression analysis where there is a continuous predictor variable it will be represented in the horizontal axis. Moreover, the lines will represent all possible combinations of the different CLASS variables. Iterpreting the Odds Ratio for a Categorical Predictor Link Let's see how to calculate the odds and the odds ratio from the logistic regression model. Here is the logistic regression model that predicts the logit of $p$: $logit(\\hat p)=ln(odds)=ln\\left ( \\frac{p_i}{1-p_i} \\right )=\\beta_0 + \\beta_1 \\cdot Gender$ According to our example the variable Gender is codified in a way that Females=1 and Males=0 , so the OR can be written: $odds_{females}=e^{\\beta_0+\\beta_1}$ $odds_{males}=e^{\\beta_0}$ $odds \\ ratio = \\frac{e^{\\beta_0+\\beta_1}}{e^{\\beta_0}}=e^{\\beta_1}$ If the 95% confidence interval does not include 1, the OR is significant at the 0.05 level indicating an association between the predictor and response variables of your model. Iterpreting the Odds Ratio for a Continuous Predictor Link For a continuous predictor variable, the OR measures the increase or decrease in odds associated with a one-unit difference of the predictor variable by default. $OR - 1 = %$ of greater odds for having one-unit of difference. Comparing Pairs to Assess the Fit of a Logistic Regression Model Link PROC LOGISTIC calculates several different goodness-of-fit measures and displayed in the Association of Predicted Probabilities and Observed Responses table. One of these goodness-of-fit methods is comparing pairs ( Pairs ). To start, PROC LOGISTIC creates two groups of observations, one for each value of the response variable. Then, the procedure selects pairs of observations, one from each group, until no more pairs can be selected. PROC LOGISTIC determines whether each pair is concordant, discordant or tied. A pair is concordant if the model predicts it correclty , i.e. if the observation with the desired outcome has a higher predicted probability , based on the model, than the observation without the outcome. A pair is discordant if the model does not predict it correctly , i.e. if the observation with the desired outcome has a lower predicted probability , based on the model, than the observation without the outcome. A pair is tied if it is neither concordant not discordant, i.e. the probabilities are the same and the model can not distinguished between them. Tied pairs aren't very common when there are continuous variables in the model. The left column of the Association of Predicted Probabilities and Observed Responses table lists the percentage of pairs of each type. At the bottom is the total number of observation pairs on which the percentages are based, i.e. the number of pairs of observations with different outcome values $(N_{event=0} \\cdot N_{event=1})$. More complex models have more than two predicted probabilities. However, regardless of the model's complexity, the same comparisons are made across all pairs of observations with different outcomes. You can use these results as goodness-of-fit measures to compare one model to another. In general, higher percentage of concordant pairs and lower percentages of discordant and tied pairs indicate a more desirable model. This table also shows the four rank correlation indices that are computed from the numbers of concordant $(n_c)$, discordant $(n_d)$ and tied $(n_t)$ pairs of observations: Somers' D (Gini coefficient) , defined as $(n_c-n_d)/(n_c+n_d+n_t)$ Goodman-Kruskal Gamma , defined as $(n_c-n_d)/(n_c+n_d)$ Kendall's Tau-a , defined as $(n_c-n_d)/(0.5 \\cdot N(N-1))$, with $N$ being the sum of observation frequencies in the data The concordance index c is the most commonly used of these values and estimates the probability of an observation with the desired outcome having a higher predicted probability than an observation without the desired outcome and is defined as $c=\\frac{n_c+0.5 \\cdot n_t}{n_c+n_d+n_t}$. Note that the concordance index, c , also gives an estimate of the area under the receiver operating characteristic (ROC) curve when the response is binary. Check these websites More information about these parameters here . In general, a model with higher values of these indices has better predictive ability than a model with lower values. Multiple Logistic Regression Model Link Introduction Link Sometimes you want to create a statistical model that explains the relationships among multiple predictors and a categorical response. You might want to examine the effect of each individual predictor on the response regardless of the levels of the other predictors to perform a more complex analysis that takes into account the interactions between the predictors In order to do this you will explore how to define and explain the adjusted odds ratio how to fit a multiple logistic regression model using the backward elimination method how to fit a multiple logistic regression model with interactions Multiple Logistic Regression Link Amultiple logistic regression model characterized the relationship between a categorical response variable and multiple predictor variables. The predictor variables can be continuous or categorical or both. $logit(p)=\\beta_0+\\beta_1 X_1 +...+\\beta_k X_k$ The goal of multiple logistic regression, like multiple linear regression, is to find the subset of variables that best explains the variability of the response variable. Models that are parsimonious or simple are more likely to be numerically stable and are also easier to generalize. The Backward Elimination Method of Variable Selection Link This method starts with a full model (a model that contains all of the main effects or predictor variables). Using an iterative process, the backward elimination method identifies and eliminates the nonsignificant predictor variables, one at a time. At each step, this method removes the least significant variable of the nonsignificant terms (the variable with the largest p-value). The smaller your significance level, the more evidence you need to keep a predictor variable in the model. This results in a more parsimonious model. The default significance level for a predictor to stay in the model is 0.05. You can change the significance level by adding SLSTAY= value or SLS= value in the MODEL statement. Adjusted Odds Ratios Link One major difference between a multiple logistic model and a logistic regression model with only one predictor variable is that the odds ratios are reported differently. Multiple logistic regression uses adjusted odds ratios . An adjusted odds ratio measures the effect of a single predictor variable on a response variable while holding all the other predictor variables constant. The adjusted odds ratio assumes that the OR for a predictor variable is the same regardless of the level of the other predictor variables . If that assumption is not true, then you need to fit a more complex model that also considers the interactions between predictor variables. The MODEL statement can include the CLODDS= option which enables the production of the OR plot ( PLOTS(ONLY)=(EFFECT ODDSRATIO) ). This option can be set to PL so that the procedure calculates profile-likelihood confidence limits for the OR of all predictor variables. These limits are based on the log likelihood and are generally preferred, especially for smaller sample sizes . Specifiying the Variable Selection Method in the MODEL Statement Link To specify the method that PROC LOGISTIC uses to select variable in a multiple logistic regression model, you add the SELECTION= option to the MODEL statement. The possible values of the SELECTION= statement are NONE | N (default): no selection method is used and the complete model is fitted BACKWARD | B : backward elimination FORWARD | F : forward selection STEPWISE | S : stepwise selection SCORE : best subset selection 1 2 3 4 PROC LOGISTIC DATA=SAS-data-set <options> ; CLASS variable < (variable_options)> ... </ options> ; MODEL response < (variable_options)> = predictors < / options SELECTION>; RUN; By default the procedure uses a $\\alpha=0.05$ significance level to determine which variables remain in the model. If you want to change the significance level, you can use the SLSTAY | SLS = option in the MODEL statement. The UNITS Statement Link The UNITS statement enables you to obtain customized odds ratio estimates for a specified unit of change in one or more continuous predictor variables. For each continuous predictor (or independent variable) that you want to modify, you specify the variable name, an equal sign, and a list of one or more units of change , separated by spaces, that are of interest for that variable. A unit of change can be a number, a standard deviation $(SD)$, or a number multiplied by the standard deviation $(n \\times SD)$. The UNITS statement is optional. If you want to use the units to change that are reflected in the stored data values, you do not need to include the UNITS statement. Specifying a Formatted Value as a Reference Level Link To ease the interpretation of the classification variable levels, text labels can be defined through a format definition. Appliying a format requires a change in the PROC LOGISTIC step. In the CLASS statement when you use the REF= option with a variable that has either a temporary or a permanent format assigned to it, you must specify the formatted value of the level instead of the stored value . Interaction between Variables Link When you fit a multiple logistic regression model, the simplest approach is to consider only the main effects (each predictor individually) on the response. In other words, this approach assumes that each variable has the same effect on the outcome regardless of the levels of the other variables. However, sometimes the effect of one variable on the outcome depends on the observed level of another variable. When this happens, we say that there is an interaction . Keep in mind that interactions that have more than two factors might be difficult to interpret . The Backward Elimination Method with Interactions in the Model Link When you use the backward elimination method with interactions in the model, PROC LOGISTIC begins by fitting th full model with all the main effects and interactions. Using an iterative process, PROC LOGISTIC eliminates nonsignificant interactions one at a time, starting with the least significant interaction (largest p-value). When only significant interactions remain, PROC LOGISTIC turns its attention to the main effects. PROC LOGISTIC eliminates, one at a time, the nonsignificant main effects that are not involved in any significant interactions. When eliminating main effects, PROC LOGISTIC must preserve the model hierarchy. According to this requirement, for any interaction that is included in the model, all main effects that the interaction contains must also be in the model, whether or not they are significant. The final model has only significant interactions, the main effects involved in the interactions, and any other significant main effects. Specifying Interactions in the MODEL Statement Link To specify interactions concisely, you can place a bar operator | between the names of each two main effects. The bar tells PROC LOGISTIC to treat the terms on either side of it as effects and also to include their combinations as interactions. If you want to limit the maximum number of variables that are involved in each interaction, you can specify @integer after the list of effects (e.g. @2 to include only the two-way interactions). The ODDSRATIO Statement Link By default, PROC LOGISTIC produces the OR only for variables that are not involved in an interaction. The OR for a main effect within an interaction would be misleading. It would only show the OR for that variable, holding constant the other variable at the value 0, which might not even be a valid value. To tell PROC LOGISTIC to produce the OR for each value of a variable that is involved in an interaction, you can use the ODDSRATIO statement. You specify a separate ODDSRATIO statement for each variable. In this statement you can optionally specify a label for the variable. The variable name is required. At the end, you can specify options following a forward slash / . In this example, there are three ODDSRATIO statements, one for each main effect (they do not have to appear in the same order that the variables are listed in the MODEL statement). 1 2 3 4 5 6 7 8 9 10 11 PROC LOGISTIC DATA = statdata . sales_inc PLOTS ( ONLY ) = ( EFFECT ODDSRATIO ); CLASS Gender ( PARAM = REF REF = ' Male ' ) IncLevel ( PARAM = REF REF = '1' ); UNITS Age = 10 ; MODEL Purchase ( EVENT = '1' ) = Gender | Age | IncLevel @2 / SELECTION = BACKWARD CLODDS = PL ; ODDSRATIO Age / CL = PL ; ODDSRATIO Gender / DIFF = REF AT ( IncLevel = ALL ) CL = PL ; ODDSRATIO IncLevel / DIFF = REF AT ( Gender = ALL ) CL = PL ; RUN ; The CL = WALD (default) | PL | BOTH option enables you to specify the type of confidence limits you want to produce: Wald, profile-likelihood or both. The DIFF = REF | ALL (default) option applies only to categorical variables. Using this option, you can specify whether PROC LOGISTIC computes the OR for a categorical variable against the reference level or against all of its levels. The AT (covariate = value-list | REF | ALL (default)) option specifies fixed levels of one or more interacting variables (also called covariates). PROC LOGISTIC computes OR at each of the specified levels. For each categorical variable, you can specify a list of one or more formatted levels of the variable, or the keyword REF to select the reference level or the keyword ALL to select all levels of the variable. This plot shows graphically what we saw with the calculated confidence intervals in the table above. Comparing the Binary and Multiple Logistic Regression Models Link Remember that, in general, when comparing the AIC and SC statistics, smaller values mean a better model fit . Note that the SC value increased with the addition of the interaction. SC selects more parsimonious models by imposing a more severe penalty for increasing the number of parameters. When comparing the c statistic values, larger values indicate a better predictive model . Interaction Plots Link To visualize the interaction terms you can produce an interaction plot. This plot explains more of the story behind the significant interaction in the output. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 proc means data = statdata . sales_inc noprint nway ; class IncLevel Gender ; var Purchase ; output out = bins sum ( Purchase ) = Purchase n ( Purchase ) = BinSize ; run ; data bins ; set bins ; Logit = log (( Purchase + 1 ) / ( BinSize - Purchase + 1 )); run ; proc sgscatter data = bins ; plot Logit * IncLevel / group = Gender markerattrs = ( size = 15 ) join ; format IncLevel incfmt .; label IncLevel = 'Income Level' ; title ; run ; quit ; If there is no interaction between two variables, the slopes shold be relatively parallel. Saving Analysis Results with the STORE Statement Link You can use the STORE statement with PROC LOGISTIC to save your analysis results as an item store for later processing. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 ODS SELECT NONE ; PROC LOGISTIC DATA = statdata . ameshousing3 ; CLASS Fireplaces ( REF = '0' ) Lot_Shape_2 ( REF = 'Regular' ) / param = ref ; MODEL Bonus ( EVENT = '1' ) = Basement_Area | Lot_Shape_2 Fireplaces ; UNITS Basement_Area = 100 ; STORE OUT = isbonus ; RUN ; ODS SELECT ALL ; DATA newhouses ; LENGTH Lot_Shape_2 $ 9 ; INPUT Fireplaces Lot_Shape_2 $ Basement_Area ; DATALINES ; 0 Regular 1060 2 Regular 775 2 Irregular 1100 1 Irregular 975 1 Regular 800 ; RUN ; PROC PLM RESTORE = isbonus ; SCORE DATA = newhouses OUT = scored_houses / ILINK ; TITLE 'Predictions using PROC PLM' ; RUN ; PROC PRINT DATA = scored_houses ; RUN ; Following the keyword STORE , you use the OUT= option to specify the name of your item store. In the PROC PLM statement, the RESTORE option specifies that the predictions will be based on the analysis results saved in the item store. The SCORE statement specifies that SAS will score the provided data set. The ILINK option requests that SAS provide the predictions in the form of predicted probabilities in lieu of logits where covariate effects are additive. Warning Be sure that you generate predictions only for new data records that fall within the range of the training data. If not, predictions could be invalid due to extrapolation. We assume that the modeled relationships between predictors and responses holds across the span of the observed data. We should not assume that this relationship holds everywhere.","title":"Categorical Data Analysis"},{"location":"statistics/categorical-data/#describing-categorical-data","text":"When you examine the distribution of a categorical variable , you want to know the values of the variable and the frequency or count of each value in the data ( one-way frequency able ). 1 2 3 4 PROC FREQ DATA = SAS - data - set ; TABLES variable1 variable2 variable3 </ options > ; < additional statements > RUN ; To look for a possible association between two or more categorical variables, you can create a crosstabulation / contingency table (when it displays statistics for two variables is also called two-way frequency able ). 1 2 3 4 PROC FREQ DATA = SAS - data - set ; TABLES variable - rows * variable - columns </ options > ; < additional statements > RUN ; Two distribution plots are associated with a frequency or crosstabulation table: a frequency plot , PLOTS=(FREQPLOT) , and a cumulative frequency plot . In PROC FREQ output, the default order for character values is alphaumeric . To reorder the values of an ordinal variable in your PROC FREQ output you can: Create a new variable in which the values are stored in logical order Apply a temporary format to the original variable How to replace the variable's name with the variable's label in PROC FREQ output 1 2 3 4 options validvarname = any ; PROC FREQ DATA = SAS - data - set ( RENAME = ( variable1 = \"Label variable 1\" n variable1 = \"Label variable 1\" n )); TABLES \"Label variable 1\" n ; RUN ; Count the distinct values of a variable : The question of how to count distinct values of a CLASS or BY variable using either PROC MEANS or PROC SUMMARY is asked frequently. While neither of these procedures has this ability, PROC SQL can count these values using the DISTINCT option and PROC FREQ using the NLEVELS option.","title":"Describing Categorical Data"},{"location":"statistics/categorical-data/#tests-of-association","text":"","title":"Tests of Association"},{"location":"statistics/categorical-data/#pearson-chi-square-test","text":"To perform a formal test of association between two categorical variables, you use the (Pearson) chi-square test which measures the difference between the observed cell frequencies and the cell frequencies that are expected if there is no association between variables ($H_0$ is true): $Expected=Row \\ total\\cdot Column\\ total/Total \\ sample \\ size$ If the sample size decreases , the chi-square value decreases and the p-value for the chi-square statistic increases Hypothesis testing: $H_0$ : no association; $H_a$ : association","title":"Pearson Chi-square Test"},{"location":"statistics/categorical-data/#cramers-v-statistic","text":"It is one measure of strength of an association between two categorical variables: For two-by-two tables, Cramer's V is in the range of -1 to 1 For larger tables, Cramer's V is in the range of 0 to 1 Values farther away from 0 indicate a relatively strong association between the variables To measure the strength of the association between a binary predictor variable and a binary outcome variable, you can use an odds ratio : $Odds \\ Ratio=\\frac{Odds \\ of \\ Outcome \\ in \\ Group \\ B}{Odds \\ of \\ Outcome \\ in \\ Group \\ A}$; $Odds=p_{event}/(1-p_{event})$ The value of the odds ratio can range from 0 to $\\infty$; it cannot be negative When the odds ratio is 1 , there is no association between variables When the odds ratio >1/<1, the group in the numerator/denominator is more likely to have the outcome The odds ratio is approximately the same regardless of the sample size To estimate the true odds ratio while taking into account the variability of the sample statistic, you can calculate confidence intervals You can use an odds ratio to test for significance between two categorical variables Odds ratio expressed as percent difference: $(odd \\ ratio -1) \\cdot 100$ 1 2 3 4 PROC FREQ DATA = SAS - data - set ; TABLES variable - rows * variable - columns / CHISQ EXPECTED </ options > ; < additional statements > RUN ; CHISQ produces the Pearson chi-square test of association, the likelihood-ratio chi-square and the Mantel-Haenszel: $\\sum \\frac{(obs. \\ freq. - exp. \\ freq.)^2}{exp. \\ freq.}$ EXPECTED prints the expected cell frequencies CELLCHI2 prints each cell's contribution to the total chi-square statistic: $ \\frac{(obs. \\ freq. - exp. \\ freq.)^2}{exp. \\ freq.}$ NOCOL / NOROW suppresses the printing of the column/row percentages NOPERCENT supresses the printing of the cell percentages RELRISK (relative risk) prints a table that contains risk ratios (probability ratios) and odds ratios; PROC FREQ uses the classification in the first column of the crosstabulation table as the outcome of interest and the first/second row in the numerator/denominator","title":"Cramer's V statistic"},{"location":"statistics/categorical-data/#mantel-haenszel-chi-square-test","text":"For ordinal associations , the Mantel-Haenszel chi-square test is a more powerful test. The levels must be in a logical order for the test results to be meaningful Hypothesis testing: $H_0$ : no ordinal association; $H_a$ : ordinal association Similarly to the Pearson case, the Mantel-Haenszel chi-square statistic/p-value indicate whether an association exists but not its magnitude and they depend on and reflect the sample size To measure the strength of the association between two ordinal variables you can use the Spearman correlation statistic. You should only use it if both variables are ordinal and are in logical order Is considered to be a rank correlation because it provides a degree of association between the ranks of the ordinal variables This statistic has a range between -1 and +1 : values close to -1/+1 indicate that there is a relatively high degree of negative/positive correlation and values close to 0 indicate a weak correlation It is not affected by the sample size 1 2 3 4 PROC FREQ DATA = SAS - data - set ; TABLES variable - rows * variable - columns / CHISQ EXPECTED </ options > ; < additional statements > RUN ; MEASURES produces the Spearman correlation statistic along with other measurement of association CL produces confidence bounds for the statistics that the MEASURES option requests The confidence bounds are valid only if the sample size is large (>25) The asymptotic standard error ( ASE ) is used for large samples and is used to calculate the confidence intervals for various measures of association (including the Spearman correlation coefficient)","title":"Mantel-Haenszel chi-square test"},{"location":"statistics/categorical-data/#introduction-to-logistic-regression","text":"Logistic Regression is a generalized linear model (like Linear Regression or ANOVA) that you can use to predict a categorical response/outcome based on one or more continuous/categorical predictor variables. There are three models:","title":"Introduction to Logistic Regression"},{"location":"statistics/categorical-data/#linear-vs-logistic","text":"","title":"Linear vs Logistic"},{"location":"statistics/categorical-data/#linear-regression-model","text":"Assumes that the expected value of the response continuous variable ($Y$) has a linear relationship with the predictor variable ($X$) The conditional mean of the response hast the linear form $E(Y|X)=\\beta_0+\\beta_1X$ and it ranges $(-\\infty,+\\infty)$ Why not to use Linear Regression to model a binary response variable Following the Linear Regression Model scheme, the response variable is calculated as $Y_i=\\beta_0+\\beta_1\\cdot X_i+\\epsilon_i$, where $\\beta_0$ and $\\beta_1$ are obtained by the method of least squares. This model assumes that the data is continuous , which is not true for the case of binary data This model assumes that the mean of the response is $\\beta_0+\\beta_1\\cdot X$ , while for binary data the mean of the response is the probability of a success If the response variable has only two levels, you cannot assume the constant variance and normality that are required for linear regression","title":"Linear Regression Model"},{"location":"statistics/categorical-data/#binary-logistic-regression-model","text":"The predictor variable ($X$) is used to estimate the probability of a specific outcome ($p$) for which you need to use a nonlinear function The mean of the response is a probability, which is between $(0, 1)$. The Inverse Logit Function binds the linear predictor between $0$ and $1$ is defined as $p_i=(1+e^{-(\\beta_0+\\beta_1 X_i)})^{-1}$ This model applies a Logit Transformation to the probabilities $logit(p_i)=ln\\left ( \\frac{p_i}{1-p_i} \\right ) = \\beta_0+\\beta_1X_i$, so that the transformed probabilities and predictor variables end up with a linear relationship The logit is the natural log of the odds (the probability of the event occurring divided by the probability of the event not occurring) We make the assumption that the logit transformation of the probabilities results in a linear relationship with the predictor variables (we can use a linear function $X$ to model the logit in order to indireclty model the probability) The logit of the probability transforms the probability into a linear function, which has no lower or upper bounds. So a logit has no lower or upper bounds .","title":"Binary Logistic Regression Model"},{"location":"statistics/categorical-data/#proc-logistic","text":"To model categorical data you use the LOGISTIC procedure. Some of the most common statements of this procedure are shown here: 1 2 3 4 5 6 PROC LOGISTIC DATA=SAS-data-set <options> ; CLASS variable < (variable_options)> ... </ options> ; MODEL response < (variable_options)> = predictors </ options> ; UNITS independent1=list... </ options> ; ODDSRATIO < 'label'> variable </ options> ; RUN; CLASS is used to define the classification (categorical) predictor variables (if any); this statement must precede the MODEL statement CLODDS = PL (profile likelihood) | WALD (default) | BOTH is an example of a general option that you can specify in the MODEL statement which computes confidence intervals for the odds ratios of all predictor variables and also enables the production of the odds ratio plot Example PROC LOGISTIC DATA=statdata.sales_inc PLOTS(ONLY)=(EFFECT ODDSRATIO); CLASS gender; MODEL purchase(EVENT='1')=gender / CLODDS=PL; RUN;","title":"PROC LOGISTIC"},{"location":"statistics/categorical-data/#classification-variables-parametrization","text":"When the predictor variable is categorical, the assumption of linearity cannot be met. To get past the obstacle of nonlinearity, the CLASS statement creates a set of one or more design variables (also called dummy variables). PROC LOGISTIC uses these variables, and not the original ones, in model calculations. Different parametrization methods for the classification variables will produce the same results regarding the significance of the categorical predictors, but understanding the parametrization method helps to interpret the results accurately. Here we present two of the most common methods of parameterizing ( PARAM = ) the classification variables. For both of them: The default reference level is the level that has the highest ranked value (or the last value) when the levels are sorted in ascending alphanumeric order The number of design variables (or $\\beta$) that are created are the number of levels of the classification variable -1","title":"Classification Variables Parametrization"},{"location":"statistics/categorical-data/#effect-coding-default","text":"Also called deviation from the mean coding , it compares the effect of each level of the variable to the average effect of all levels . Example Using this parametrization scheme the model will be described as follows $logit(p)=\\beta_0+\\beta_1\\cdot D_{Low \\ Income} + \\beta_2\\cdot D_{Medium \\ Income}$ $\\beta_0$ is the average value of the logit across all income levels $\\beta_1$ is the difference between the logit for income level 1 and $\\beta_0$ $\\beta_2$ is the difference between the logit for income level 2 and $\\beta_0$ Here's the Analysis of Maximum Likelihood Estimates table that PROC LOGISTIC generates for this example on which the parameter estimates ($\\beta_0$, $\\beta_1$, $\\beta_2$) and p-values reflect differences from the overall mean value over all levels. The p-values indicate whether each particular level is significant compared to the average effect of all levels. The p-values for $\\beta_1$ and $\\beta_2$ not significant meaning that the effect of those levels is not different than the average effect of low, medium and high income.","title":"Effect coding (default)"},{"location":"statistics/categorical-data/#reference-cell-coding","text":"It compares the effect of each level of the predictor to the effect of another level that is the designated reference level . Example To use this scheme the classification variable has to be defined in the following way CLASS gender (PARAM=REF REF='Male'); Using this parametrization scheme the model will be described as follows $logit(p)=\\beta_0+\\beta_1\\cdot D_{Low \\ Income} + \\beta_2\\cdot D_{Medium \\ Income}$ $\\beta_0$ is the intercept, but not in terms of where you cross the $Y$ axis, instead is the value of the logit of the probability when income is high (or at the reference level) $\\beta_1$ is the difference between the logit of the probability for low and high income $\\beta_2$ is the difference between the logit of the probability for medium and high income Here's the Analysis of Maximum Likelihood Estimates table that PROC LOGISTIC generates for this example on which the parameter estimates ($\\beta_0$, $\\beta_1$, $\\beta_2$) and p-values reflect differences with respect to the reference level. The p-values indicate whether each particular level is significant compared to the reference level. The p-value for $\\beta_1<0.05$ is significant meaning that the effect of a low income is statistically different than the effect of a high income on the probability that people will spend at least $100\\$$. The same applies to $\\beta_2$.","title":"Reference cell coding"},{"location":"statistics/categorical-data/#fitting-a-binary-logistic-regression","text":"1 2 3 4 5 PROC LOGISTIC DATA = statdata . sales_inc PLOTS ( ONLY ) = ( EFFECT ); CLASS Gender ( PARAM = REF REF = 'Male' ); MODEL Purchase ( event = '1' ) = Gender ; title1 'LOGISTIC MODEL (1): Purchase = Gender' ; RUN ; We look at the first few tables to make sure that the model is set up the way we want The Model Information table describes the data set, the response variable, the number of response levels, the type of model, the algorithm used to obtain the parameter estimates, and the number of observations read and used. The Response Profile table shows the values of the response variable, listed according to their ordered value and frequency. By default, PROC LOGISTIC orders the values of the response variable alphanumercally and bases the logistic regression model on the probability of the lowest value. However, we set the EVENT=1 , the highest value, so this model is based on the probability that Purchase=1 . Below this table, we see the probability that PROC LOGISTIC is modeling, as shown in the log. The Class Level Information table displays the predictor variable in the CLASS statement Gender (in the model we fixed 'Male' as the reference level, so the design variable is 1 when Gender='Female' and 0 when Gender='Male' ). The Model Convergence Status simply indicates that the convergence criterion was met. There are a number of options to control the convergence criterion, but the default is the gradient convergence criterion with a default value of $10^{-8}$. The Model Fit Statistic table reports the resuls of three tests (for the model with the intercept only and the model with the intercept and the predictor variables): AIC, SC and -2$\\cdot$Log(likelihood). AIC and SC are goodness-of-fit measures that you can use to compare one model to another (lower values indicate more desirable model, although there is no standard for determining how much of a difference indicates an improvement) and are not dependent on the number of terms in the model. Akaike's Information Criterion (AIC) : it adjusts for the number of predictor valriables. It is the best statitstic to come up with the best explanatory model . Schwarz's Bayesian Criterion (SC) : it adjusts for the number of predictor variables and the number of observations. This test uses a bigger penalty for extra variables and therefore favors more parsimonious models. It is the best statitstic to come up with the best predictive model . The Testing Global Null Hypothesis: BETA=0 table provides three statistics to test $H_0$ that all the regression coefficients in the model are 0. The Likelihood Ratio is the most reliable test, specially for small sample sizes. It is similar to the overall F test in linear regression. The Type 3 Analysis of Effects table is generated when CLASS specifies a categorical predictor variable. It shows the results of testing whether each individual parameter estimate is statistically different from 0 ( Pr>ChiSq $<0.05$). The Wald Chi-Square statistic tests the listed effects. When there is only one predictor variable in the model, the value listed in the table will be identical to the Wald test in the Testing Global Null Hypothesis table. The Analysis of Maximum Likelihood Estimates table lists the estimated model parameters (the betas), their standard errors, Wald test statistics and corresponding p-values. The parameter estimates are the estimated coefficients of the fitted logistic regression model. We can use these estimates to construct the logistic regression equation $logit(\\beta)=\\beta_0+\\beta_1 \\cdot Categorical \\ predictor$. The Odds Ratio Estimates table ( ONLY for binary logistic regression ) shows the OR ratio for the modeled event. Notice that PROC LOGISTIC calculates Wald confidence limits by default. The Association of Predicted Probabilities and Observed Responses table lists several goodness-of-fit measures. The Odds Ratio Estimates and Profile-Likelihood Confidence Intervals table ( ONLY for multiple logistic regression ) contains the OR estimates and the profile-likelihood confidence limits that the CLODDS= option specified for the main effects. For multiple logistic regression, remember that PROC LOGISTIC calculates the adjusted odds ratios. For a continuous predictor, the odds ratio measures the change in odds associated with a one-unit difference of the predictor variable by default although it is specified otherwise in the UNITS statement. In the Odds Ratios plot , the dots represent the OR estimates and the horizontal lines represent the confidence intervals for those estimates. There is a reference line at one to check if the confidence intervals cross this value meaning that it is not statistically different from 1. The Effect plot shows the levels of the CLASS predictor variable vs the probability of the desired outcome. If you are performing a multiple logistic regression analysis where there is a continuous predictor variable it will be represented in the horizontal axis. Moreover, the lines will represent all possible combinations of the different CLASS variables.","title":"Fitting a Binary Logistic Regression"},{"location":"statistics/categorical-data/#iterpreting-the-odds-ratio-for-a-categorical-predictor","text":"Let's see how to calculate the odds and the odds ratio from the logistic regression model. Here is the logistic regression model that predicts the logit of $p$: $logit(\\hat p)=ln(odds)=ln\\left ( \\frac{p_i}{1-p_i} \\right )=\\beta_0 + \\beta_1 \\cdot Gender$ According to our example the variable Gender is codified in a way that Females=1 and Males=0 , so the OR can be written: $odds_{females}=e^{\\beta_0+\\beta_1}$ $odds_{males}=e^{\\beta_0}$ $odds \\ ratio = \\frac{e^{\\beta_0+\\beta_1}}{e^{\\beta_0}}=e^{\\beta_1}$ If the 95% confidence interval does not include 1, the OR is significant at the 0.05 level indicating an association between the predictor and response variables of your model.","title":"Iterpreting the Odds Ratio for a Categorical Predictor"},{"location":"statistics/categorical-data/#iterpreting-the-odds-ratio-for-a-continuous-predictor","text":"For a continuous predictor variable, the OR measures the increase or decrease in odds associated with a one-unit difference of the predictor variable by default. $OR - 1 = %$ of greater odds for having one-unit of difference.","title":"Iterpreting the Odds Ratio for a Continuous Predictor"},{"location":"statistics/categorical-data/#comparing-pairs-to-assess-the-fit-of-a-logistic-regression-model","text":"PROC LOGISTIC calculates several different goodness-of-fit measures and displayed in the Association of Predicted Probabilities and Observed Responses table. One of these goodness-of-fit methods is comparing pairs ( Pairs ). To start, PROC LOGISTIC creates two groups of observations, one for each value of the response variable. Then, the procedure selects pairs of observations, one from each group, until no more pairs can be selected. PROC LOGISTIC determines whether each pair is concordant, discordant or tied. A pair is concordant if the model predicts it correclty , i.e. if the observation with the desired outcome has a higher predicted probability , based on the model, than the observation without the outcome. A pair is discordant if the model does not predict it correctly , i.e. if the observation with the desired outcome has a lower predicted probability , based on the model, than the observation without the outcome. A pair is tied if it is neither concordant not discordant, i.e. the probabilities are the same and the model can not distinguished between them. Tied pairs aren't very common when there are continuous variables in the model. The left column of the Association of Predicted Probabilities and Observed Responses table lists the percentage of pairs of each type. At the bottom is the total number of observation pairs on which the percentages are based, i.e. the number of pairs of observations with different outcome values $(N_{event=0} \\cdot N_{event=1})$. More complex models have more than two predicted probabilities. However, regardless of the model's complexity, the same comparisons are made across all pairs of observations with different outcomes. You can use these results as goodness-of-fit measures to compare one model to another. In general, higher percentage of concordant pairs and lower percentages of discordant and tied pairs indicate a more desirable model. This table also shows the four rank correlation indices that are computed from the numbers of concordant $(n_c)$, discordant $(n_d)$ and tied $(n_t)$ pairs of observations: Somers' D (Gini coefficient) , defined as $(n_c-n_d)/(n_c+n_d+n_t)$ Goodman-Kruskal Gamma , defined as $(n_c-n_d)/(n_c+n_d)$ Kendall's Tau-a , defined as $(n_c-n_d)/(0.5 \\cdot N(N-1))$, with $N$ being the sum of observation frequencies in the data The concordance index c is the most commonly used of these values and estimates the probability of an observation with the desired outcome having a higher predicted probability than an observation without the desired outcome and is defined as $c=\\frac{n_c+0.5 \\cdot n_t}{n_c+n_d+n_t}$. Note that the concordance index, c , also gives an estimate of the area under the receiver operating characteristic (ROC) curve when the response is binary. Check these websites More information about these parameters here . In general, a model with higher values of these indices has better predictive ability than a model with lower values.","title":"Comparing Pairs to Assess the Fit of a Logistic Regression Model"},{"location":"statistics/categorical-data/#multiple-logistic-regression-model","text":"","title":"Multiple Logistic Regression Model"},{"location":"statistics/categorical-data/#introduction","text":"Sometimes you want to create a statistical model that explains the relationships among multiple predictors and a categorical response. You might want to examine the effect of each individual predictor on the response regardless of the levels of the other predictors to perform a more complex analysis that takes into account the interactions between the predictors In order to do this you will explore how to define and explain the adjusted odds ratio how to fit a multiple logistic regression model using the backward elimination method how to fit a multiple logistic regression model with interactions","title":"Introduction"},{"location":"statistics/categorical-data/#multiple-logistic-regression","text":"Amultiple logistic regression model characterized the relationship between a categorical response variable and multiple predictor variables. The predictor variables can be continuous or categorical or both. $logit(p)=\\beta_0+\\beta_1 X_1 +...+\\beta_k X_k$ The goal of multiple logistic regression, like multiple linear regression, is to find the subset of variables that best explains the variability of the response variable. Models that are parsimonious or simple are more likely to be numerically stable and are also easier to generalize.","title":"Multiple Logistic Regression"},{"location":"statistics/categorical-data/#the-backward-elimination-method-of-variable-selection","text":"This method starts with a full model (a model that contains all of the main effects or predictor variables). Using an iterative process, the backward elimination method identifies and eliminates the nonsignificant predictor variables, one at a time. At each step, this method removes the least significant variable of the nonsignificant terms (the variable with the largest p-value). The smaller your significance level, the more evidence you need to keep a predictor variable in the model. This results in a more parsimonious model. The default significance level for a predictor to stay in the model is 0.05. You can change the significance level by adding SLSTAY= value or SLS= value in the MODEL statement.","title":"The Backward Elimination Method of Variable Selection"},{"location":"statistics/categorical-data/#adjusted-odds-ratios","text":"One major difference between a multiple logistic model and a logistic regression model with only one predictor variable is that the odds ratios are reported differently. Multiple logistic regression uses adjusted odds ratios . An adjusted odds ratio measures the effect of a single predictor variable on a response variable while holding all the other predictor variables constant. The adjusted odds ratio assumes that the OR for a predictor variable is the same regardless of the level of the other predictor variables . If that assumption is not true, then you need to fit a more complex model that also considers the interactions between predictor variables. The MODEL statement can include the CLODDS= option which enables the production of the OR plot ( PLOTS(ONLY)=(EFFECT ODDSRATIO) ). This option can be set to PL so that the procedure calculates profile-likelihood confidence limits for the OR of all predictor variables. These limits are based on the log likelihood and are generally preferred, especially for smaller sample sizes .","title":"Adjusted Odds Ratios"},{"location":"statistics/categorical-data/#specifiying-the-variable-selection-method-in-the-model-statement","text":"To specify the method that PROC LOGISTIC uses to select variable in a multiple logistic regression model, you add the SELECTION= option to the MODEL statement. The possible values of the SELECTION= statement are NONE | N (default): no selection method is used and the complete model is fitted BACKWARD | B : backward elimination FORWARD | F : forward selection STEPWISE | S : stepwise selection SCORE : best subset selection 1 2 3 4 PROC LOGISTIC DATA=SAS-data-set <options> ; CLASS variable < (variable_options)> ... </ options> ; MODEL response < (variable_options)> = predictors < / options SELECTION>; RUN; By default the procedure uses a $\\alpha=0.05$ significance level to determine which variables remain in the model. If you want to change the significance level, you can use the SLSTAY | SLS = option in the MODEL statement.","title":"Specifiying the Variable Selection Method in the MODEL Statement"},{"location":"statistics/categorical-data/#the-units-statement","text":"The UNITS statement enables you to obtain customized odds ratio estimates for a specified unit of change in one or more continuous predictor variables. For each continuous predictor (or independent variable) that you want to modify, you specify the variable name, an equal sign, and a list of one or more units of change , separated by spaces, that are of interest for that variable. A unit of change can be a number, a standard deviation $(SD)$, or a number multiplied by the standard deviation $(n \\times SD)$. The UNITS statement is optional. If you want to use the units to change that are reflected in the stored data values, you do not need to include the UNITS statement.","title":"The UNITS Statement"},{"location":"statistics/categorical-data/#specifying-a-formatted-value-as-a-reference-level","text":"To ease the interpretation of the classification variable levels, text labels can be defined through a format definition. Appliying a format requires a change in the PROC LOGISTIC step. In the CLASS statement when you use the REF= option with a variable that has either a temporary or a permanent format assigned to it, you must specify the formatted value of the level instead of the stored value .","title":"Specifying a Formatted Value as a Reference Level"},{"location":"statistics/categorical-data/#interaction-between-variables","text":"When you fit a multiple logistic regression model, the simplest approach is to consider only the main effects (each predictor individually) on the response. In other words, this approach assumes that each variable has the same effect on the outcome regardless of the levels of the other variables. However, sometimes the effect of one variable on the outcome depends on the observed level of another variable. When this happens, we say that there is an interaction . Keep in mind that interactions that have more than two factors might be difficult to interpret .","title":"Interaction between Variables"},{"location":"statistics/categorical-data/#the-backward-elimination-method-with-interactions-in-the-model","text":"When you use the backward elimination method with interactions in the model, PROC LOGISTIC begins by fitting th full model with all the main effects and interactions. Using an iterative process, PROC LOGISTIC eliminates nonsignificant interactions one at a time, starting with the least significant interaction (largest p-value). When only significant interactions remain, PROC LOGISTIC turns its attention to the main effects. PROC LOGISTIC eliminates, one at a time, the nonsignificant main effects that are not involved in any significant interactions. When eliminating main effects, PROC LOGISTIC must preserve the model hierarchy. According to this requirement, for any interaction that is included in the model, all main effects that the interaction contains must also be in the model, whether or not they are significant. The final model has only significant interactions, the main effects involved in the interactions, and any other significant main effects.","title":"The Backward Elimination Method with Interactions in the Model"},{"location":"statistics/categorical-data/#specifying-interactions-in-the-model-statement","text":"To specify interactions concisely, you can place a bar operator | between the names of each two main effects. The bar tells PROC LOGISTIC to treat the terms on either side of it as effects and also to include their combinations as interactions. If you want to limit the maximum number of variables that are involved in each interaction, you can specify @integer after the list of effects (e.g. @2 to include only the two-way interactions).","title":"Specifying Interactions in the MODEL Statement"},{"location":"statistics/categorical-data/#the-oddsratio-statement","text":"By default, PROC LOGISTIC produces the OR only for variables that are not involved in an interaction. The OR for a main effect within an interaction would be misleading. It would only show the OR for that variable, holding constant the other variable at the value 0, which might not even be a valid value. To tell PROC LOGISTIC to produce the OR for each value of a variable that is involved in an interaction, you can use the ODDSRATIO statement. You specify a separate ODDSRATIO statement for each variable. In this statement you can optionally specify a label for the variable. The variable name is required. At the end, you can specify options following a forward slash / . In this example, there are three ODDSRATIO statements, one for each main effect (they do not have to appear in the same order that the variables are listed in the MODEL statement). 1 2 3 4 5 6 7 8 9 10 11 PROC LOGISTIC DATA = statdata . sales_inc PLOTS ( ONLY ) = ( EFFECT ODDSRATIO ); CLASS Gender ( PARAM = REF REF = ' Male ' ) IncLevel ( PARAM = REF REF = '1' ); UNITS Age = 10 ; MODEL Purchase ( EVENT = '1' ) = Gender | Age | IncLevel @2 / SELECTION = BACKWARD CLODDS = PL ; ODDSRATIO Age / CL = PL ; ODDSRATIO Gender / DIFF = REF AT ( IncLevel = ALL ) CL = PL ; ODDSRATIO IncLevel / DIFF = REF AT ( Gender = ALL ) CL = PL ; RUN ; The CL = WALD (default) | PL | BOTH option enables you to specify the type of confidence limits you want to produce: Wald, profile-likelihood or both. The DIFF = REF | ALL (default) option applies only to categorical variables. Using this option, you can specify whether PROC LOGISTIC computes the OR for a categorical variable against the reference level or against all of its levels. The AT (covariate = value-list | REF | ALL (default)) option specifies fixed levels of one or more interacting variables (also called covariates). PROC LOGISTIC computes OR at each of the specified levels. For each categorical variable, you can specify a list of one or more formatted levels of the variable, or the keyword REF to select the reference level or the keyword ALL to select all levels of the variable. This plot shows graphically what we saw with the calculated confidence intervals in the table above.","title":"The ODDSRATIO Statement"},{"location":"statistics/categorical-data/#comparing-the-binary-and-multiple-logistic-regression-models","text":"Remember that, in general, when comparing the AIC and SC statistics, smaller values mean a better model fit . Note that the SC value increased with the addition of the interaction. SC selects more parsimonious models by imposing a more severe penalty for increasing the number of parameters. When comparing the c statistic values, larger values indicate a better predictive model .","title":"Comparing the Binary and Multiple Logistic Regression Models"},{"location":"statistics/categorical-data/#interaction-plots","text":"To visualize the interaction terms you can produce an interaction plot. This plot explains more of the story behind the significant interaction in the output. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 proc means data = statdata . sales_inc noprint nway ; class IncLevel Gender ; var Purchase ; output out = bins sum ( Purchase ) = Purchase n ( Purchase ) = BinSize ; run ; data bins ; set bins ; Logit = log (( Purchase + 1 ) / ( BinSize - Purchase + 1 )); run ; proc sgscatter data = bins ; plot Logit * IncLevel / group = Gender markerattrs = ( size = 15 ) join ; format IncLevel incfmt .; label IncLevel = 'Income Level' ; title ; run ; quit ; If there is no interaction between two variables, the slopes shold be relatively parallel.","title":"Interaction Plots"},{"location":"statistics/categorical-data/#saving-analysis-results-with-the-store-statement","text":"You can use the STORE statement with PROC LOGISTIC to save your analysis results as an item store for later processing. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 ODS SELECT NONE ; PROC LOGISTIC DATA = statdata . ameshousing3 ; CLASS Fireplaces ( REF = '0' ) Lot_Shape_2 ( REF = 'Regular' ) / param = ref ; MODEL Bonus ( EVENT = '1' ) = Basement_Area | Lot_Shape_2 Fireplaces ; UNITS Basement_Area = 100 ; STORE OUT = isbonus ; RUN ; ODS SELECT ALL ; DATA newhouses ; LENGTH Lot_Shape_2 $ 9 ; INPUT Fireplaces Lot_Shape_2 $ Basement_Area ; DATALINES ; 0 Regular 1060 2 Regular 775 2 Irregular 1100 1 Irregular 975 1 Regular 800 ; RUN ; PROC PLM RESTORE = isbonus ; SCORE DATA = newhouses OUT = scored_houses / ILINK ; TITLE 'Predictions using PROC PLM' ; RUN ; PROC PRINT DATA = scored_houses ; RUN ; Following the keyword STORE , you use the OUT= option to specify the name of your item store. In the PROC PLM statement, the RESTORE option specifies that the predictions will be based on the analysis results saved in the item store. The SCORE statement specifies that SAS will score the provided data set. The ILINK option requests that SAS provide the predictions in the form of predicted probabilities in lieu of logits where covariate effects are additive. Warning Be sure that you generate predictions only for new data records that fall within the range of the training data. If not, predictions could be invalid due to extrapolation. We assume that the modeled relationships between predictors and responses holds across the span of the observed data. We should not assume that this relationship holds everywhere.","title":"Saving Analysis Results with the STORE Statement"},{"location":"statistics/inference/","text":"How to verify the assumptions and diagnose problems that you encounter in linear regression ? Examining Residuals Link You can use the residual values (difference between each observed value of $Y$ and its predicted value) from the regression analysis to verify the assumptions of the linear regression . Residuals are estimates of the errors, so you can plot the residuals to check the assumptions of the errors . You can plot residuals vs the predicted values to check for violations of equal variances You can also use this plot to check for violations of linearity and independence You can plot the residuals vs the values of the independent variables to further examine any violations of equal variances (you can see which predictor contributes to the violation of the assumption) You can use a histogram or a normal probability plot of the residuals to determine whether or not the errors are normally distributed You want to see a random scatter of the residual values above and below the reference line at 0. If you see patterns or trends in the residual values, the assumptions might not be valid and the models might have problems. Note To take autocorrelation (correlated over time) into account, you might need to use a regression procedure such as PROC AUTOREG You can also use these plots to detect outliers , which often reflect data errors or unusual circumstances. They can affect your regression results, so you want to know whether any outliers are present and causing problems and investigate if they result from data entry error or some other problem that you can correct. PROC REG Link 1 2 3 4 5 PROC REG DATA=SAS-data-set PLOTS(ONLY)=(QQ RESIDUALBYPREDICTED RESIDUALS) <options> ; <label:> MODEL dependent=regressor(s) </options> ; ID variable4identification; RUN; QUIT; QQ requests a residual quantile-quantile plot to assess the normality of the residual error RESIDUALBYPREDICTED requests a plot of residuals by predicted values to verify the equal variance assumption, the independence assumption and model adequacy RESIDUALS requests a panel of plots of residuals by the predictor variables in the model: if any of the Residual by Regressors plots show signs of unequal variance, we can determine which predictor variable is involved in the problem Identifying Influential Observations Link An influential observation is different from an outlier. An outlier is an unusual observation that has a large residual compare to the rest of the points. An influential observation can sometimes have a large residual compared to the rest of the points, but it is an observation so far away from the rest of the data that it singlehandedly exerts influence on the slope of the regression line. Using STUDENT residuals to detect outliers Link Also known as studientized or standardized residuals , the STUDENT residuals are calculated by dividing the residual by their standard errors , so you can think of them as roughly equivalent to a z-score. For relatively small sample sizes , if the absolute value of the STUDENT residual is $>2$ , you can suspect that the corresponding observation is an outlier For large sample sizes , it's very likely that even more STUDENT residuals greater than $\\pm2$ will occur just by chance, so you should typically use a larger cutoff value of $>3$ Using COOKSD statistics to detect influential observations Link For each observation, the Cook's D statistic is calculated as if that observation weren't in the data set as well as the set of parameter estimates with all the observations in your regression analysis. If any observation has a Cook's D statistic $>4/n$ that observation is influential The Cook's D statistic is most useful for identifying influential observations when the purpose of your model is parameter estimation Using RSTUDENT residuals to detect influential observations Link RSTUDENT residuals are similar to STUDENT residuals. For each observation, the RSTUDENT residual is the residual divided by the standard error estimated with the current observation deleted . If the RSTUDENT residual is different from the STUDENT residual, the observation is probably influential If the absolute value of the RSTUDENT residuals is $>2$ or $>3$, you've probably detected an influential observation Using DFFITS statistics to detect influential observations Link DFFITS measures the impact that each observation has on its own predicted value. For each observation, DFFITS is calculated using two predicted values : The first predicted value is calculated from a model using the entire data set to estimate model parameters The second predicted value is calculated from a model using the data set with that particular observation removed to estimate model parameters The difference between the two predicted values is divided by the standard error of the predicted value, without the observation If the standardized difference between these predicted values is large , that particular observation has a large effect on the model fit . The general cutoff value is $2$ The more precise cutoff is $2 \\cdot sqrt(p/n)$ If the absolute value of DFFITS for any observation is $>$ cutoff value, you've detected an influential observation DFFITS is most useful for predictive models Using DFBETAS statistics to explore the influenced predictor variable Link To help identifying which parameter the observation might be influencing most you can use DFBETAS (difference in betas). It measure the change in each parameter estimate. One DFBETAS is calculated per predictor variable per observation Each value is calculated by taking the estimated coefficient for that particular predictor variable using all the data , subtracting the estimated coefficient for that particular predictor variable with the current observation removed and dividing by its standard error Large DFBETAS indicate observations that are influential in estimating a given parameter: The general cutoff value is $2$ The more precise cutoff is $2 \\cdot sqrt(1/n)$ PROC GLMSELECT Link 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 PROC GLMSELECT DATA = SAS - data - set < options > ; < label : > MODEL dependent ( s ) = regressor ( s ) / </ options > ; RUN ; QUIT ; ODS OUTPUT RSTUDENTBYPREDICTED = name - rstud - data - set COOKSDPLOT = name - cooksd - data - set DFFITSPLOT = name - dffits - data - set DFBETASPANEL = name - dfbs - data - set ; PROC REG DATA = SAS - data - set PLOTS ( ONLY LABEL ) = ( RSTUDENTBYPREDICTED COOKSD DFFITS DFBETAS ) < options > ; < label : > MODEL dependent =& _GLSIND </ options > ; ID variable4identification ; RUN ; QUIT ; DATA influential ; MERGE name - rstud - data - set name - cooksd - data - set name - dffits - data - set name - dfbs - data - set ; BY observation ; IF ( ABS ( RSTUDENT ) > 3 ) OR ( COOKSDLABEL NE ' ' ) OR DFFITSOUT THEN FLAG = 1 ; ARRAY DFBETAS { * } _DFBETASOUT : ; DO I = 2 TO DIM ( DFBETAS ) ; IF DFBETAS { I } THEN FLAG = 1 ; END ; IF ABS ( RSTUDENT ) <= 3 THEN RSTUDENT = . ; IF COOKSDLABEL EQ ' ' THEN COOKSD = . ; IF FLAG = 1 ; DROP I FLAG ; RUN ; PROC PRINT DATA = influential ; ID observation ; VAR RSTUDENT COOKSD DFFITSOUT _DFBETASOUT : ; RUN ; PROC GLMSELECT automatically creates the &_GLSIND macro variable which stores the list of effects that are in the model whose variable order can be checked in the Influence Diagnostics panel The ODS statement takes the data that creates each of the requested plots and saves it in the specified data set The LABEL option includes a label for the extreme observations in the plot (labeled with the observation numbers if there is not ID specified) Having influential observations doesn't violate regression assumptions , but it's a major nuisance that you need to address: Recheck for data entry errors If the data appears to be valid, consider whether you have an adequate model (a different model might fit the data better). Divide the number of influential observations you detect by the number of observations in you data set: if the result is $>5\\%$ you probably have the wrong model . Determine whether the influential observation is valid but just unusual As a general rule you should not exclude data (some unusual observations contain important information) If you choose to exclude some observations, include in your report a description of the types of observations that you excluded and why and discuss the limitation of the conclusions given the exclusions Detecting Collinearity Link Collinearity (or multicollinearity) is a problem that you face in multiple regression. It occurs when two or more predictor variables are highly correlated with each other ( redundant information among them, the predictor variables explain much of the same variation in the response). Collinearity doesn't violate the assumptions of multiple regression. Collinearity can hide significant effects (if you include only one of the collinear variables in the model it is significant but when there are more than one included none of them are significant) Collinearity increases the variance of the parameter estimates, making them unstable (the data points don't spread out enough in the space to provide stable support for the plane defined by the model) and, in turn, this increases the prediction error of the model When an overall model is highly significant but the individual variables don't tell the same story, it's a warning sign of collinearity . When the standard error for an estimate is larger than the parameter estimate itself, it's not going to be statistically significant. The SE tells us how variable the corresponding parameter estimate is: when the standard errors are high, the model lacks stability . 1 2 3 4 PROC REG DATA=SAS-data-set <options> ; <label:> MODEL dependent = regressors / VIF </options> ; RUN; QUIT; The VIF (variance inflation factor, $VIF_i=1/(1-R_i^2)$) option measures the magnitude of collinearity in a model (VIF$>10$ for any predictor in the model, those predictors are probably involved in collinearity) Other options are COLLIN (includes the intercept when analyzing collinearity and helps identify the predictors that are causing the problem) and COLLINOINT (requests the same analysis as COLLIN but excludes the intercept) Effective modeling cycle Link You want to get to know your data by performing preliminary analysis : Plot your data Calculate descriptive statistics Perform correlation analysis Identify some good candidate models using PROC REG : First check for collinearity Use all-possible regression or stepwise selection methods and subject matter knowledge to select model candidates Identify the good ones with the Mallows' (prediction) or Hocking's (explanatory) criterion for $C_p$ Check and validate your assumtions by creating residual plots and conducting a few other statistical tests Deal with any problems in your data : Determine whether any influential observations might be throwing off your model calculations Determine whether any variables are collinear Revise your model Validate your model with data not used to build the model (prediction testing)","title":"Model Post-Fitting for Inference"},{"location":"statistics/inference/#examining-residuals","text":"You can use the residual values (difference between each observed value of $Y$ and its predicted value) from the regression analysis to verify the assumptions of the linear regression . Residuals are estimates of the errors, so you can plot the residuals to check the assumptions of the errors . You can plot residuals vs the predicted values to check for violations of equal variances You can also use this plot to check for violations of linearity and independence You can plot the residuals vs the values of the independent variables to further examine any violations of equal variances (you can see which predictor contributes to the violation of the assumption) You can use a histogram or a normal probability plot of the residuals to determine whether or not the errors are normally distributed You want to see a random scatter of the residual values above and below the reference line at 0. If you see patterns or trends in the residual values, the assumptions might not be valid and the models might have problems. Note To take autocorrelation (correlated over time) into account, you might need to use a regression procedure such as PROC AUTOREG You can also use these plots to detect outliers , which often reflect data errors or unusual circumstances. They can affect your regression results, so you want to know whether any outliers are present and causing problems and investigate if they result from data entry error or some other problem that you can correct.","title":"Examining Residuals"},{"location":"statistics/inference/#proc-reg","text":"1 2 3 4 5 PROC REG DATA=SAS-data-set PLOTS(ONLY)=(QQ RESIDUALBYPREDICTED RESIDUALS) <options> ; <label:> MODEL dependent=regressor(s) </options> ; ID variable4identification; RUN; QUIT; QQ requests a residual quantile-quantile plot to assess the normality of the residual error RESIDUALBYPREDICTED requests a plot of residuals by predicted values to verify the equal variance assumption, the independence assumption and model adequacy RESIDUALS requests a panel of plots of residuals by the predictor variables in the model: if any of the Residual by Regressors plots show signs of unequal variance, we can determine which predictor variable is involved in the problem","title":"PROC REG"},{"location":"statistics/inference/#identifying-influential-observations","text":"An influential observation is different from an outlier. An outlier is an unusual observation that has a large residual compare to the rest of the points. An influential observation can sometimes have a large residual compared to the rest of the points, but it is an observation so far away from the rest of the data that it singlehandedly exerts influence on the slope of the regression line.","title":"Identifying Influential Observations"},{"location":"statistics/inference/#using-student-residuals-to-detect-outliers","text":"Also known as studientized or standardized residuals , the STUDENT residuals are calculated by dividing the residual by their standard errors , so you can think of them as roughly equivalent to a z-score. For relatively small sample sizes , if the absolute value of the STUDENT residual is $>2$ , you can suspect that the corresponding observation is an outlier For large sample sizes , it's very likely that even more STUDENT residuals greater than $\\pm2$ will occur just by chance, so you should typically use a larger cutoff value of $>3$","title":"Using STUDENT residuals to detect outliers"},{"location":"statistics/inference/#using-cooksd-statistics-to-detect-influential-observations","text":"For each observation, the Cook's D statistic is calculated as if that observation weren't in the data set as well as the set of parameter estimates with all the observations in your regression analysis. If any observation has a Cook's D statistic $>4/n$ that observation is influential The Cook's D statistic is most useful for identifying influential observations when the purpose of your model is parameter estimation","title":"Using COOKSD statistics to detect influential observations"},{"location":"statistics/inference/#using-rstudent-residuals-to-detect-influential-observations","text":"RSTUDENT residuals are similar to STUDENT residuals. For each observation, the RSTUDENT residual is the residual divided by the standard error estimated with the current observation deleted . If the RSTUDENT residual is different from the STUDENT residual, the observation is probably influential If the absolute value of the RSTUDENT residuals is $>2$ or $>3$, you've probably detected an influential observation","title":"Using RSTUDENT residuals to detect influential observations"},{"location":"statistics/inference/#using-dffits-statistics-to-detect-influential-observations","text":"DFFITS measures the impact that each observation has on its own predicted value. For each observation, DFFITS is calculated using two predicted values : The first predicted value is calculated from a model using the entire data set to estimate model parameters The second predicted value is calculated from a model using the data set with that particular observation removed to estimate model parameters The difference between the two predicted values is divided by the standard error of the predicted value, without the observation If the standardized difference between these predicted values is large , that particular observation has a large effect on the model fit . The general cutoff value is $2$ The more precise cutoff is $2 \\cdot sqrt(p/n)$ If the absolute value of DFFITS for any observation is $>$ cutoff value, you've detected an influential observation DFFITS is most useful for predictive models","title":"Using DFFITS statistics to detect influential observations"},{"location":"statistics/inference/#using-dfbetas-statistics-to-explore-the-influenced-predictor-variable","text":"To help identifying which parameter the observation might be influencing most you can use DFBETAS (difference in betas). It measure the change in each parameter estimate. One DFBETAS is calculated per predictor variable per observation Each value is calculated by taking the estimated coefficient for that particular predictor variable using all the data , subtracting the estimated coefficient for that particular predictor variable with the current observation removed and dividing by its standard error Large DFBETAS indicate observations that are influential in estimating a given parameter: The general cutoff value is $2$ The more precise cutoff is $2 \\cdot sqrt(1/n)$","title":"Using DFBETAS statistics to explore the influenced predictor variable"},{"location":"statistics/inference/#proc-glmselect","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 PROC GLMSELECT DATA = SAS - data - set < options > ; < label : > MODEL dependent ( s ) = regressor ( s ) / </ options > ; RUN ; QUIT ; ODS OUTPUT RSTUDENTBYPREDICTED = name - rstud - data - set COOKSDPLOT = name - cooksd - data - set DFFITSPLOT = name - dffits - data - set DFBETASPANEL = name - dfbs - data - set ; PROC REG DATA = SAS - data - set PLOTS ( ONLY LABEL ) = ( RSTUDENTBYPREDICTED COOKSD DFFITS DFBETAS ) < options > ; < label : > MODEL dependent =& _GLSIND </ options > ; ID variable4identification ; RUN ; QUIT ; DATA influential ; MERGE name - rstud - data - set name - cooksd - data - set name - dffits - data - set name - dfbs - data - set ; BY observation ; IF ( ABS ( RSTUDENT ) > 3 ) OR ( COOKSDLABEL NE ' ' ) OR DFFITSOUT THEN FLAG = 1 ; ARRAY DFBETAS { * } _DFBETASOUT : ; DO I = 2 TO DIM ( DFBETAS ) ; IF DFBETAS { I } THEN FLAG = 1 ; END ; IF ABS ( RSTUDENT ) <= 3 THEN RSTUDENT = . ; IF COOKSDLABEL EQ ' ' THEN COOKSD = . ; IF FLAG = 1 ; DROP I FLAG ; RUN ; PROC PRINT DATA = influential ; ID observation ; VAR RSTUDENT COOKSD DFFITSOUT _DFBETASOUT : ; RUN ; PROC GLMSELECT automatically creates the &_GLSIND macro variable which stores the list of effects that are in the model whose variable order can be checked in the Influence Diagnostics panel The ODS statement takes the data that creates each of the requested plots and saves it in the specified data set The LABEL option includes a label for the extreme observations in the plot (labeled with the observation numbers if there is not ID specified) Having influential observations doesn't violate regression assumptions , but it's a major nuisance that you need to address: Recheck for data entry errors If the data appears to be valid, consider whether you have an adequate model (a different model might fit the data better). Divide the number of influential observations you detect by the number of observations in you data set: if the result is $>5\\%$ you probably have the wrong model . Determine whether the influential observation is valid but just unusual As a general rule you should not exclude data (some unusual observations contain important information) If you choose to exclude some observations, include in your report a description of the types of observations that you excluded and why and discuss the limitation of the conclusions given the exclusions","title":"PROC GLMSELECT"},{"location":"statistics/inference/#detecting-collinearity","text":"Collinearity (or multicollinearity) is a problem that you face in multiple regression. It occurs when two or more predictor variables are highly correlated with each other ( redundant information among them, the predictor variables explain much of the same variation in the response). Collinearity doesn't violate the assumptions of multiple regression. Collinearity can hide significant effects (if you include only one of the collinear variables in the model it is significant but when there are more than one included none of them are significant) Collinearity increases the variance of the parameter estimates, making them unstable (the data points don't spread out enough in the space to provide stable support for the plane defined by the model) and, in turn, this increases the prediction error of the model When an overall model is highly significant but the individual variables don't tell the same story, it's a warning sign of collinearity . When the standard error for an estimate is larger than the parameter estimate itself, it's not going to be statistically significant. The SE tells us how variable the corresponding parameter estimate is: when the standard errors are high, the model lacks stability . 1 2 3 4 PROC REG DATA=SAS-data-set <options> ; <label:> MODEL dependent = regressors / VIF </options> ; RUN; QUIT; The VIF (variance inflation factor, $VIF_i=1/(1-R_i^2)$) option measures the magnitude of collinearity in a model (VIF$>10$ for any predictor in the model, those predictors are probably involved in collinearity) Other options are COLLIN (includes the intercept when analyzing collinearity and helps identify the predictors that are causing the problem) and COLLINOINT (requests the same analysis as COLLIN but excludes the intercept)","title":"Detecting Collinearity"},{"location":"statistics/inference/#effective-modeling-cycle","text":"You want to get to know your data by performing preliminary analysis : Plot your data Calculate descriptive statistics Perform correlation analysis Identify some good candidate models using PROC REG : First check for collinearity Use all-possible regression or stepwise selection methods and subject matter knowledge to select model candidates Identify the good ones with the Mallows' (prediction) or Hocking's (explanatory) criterion for $C_p$ Check and validate your assumtions by creating residual plots and conducting a few other statistical tests Deal with any problems in your data : Determine whether any influential observations might be throwing off your model calculations Determine whether any variables are collinear Revise your model Validate your model with data not used to build the model (prediction testing)","title":"Effective modeling cycle"},{"location":"statistics/introduction/","text":"Basic Statistical Concepts Link Descriptive statistics (exploratory data analysis, EDA) Explore your data Inferential statistics (explanatory modelling) How is X related to Y? Sample sizes are typically small and include few variables The focus is on the parameters of the model To assess the model, you use p-values and confidence intervals Predictive modelling If you know X, can you predict Y? Sample sizes are large and include many predictive (input) variables The focus is on the predictions of observations rather than the parameters of the model To assess a predictive model, you validate predictions using holdout sample data Parameters : numerical values (typically unknown, you can't measure the entire population) that summarize characteristics of a population (greek letters) Statistics : summarizes characteristics of a sample (standard alphabet) You use statistics to estimate parameters Independent variable : it can take different values, it affects or determines a dependent variable . It can be called predictor, explanatory, control or input variable. Dependent variable : it can take different values in response to an independent variable . Also known as response, outcome or target variable. Scale of measurement : variable's classification Quantitative/numerical variables : counts or measurements, you can perform arithmetical operations with it Discrete data : variables that can have only a countable number of values within a measurement range Continuous data : variables that are measured on a scale that has infinite number of values and has no breaks or jumps Interval scale data : it can be rank-ordered like ordinal data but also has a sensible spacing of observations such that differenes between measurements are meaningful but it lacks a true zero (ratios are meaningless) Ratio scale data : it is rank-ordered with meaningful spacing and also includes a true zero point and can therefore accurately indicate the ratio difference between two spaces on the measurement scale Categorical/attribute variables : variables that denote groupings or labels Nominal data (qualitative/classification variable) : exhibits no ordering within its observed levels, groups or categories Ordinal data : the observed labels can be ordered in some meaningful way that implies that the differences between the groups or categories are due to magnitude Univariate analysis provides techniques for analyzing and describing a sigle variable. It reveals patterns in the data by looking at the range of values, measures of dispersion , the central tendecy of the values and frequency distribution . Bivariate analysis describes and explains the relationships between two variables and how they change or covary together. It include techniques such as correlation analysis and chi-square tests of independance . Multivariate/Multivariable analysis examines two or more variables at the same time in order to understand the relationships among them. Techniques such as mutiple linear regression and n-way ANOVA are typically called multivariable analysis (only one response variable). Techniques such as factora analysis and clustering are typically called mutivariate analysis (they consider more than one response variable). Descriptive Statistics Measures of central tendencies : mean (affected by outliers), median (less sensitive to outliers), mode Percentile Quartile 25th 1st / lower / Q1 50th 2nd / middle / Q2 Median 75th 3rd / upper / Q3 The interquartile range (IQR) is the difference between Q1 and Q3, it is a robust estimate of the variability because changes in the upper/lower 25% of the data do not affect it. If there are outliers in the data, then the IQR is a more reliable measure of the spread than the overall range. The coefficient of variation (CV) is a measure of the standard deviation expressed as a percentage of the mean ($c_v = \\sigma / 100 \\mu$). Normal distribution Intervals Percentage contained $\\mu \\pm \\sigma$ 68% $\\mu \\pm 2 \\sigma$ 95% $\\mu \\pm 3 \\sigma$ 99% How to check the normality of a sample? Compare the mean and the median : if they are nearly equal, that is an indicator of symmetry (requirement for normality) Check that skewness and kurtosis are close to 0: If both are greater than 1 or less than -1: data is not normal If either is greater than 2 or less than -2: data is not normal Statistical summaries Skewness and kurtosis measure certain aspects of the shape of a distribution (they are 0 and 3 for a normal distribution, although SAS has standardized both to 0) Skewness measures the tendency of your data to be more spread out on one side of the mean than on the other (asymmetry of the distribution). You can think of the direction of skewness as the direction the data is trailing off to. A right-skewed distribution tells us that the mean is greater than the median . Kurtosis measures the tendency of your data to be concentrated toward the center or toward the tails of the distribution (peakedness of the data, tail thickness). A negative kurtosis (platykurtic distribution) means that the data has lighter tails than in a normal distribution. A positive kurtosis (leptokurtic/heavy-tailed/outlier-prone distribution) means that the data has heavier tails and is more concentrated around the mean than a normal distribution. Rectangular, bimodal and multimodal distributions tend to have low values of kurtosis. Asymmetric distributions also tend to have nonzero kurtosis. In these cases, understanding kurtosis is considerably more complex and can be difficult to assess visually. PROC SURVEYSELECT Link How to generate random (representative) samples (population subsets): 1 2 3 4 5 6 7 PROC SURVEYSELECT DATA = SAS - data - set OUT = name - of - output - data - set METHOD = method - of - random - sampling SEED = seed - value SAMPSIZE = number - of - observations - desired ; < STRATA stratification - variable ( s ) ;> RUN ; METHOD specifies the random sampling method to be used. For simple random sampling without replacement, use METHOD=SRS . For simple random sampling with replacement, use METHOD=URS . For other selection methods and details on sampling algorithms, see the SAS online documentation for PROC SURVEYSELECT . SEED specifies the initial seed for random number generation. If no SEED option is specified, SAS uses the system time as its seed value. This creates a different random sample every time the procedure is run. SAMPSIZE indicates the number of observations to be included in the sample. To select a certain fraction of the original data set rather than a given number of observations, use the SAMPRATE option. Picturing Your Data Link PROC UNIVARIATE Link Plots that can be produced with this procedure: Histograms Normal probability plots : expected percentiles from standard normal vs actual data values PROC SGSCATTER Link Plots that can be produced with this procedure: Scatter plots : you can create a single-cell (simple Y by X) scatter plot, a multi-cell scatter plot with multiple independent scatter plots in a grid and a scatter plot matrix , which produces a matrix of scatter plots comparing multiple variables. PROC SGPLOT Link Plots that can be produced with this procedure: 1 2 3 4 5 6 7 8 9 10 11 PROC SGPLOT DATA=SAS-data-set <options> ; DOT category-variable </options> ; HBAR category-variable </options> ; VBAR category-variable </options> ; HBOX response-variable </options> ; VBOX response-variable </options> ; HISTOGRAM response-variable </options> ; SCATTER X=variable Y=variable </options> ; NEEDLE X=variable Y=numeric-variable </options> ; REG X=numeric-variable Y=numeric-variable </options> ; RUN; Anywhere in the procedure you can add reference lines : 1 2 3 4 REFLINE variable | value-1 <... value-n > < /option(s)> /* Example: */ REFLINE 1200 / axis=y lineattrs=(color=blue); Note The order on which you define the parts of the plot will the determined the order on which it is displayed (if you want to send a REFLINE to the back, define it first) Scatter plots ( SCATTER ) Line graphs Histograms ( HISTOGRAM ) with overlaid distribution curves Regression lines ( REG ) with confidence and prediction bands Dot plots ( DOT ) Box plots ( HBOX / VBOX ) : it makes it easy to see how spread out your data is and if there are any outliers. The box represents the middle 50% of your data (IQR). The lower/middle/upper line of the box represent Q1/Q2/Q3. The diamond denotes the mean (easy to check how close the mean is to the median). The whiskers extend as far as the data extends to a maximum length of 1.5 times the IQR above Q3. Any data points farther than this distance are considered possible outliers and are represented in this plot as circles . Bar charts ( HBAR / VBAR ) Needle plot ( NEEDLE ) : creates a plot with needles connecting each point to the baseline You can also overlay plots together to produce many different types of graphs PROC SGPANEL Link Plots that can be produced with this procedure: Panels of plots for different levels of a factor or several different time periods depending on the classification variable Side-by-side histograms which provide a visual comparison for your data PROC SGRENDER Link Plots from graphs templates you have modified or written yourself To specify options for graphs you submit the ODS GRAPHICS statement: 1 ODS GRAPHICS ON < options > ; To select/exclude specific test results, graphs or tables from you output, you can use ODS SELECT and ODS EXCLUDE statements. You can use ODS templates to modify the layout and details of each graph You can use ODS styles to control the general appearance and consistency of yous graphs and tables (by default HTMLBLUE ). Another way to control your output is to use the PLOTS option which is usually available in the procedure statement: 1 PROC UNIVARIATE DATA = SAS - data - set PLOTS = options ; This option enables you to specify which graphs SAS should create, either in addtion or instead of the default plots. Confidence Intervals for the Mean Link A point estimator is a sample statistic used to estimate a population parameter An estimator takes on different values from sample to sample, so it's important to know its variance A statistic that measures the variability of your estimator is the standard error It differs from the standard deviation: the standard deviation deals with the variability of your data while standard error deals with the variability of you sample statistic E.g.: $standard \\ error \\ of \\ the \\ mean = standard \\ deviation/ \\sqrt{sample \\ size}$ The distribution of sample means is always less variable than the data. Because we know that point estimators vary from sample to sample, it would be nice to have an estimator of the mean that directly accounts for this natural variability The interval estimator gives us a range of values that is likely to contain the population mean It is calculated from the standard error and a value that is determined by the degree of certainty we require ( significance level ) Confidence intervals are a type of interval estimator used to estimate the population mean You can make the confidence interval narrower by increasing the sample size and by decreasing the confidence level $CI = sample \\ mean \\pm quantile \\cdot standard \\ error$ The CLM option of PROC MEANS calculates the confidence limits for the mean, you can add ALPHA= to change the default 0.05 value for a 95% confidence level The central limit theorem states that the distribution of sample means is approximately normal regardless of the population distribution's shape, if the sample size is large enough (~30 observations) Hypothesis Testing Link The null hypothesis ($H_0$) is what you assume to be true when you start your analysis The alternative hypothesis ($H_a$ or $H_1$) is your initial research hypothesis, that is, your proposed explanation Decision-making process: Define null and alternative hypothesis Specify significance level (type I error rate) Collect data Reject or fail to reject the null hypothesis The type I and II errors are inversely related : as one type increases the other decreases The power is the probability of a correct rejection = 1 - \u03b2 It is the ability of the statistical test to detect a true difference It is the ability to successfully reject a false null hypothesis A p-value measures the probability of observing a value as extreme as the one observed The p-value is used to determine statistical significance It helps you assess whether you should reject the null hypothesis The p-value is affected by: The effect size : the difference between the observed statistic and the hypothesized value The sample size : the larger the sample size, the more sure you are about the sample statistics, the lower the p-value is A reference distribution enables you to quantify the probability (p-value) of observing a particular outcome (the calculated sample statistic) or a more extreme outcome, if the nul hypothesis is true Two common reference distributions for statistical hypothesis testing are the t distribution and the F distribution These distributions are characterized by the degrees of freedom associated with your data The t distribution arises when you're making inferences about a population mean and the population standard deviation is unknown and has to be estimated from the data It is approximately normal as the sample size grows larger The t distribution is a symmetric distribution like the normal distribution except that the t distribution has thicker tails The t statistic is positive/negative when the sample is more/less than the hypothesized mean If the data doesn't come from a normal distribution, then the t statistic approximately follows a t distribution as long as the sample size is large ( central limit theorem ) Calculation with PROC UNIVARIATE : 1 2 3 4 5 6 7 8 ODS SELECT TESTSFORLOCATION; PROC UNIVARIATE DATA=SAS-data-set MU0=number alpha=number; VAR variable(s); ID variable_to_relate; HISTOGRAM variables </options> ; PROBPLOT variables </options> ; INSET keywords </options> ; RUN; TESTSFORLOCATION displays only the p-values calculation By default MU0 = 0","title":"Introduction"},{"location":"statistics/introduction/#basic-statistical-concepts","text":"Descriptive statistics (exploratory data analysis, EDA) Explore your data Inferential statistics (explanatory modelling) How is X related to Y? Sample sizes are typically small and include few variables The focus is on the parameters of the model To assess the model, you use p-values and confidence intervals Predictive modelling If you know X, can you predict Y? Sample sizes are large and include many predictive (input) variables The focus is on the predictions of observations rather than the parameters of the model To assess a predictive model, you validate predictions using holdout sample data Parameters : numerical values (typically unknown, you can't measure the entire population) that summarize characteristics of a population (greek letters) Statistics : summarizes characteristics of a sample (standard alphabet) You use statistics to estimate parameters Independent variable : it can take different values, it affects or determines a dependent variable . It can be called predictor, explanatory, control or input variable. Dependent variable : it can take different values in response to an independent variable . Also known as response, outcome or target variable. Scale of measurement : variable's classification Quantitative/numerical variables : counts or measurements, you can perform arithmetical operations with it Discrete data : variables that can have only a countable number of values within a measurement range Continuous data : variables that are measured on a scale that has infinite number of values and has no breaks or jumps Interval scale data : it can be rank-ordered like ordinal data but also has a sensible spacing of observations such that differenes between measurements are meaningful but it lacks a true zero (ratios are meaningless) Ratio scale data : it is rank-ordered with meaningful spacing and also includes a true zero point and can therefore accurately indicate the ratio difference between two spaces on the measurement scale Categorical/attribute variables : variables that denote groupings or labels Nominal data (qualitative/classification variable) : exhibits no ordering within its observed levels, groups or categories Ordinal data : the observed labels can be ordered in some meaningful way that implies that the differences between the groups or categories are due to magnitude Univariate analysis provides techniques for analyzing and describing a sigle variable. It reveals patterns in the data by looking at the range of values, measures of dispersion , the central tendecy of the values and frequency distribution . Bivariate analysis describes and explains the relationships between two variables and how they change or covary together. It include techniques such as correlation analysis and chi-square tests of independance . Multivariate/Multivariable analysis examines two or more variables at the same time in order to understand the relationships among them. Techniques such as mutiple linear regression and n-way ANOVA are typically called multivariable analysis (only one response variable). Techniques such as factora analysis and clustering are typically called mutivariate analysis (they consider more than one response variable). Descriptive Statistics Measures of central tendencies : mean (affected by outliers), median (less sensitive to outliers), mode Percentile Quartile 25th 1st / lower / Q1 50th 2nd / middle / Q2 Median 75th 3rd / upper / Q3 The interquartile range (IQR) is the difference between Q1 and Q3, it is a robust estimate of the variability because changes in the upper/lower 25% of the data do not affect it. If there are outliers in the data, then the IQR is a more reliable measure of the spread than the overall range. The coefficient of variation (CV) is a measure of the standard deviation expressed as a percentage of the mean ($c_v = \\sigma / 100 \\mu$). Normal distribution Intervals Percentage contained $\\mu \\pm \\sigma$ 68% $\\mu \\pm 2 \\sigma$ 95% $\\mu \\pm 3 \\sigma$ 99% How to check the normality of a sample? Compare the mean and the median : if they are nearly equal, that is an indicator of symmetry (requirement for normality) Check that skewness and kurtosis are close to 0: If both are greater than 1 or less than -1: data is not normal If either is greater than 2 or less than -2: data is not normal Statistical summaries Skewness and kurtosis measure certain aspects of the shape of a distribution (they are 0 and 3 for a normal distribution, although SAS has standardized both to 0) Skewness measures the tendency of your data to be more spread out on one side of the mean than on the other (asymmetry of the distribution). You can think of the direction of skewness as the direction the data is trailing off to. A right-skewed distribution tells us that the mean is greater than the median . Kurtosis measures the tendency of your data to be concentrated toward the center or toward the tails of the distribution (peakedness of the data, tail thickness). A negative kurtosis (platykurtic distribution) means that the data has lighter tails than in a normal distribution. A positive kurtosis (leptokurtic/heavy-tailed/outlier-prone distribution) means that the data has heavier tails and is more concentrated around the mean than a normal distribution. Rectangular, bimodal and multimodal distributions tend to have low values of kurtosis. Asymmetric distributions also tend to have nonzero kurtosis. In these cases, understanding kurtosis is considerably more complex and can be difficult to assess visually.","title":"Basic Statistical Concepts"},{"location":"statistics/introduction/#proc-surveyselect","text":"How to generate random (representative) samples (population subsets): 1 2 3 4 5 6 7 PROC SURVEYSELECT DATA = SAS - data - set OUT = name - of - output - data - set METHOD = method - of - random - sampling SEED = seed - value SAMPSIZE = number - of - observations - desired ; < STRATA stratification - variable ( s ) ;> RUN ; METHOD specifies the random sampling method to be used. For simple random sampling without replacement, use METHOD=SRS . For simple random sampling with replacement, use METHOD=URS . For other selection methods and details on sampling algorithms, see the SAS online documentation for PROC SURVEYSELECT . SEED specifies the initial seed for random number generation. If no SEED option is specified, SAS uses the system time as its seed value. This creates a different random sample every time the procedure is run. SAMPSIZE indicates the number of observations to be included in the sample. To select a certain fraction of the original data set rather than a given number of observations, use the SAMPRATE option.","title":"PROC SURVEYSELECT"},{"location":"statistics/introduction/#picturing-your-data","text":"","title":"Picturing Your Data"},{"location":"statistics/introduction/#proc-univariate","text":"Plots that can be produced with this procedure: Histograms Normal probability plots : expected percentiles from standard normal vs actual data values","title":"PROC UNIVARIATE"},{"location":"statistics/introduction/#proc-sgscatter","text":"Plots that can be produced with this procedure: Scatter plots : you can create a single-cell (simple Y by X) scatter plot, a multi-cell scatter plot with multiple independent scatter plots in a grid and a scatter plot matrix , which produces a matrix of scatter plots comparing multiple variables.","title":"PROC SGSCATTER"},{"location":"statistics/introduction/#proc-sgplot","text":"Plots that can be produced with this procedure: 1 2 3 4 5 6 7 8 9 10 11 PROC SGPLOT DATA=SAS-data-set <options> ; DOT category-variable </options> ; HBAR category-variable </options> ; VBAR category-variable </options> ; HBOX response-variable </options> ; VBOX response-variable </options> ; HISTOGRAM response-variable </options> ; SCATTER X=variable Y=variable </options> ; NEEDLE X=variable Y=numeric-variable </options> ; REG X=numeric-variable Y=numeric-variable </options> ; RUN; Anywhere in the procedure you can add reference lines : 1 2 3 4 REFLINE variable | value-1 <... value-n > < /option(s)> /* Example: */ REFLINE 1200 / axis=y lineattrs=(color=blue); Note The order on which you define the parts of the plot will the determined the order on which it is displayed (if you want to send a REFLINE to the back, define it first) Scatter plots ( SCATTER ) Line graphs Histograms ( HISTOGRAM ) with overlaid distribution curves Regression lines ( REG ) with confidence and prediction bands Dot plots ( DOT ) Box plots ( HBOX / VBOX ) : it makes it easy to see how spread out your data is and if there are any outliers. The box represents the middle 50% of your data (IQR). The lower/middle/upper line of the box represent Q1/Q2/Q3. The diamond denotes the mean (easy to check how close the mean is to the median). The whiskers extend as far as the data extends to a maximum length of 1.5 times the IQR above Q3. Any data points farther than this distance are considered possible outliers and are represented in this plot as circles . Bar charts ( HBAR / VBAR ) Needle plot ( NEEDLE ) : creates a plot with needles connecting each point to the baseline You can also overlay plots together to produce many different types of graphs","title":"PROC SGPLOT"},{"location":"statistics/introduction/#proc-sgpanel","text":"Plots that can be produced with this procedure: Panels of plots for different levels of a factor or several different time periods depending on the classification variable Side-by-side histograms which provide a visual comparison for your data","title":"PROC SGPANEL"},{"location":"statistics/introduction/#proc-sgrender","text":"Plots from graphs templates you have modified or written yourself To specify options for graphs you submit the ODS GRAPHICS statement: 1 ODS GRAPHICS ON < options > ; To select/exclude specific test results, graphs or tables from you output, you can use ODS SELECT and ODS EXCLUDE statements. You can use ODS templates to modify the layout and details of each graph You can use ODS styles to control the general appearance and consistency of yous graphs and tables (by default HTMLBLUE ). Another way to control your output is to use the PLOTS option which is usually available in the procedure statement: 1 PROC UNIVARIATE DATA = SAS - data - set PLOTS = options ; This option enables you to specify which graphs SAS should create, either in addtion or instead of the default plots.","title":"PROC SGRENDER"},{"location":"statistics/introduction/#confidence-intervals-for-the-mean","text":"A point estimator is a sample statistic used to estimate a population parameter An estimator takes on different values from sample to sample, so it's important to know its variance A statistic that measures the variability of your estimator is the standard error It differs from the standard deviation: the standard deviation deals with the variability of your data while standard error deals with the variability of you sample statistic E.g.: $standard \\ error \\ of \\ the \\ mean = standard \\ deviation/ \\sqrt{sample \\ size}$ The distribution of sample means is always less variable than the data. Because we know that point estimators vary from sample to sample, it would be nice to have an estimator of the mean that directly accounts for this natural variability The interval estimator gives us a range of values that is likely to contain the population mean It is calculated from the standard error and a value that is determined by the degree of certainty we require ( significance level ) Confidence intervals are a type of interval estimator used to estimate the population mean You can make the confidence interval narrower by increasing the sample size and by decreasing the confidence level $CI = sample \\ mean \\pm quantile \\cdot standard \\ error$ The CLM option of PROC MEANS calculates the confidence limits for the mean, you can add ALPHA= to change the default 0.05 value for a 95% confidence level The central limit theorem states that the distribution of sample means is approximately normal regardless of the population distribution's shape, if the sample size is large enough (~30 observations)","title":"Confidence Intervals for the Mean"},{"location":"statistics/introduction/#hypothesis-testing","text":"The null hypothesis ($H_0$) is what you assume to be true when you start your analysis The alternative hypothesis ($H_a$ or $H_1$) is your initial research hypothesis, that is, your proposed explanation Decision-making process: Define null and alternative hypothesis Specify significance level (type I error rate) Collect data Reject or fail to reject the null hypothesis The type I and II errors are inversely related : as one type increases the other decreases The power is the probability of a correct rejection = 1 - \u03b2 It is the ability of the statistical test to detect a true difference It is the ability to successfully reject a false null hypothesis A p-value measures the probability of observing a value as extreme as the one observed The p-value is used to determine statistical significance It helps you assess whether you should reject the null hypothesis The p-value is affected by: The effect size : the difference between the observed statistic and the hypothesized value The sample size : the larger the sample size, the more sure you are about the sample statistics, the lower the p-value is A reference distribution enables you to quantify the probability (p-value) of observing a particular outcome (the calculated sample statistic) or a more extreme outcome, if the nul hypothesis is true Two common reference distributions for statistical hypothesis testing are the t distribution and the F distribution These distributions are characterized by the degrees of freedom associated with your data The t distribution arises when you're making inferences about a population mean and the population standard deviation is unknown and has to be estimated from the data It is approximately normal as the sample size grows larger The t distribution is a symmetric distribution like the normal distribution except that the t distribution has thicker tails The t statistic is positive/negative when the sample is more/less than the hypothesized mean If the data doesn't come from a normal distribution, then the t statistic approximately follows a t distribution as long as the sample size is large ( central limit theorem ) Calculation with PROC UNIVARIATE : 1 2 3 4 5 6 7 8 ODS SELECT TESTSFORLOCATION; PROC UNIVARIATE DATA=SAS-data-set MU0=number alpha=number; VAR variable(s); ID variable_to_relate; HISTOGRAM variables </options> ; PROBPLOT variables </options> ; INSET keywords </options> ; RUN; TESTSFORLOCATION displays only the p-values calculation By default MU0 = 0","title":"Hypothesis Testing"},{"location":"statistics/miscellanea/","text":"One-sided vs Two-sided Tests: How do their p-values compare? Link The default among statistical packages performing tests is to report two-tailed p-values. Because the most commonly used test statistic distributions (standard normal, Student's t) are symmetric about zero, most one-tailed p-values can be derived from the two-tailed p-values. The null hypothesis is that the difference in means is zero. The two-sided alternative is that the difference in means is not zero. There are two one-sided alternatives that one could opt to test instead: that the difference is positive ($diff > 0$) or that the difference is negative ($diff < 0$). Note that the test statistic is the same for all of these tests. The two-tailed p-value is $P > |t|$. This can be rewritten as $P(>x)+P(<x)$. Because the t-distribution is symmetric about zero, these two probabilities are equal: $P > |t| = 2 \\cdot P(<x)$. Thus, we can see that the two-tailed p-value is twice the one-tailed p-value for the alternative hypothesis that ($diff < 0$). The other one-tailed alternative hypothesis has a p-value of $P(>x)=1-(P<x)$. So, depending on the direction of the one-tailed hypothesis, its p-value is either $0.5 \\cdot$(two-tailed p-value) or $1-0.5 \\cdot$(two-tailed p-value) if the test statistic symmetrically distributed around zero. In summary, to understand the connection between the results, you have to carefully review the $H_a$ for each case: the one-sided analysis (there is a difference with a certain sign) is more restrictive and demanding than the two-sided (there is a difference). Therefore, it follows that the one-tail p-value is half the two-tail p-value and the two-tail p-value is twice the one-tail p-value (assuming you correctly predicted the direction of the difference). Check these websites What are the differences between one-tailed and two-tailed tests? One-tail vs two-tail p-values One-sided and two-sided tests and p-values McNemar's Test vs Cohen's Kappa Coefficient Link McNemar's test is a statistical test used on paired nominal data. It is applied to $2 \\times 2$ contingency tables with a dichotomous trait, with matched pairs of subjects, to determine whether the row and column marginal frequencies are equal (that is, whether there is \"marginal homogeneity\"). The null hypothesis of marginal homogeneity states that the two marginal probabilities for each outcome are the same. 1 2 3 4 5 6 7 ODS EXCLUDE ALL ; PROC FREQ DATA = SAS - data - set ; TABLE variable1 * variable2 ; EXACT MCNEM ; ODS OUTPUT MCNEMARSTEST = mcnemarresults ; RUN ; ODS EXCLUDE NONE ; Cohen's kappa coefficient is a statistic which measures inter-rater agreement for qualitative (categorical) items. It is generally thought to be a more robust measure than simple percent agreement calculation, since $\\kappa$ takes into account the possibility of the agreement occurring by chance. There is controversy surrounding Cohen\u2019s Kappa due to the difficulty in interpreting indices of agreement. Some researchers have suggested that it is conceptually simpler to evaluate disagreement between items. If the raters are in complete agreement then $\\kappa=1$. If there is no agreement among the raters other than what would be expected by chance (as given by pe), $\\kappa \\le 0$. Note that Cohen's kappa measures agreement between two raters only . The Fleiss kappa is a multi-rater generalization of Scott's pi statistic. Kappa is also used to compare performance in machine learning but the directional version known as Informedness or Youden's J statistic is argued to be more appropriate for supervised learning. Check these websites McNemar vs. Cohen's Kappa Yates' Correction for Continuity Link Yates' correction for continuity (or Yates' chi-squared test) is used in certain situations when testing for independence in a contingency table. It is a correction made to account for the fact that both Pearson\u2019s chi-square test and McNemar\u2019s chi-square test are biased upwards for a 2 x 2 contingency table. An upwards bias tends to make results larger than they should be. If you are creating a 2 x 2 contingency table that uses either of these two tests, the Yates correction is usually recommended Cochran-Mantel-Haenszel Test Link In statistics, the Cochran\u2013Mantel\u2013Haenszel test (CMH) is a test used in the analysis of stratified or matched categorical data . It allows an investigator to test the association between a binary predictor or treatment and a binary outcome such as case or control status while taking into account the stratification. It is often used in observational studies where random assignment of subjects to different treatments cannot be controlled, but confounding covariates can be measured. The null hypothesis is that there is no association between the treatment and the outcome. More precisely, the null hypothesis is $H_{0}:R=1$ and the alternative hypothesis is $H_{1}:R\\neq 1$. It follows a $\\chi^{2}$ distribution asymptotically under $H_{0}$. The CMH test is a generalization of the McNemar test as their test statistics are identical when the strata are pairs. Unlike the McNemar test which can only handle pairs, the CMH test handles arbitrary strata size . Conditional logistic regression is more general than the CMH test as it can handle continuous variable and perform multivariate analysis. When the CMH test can be applied, the CMH test statistic and the score test statistic of the conditional logistic regression are identical. The CMH test supposes that the effect of the treatment is homogeneous in all strata. The Breslow-Day for homogeneous association test allows to test this assumption. It should be noted that this is not a concern if the strata are small e.g. pairs. Warning The Mantel-Haenszel statistic has 1 degree of freedom and assumes that either exposure or disease are measured on an ordinal (or interval) scales when you have more than two levels . Check these websites Example of CMH vs Fisher Chi-Square test vs T-test Link Characteristics Link A t-test can be either one-sided or two-sided. The chi-square is The chi-squared test is essentially always a one-sided test. Here is a loose way to think about it: the chi-squared test is basically a goodness of fit test. Sometimes it is explicitly referred to as such, but even when it's not, it is still often in essence a goodness of fit. When the realized chi-squared value is way out on the right tail of it's distribution, it indicates a poor fit , and if it is far enough, relative to some pre-specified threshold, we might conclude that it is so poor that we don't believe the data are from that reference distribution. If we were to use the chi-squared test as a two-sided test , we would also be worried if the statistic were too far into the left side of the chi-squared distribution. This would mean that we are worried the fit might be too good . This is simply not something we are typically worried about. As a historical side-note, this is related to the controversy of whether Mendel fudged his data. The idea was that his data were too good to be true. See here for more info if you're curious. In summary, the $\\chi^2$ is a two-sided test from which we are usually interested in only one of the tails of the distribution, indicating more disagreement, rather than less disagreement than one expects by chance. Null Hypothesis Tested Link A t-test tests a null hypothesis about two means ; most often, it tests the hypothesis that two means are equal, or that the difference between them is zero. A chi-square test tests a null hypothesis about the relationship between two variables (even with more than two levels). Types of Data Link A t-test requires two variables; one must be categorical and have exactly two levels, and the other must be quantitative and be estimable by a mean . A chi-square test requires categorical variables , usually only two, but each may have any number of levels . Relationship Between These Tests Link You can refer $z$ to the standard normal table to get one-sided or two-sided $P-$values. Equivalently, for the two-sided alternative $H_0:\\beta \\ne \\beta_0$, $z^2$ has a chi-squared distribution with $df = 1$. The $P-$value is then the right-tail chi-squared probability above the observed value. The two-tail probability beyond $\\pm z$ for the standard normal distribution equals the right-tail probability above $z^2$ for the chi-squared distribution with $df = 1$ . For example, the two-tail standard normal probability of $0.05$ that falls below $\u22121.96$ and above $1.96$ equals the right-tail chi-squared probability above $(1.96)^2 = 3.84$ when $df = 1$. This relationship between normal and chi-square distributions can be extended to the relationship between t-test distribution and chi-square ones. Check these websites The difference between a t-test & a chi-square Introduction to Categorical Data Analysis, A. Agresti, 2007 (2nd ed.) page 11","title":"Miscellanea"},{"location":"statistics/miscellanea/#one-sided-vs-two-sided-tests-how-do-their-p-values-compare","text":"The default among statistical packages performing tests is to report two-tailed p-values. Because the most commonly used test statistic distributions (standard normal, Student's t) are symmetric about zero, most one-tailed p-values can be derived from the two-tailed p-values. The null hypothesis is that the difference in means is zero. The two-sided alternative is that the difference in means is not zero. There are two one-sided alternatives that one could opt to test instead: that the difference is positive ($diff > 0$) or that the difference is negative ($diff < 0$). Note that the test statistic is the same for all of these tests. The two-tailed p-value is $P > |t|$. This can be rewritten as $P(>x)+P(<x)$. Because the t-distribution is symmetric about zero, these two probabilities are equal: $P > |t| = 2 \\cdot P(<x)$. Thus, we can see that the two-tailed p-value is twice the one-tailed p-value for the alternative hypothesis that ($diff < 0$). The other one-tailed alternative hypothesis has a p-value of $P(>x)=1-(P<x)$. So, depending on the direction of the one-tailed hypothesis, its p-value is either $0.5 \\cdot$(two-tailed p-value) or $1-0.5 \\cdot$(two-tailed p-value) if the test statistic symmetrically distributed around zero. In summary, to understand the connection between the results, you have to carefully review the $H_a$ for each case: the one-sided analysis (there is a difference with a certain sign) is more restrictive and demanding than the two-sided (there is a difference). Therefore, it follows that the one-tail p-value is half the two-tail p-value and the two-tail p-value is twice the one-tail p-value (assuming you correctly predicted the direction of the difference). Check these websites What are the differences between one-tailed and two-tailed tests? One-tail vs two-tail p-values One-sided and two-sided tests and p-values","title":"One-sided vs Two-sided Tests: How do their p-values compare?"},{"location":"statistics/miscellanea/#mcnemars-test-vs-cohens-kappa-coefficient","text":"McNemar's test is a statistical test used on paired nominal data. It is applied to $2 \\times 2$ contingency tables with a dichotomous trait, with matched pairs of subjects, to determine whether the row and column marginal frequencies are equal (that is, whether there is \"marginal homogeneity\"). The null hypothesis of marginal homogeneity states that the two marginal probabilities for each outcome are the same. 1 2 3 4 5 6 7 ODS EXCLUDE ALL ; PROC FREQ DATA = SAS - data - set ; TABLE variable1 * variable2 ; EXACT MCNEM ; ODS OUTPUT MCNEMARSTEST = mcnemarresults ; RUN ; ODS EXCLUDE NONE ; Cohen's kappa coefficient is a statistic which measures inter-rater agreement for qualitative (categorical) items. It is generally thought to be a more robust measure than simple percent agreement calculation, since $\\kappa$ takes into account the possibility of the agreement occurring by chance. There is controversy surrounding Cohen\u2019s Kappa due to the difficulty in interpreting indices of agreement. Some researchers have suggested that it is conceptually simpler to evaluate disagreement between items. If the raters are in complete agreement then $\\kappa=1$. If there is no agreement among the raters other than what would be expected by chance (as given by pe), $\\kappa \\le 0$. Note that Cohen's kappa measures agreement between two raters only . The Fleiss kappa is a multi-rater generalization of Scott's pi statistic. Kappa is also used to compare performance in machine learning but the directional version known as Informedness or Youden's J statistic is argued to be more appropriate for supervised learning. Check these websites McNemar vs. Cohen's Kappa","title":"McNemar's Test vs Cohen's Kappa Coefficient"},{"location":"statistics/miscellanea/#yates-correction-for-continuity","text":"Yates' correction for continuity (or Yates' chi-squared test) is used in certain situations when testing for independence in a contingency table. It is a correction made to account for the fact that both Pearson\u2019s chi-square test and McNemar\u2019s chi-square test are biased upwards for a 2 x 2 contingency table. An upwards bias tends to make results larger than they should be. If you are creating a 2 x 2 contingency table that uses either of these two tests, the Yates correction is usually recommended","title":"Yates' Correction for Continuity"},{"location":"statistics/miscellanea/#cochran-mantel-haenszel-test","text":"In statistics, the Cochran\u2013Mantel\u2013Haenszel test (CMH) is a test used in the analysis of stratified or matched categorical data . It allows an investigator to test the association between a binary predictor or treatment and a binary outcome such as case or control status while taking into account the stratification. It is often used in observational studies where random assignment of subjects to different treatments cannot be controlled, but confounding covariates can be measured. The null hypothesis is that there is no association between the treatment and the outcome. More precisely, the null hypothesis is $H_{0}:R=1$ and the alternative hypothesis is $H_{1}:R\\neq 1$. It follows a $\\chi^{2}$ distribution asymptotically under $H_{0}$. The CMH test is a generalization of the McNemar test as their test statistics are identical when the strata are pairs. Unlike the McNemar test which can only handle pairs, the CMH test handles arbitrary strata size . Conditional logistic regression is more general than the CMH test as it can handle continuous variable and perform multivariate analysis. When the CMH test can be applied, the CMH test statistic and the score test statistic of the conditional logistic regression are identical. The CMH test supposes that the effect of the treatment is homogeneous in all strata. The Breslow-Day for homogeneous association test allows to test this assumption. It should be noted that this is not a concern if the strata are small e.g. pairs. Warning The Mantel-Haenszel statistic has 1 degree of freedom and assumes that either exposure or disease are measured on an ordinal (or interval) scales when you have more than two levels . Check these websites Example of CMH vs Fisher","title":"Cochran-Mantel-Haenszel Test"},{"location":"statistics/miscellanea/#chi-square-test-vs-t-test","text":"","title":"Chi-Square test vs T-test"},{"location":"statistics/miscellanea/#characteristics","text":"A t-test can be either one-sided or two-sided. The chi-square is The chi-squared test is essentially always a one-sided test. Here is a loose way to think about it: the chi-squared test is basically a goodness of fit test. Sometimes it is explicitly referred to as such, but even when it's not, it is still often in essence a goodness of fit. When the realized chi-squared value is way out on the right tail of it's distribution, it indicates a poor fit , and if it is far enough, relative to some pre-specified threshold, we might conclude that it is so poor that we don't believe the data are from that reference distribution. If we were to use the chi-squared test as a two-sided test , we would also be worried if the statistic were too far into the left side of the chi-squared distribution. This would mean that we are worried the fit might be too good . This is simply not something we are typically worried about. As a historical side-note, this is related to the controversy of whether Mendel fudged his data. The idea was that his data were too good to be true. See here for more info if you're curious. In summary, the $\\chi^2$ is a two-sided test from which we are usually interested in only one of the tails of the distribution, indicating more disagreement, rather than less disagreement than one expects by chance.","title":"Characteristics"},{"location":"statistics/miscellanea/#null-hypothesis-tested","text":"A t-test tests a null hypothesis about two means ; most often, it tests the hypothesis that two means are equal, or that the difference between them is zero. A chi-square test tests a null hypothesis about the relationship between two variables (even with more than two levels).","title":"Null Hypothesis Tested"},{"location":"statistics/miscellanea/#types-of-data","text":"A t-test requires two variables; one must be categorical and have exactly two levels, and the other must be quantitative and be estimable by a mean . A chi-square test requires categorical variables , usually only two, but each may have any number of levels .","title":"Types of Data"},{"location":"statistics/miscellanea/#relationship-between-these-tests","text":"You can refer $z$ to the standard normal table to get one-sided or two-sided $P-$values. Equivalently, for the two-sided alternative $H_0:\\beta \\ne \\beta_0$, $z^2$ has a chi-squared distribution with $df = 1$. The $P-$value is then the right-tail chi-squared probability above the observed value. The two-tail probability beyond $\\pm z$ for the standard normal distribution equals the right-tail probability above $z^2$ for the chi-squared distribution with $df = 1$ . For example, the two-tail standard normal probability of $0.05$ that falls below $\u22121.96$ and above $1.96$ equals the right-tail chi-squared probability above $(1.96)^2 = 3.84$ when $df = 1$. This relationship between normal and chi-square distributions can be extended to the relationship between t-test distribution and chi-square ones. Check these websites The difference between a t-test & a chi-square Introduction to Categorical Data Analysis, A. Agresti, 2007 (2nd ed.) page 11","title":"Relationship Between These Tests"},{"location":"statistics/prediction/","text":"Inferential models, such as ANOVA and linear regression, are used to test hypotheses about the data and characterize the relationships between various types of predictor variables and a response variable. But, what if you want to go beyond explaining the relationship and predict future values of the response variable ? In predictive modeling, a statistical model is used to predict future values of a response variable, based on the existing values of predictor variables. In predictive modeling, the terms used to refer to the variables are often different from the terms used in explanatory modeling: Predictors are often called inputs, features, explanatory variables or independent variables The response variable is often called a target, an outcome or dependent variable In predictive modeling, the inputs and the target can be continuous, categorical, binary or any combination of these types. Introduction to Predictive Modeling Link Before you can predict values, you must first build a predictive model. To build a predictive model, you can use PROC GLMSELECT . Describe the goal and uses of predictive modeling Explain how data partitioning is used in the hones assessment method of model selection Describe the relationship between model complexity and flexibility Use PROC GLMSELECT to build a predictive model What Is Predictive Modeling? Link Predictive modeling uses historical data to predict future outcomes. The process of building and scoring a predictive model has two main parts: building the predictive model on existing data and then deploying the model to make predictions on new data (using a process call scoring). A predictive model consists of either a formula or rules (depending on the type of analysis that you use) based on a set of input variables that are moes likely to predict the values of a target variable. Here we will deal with predictive models based on regression models, which are parametric and have formulas . Predictive models can also be based on non-parametric models such as decision trees, which have rules . Model-based predictions are often called fact-based predictions . In contrast, decisions that are based completely on people's business expertise are often referred to as intuition-based decisions . Prediction modeling takes the guesswork out of the prediction process. Model Complexity Link Whether you are doing predictive modeling or inferential modeling, you want to select a model that generalizes well, that is, the model that best fits the entire population. You assume that a sample that is used to fit the model is representative of the population. However, any given sample typically has idiosyncracies that are not found in the population. The model that best fits a sample and the population is the model that has the right complexity. A naive modeler might assume that most complex model should always outperform the others, but this is not the case. An overly complex model might be too flexible . This leads to overfitting that is, accomodating nuances of the random noise (the chance relationships) in the particular sample. Overfitting leads to models that have higher variance when they are applied to a population . For regression, including more terms in the model increases complexity. On the other hand, an inssufficiently complex model might not be flexible enough . This leads to underfitting that is, systematically missing the signal (the true relationships). This leads to biased inferences , which are inferences that are not true of the population. A model with just enough complexity, which also means just enough flexibility , gives the best generalization. The important thing to realize is that there is not one perfect model; there is always a balance between overfitting and underfitting. Building a Predictive Model Link The first part of the predictive modeling process is building the model. There are two steps to building the model: fitting the model and then, assessing model performance in order to select the model that will be deployed. To build a predictive model, a method called honest assessment is commonly used to ensure that the best model is selected. Honest assessment means that the assessment is done on a different data set than the one that was used to build the model and thus, it involves partitioning the available data typically into two data sets: a training data set and a validation data set. Both data sets contain the inputs and the target. The training data set is used to fit the model. In the training data set, an observation is called a training case . Other synonyms for \"observation\" are example, instance and record. The validation data set is a holdout sample that is used to assess model performance and select the model that has the best generalization. Sometimes there is a third partition of the data, the test data set , that is used to perform a final test on the model before the model is used for scoring. This final test can be referred to as a final honest estimate of generalization. Like the validation data set, the test data set is also referred to as a holdout data set. In practice, many analysts see no need for a final honest assessment of generalization based on a test data set. Instead, an optimal model is chosen using the validation data. The model assessment that is measured on the validation data, is reported as an upper bound on the performance that is expected when the model is deployed for scoring. Partitioning the data avoids overfitting problems. The classical example of overfitting is selecting linear regression models based on R square. Is it always best to partition the data set when you build a predictive model? This depends on the size of the data set. If you start with a small or medium-size data set, partitioning the data might not be efficient. The reduced sample size can severely degrade the fit of the model. In fact, computer-intensive methods, such as the cross-validation and bootstrap methods , were developed so that all the data can be used for both fitting and honest assessment. However, predictive modeling usually involves very large data sets, so partitioning the data is usually appropriate. Let's take a closer look at using honest assessment to build a predictive model. During model fitting, the training data is used to model the target. You can use one of several model selection methods. The forward selection process generates a number of possible models, which increases in complexity as variables are added to the model. Variable continue to be added as long as they meet the criterion for inclusion. For example, if you use the AICC criterion, variables will be added as long as the criterion value continues to decrease and then the process stops. From a number of models of different complexity generated with the training data on which the validation data is used to assess their performance, the chosen model will be the simplest model (more parsimonious) with the highest validation assessment or best performance on the validation data . Using PROC GLMSELECT to Build a Predictive Model Link This procedure can build a model in two ways. The method that you use depends on the state of your data before model building begins. If your data is already partitioned into a trining data set and a validation data set, you can simply reference both data sets in the procedure. If you start with a single data set, PROC GLMSELECT can partition the data for you. If the validation data set already exists: 1 2 3 4 PROC GLMSELECT DATA = training - data - set VALDATA = validation - data - set ; MODEL target ( s ) = input ( s ) </ options > ; RUN ; If you start with a data set that is not yet partitioned: 1 2 3 4 5 PROC GLMSELECT DATA=training-data-set <SEED =number > ; MODEL target(s)=input(s) </ options> ; PARTITION FRACTION ( <TEST =fraction ><VALIDATE =fraction > ); RUN; The sum of the specified fractions must be less than 1 and the remaining fraction of the cases in the inputa data set are assigned the training role. The PARTITION statement requires a pseudo-random number generator to start the random selection process and a starting seed seed is needed which must be an integer. If you need to be able to reproduce your results in the future, you specify an integer that is greater than zero in the SEED= option in order to get the same results every time. If the SEED= value specifies an invalid value or no value, the seed is automatically generated from reading the time of day from the computer's clock. In most situations, it is recommended that you use the SEED= option and specify an integer greater than zero. Building a Predictive Model Link 1 2 3 4 5 6 7 8 9 10 11 ODS GRAPHICS ; PROC GLMSELECT DATA = training - data - set PLOTS = all VALDATA = validation - data - set ; CLASS categorical1 categorical2 / PARAM = glm REF = first ; MODEL target = categorical1 categorical2 interval1 interval2 / SELECTION = backward | forward | stepwise SELECT = sbc | aic CHOOSE = validate ; STORE OUT = output - data - set ; title \"Selecting the Best Model using Honest Assessment\" ; RUN ; The PARAM= option specifies the parametrization method. In this case glm parametrization is used (same method used in PROC GLM ). With glm parametrization, one design variable is produced per level of each CLASS variable. We specify REF=first so that, for each design variable, we can compare the output at the current level with the first level. By default, the reference level is the last level. The SELECTION= option specifies the backward selection model of variable selection. SELECT=sbc indicates that the Schwarz-Bayesian criterion will be used to determine which variables remain in the model. CHOOSE=validate specifies that PROC GLMSELECT will select the best model based on the validation data (the one that has the smallest overall validation error, specifically the average squared error --ASE--, which is the sum of the squared differences between the observed value and the predicted value using the model). The STORE statement saves the context and results of the statistical analysis in a file called an item store. It is convenient to create an item store because you can reference it in later programs and avoid having to rebuild the model. For example, you can use the item store to score new data in the next step of predictive modeling. Let's describe now the typical obtained results: The first table summarizes model information, including the data sets, variables, selction method and criteria that are used to select the variables and the final model. Then there are two Observation Profile for Analysis tables, one for the analysis (that is, training) data and the other for the validation data. The Class Level Information table shows the number of categorical variables included in the analysis. The total degrees of freedom are much higher than this number because most categorical variables have more than one level, which results in more than one design variable. The number of non-redundant design variables or degrees of freedom of a categorical variable is the total number of levels minus one. In the Dimensions table, you can see the number of effects and parameters of the analysis. The number of parameters is higher than the number of effects because some of the effects have multiple parameters. The number of effects = number of interval variables + number of categorical variables + intercept The number of parameters = number of interval variables + number of degrees of freedom of categorical variables (addition of total number of levels - the number of categorical variables) + intercept In the Backward Selection Summary table, the Step 0 row shows the number of effects and non-redundant parameters (smaller than the number of parameters that is shown in the Dimensions table) that we start with. The difference is the number of variables because there is one redundant design variable for each categorical variable. The SBC is assessed on the training data and variables are removed from the model while this produces a reduction on it. In the last row, the SBC value is followed by an asterisk, that means Optimal Value of Criterion . Remember that the SBC is also the stopping criterion because we didn't specify a different criterion. So, based on the SBC for the training data, the model at the last step is the best model. The next column reports the training average squared error, but this is not used to select the model. It seems contradictory that, as you remove variables from the model, the training ASE tends to increase. However, this is not true for the validation data set, so the validation data set gives a better idea of how the model performs on data that was not used to build the model. Remember that the model is chosen based on the validation ASE. Going down the column for the validation ASE, the values continue to decrease until a value marked with an asterisk, meaning that this is the best model based on the validation data. The Stop Details table summarizes the information that we just discussed. In the first plot, Coefficient Progression for [target] , the lower section shows the performance of all the models based on the validation ASE. In the top section you can compare the models and see how the parameters changed as variables were removed from the model. A vertical line extends up through the point for the selected model. The parameter estimates on this vertical line are of interest to us. In the Progression of Average Squared Errors by Role for [target] plot, the ASE for the validation data is plotted on the top line and the training data on the bottom line. Looking at the training data plot, notice that the ASE cannot go down; it can only go up because the backwards elimination criterion is used to remove variables from the model. For the training data, the ASE can go down only if variables are added. In the Analysis of Variance table, we see some summary information. Remember that the number of degrees of freedom in the selected model is not the number of variables. The Parameter Estimates table. For each categorical variable, the selected levels are shown and the table and the lowest one (because the REF=first option was defined) is used as the reference level and is then set to zero with zero degrees of freedom, which makes it the redundant design variable. This table also shows t values. We could also get p-values, but that is not useful because this model was already selected. Notice that this table lists all the categorical variables before the continuous variables. Scoring Predictive Models Link After you build a predictive model, you are ready to deploy the model. To score new data you can use PROC GLMSELECT and PROC PLM . Preparing for Scoring Link Before you start using a newly-built model to score data, some preparation of the model, the data or both is usually required. It is essential for the scoring data to be comparable to the training and validation data that were used to build the model. Before the model is build, modifications are often made to the training data, such as missing value imputation, transformations and derivation of inputs through standardization of the creation of composite variables from existing variables. The same modifications must be made to the validation data before validating the model and to the scoring data before scoring. Making the same modifications becomes more complex if the original modifications were based on parameters derived from the training data set, such as the mean or standard deviation. Methods of Scoring Link When you score, you do not rerun the algorithm that was used to build the model. Instead, you apply the score code that is, the equations obtained from the final model to the scoring data. Let's look at three methods of scoring your data. Method 1 : a SCORE statement is added to the PROC GLMSELECT step that is used to create the model. This method is useful because you can build and score a model in one step. However, this method is inefficient if you want to score more than once or use a large data set to build a model. With this method, the model must be built from the training data each time the program is run. Method 2 : a STORE statement in the PROC GLMSELECT step and then a SCORE statement in PROC PLM . This method enables you to build the model only once, along with an item store, using PROC GLMSELECT . You can then use PROC PLM to score new data using the item store. Separating the code for model building and model scoring is especially helpful if your model is based on a very large training data set or if you want to scroe more than once. One potential problem with this method is that others might not be able to use this code with earlier versions of SAS or you might not want to share the entire item store. Method 3 : a STORE statement in PROC GLMSELECT , a CODE statement in PROC PLM to output SAS code for scoring and then a DATA step to do the scoring. Some of the previous method's problems are solved by using PROC PLM to write detailed scoring code, based on the item store, that is compatible with earlier versions of SAS. You can provide this code to others without having to share other information that is in the item store. The DATA step is then used for scoring. Scoring Data Link Let's use the item store that we created in the last example to score data. When you score data in your work environment, you'll be scoring data that was not used in either training or validation. Here we use code that scores the data in two different ways and then compares the output from the two methods. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 PROC PLM RESTORE = output - data - set ; SCORE DATA = data - set - to - score OUT = scored - data - set1 ; CODE FILE = \"/folders/myfolders/scoring.sas\" ; RUN ; DATA scored - data - set2 ; SET data - set - to - score ; %include \"/folders/myfolders/scoring.sas\" ; RUN ; PROC COMPARE BASE = scored - data - set1 COMPARE = scored - data - set2 CRITERION = 0.0001 ; VAR predicted1 ; WITH predicted2 ; RUN ; The PROC PLM uses the SCORE statement to score the data. By default the name Predicted is used for the scored variable. An statement of this kind can be used also in the PROC GLMSELECT to score data. The second method generates scoring code by using the CODE statement in PROC PLM , and then uses a DATA step to do the scoring. The PROC COMPARE step compares the values of the scored variable in the two output data sets. There's no need to do any preliminary matching or sorting in this case because the output data sets are based ont he same input data set; they have the same number of variables and observations, in the same order. By default, the criterion for judging the equality of the numeric values is 0.00001 which can be changed by specifying a different criterion using the CRITERION= option. The scored variable has a different name in the two data sets, so the two names are specified in the VAR (for the BASE= ) and WITH (for the COMPARE= ) statements. In the results, we want to look at the Values Comparison Summary to see whether the two methods produced similar predictions and check for values that are compared as unequal. You can find Values not EXACTLY Equal but, as the Maximum Difference Criterion Value indicates, the differences are too small to be important.","title":"Model Building and Scoring for Prediction"},{"location":"statistics/prediction/#introduction-to-predictive-modeling","text":"Before you can predict values, you must first build a predictive model. To build a predictive model, you can use PROC GLMSELECT . Describe the goal and uses of predictive modeling Explain how data partitioning is used in the hones assessment method of model selection Describe the relationship between model complexity and flexibility Use PROC GLMSELECT to build a predictive model","title":"Introduction to Predictive Modeling"},{"location":"statistics/prediction/#what-is-predictive-modeling","text":"Predictive modeling uses historical data to predict future outcomes. The process of building and scoring a predictive model has two main parts: building the predictive model on existing data and then deploying the model to make predictions on new data (using a process call scoring). A predictive model consists of either a formula or rules (depending on the type of analysis that you use) based on a set of input variables that are moes likely to predict the values of a target variable. Here we will deal with predictive models based on regression models, which are parametric and have formulas . Predictive models can also be based on non-parametric models such as decision trees, which have rules . Model-based predictions are often called fact-based predictions . In contrast, decisions that are based completely on people's business expertise are often referred to as intuition-based decisions . Prediction modeling takes the guesswork out of the prediction process.","title":"What Is Predictive Modeling?"},{"location":"statistics/prediction/#model-complexity","text":"Whether you are doing predictive modeling or inferential modeling, you want to select a model that generalizes well, that is, the model that best fits the entire population. You assume that a sample that is used to fit the model is representative of the population. However, any given sample typically has idiosyncracies that are not found in the population. The model that best fits a sample and the population is the model that has the right complexity. A naive modeler might assume that most complex model should always outperform the others, but this is not the case. An overly complex model might be too flexible . This leads to overfitting that is, accomodating nuances of the random noise (the chance relationships) in the particular sample. Overfitting leads to models that have higher variance when they are applied to a population . For regression, including more terms in the model increases complexity. On the other hand, an inssufficiently complex model might not be flexible enough . This leads to underfitting that is, systematically missing the signal (the true relationships). This leads to biased inferences , which are inferences that are not true of the population. A model with just enough complexity, which also means just enough flexibility , gives the best generalization. The important thing to realize is that there is not one perfect model; there is always a balance between overfitting and underfitting.","title":"Model Complexity"},{"location":"statistics/prediction/#building-a-predictive-model","text":"The first part of the predictive modeling process is building the model. There are two steps to building the model: fitting the model and then, assessing model performance in order to select the model that will be deployed. To build a predictive model, a method called honest assessment is commonly used to ensure that the best model is selected. Honest assessment means that the assessment is done on a different data set than the one that was used to build the model and thus, it involves partitioning the available data typically into two data sets: a training data set and a validation data set. Both data sets contain the inputs and the target. The training data set is used to fit the model. In the training data set, an observation is called a training case . Other synonyms for \"observation\" are example, instance and record. The validation data set is a holdout sample that is used to assess model performance and select the model that has the best generalization. Sometimes there is a third partition of the data, the test data set , that is used to perform a final test on the model before the model is used for scoring. This final test can be referred to as a final honest estimate of generalization. Like the validation data set, the test data set is also referred to as a holdout data set. In practice, many analysts see no need for a final honest assessment of generalization based on a test data set. Instead, an optimal model is chosen using the validation data. The model assessment that is measured on the validation data, is reported as an upper bound on the performance that is expected when the model is deployed for scoring. Partitioning the data avoids overfitting problems. The classical example of overfitting is selecting linear regression models based on R square. Is it always best to partition the data set when you build a predictive model? This depends on the size of the data set. If you start with a small or medium-size data set, partitioning the data might not be efficient. The reduced sample size can severely degrade the fit of the model. In fact, computer-intensive methods, such as the cross-validation and bootstrap methods , were developed so that all the data can be used for both fitting and honest assessment. However, predictive modeling usually involves very large data sets, so partitioning the data is usually appropriate. Let's take a closer look at using honest assessment to build a predictive model. During model fitting, the training data is used to model the target. You can use one of several model selection methods. The forward selection process generates a number of possible models, which increases in complexity as variables are added to the model. Variable continue to be added as long as they meet the criterion for inclusion. For example, if you use the AICC criterion, variables will be added as long as the criterion value continues to decrease and then the process stops. From a number of models of different complexity generated with the training data on which the validation data is used to assess their performance, the chosen model will be the simplest model (more parsimonious) with the highest validation assessment or best performance on the validation data .","title":"Building a Predictive Model"},{"location":"statistics/prediction/#using-proc-glmselect-to-build-a-predictive-model","text":"This procedure can build a model in two ways. The method that you use depends on the state of your data before model building begins. If your data is already partitioned into a trining data set and a validation data set, you can simply reference both data sets in the procedure. If you start with a single data set, PROC GLMSELECT can partition the data for you. If the validation data set already exists: 1 2 3 4 PROC GLMSELECT DATA = training - data - set VALDATA = validation - data - set ; MODEL target ( s ) = input ( s ) </ options > ; RUN ; If you start with a data set that is not yet partitioned: 1 2 3 4 5 PROC GLMSELECT DATA=training-data-set <SEED =number > ; MODEL target(s)=input(s) </ options> ; PARTITION FRACTION ( <TEST =fraction ><VALIDATE =fraction > ); RUN; The sum of the specified fractions must be less than 1 and the remaining fraction of the cases in the inputa data set are assigned the training role. The PARTITION statement requires a pseudo-random number generator to start the random selection process and a starting seed seed is needed which must be an integer. If you need to be able to reproduce your results in the future, you specify an integer that is greater than zero in the SEED= option in order to get the same results every time. If the SEED= value specifies an invalid value or no value, the seed is automatically generated from reading the time of day from the computer's clock. In most situations, it is recommended that you use the SEED= option and specify an integer greater than zero.","title":"Using PROC GLMSELECT to Build a Predictive Model"},{"location":"statistics/prediction/#building-a-predictive-model_1","text":"1 2 3 4 5 6 7 8 9 10 11 ODS GRAPHICS ; PROC GLMSELECT DATA = training - data - set PLOTS = all VALDATA = validation - data - set ; CLASS categorical1 categorical2 / PARAM = glm REF = first ; MODEL target = categorical1 categorical2 interval1 interval2 / SELECTION = backward | forward | stepwise SELECT = sbc | aic CHOOSE = validate ; STORE OUT = output - data - set ; title \"Selecting the Best Model using Honest Assessment\" ; RUN ; The PARAM= option specifies the parametrization method. In this case glm parametrization is used (same method used in PROC GLM ). With glm parametrization, one design variable is produced per level of each CLASS variable. We specify REF=first so that, for each design variable, we can compare the output at the current level with the first level. By default, the reference level is the last level. The SELECTION= option specifies the backward selection model of variable selection. SELECT=sbc indicates that the Schwarz-Bayesian criterion will be used to determine which variables remain in the model. CHOOSE=validate specifies that PROC GLMSELECT will select the best model based on the validation data (the one that has the smallest overall validation error, specifically the average squared error --ASE--, which is the sum of the squared differences between the observed value and the predicted value using the model). The STORE statement saves the context and results of the statistical analysis in a file called an item store. It is convenient to create an item store because you can reference it in later programs and avoid having to rebuild the model. For example, you can use the item store to score new data in the next step of predictive modeling. Let's describe now the typical obtained results: The first table summarizes model information, including the data sets, variables, selction method and criteria that are used to select the variables and the final model. Then there are two Observation Profile for Analysis tables, one for the analysis (that is, training) data and the other for the validation data. The Class Level Information table shows the number of categorical variables included in the analysis. The total degrees of freedom are much higher than this number because most categorical variables have more than one level, which results in more than one design variable. The number of non-redundant design variables or degrees of freedom of a categorical variable is the total number of levels minus one. In the Dimensions table, you can see the number of effects and parameters of the analysis. The number of parameters is higher than the number of effects because some of the effects have multiple parameters. The number of effects = number of interval variables + number of categorical variables + intercept The number of parameters = number of interval variables + number of degrees of freedom of categorical variables (addition of total number of levels - the number of categorical variables) + intercept In the Backward Selection Summary table, the Step 0 row shows the number of effects and non-redundant parameters (smaller than the number of parameters that is shown in the Dimensions table) that we start with. The difference is the number of variables because there is one redundant design variable for each categorical variable. The SBC is assessed on the training data and variables are removed from the model while this produces a reduction on it. In the last row, the SBC value is followed by an asterisk, that means Optimal Value of Criterion . Remember that the SBC is also the stopping criterion because we didn't specify a different criterion. So, based on the SBC for the training data, the model at the last step is the best model. The next column reports the training average squared error, but this is not used to select the model. It seems contradictory that, as you remove variables from the model, the training ASE tends to increase. However, this is not true for the validation data set, so the validation data set gives a better idea of how the model performs on data that was not used to build the model. Remember that the model is chosen based on the validation ASE. Going down the column for the validation ASE, the values continue to decrease until a value marked with an asterisk, meaning that this is the best model based on the validation data. The Stop Details table summarizes the information that we just discussed. In the first plot, Coefficient Progression for [target] , the lower section shows the performance of all the models based on the validation ASE. In the top section you can compare the models and see how the parameters changed as variables were removed from the model. A vertical line extends up through the point for the selected model. The parameter estimates on this vertical line are of interest to us. In the Progression of Average Squared Errors by Role for [target] plot, the ASE for the validation data is plotted on the top line and the training data on the bottom line. Looking at the training data plot, notice that the ASE cannot go down; it can only go up because the backwards elimination criterion is used to remove variables from the model. For the training data, the ASE can go down only if variables are added. In the Analysis of Variance table, we see some summary information. Remember that the number of degrees of freedom in the selected model is not the number of variables. The Parameter Estimates table. For each categorical variable, the selected levels are shown and the table and the lowest one (because the REF=first option was defined) is used as the reference level and is then set to zero with zero degrees of freedom, which makes it the redundant design variable. This table also shows t values. We could also get p-values, but that is not useful because this model was already selected. Notice that this table lists all the categorical variables before the continuous variables.","title":"Building a Predictive Model"},{"location":"statistics/prediction/#scoring-predictive-models","text":"After you build a predictive model, you are ready to deploy the model. To score new data you can use PROC GLMSELECT and PROC PLM .","title":"Scoring Predictive Models"},{"location":"statistics/prediction/#preparing-for-scoring","text":"Before you start using a newly-built model to score data, some preparation of the model, the data or both is usually required. It is essential for the scoring data to be comparable to the training and validation data that were used to build the model. Before the model is build, modifications are often made to the training data, such as missing value imputation, transformations and derivation of inputs through standardization of the creation of composite variables from existing variables. The same modifications must be made to the validation data before validating the model and to the scoring data before scoring. Making the same modifications becomes more complex if the original modifications were based on parameters derived from the training data set, such as the mean or standard deviation.","title":"Preparing for Scoring"},{"location":"statistics/prediction/#methods-of-scoring","text":"When you score, you do not rerun the algorithm that was used to build the model. Instead, you apply the score code that is, the equations obtained from the final model to the scoring data. Let's look at three methods of scoring your data. Method 1 : a SCORE statement is added to the PROC GLMSELECT step that is used to create the model. This method is useful because you can build and score a model in one step. However, this method is inefficient if you want to score more than once or use a large data set to build a model. With this method, the model must be built from the training data each time the program is run. Method 2 : a STORE statement in the PROC GLMSELECT step and then a SCORE statement in PROC PLM . This method enables you to build the model only once, along with an item store, using PROC GLMSELECT . You can then use PROC PLM to score new data using the item store. Separating the code for model building and model scoring is especially helpful if your model is based on a very large training data set or if you want to scroe more than once. One potential problem with this method is that others might not be able to use this code with earlier versions of SAS or you might not want to share the entire item store. Method 3 : a STORE statement in PROC GLMSELECT , a CODE statement in PROC PLM to output SAS code for scoring and then a DATA step to do the scoring. Some of the previous method's problems are solved by using PROC PLM to write detailed scoring code, based on the item store, that is compatible with earlier versions of SAS. You can provide this code to others without having to share other information that is in the item store. The DATA step is then used for scoring.","title":"Methods of Scoring"},{"location":"statistics/prediction/#scoring-data","text":"Let's use the item store that we created in the last example to score data. When you score data in your work environment, you'll be scoring data that was not used in either training or validation. Here we use code that scores the data in two different ways and then compares the output from the two methods. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 PROC PLM RESTORE = output - data - set ; SCORE DATA = data - set - to - score OUT = scored - data - set1 ; CODE FILE = \"/folders/myfolders/scoring.sas\" ; RUN ; DATA scored - data - set2 ; SET data - set - to - score ; %include \"/folders/myfolders/scoring.sas\" ; RUN ; PROC COMPARE BASE = scored - data - set1 COMPARE = scored - data - set2 CRITERION = 0.0001 ; VAR predicted1 ; WITH predicted2 ; RUN ; The PROC PLM uses the SCORE statement to score the data. By default the name Predicted is used for the scored variable. An statement of this kind can be used also in the PROC GLMSELECT to score data. The second method generates scoring code by using the CODE statement in PROC PLM , and then uses a DATA step to do the scoring. The PROC COMPARE step compares the values of the scored variable in the two output data sets. There's no need to do any preliminary matching or sorting in this case because the output data sets are based ont he same input data set; they have the same number of variables and observations, in the same order. By default, the criterion for judging the equality of the numeric values is 0.00001 which can be changed by specifying a different criterion using the CRITERION= option. The scored variable has a different name in the two data sets, so the two names are specified in the VAR (for the BASE= ) and WITH (for the COMPARE= ) statements. In the results, we want to look at the Values Comparison Summary to see whether the two methods produced similar predictions and check for values that are compared as unequal. You can find Values not EXACTLY Equal but, as the Maximum Difference Criterion Value indicates, the differences are too small to be important.","title":"Scoring Data"},{"location":"statistics/regression/","text":"Exploratory Data Analysis Link A useful set of techniques for investigating your data is known as exploratory data analysis . PROC SGCATTER : Scatter Plots Link 1 2 3 PROC SGSCATTER DATA = SAS - data - base ; PLOT variableY * ( variableX1 variableX2 ) / REG ; RUN ; If you have so many observations that the scatter plot of the whole data set is difficult to interpret, you might run PROC SGSCATTER on a random sample of observations PROC CORR : Correlation Analysis Link There are two ways of calculating correlations: the Pearson correlation coefficient (specially used for normal distributed data) and the Spearman's rank correlation coefficient (which is best fitted when your data presents outliers). The Spearman correlation between two variables is equal to the Pearson correlation between the rank values of those two variables; while Pearson's correlation assesses linear relationships, Spearman's correlation assesses monotonic relationships (whether linear or not). Spearman's coefficient is appropriate for both continuous and discrete ordinal variables. The closer the Pearson correlation coefficient is to +1/-1, the stronger the positive/negative linear relationship is between the two variables. The closer the correlation coefficient is to 0, the weaker the linear relationship and if it is 0 variables are uncorrelated. When you interpret the correlation, be cautious about the effect of large sample sizes : even a correlation of 0.01 can be statistically significant with a large enough sample size and you would almost always reject the hypothesis $H_0$: $\\rho =0$, even if the value of your correlation is small for all practical purposes Some common errors on interpreting correlations are concluding a cause-and-effect relationship between the variables misinterpreting the kind of relationship between the variables and failing to recognize the influence of outliers on the correlation The variables might be related but not causally Correlation coefficients can be large because both variables are affected by other variables Variables might be strongly correlated by chance Just because the correlation coefficient is close to 0 doesn't mean that no relationship exists between the two variables: they might have a non-linear relationship Another common error is failing to recognize the influence of outliers on the correlation If you have an outlier you should report both correlation coefficients (with and without the outlier) to report how influential the unusual data point is in your analysis The PROC CORR also produces scatter plots or a scatter plot matrix . 1 2 3 4 5 PROC CORR DATA = SAS - data - set RANK | NOSIMPLE PLOTS ( ONLY ) = MATRIX ( NVAR = ALL HISTOGRAM ) | SCATTER ( NVAR = ALL ELLIPSE = NONE ) < options > ; VAR variable ( s ) X ; WITH variable ( s ) Y ; ID variable4label ; RUN ; Simple Linear Regression Link You use correlation analysis to determine the strength of the linear relationship between continuous response variables. Now you need to go a step further and define the linear relationship itself : $Y= \\beta_0+\\beta_1 \\cdot X+\\epsilon$ $Y$ is the response variable $X$ is the predictor variable $\\beta_0$ is the intercept parameter $\\beta_1$ is the slope parameter $\\epsilon$ is the error term The method of least squares produces parameter estimates $\\hat \\beta_0$ and $\\hat \\beta_1$ with certain optimum properties which make them the Best Linear Unbiased Estimators ( BLUE ): They are unbiased estimates of the population parameters They have minimum variance To find out how much better is the model that takes the predictor variable into account than a model that ignores the predictor variable, you can compare the simple linear regression model to a baseline model ($Y= \\bar Y$ independent of $X$). For your comparison, you calculate the explained , unexplained and total variability in the simple linear regression model. The explained variability (SSM) is the difference between the regression line and the mean of the response variable: $\\sum(\\hat Y_i-\\bar Y)^2$ The unexplained variability (SSE) is the difference between the observed values and the regression line: $\\sum(Y_i-\\hat Y_i)^2$ The total variability is the difference between the observed values and the mean of the response variable: $\\sum(Y_i-\\bar Y)^2$ If we consider hypothesis testing for linear regression: $H_0$: the regression model does not fit the data better than the baseline model (slope $= 0$) $H_a$: the regression model does fit the data better than the baseline model (slope $= \\hat\\beta_1 \\ne 0$) These assumptions underlie the hypothesis test for the regression model and have to be met for a simple linear regression analysis to be valid (last three assumptions are the same as for ANOVA): The mean of the response variable is linearly related to the value of the predictor variable The error terms are normally distributed with a mean of 0 The error terms have equal variances The error terms are independent at each value of the predictor variable PROC REG Link 1 2 3 4 5 PROC REG DATA=SAS-data-set <options> ; MODEL dependent=regressor / CLM CLI </options> ; ID regressor; RUN; QUIT; To asses the level of precision around the mean estimates you can produce confidence intervals around the means. Confidence intervals become wider as you move away from the mean of the predictor variable. The wider the confidence interval the less precise it is. You might also want to construct prediction intervals for a single observation. A prediction interval is wider than a confidence interval because single observations have more variability than sample means . For producing predicted values with PROC REG : Create a data set containing the values of the independent variables for which you want to make predictions Concatenate the new data set with the original data set Fit a simple linear regression model to the new data set and specify the P option in the MODEL statement Because the concatenated observations contain missing values for the response variable, PROC REG does not include these observations when fitting the regression model. However, PROC REG does produce predicted values for these observations. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 DATA SAS - predictions - data - set ; INPUT dependent @@ ; DATALINES ; [ new values separated with blanks ] ; RUN ; DATA SAS - new - data - set ; SET SAS - predictions - data - set SAS - original - data - set ; RUN ; PROC REG DATA = SAS - new - data - set ; MODEL dependent = regressor / P ; ID regressor ; RUN ; QUIT ; When you use a model to predict future values of the response variable given certain values of the predictor variable, you must stay within (or near) the range of values for the predictor variable used to create the model . The relationship between the predictor variable and the response variable might be different beyond the range of the data. If you have a large data set and have already fitted the regression model, you can predict values more efficiently by using PROC REG and PROC SCORE : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 PROC REG DATA = SAS - original - data - set NOPRINT OUTEST = SAS - estimates - data - set ; MODEL dependent = regressor </ options > ; ID regressor ; RUN ; QUIT ; PROC SCORE DATA = SAS - predictions - data - set SCORE = SAS - estimates - data - set OUT = SAS - scored - data - set TYPE = PARMS < options > ; VAR variable ( s ); RUN ; QUIT ; Multiple Regression Link In multiple regression you can model the relationship between the response variable and more than one predictor variable . It is a powerful tool for both analytical or explanatory analysis and for prediction . $Y=\\beta_0+\\beta_1X_1+\\beta_2X_2+...+\\beta_kX_k+\\epsilon$ ($k+1$ parameters) Advantages Multiple linear regression is a more powerful tool You can determine whether a relationship exists between the response variable and more than one predictor variable at the same time Disadvantages You need to perform a selection process to decide which model to use The more predictors you have, the more complicated interpreting the model becomes If we consider hypothesis testing for linear regression: $H_0$: the regression model does not fit the data better than the baseline model $(\\beta_1=\\beta_2=...=\\beta_k= 0)$ $H_a$: the regression model does fit the data better than the baseline model (at least one $\\beta_i \\ne 0$) These assumptions have to be met for a multiple linear regression analysis to be valid (last three assumptions are the same as for ANOVA): A linear function of the $X$s accurately models the mean of the $Y$s The error terms are normally distributed with a mean of 0 The error terms have constant variances The error terms are independent at each value of the predictor variable The regular $R^2$ values never decrease when you add more terms to the model, but the adjusted $R^2$ value takes into account the number of terms in the model by including a penalty for the complexity of the model. The adjusted $R^2$ value increases only if new terms that you add significantly improve the model enough to warrant increasing the complexity of the model. It enables proper comparison between models with different parameter counts. When an adjusted $R^2$ increases by removing a variable from the models, it strongly implies that the removed variable was not necessary . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 PROC REG DATA=SAS-data-set <options> ; MODEL dependent=regressor1 regressor2 </options> ; RUN; QUIT; PROC GLM DATA=SAS-data-set PLOTS(ONLY)=(CONTOURFIT); MODEL dependent=regressor1 regressor2; STORE OUT=SAS-multiple-data-set; RUN; QUIT; PROC PLM RESTORE=SAS-multiple-data-set PLOTS=ALL; EFFECTPLOT CONTOUR (Y=regressor1 X=regressor2); EFFECTPLOT SLICEFIT (X=regressor2 SLICEBY=regressor1=250 to 1000 by 250); RUN; In PROC GLM , when you run a linear regression model with only two predictor variables, the output includes a contour fit plot by default. We specify CONTOURFIT to tell SAS to overlay the contour plot with a scatter plot of the observed data The plot shows predicted values of the response variable as gradations of the background color from blue, representing low values, to red, representing high values. The dots , which are similarly coloured, represent the actual data . Observations that are perfectly fit would show the same color within the circle as outside the circle. The lines on the graph help you read the actual predictions at even intervals. The CONTOUR option displays a contour plot of predicted values against two continuous covariates The SLICEFIT option displys a curve of predicted values vs a continuous variable grouped by the levels of another effect Clearly the PROC GLM contour fit plot is more useful . However, if you do not have access to the original data set and can run PROC PLM only on the item store, this plot still gives you an idea of the relationship between the predictor variables and predicted values. Model Building and Interpretation Link The brute force approach to find a good model is to start including all the predictor variables available and rerun the model removing the least significant remaining term each time until you're left with a model where only significant terms remain . With a small number of predictor variables a manual approach isn't too difficult but with a large number of predictor variables it's very tedious. Fortunately, if you specify the model selection technique to use, SAS finds good candidate models in an automatic way. All-possible regression methods Link SAS computes all possible models and ranks the results. Then, to evaluate the models, you compare statistics side by side ($R^2$, adjusted $R^2$ and $C_p$ statistic). Mallows' $C_p$ statistic helps you detect model bias if you are underfitting/overfitting the model, it is a simple indicator of effective variable selection within a model To select the best model for prediction (most accurate model for predicting future values of $Y$), you should use the Mallows' criterion : $C_p \\le p$, which is the number of parameters in the model including the intercept To select the best model for parameter estimation (analytical or explanatory analysis), you should use Hocking's criterion : $C_p\\le2p-p_{full}+1$ 1 2 3 4 PROC REG DATA=SASdata-set PLOTS(ONLY)=(CP) <options> ; <label:> MODEL dependent=regressors / SELECTION=CP RSQUARE ADJRSQ BEST=n </options> ; RUN; QUIT; BEST prints an specific number of the best candidate models according to a few different statistical criteria SELECTION option is used to specify the method used to select the model ( CP , RSQUARE and ADJRSQ to calculate with the all-possible regression model; the first statistic determines the sorting order) For this all-possible regression model, we add the label ALL_REG: With PLOTS=(CP) we produce a plot: Each star represents the best model for a given number of parameters. The solid blue line represents Mallows' criterion for $C_p$, so using this line helps us find a good candidate model for prediction. Because we want the smallest model possible , we start at the left side of the graph, with the fewest number of parameters moving to the right until we find the first model that falls below the solid blue line . To find models for parameter estimation we have to look for models that falls below the red solid line which represent the Hocking's criterion for $C_p$ parameter estimation. If we hover over the star, we can see which variables are included in this model. Stepwise selection methods Link Here you choose a selection method ( stepwise , forward or backward approaches) and SAS constructs a model based on that method. When you have a large number of potential predictor variables , the stepwise regression methods might be a better option. You can use either the REG procedure or the GLMSELECT procedure to perform stepwise selection methods Forward selection starts with no predictor variables in the model It selects the best one-variable model It selects the best two-variable model that includes the variable from the first model (after a variable is added to the model, it stays in even if it becomes insignificant later) It keeps adding variables, one at a time, until no significant terms are left to add Backward selection/elimination starts with all predictor variables in the model It removes variables one at a time, starting with the most non-significant variable (after a variable is removed from the model, it cannot reenter) It stops when only significant terms are left in the model Stepwise selection combines aspects of both forward and backward selection It starts with no predictor variables in the model and starts adding variables, one at a time, as in forward selection However, as in backward selection, stepwise selection can drop non-significant variables, one at a time It stops when everything in the model is currently significant and everything not in the model is not significant Statisticians in general agree on first using stepwise methods to identify several good candidates models and then applying your subject matter expertise to choose the best model. Because the techniques for selecting or eliminating variables differ between the three selection methods, they don't always produce the same final model . There is no one method that is best and you need to be cautious when reporting statistical quantities produced by these methods: Using automated model selections results in biases in parameter estimates , predictions and standard errors Incorrect calculation of degrees of freedom p-values that tend to err on the side of overestimating significance How can you avoid these issues ? You can hold out some of your data in order to perform an honest assessment of how well your model performs on a different sample of data ( holdout/validation data ) than you use to develop the model ( training data ) Other honest assessment approaches include cross-validation (if your data set is not large enough to split) or bootstraping (a resampling method that tries to approximate the distribution of the parameter estimates to estimate the standard error and p-values) Using Lasso for Predictor Selection Link The most widely researched and implemented modern method is the least absolute shrinkage and selection operator (Lasso) , which fits within the broader least angle regression framework (LARS) that can estimate Lasso with the computational complexity of ordinary least squares (OLS). Despite the overwhelming support for modern methods like Lasso in the statistical literature, more traditional methods such as p values or automatic selection methods such as forward, backward, or stepwise selection are still widely used even though short-comings of these methods have been disseminated for over a decade. When fitting any type of regression model, random noise can become entangled with signal, especially with small or moderate sample sizes. This can lead to estimates that overstate the impact of particular predictor variables and attribute more predictive power to the model than is present in the population. When this occurs, the model is said to be overfit with the consequence that regression coefficients have inflated magnitude, standard errors are underestimated, p values are consequently too small, $R^2$ values are consequently too large compared to their population values , and the model is not parsimonious because extraneous predictors may be seen as important. Tip A widely quoted rule is that you need 10 or 15 observations per independent variable in a regression model. To avoid over-fitting in a binary logistic regression model, you need to focus on the number of events per variable (EPV), not the total number of cases (i.e. events plus non-events). An event is defined as the outcome category (0 or 1) with the lower frequency. For example, if a sample of 200 patients are studied and 180 patients die during the study (so that 20 patients survive), only two pre-specified predictors can reliably be fitted to the total data. Similarly, if 120 patients die during the study (so that 80 patients survive), eight pre-specified predictors (based on the smallest of the two counts, being 80) can be fitted reliably. If more are fitted, overfitting is likely and the results will not predict well outside the training data. Sometimes this rule is too conservative and can be relaxed . Notice that having a sample size large enough to avoid over-fitting is not the same thing as having a sample size large enough to ensure adequate power. That's a separate issue. PROC GLMSELECT Link 1 2 3 4 5 PROC GLMSELECT DATA=SAS-data-set <options> ; CLASS variables; <label:> MODEL dependent(s) = regressor(s) / </options> ; RUN; QUIT; The SELECTION option specifies the method to be used to select the model ( FORWARD | BACKWARD | STEPWISE = default value) The SELECT option specifies the criterion to be used to determine which variable to add/remove from the model ( SL = significance level as the selection criterion) The SLENTRY option determines the significance level for a variable to enter the model (default = 0.5 for forward and 0.15 for stepwise) The SLSTAY option determines the significance level for a variable to stay in the model (default = 0.1 for backward and 0.15 for stepwise) You can display p-values in the Parameter Estimates table by including the SHOWPVALUES option int he MODEL statement The DETAILS option specifies the level of detail produced ( ALL | STEPS | SUMMARY ) Recommendations to decide which model is best for your needs: Run all model selection methods Look for commonalities across the results Narrow down your choice of models by using your subject matter knowledge Information Criterion and Other Selection Options Link There are other selection criteria that you can use to select variables for a model as well as evaluate competing models. These statistics are collectively referred to as information criteria . Each information criterion searched for a model that minimizes the unexplained variability with as few effects in the model as possible . The model with the smaller information criterion is considered to be better . For types are available in PROC GLMSELECT : Akaike's information criterion ( SELECT=AIC ) Correcterd Akaike's information criterion ( SELECT=AICC ) Sawa Bayesian information criterion ( SELECT=BIC ) Schwarz Bayesian information criterion ( SELECT=SBC , it could be called BIC in some other SAS procedures) The calculations of all information criteria begin the same way: First you calculate $n\\cdot log(SSE/n)$ Then, each criterion adds a penalty that represents the complexity of the model (each type of information criterion invokes a different penalty component) AIC : $2p+n+2$ AICC : $n(n+p)/(n-p-2)$ BIC : $2(p+2)1-2q^2$ SBC : $p\\cdot log(n)$","title":"Regression"},{"location":"statistics/regression/#exploratory-data-analysis","text":"A useful set of techniques for investigating your data is known as exploratory data analysis .","title":"Exploratory Data Analysis"},{"location":"statistics/regression/#proc-sgcatter-scatter-plots","text":"1 2 3 PROC SGSCATTER DATA = SAS - data - base ; PLOT variableY * ( variableX1 variableX2 ) / REG ; RUN ; If you have so many observations that the scatter plot of the whole data set is difficult to interpret, you might run PROC SGSCATTER on a random sample of observations","title":"PROC SGCATTER: Scatter Plots"},{"location":"statistics/regression/#proc-corr-correlation-analysis","text":"There are two ways of calculating correlations: the Pearson correlation coefficient (specially used for normal distributed data) and the Spearman's rank correlation coefficient (which is best fitted when your data presents outliers). The Spearman correlation between two variables is equal to the Pearson correlation between the rank values of those two variables; while Pearson's correlation assesses linear relationships, Spearman's correlation assesses monotonic relationships (whether linear or not). Spearman's coefficient is appropriate for both continuous and discrete ordinal variables. The closer the Pearson correlation coefficient is to +1/-1, the stronger the positive/negative linear relationship is between the two variables. The closer the correlation coefficient is to 0, the weaker the linear relationship and if it is 0 variables are uncorrelated. When you interpret the correlation, be cautious about the effect of large sample sizes : even a correlation of 0.01 can be statistically significant with a large enough sample size and you would almost always reject the hypothesis $H_0$: $\\rho =0$, even if the value of your correlation is small for all practical purposes Some common errors on interpreting correlations are concluding a cause-and-effect relationship between the variables misinterpreting the kind of relationship between the variables and failing to recognize the influence of outliers on the correlation The variables might be related but not causally Correlation coefficients can be large because both variables are affected by other variables Variables might be strongly correlated by chance Just because the correlation coefficient is close to 0 doesn't mean that no relationship exists between the two variables: they might have a non-linear relationship Another common error is failing to recognize the influence of outliers on the correlation If you have an outlier you should report both correlation coefficients (with and without the outlier) to report how influential the unusual data point is in your analysis The PROC CORR also produces scatter plots or a scatter plot matrix . 1 2 3 4 5 PROC CORR DATA = SAS - data - set RANK | NOSIMPLE PLOTS ( ONLY ) = MATRIX ( NVAR = ALL HISTOGRAM ) | SCATTER ( NVAR = ALL ELLIPSE = NONE ) < options > ; VAR variable ( s ) X ; WITH variable ( s ) Y ; ID variable4label ; RUN ;","title":"PROC CORR: Correlation Analysis"},{"location":"statistics/regression/#simple-linear-regression","text":"You use correlation analysis to determine the strength of the linear relationship between continuous response variables. Now you need to go a step further and define the linear relationship itself : $Y= \\beta_0+\\beta_1 \\cdot X+\\epsilon$ $Y$ is the response variable $X$ is the predictor variable $\\beta_0$ is the intercept parameter $\\beta_1$ is the slope parameter $\\epsilon$ is the error term The method of least squares produces parameter estimates $\\hat \\beta_0$ and $\\hat \\beta_1$ with certain optimum properties which make them the Best Linear Unbiased Estimators ( BLUE ): They are unbiased estimates of the population parameters They have minimum variance To find out how much better is the model that takes the predictor variable into account than a model that ignores the predictor variable, you can compare the simple linear regression model to a baseline model ($Y= \\bar Y$ independent of $X$). For your comparison, you calculate the explained , unexplained and total variability in the simple linear regression model. The explained variability (SSM) is the difference between the regression line and the mean of the response variable: $\\sum(\\hat Y_i-\\bar Y)^2$ The unexplained variability (SSE) is the difference between the observed values and the regression line: $\\sum(Y_i-\\hat Y_i)^2$ The total variability is the difference between the observed values and the mean of the response variable: $\\sum(Y_i-\\bar Y)^2$ If we consider hypothesis testing for linear regression: $H_0$: the regression model does not fit the data better than the baseline model (slope $= 0$) $H_a$: the regression model does fit the data better than the baseline model (slope $= \\hat\\beta_1 \\ne 0$) These assumptions underlie the hypothesis test for the regression model and have to be met for a simple linear regression analysis to be valid (last three assumptions are the same as for ANOVA): The mean of the response variable is linearly related to the value of the predictor variable The error terms are normally distributed with a mean of 0 The error terms have equal variances The error terms are independent at each value of the predictor variable","title":"Simple Linear Regression"},{"location":"statistics/regression/#proc-reg","text":"1 2 3 4 5 PROC REG DATA=SAS-data-set <options> ; MODEL dependent=regressor / CLM CLI </options> ; ID regressor; RUN; QUIT; To asses the level of precision around the mean estimates you can produce confidence intervals around the means. Confidence intervals become wider as you move away from the mean of the predictor variable. The wider the confidence interval the less precise it is. You might also want to construct prediction intervals for a single observation. A prediction interval is wider than a confidence interval because single observations have more variability than sample means . For producing predicted values with PROC REG : Create a data set containing the values of the independent variables for which you want to make predictions Concatenate the new data set with the original data set Fit a simple linear regression model to the new data set and specify the P option in the MODEL statement Because the concatenated observations contain missing values for the response variable, PROC REG does not include these observations when fitting the regression model. However, PROC REG does produce predicted values for these observations. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 DATA SAS - predictions - data - set ; INPUT dependent @@ ; DATALINES ; [ new values separated with blanks ] ; RUN ; DATA SAS - new - data - set ; SET SAS - predictions - data - set SAS - original - data - set ; RUN ; PROC REG DATA = SAS - new - data - set ; MODEL dependent = regressor / P ; ID regressor ; RUN ; QUIT ; When you use a model to predict future values of the response variable given certain values of the predictor variable, you must stay within (or near) the range of values for the predictor variable used to create the model . The relationship between the predictor variable and the response variable might be different beyond the range of the data. If you have a large data set and have already fitted the regression model, you can predict values more efficiently by using PROC REG and PROC SCORE : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 PROC REG DATA = SAS - original - data - set NOPRINT OUTEST = SAS - estimates - data - set ; MODEL dependent = regressor </ options > ; ID regressor ; RUN ; QUIT ; PROC SCORE DATA = SAS - predictions - data - set SCORE = SAS - estimates - data - set OUT = SAS - scored - data - set TYPE = PARMS < options > ; VAR variable ( s ); RUN ; QUIT ;","title":"PROC REG"},{"location":"statistics/regression/#multiple-regression","text":"In multiple regression you can model the relationship between the response variable and more than one predictor variable . It is a powerful tool for both analytical or explanatory analysis and for prediction . $Y=\\beta_0+\\beta_1X_1+\\beta_2X_2+...+\\beta_kX_k+\\epsilon$ ($k+1$ parameters) Advantages Multiple linear regression is a more powerful tool You can determine whether a relationship exists between the response variable and more than one predictor variable at the same time Disadvantages You need to perform a selection process to decide which model to use The more predictors you have, the more complicated interpreting the model becomes If we consider hypothesis testing for linear regression: $H_0$: the regression model does not fit the data better than the baseline model $(\\beta_1=\\beta_2=...=\\beta_k= 0)$ $H_a$: the regression model does fit the data better than the baseline model (at least one $\\beta_i \\ne 0$) These assumptions have to be met for a multiple linear regression analysis to be valid (last three assumptions are the same as for ANOVA): A linear function of the $X$s accurately models the mean of the $Y$s The error terms are normally distributed with a mean of 0 The error terms have constant variances The error terms are independent at each value of the predictor variable The regular $R^2$ values never decrease when you add more terms to the model, but the adjusted $R^2$ value takes into account the number of terms in the model by including a penalty for the complexity of the model. The adjusted $R^2$ value increases only if new terms that you add significantly improve the model enough to warrant increasing the complexity of the model. It enables proper comparison between models with different parameter counts. When an adjusted $R^2$ increases by removing a variable from the models, it strongly implies that the removed variable was not necessary . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 PROC REG DATA=SAS-data-set <options> ; MODEL dependent=regressor1 regressor2 </options> ; RUN; QUIT; PROC GLM DATA=SAS-data-set PLOTS(ONLY)=(CONTOURFIT); MODEL dependent=regressor1 regressor2; STORE OUT=SAS-multiple-data-set; RUN; QUIT; PROC PLM RESTORE=SAS-multiple-data-set PLOTS=ALL; EFFECTPLOT CONTOUR (Y=regressor1 X=regressor2); EFFECTPLOT SLICEFIT (X=regressor2 SLICEBY=regressor1=250 to 1000 by 250); RUN; In PROC GLM , when you run a linear regression model with only two predictor variables, the output includes a contour fit plot by default. We specify CONTOURFIT to tell SAS to overlay the contour plot with a scatter plot of the observed data The plot shows predicted values of the response variable as gradations of the background color from blue, representing low values, to red, representing high values. The dots , which are similarly coloured, represent the actual data . Observations that are perfectly fit would show the same color within the circle as outside the circle. The lines on the graph help you read the actual predictions at even intervals. The CONTOUR option displays a contour plot of predicted values against two continuous covariates The SLICEFIT option displys a curve of predicted values vs a continuous variable grouped by the levels of another effect Clearly the PROC GLM contour fit plot is more useful . However, if you do not have access to the original data set and can run PROC PLM only on the item store, this plot still gives you an idea of the relationship between the predictor variables and predicted values.","title":"Multiple Regression"},{"location":"statistics/regression/#model-building-and-interpretation","text":"The brute force approach to find a good model is to start including all the predictor variables available and rerun the model removing the least significant remaining term each time until you're left with a model where only significant terms remain . With a small number of predictor variables a manual approach isn't too difficult but with a large number of predictor variables it's very tedious. Fortunately, if you specify the model selection technique to use, SAS finds good candidate models in an automatic way.","title":"Model Building and Interpretation"},{"location":"statistics/regression/#all-possible-regression-methods","text":"SAS computes all possible models and ranks the results. Then, to evaluate the models, you compare statistics side by side ($R^2$, adjusted $R^2$ and $C_p$ statistic). Mallows' $C_p$ statistic helps you detect model bias if you are underfitting/overfitting the model, it is a simple indicator of effective variable selection within a model To select the best model for prediction (most accurate model for predicting future values of $Y$), you should use the Mallows' criterion : $C_p \\le p$, which is the number of parameters in the model including the intercept To select the best model for parameter estimation (analytical or explanatory analysis), you should use Hocking's criterion : $C_p\\le2p-p_{full}+1$ 1 2 3 4 PROC REG DATA=SASdata-set PLOTS(ONLY)=(CP) <options> ; <label:> MODEL dependent=regressors / SELECTION=CP RSQUARE ADJRSQ BEST=n </options> ; RUN; QUIT; BEST prints an specific number of the best candidate models according to a few different statistical criteria SELECTION option is used to specify the method used to select the model ( CP , RSQUARE and ADJRSQ to calculate with the all-possible regression model; the first statistic determines the sorting order) For this all-possible regression model, we add the label ALL_REG: With PLOTS=(CP) we produce a plot: Each star represents the best model for a given number of parameters. The solid blue line represents Mallows' criterion for $C_p$, so using this line helps us find a good candidate model for prediction. Because we want the smallest model possible , we start at the left side of the graph, with the fewest number of parameters moving to the right until we find the first model that falls below the solid blue line . To find models for parameter estimation we have to look for models that falls below the red solid line which represent the Hocking's criterion for $C_p$ parameter estimation. If we hover over the star, we can see which variables are included in this model.","title":"All-possible regression methods"},{"location":"statistics/regression/#stepwise-selection-methods","text":"Here you choose a selection method ( stepwise , forward or backward approaches) and SAS constructs a model based on that method. When you have a large number of potential predictor variables , the stepwise regression methods might be a better option. You can use either the REG procedure or the GLMSELECT procedure to perform stepwise selection methods Forward selection starts with no predictor variables in the model It selects the best one-variable model It selects the best two-variable model that includes the variable from the first model (after a variable is added to the model, it stays in even if it becomes insignificant later) It keeps adding variables, one at a time, until no significant terms are left to add Backward selection/elimination starts with all predictor variables in the model It removes variables one at a time, starting with the most non-significant variable (after a variable is removed from the model, it cannot reenter) It stops when only significant terms are left in the model Stepwise selection combines aspects of both forward and backward selection It starts with no predictor variables in the model and starts adding variables, one at a time, as in forward selection However, as in backward selection, stepwise selection can drop non-significant variables, one at a time It stops when everything in the model is currently significant and everything not in the model is not significant Statisticians in general agree on first using stepwise methods to identify several good candidates models and then applying your subject matter expertise to choose the best model. Because the techniques for selecting or eliminating variables differ between the three selection methods, they don't always produce the same final model . There is no one method that is best and you need to be cautious when reporting statistical quantities produced by these methods: Using automated model selections results in biases in parameter estimates , predictions and standard errors Incorrect calculation of degrees of freedom p-values that tend to err on the side of overestimating significance How can you avoid these issues ? You can hold out some of your data in order to perform an honest assessment of how well your model performs on a different sample of data ( holdout/validation data ) than you use to develop the model ( training data ) Other honest assessment approaches include cross-validation (if your data set is not large enough to split) or bootstraping (a resampling method that tries to approximate the distribution of the parameter estimates to estimate the standard error and p-values)","title":"Stepwise selection methods"},{"location":"statistics/regression/#using-lasso-for-predictor-selection","text":"The most widely researched and implemented modern method is the least absolute shrinkage and selection operator (Lasso) , which fits within the broader least angle regression framework (LARS) that can estimate Lasso with the computational complexity of ordinary least squares (OLS). Despite the overwhelming support for modern methods like Lasso in the statistical literature, more traditional methods such as p values or automatic selection methods such as forward, backward, or stepwise selection are still widely used even though short-comings of these methods have been disseminated for over a decade. When fitting any type of regression model, random noise can become entangled with signal, especially with small or moderate sample sizes. This can lead to estimates that overstate the impact of particular predictor variables and attribute more predictive power to the model than is present in the population. When this occurs, the model is said to be overfit with the consequence that regression coefficients have inflated magnitude, standard errors are underestimated, p values are consequently too small, $R^2$ values are consequently too large compared to their population values , and the model is not parsimonious because extraneous predictors may be seen as important. Tip A widely quoted rule is that you need 10 or 15 observations per independent variable in a regression model. To avoid over-fitting in a binary logistic regression model, you need to focus on the number of events per variable (EPV), not the total number of cases (i.e. events plus non-events). An event is defined as the outcome category (0 or 1) with the lower frequency. For example, if a sample of 200 patients are studied and 180 patients die during the study (so that 20 patients survive), only two pre-specified predictors can reliably be fitted to the total data. Similarly, if 120 patients die during the study (so that 80 patients survive), eight pre-specified predictors (based on the smallest of the two counts, being 80) can be fitted reliably. If more are fitted, overfitting is likely and the results will not predict well outside the training data. Sometimes this rule is too conservative and can be relaxed . Notice that having a sample size large enough to avoid over-fitting is not the same thing as having a sample size large enough to ensure adequate power. That's a separate issue.","title":"Using Lasso for Predictor Selection"},{"location":"statistics/regression/#proc-glmselect","text":"1 2 3 4 5 PROC GLMSELECT DATA=SAS-data-set <options> ; CLASS variables; <label:> MODEL dependent(s) = regressor(s) / </options> ; RUN; QUIT; The SELECTION option specifies the method to be used to select the model ( FORWARD | BACKWARD | STEPWISE = default value) The SELECT option specifies the criterion to be used to determine which variable to add/remove from the model ( SL = significance level as the selection criterion) The SLENTRY option determines the significance level for a variable to enter the model (default = 0.5 for forward and 0.15 for stepwise) The SLSTAY option determines the significance level for a variable to stay in the model (default = 0.1 for backward and 0.15 for stepwise) You can display p-values in the Parameter Estimates table by including the SHOWPVALUES option int he MODEL statement The DETAILS option specifies the level of detail produced ( ALL | STEPS | SUMMARY ) Recommendations to decide which model is best for your needs: Run all model selection methods Look for commonalities across the results Narrow down your choice of models by using your subject matter knowledge","title":"PROC GLMSELECT"},{"location":"statistics/regression/#information-criterion-and-other-selection-options","text":"There are other selection criteria that you can use to select variables for a model as well as evaluate competing models. These statistics are collectively referred to as information criteria . Each information criterion searched for a model that minimizes the unexplained variability with as few effects in the model as possible . The model with the smaller information criterion is considered to be better . For types are available in PROC GLMSELECT : Akaike's information criterion ( SELECT=AIC ) Correcterd Akaike's information criterion ( SELECT=AICC ) Sawa Bayesian information criterion ( SELECT=BIC ) Schwarz Bayesian information criterion ( SELECT=SBC , it could be called BIC in some other SAS procedures) The calculations of all information criteria begin the same way: First you calculate $n\\cdot log(SSE/n)$ Then, each criterion adds a penalty that represents the complexity of the model (each type of information criterion invokes a different penalty component) AIC : $2p+n+2$ AICC : $n(n+p)/(n-p-2)$ BIC : $2(p+2)1-2q^2$ SBC : $p\\cdot log(n)$","title":"Information Criterion and Other Selection Options"},{"location":"word/fieldcodes/","text":"Numbering with Sequence Fields Link SEQ Identifier [Bookmark] [Switches] 1 SEQ Listing_ \\ r 19 \\ * ARABIC \\ s 2 SEQ : defines a sequence of numbers/characters Listing_ : sequence identifier/name \\r 19 (reset switch): resets the listing to 19 \\* : ARABIC : use arabic numbers for this list \\s 2 :","title":"Fieldcodes"},{"location":"word/fieldcodes/#numbering-with-sequence-fields","text":"SEQ Identifier [Bookmark] [Switches] 1 SEQ Listing_ \\ r 19 \\ * ARABIC \\ s 2 SEQ : defines a sequence of numbers/characters Listing_ : sequence identifier/name \\r 19 (reset switch): resets the listing to 19 \\* : ARABIC : use arabic numbers for this list \\s 2 :","title":"Numbering with Sequence Fields"}]}