{
    "docs": [
        {
            "location": "/", 
            "text": "Hope this site help you with your SAS programming.\nContact me if you want any content to be added to this brief (hopefully not so brief in the future) manual. \nI will be glad to include it.\n\n\nEnjoy! :)\n\n\n\n\n\n\nIf this is your starting point with SAS programming, maybe these readings could be useful\n\n\n\n\nGetting Started with SAS Programming\n\n\nWorking with SAS Programs\n\n\nSAS proceedings papers repository\n\n\nSAS seminars by the UCLA Statistical Consulting Group\n\n\nInteresting configuration tips and tricks\n\n\n\n\n\n\n\n\nPrint out the available SAS packages according to your license and the expiration dates: \n\n\n\n\n1\n2\nPROC SETINIT;\n \n\nRUN;\n\n\n\n\n\n\n\n\n\nComments:\n\n\n\n\n1\n2\n/* comment */\n\n\n* comment statement;\n\n\n\n\n\n\n\n\n\n\n\nWhen you name a process flow \nAutoexec\n, SAS Enterprise Guide prompts you to run the process flow when you open the project. This makes it easy to recreate your data when you return to the project.\n\n\n\n\n\n\nHow to \ncompare SAS programs\n in SAS Enterprise Guide\n\n\n\n\n\n\nSend an \nemail\n with some coding\n\n\n\n\n\n\nShortcuts\n\n\n\n\n\n\n\n\nShortcut\n\n\nFunction\n\n\n\n\n\n\n\n\n\n\nF3\n\n\nRun selection or run all if there's nothing selected\n\n\n\n\n\n\nCtrl + I\n\n\nBeautify code (proper indentation)\n\n\n\n\n\n\nCtrl + Shift + U\n\n\nConvert to uppercase\n\n\n\n\n\n\nCtrl + Shift + L\n\n\nConvert to lowercase\n\n\n\n\n\n\nCtrl + /\n\n\nWrap selection (or current line) in a comment\n\n\n\n\n\n\nCtrl + Shift + /\n\n\nUnwrap selection (or current line) from a comment\n\n\n\n\n\n\nCtrl + G\n\n\nGo to line (prompts for a line number)\n\n\n\n\n\n\nCtrl + [, Ctrl + ]\n\n\nMove caret to matching parenthesis/brace\n\n\n\n\n\n\nAlt + [, Alt + ]\n\n\nMove caret to matching DO/END keyword\n\n\n\n\n\n\n\n\n\n\nCheck these websites\n\n\n\n\n5 keyboard shortcuts in SAS that will change your life", 
            "title": "Gettin' Started!"
        }, 
        {
            "location": "/#shortcuts", 
            "text": "Shortcut  Function      F3  Run selection or run all if there's nothing selected    Ctrl + I  Beautify code (proper indentation)    Ctrl + Shift + U  Convert to uppercase    Ctrl + Shift + L  Convert to lowercase    Ctrl + /  Wrap selection (or current line) in a comment    Ctrl + Shift + /  Unwrap selection (or current line) from a comment    Ctrl + G  Go to line (prompts for a line number)    Ctrl + [, Ctrl + ]  Move caret to matching parenthesis/brace    Alt + [, Alt + ]  Move caret to matching DO/END keyword      Check these websites   5 keyboard shortcuts in SAS that will change your life", 
            "title": "Shortcuts"
        }, 
        {
            "location": "/essentials/accessing/", 
            "text": "Chapter summary in SAS\n\n\nAccessing SAS libraries\n\n\nlibref\n is a \nlibrary reference name\n (a shortcut to the physical location). There are three rules for valid librefs:\n\n\n\n\nA length of one to eight characters\n\n\nBegin with a letter or underscore\n\n\nThe remaining characters are letters, numbers, or underscores\n\n\n\n\nValid \nvariable names\n begin with a letter or underscore, and continue with letters, numbers, or underscores. The \nVALIDVARNAME\n system option specifies the rules for valid SAS variable names that can be created and processed during a SAS session: \n\n\n1\nOPTIONS VALIDVARNAME=V7 (default) | UPCASE | ANY;\n\n\n\n\n\n\n\n\nlibref.data-set-name\n: data set reference two-level name\n\n\ndata-set-name\n: when the data set belongs to a temporary library, you can optionally use a one-level name (SAS assumes that it is contained in the \nwork\n library, which is the default)\n\n\nThe \nLIBNAME\n statement associates the \nlibref\n with the physical location of the library/data for the current SAS session\n\n\n\n\n1\n2\n3\n4\n5\n6\nLIBNAME\n \nlibref\n-\nname\n \nSAS\n-\nlibrary\n-\nfolder\n-\npath\n \noptions\n;\n\n\n\n/* Example */\n\n\n\n%let\n \npath\n=/\nfolders\n/\nmyfolders\n/\necprg193\n;\n \n\nlibname\n \norion\n \npath\n;\n\n\n\n\n\n\n\nErase the association between SAS and a custom library\n\n\n1\nLIBNAME libref-name CLEAR;\n\n\n\n\n\n\nRemove data sets and libraries\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n*\n \nDelete\n \ndata\n \nsets\n \n;\n\n\n*\n------------------\n;\n\n\nPROC\n \nDELETE\n \nDATA\n=\ndata1\n \ndata2\n \ndata3\n;\n\n\nRUN\n;\n\n\n\n*\n \nDelete\n \nfull\n \nlibrary\n \n;\n\n\n*\n---------------------\n;\n\n\nPROC\n \nDATASETS\n \nLIB\n=\nlibrary-name\n \nMEMTYPE\n=\nDATA\n \nKILL\n \nNOLIST\n;\n\n\nRUN\n;\n\n\nQUIT\n;\n\n\n\n*\n \nDelete\n \nspecific\n \ndata\n \nsets\n \n;\n\n\n*\n---------------------------\n;\n\n\nPROC\n \nDATASETS\n \nLIB\n=\nlibrary-name\n \nNOWARN\n \nNOLIST\n \nNODETAILS\n;\n\n    \nDELETE\n \nprefix-data-set\n:\n \nother-data-set\n;\n\n\nRUN\n;\n\n\nQUIT\n;\n\n\n\n\n\n\n\nCopy data sets and libraries\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n* Copy full library ;\n*-------------------;\nPROC COPY IN=old-library OUT=new-library;\nRUN;\n\n* Copy specific data sets ;\n*-------------------------;\nPROC COPY IN=old-library OUT=new-library MEMTYPE=DATA;\n   SELECT data-set1 data-set2;\nRUN;\n\n\n\n\n\n\nRename data sets\n\n\n1\n2\n3\nPROC DATASETS LIBRARY=library-name;\n   CHANGE data-set-name1=data-set-new-name1 data-set-name2=data-set-new-name2;\nRUN;\n\n\n\n\n\n\nCheck the contents of a library\n\n\n1\n2\nPROC CONTENTS DATA=libref._ALL_;\nRUN;\n\n\n\n\n\n\nTo hide the descriptors of all data sets in the library (it could generate a very long report) you can add the option \nnods\n (only compatible with the keybord \n_all_\n):\n\n\n1\n2\nPROC CONTENTS DATA=libref._ALL_ NODS;\nRUN;\n\n\n\n\n\n\nPrint the full data set\n\n\n1\n2\nPROC PRINT DATA=SAS-data-set;\nRUN;\n\n\n\n\n\n\nExamining SAS Data Sets\n\n\nParts of a library (SAS notation):\n\n\n\n\nTable = \ndata set\n\n\nColumn = \nvariable\n\n\nRow = \nobservation\n\n\n\n\nPROC CONTENTS\n\n\nThe \ndescriptor portion\n contains information about the attributes of the data set (metadata), including the variable names. It is show in three tables:\n\n\n\n\nTable 1:\n general information about the data set (name, creation date/time, etc.)\n\n\nTable 2:\n operating environment information, file location, etc.\n\n\nTable 3:\n alphabetic list of variables in the data set and their attributes\n\n\n\n\nPROC PRINT\n\n\nThe \ndata portion\n contains the data values, stored in variables (numeric/character)\n\n\n\n\nNumeric values:\n right-aligned digits 0-9, minus sign, single decimal point, scientific notation (E)\n\n\nCharacter values:\n left-aligned; letters, numbers, special characters and blanks\n\n\nMissing values:\n \nblank\n for character variables and \nperiod\n for numeric ones. To change this default behaviour use  \nMISSING='new-character'\n \n\n\nValues length:\n for character variables 1 byte = 1 character, numeric variables have 8 bytes of storage by default (16-17 significant digits)\n\n\nOther attributes:\n \nformat\n, \ninformat\n, \nlabel", 
            "title": "Accessing Data"
        }, 
        {
            "location": "/essentials/accessing/#accessing-sas-libraries", 
            "text": "libref  is a  library reference name  (a shortcut to the physical location). There are three rules for valid librefs:   A length of one to eight characters  Begin with a letter or underscore  The remaining characters are letters, numbers, or underscores   Valid  variable names  begin with a letter or underscore, and continue with letters, numbers, or underscores. The  VALIDVARNAME  system option specifies the rules for valid SAS variable names that can be created and processed during a SAS session:   1 OPTIONS VALIDVARNAME=V7 (default) | UPCASE | ANY;    libref.data-set-name : data set reference two-level name  data-set-name : when the data set belongs to a temporary library, you can optionally use a one-level name (SAS assumes that it is contained in the  work  library, which is the default)  The  LIBNAME  statement associates the  libref  with the physical location of the library/data for the current SAS session   1\n2\n3\n4\n5\n6 LIBNAME   libref - name   SAS - library - folder - path   options ;  /* Example */  %let   path =/ folders / myfolders / ecprg193 ;   libname   orion   path ;", 
            "title": "Accessing SAS libraries"
        }, 
        {
            "location": "/essentials/accessing/#erase-the-association-between-sas-and-a-custom-library", 
            "text": "1 LIBNAME libref-name CLEAR;", 
            "title": "Erase the association between SAS and a custom library"
        }, 
        {
            "location": "/essentials/accessing/#remove-data-sets-and-libraries", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17 *   Delete   data   sets   ;  * ------------------ ;  PROC   DELETE   DATA = data1   data2   data3 ;  RUN ;  *   Delete   full   library   ;  * --------------------- ;  PROC   DATASETS   LIB = library-name   MEMTYPE = DATA   KILL   NOLIST ;  RUN ;  QUIT ;  *   Delete   specific   data   sets   ;  * --------------------------- ;  PROC   DATASETS   LIB = library-name   NOWARN   NOLIST   NODETAILS ; \n     DELETE   prefix-data-set :   other-data-set ;  RUN ;  QUIT ;", 
            "title": "Remove data sets and libraries"
        }, 
        {
            "location": "/essentials/accessing/#copy-data-sets-and-libraries", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 * Copy full library ;\n*-------------------;\nPROC COPY IN=old-library OUT=new-library;\nRUN;\n\n* Copy specific data sets ;\n*-------------------------;\nPROC COPY IN=old-library OUT=new-library MEMTYPE=DATA;\n   SELECT data-set1 data-set2;\nRUN;", 
            "title": "Copy data sets and libraries"
        }, 
        {
            "location": "/essentials/accessing/#rename-data-sets", 
            "text": "1\n2\n3 PROC DATASETS LIBRARY=library-name;\n   CHANGE data-set-name1=data-set-new-name1 data-set-name2=data-set-new-name2;\nRUN;", 
            "title": "Rename data sets"
        }, 
        {
            "location": "/essentials/accessing/#check-the-contents-of-a-library", 
            "text": "1\n2 PROC CONTENTS DATA=libref._ALL_;\nRUN;   To hide the descriptors of all data sets in the library (it could generate a very long report) you can add the option  nods  (only compatible with the keybord  _all_ ):  1\n2 PROC CONTENTS DATA=libref._ALL_ NODS;\nRUN;", 
            "title": "Check the contents of a library"
        }, 
        {
            "location": "/essentials/accessing/#print-the-full-data-set", 
            "text": "1\n2 PROC PRINT DATA=SAS-data-set;\nRUN;", 
            "title": "Print the full data set"
        }, 
        {
            "location": "/essentials/accessing/#examining-sas-data-sets", 
            "text": "Parts of a library (SAS notation):   Table =  data set  Column =  variable  Row =  observation", 
            "title": "Examining SAS Data Sets"
        }, 
        {
            "location": "/essentials/accessing/#proc-contents", 
            "text": "The  descriptor portion  contains information about the attributes of the data set (metadata), including the variable names. It is show in three tables:   Table 1:  general information about the data set (name, creation date/time, etc.)  Table 2:  operating environment information, file location, etc.  Table 3:  alphabetic list of variables in the data set and their attributes", 
            "title": "PROC CONTENTS"
        }, 
        {
            "location": "/essentials/accessing/#proc-print", 
            "text": "The  data portion  contains the data values, stored in variables (numeric/character)   Numeric values:  right-aligned digits 0-9, minus sign, single decimal point, scientific notation (E)  Character values:  left-aligned; letters, numbers, special characters and blanks  Missing values:   blank  for character variables and  period  for numeric ones. To change this default behaviour use   MISSING='new-character'    Values length:  for character variables 1 byte = 1 character, numeric variables have 8 bytes of storage by default (16-17 significant digits)  Other attributes:   format ,  informat ,  label", 
            "title": "PROC PRINT"
        }, 
        {
            "location": "/essentials/reporting/", 
            "text": "Chapter summary in SAS\n\n\nSubsetting Report Data\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nPROC PRINT DATA=SAS-data-set(OBS=3) NOOBS;  /* OBS=3 prints only 3 elements | NOOBS hides the \nObs\n */\n    VAR variable1 variable2 variable3;      /* prints out only this variables in the report */\n    SUM variable1 variable2;                /* adds an extra line at the end with the total */\n    WHERE variable3\n1000; variable3\n1000;   /* operators: \n \n \n= \n= = ^= in + - / * ** \n | ~ ^ ? */\n    WHERE variable4 in (\nChild\n,\nElder\n);   /* only the last WHERE condition is applied */\n    WHERE variable1=20 AND variable4 CONTAINS \ncase-sensitive-substring\n;  /* CONTAINS = ? */\n    IDWHERE ANYALPHA(variable) NE 0         /* only values containing at least a letter */\n    ID variable1                            /* replaces the \nObs\n column by a selected variable values */\n    BY variable3variable3                   /* separate in different tables for different variable values (sort first) */\nRUN;\n\n\n\n\n\n\nSpecial \nWHERE\n operators:\n\n\n\n\nBETWEEN x AND y\n: an inclusive range\n\n\nWHERE SAME AND\n: augment a previous where expression (both applied)\n\n\nIS NULL\n: a missing value\n\n\nIS MISSING\n: a missing value\n\n\nLIKE\n: matches a pattern (% = any number of characters, _ = one character). E.g.: \n'T_m%'\n\n\nThe \nSOUNDS-LIKE (=\\*)\n operator selects observations that contain a spelling variation of a specified word or words. This operator uses the \nSoundex\n algorithm to compare the variable value and the operand.\n\n\nANYVALUE\n is an interesting function that searches a character string for an alphabetic character, and returns the first position at which the character is found\n\n\n\n\n\n\nNote\n\n\nTo compare with a SAS date value you need to express is as a SAS date constant: \n'DDMM\n\\YY\nYY'D\n.\n\n\n\n\nSorting and Grouping Report Data\n\n\n1\n2\n3\n4\n5\nPROC SORT DATA=SAS-data-set\n    OUT=new-SAS-data-set NODUPKEY;                                           /* optional */\n    DUPOUT=work.duplicates;                                                  /* optional */\n    BY ASCENDING variable1-to-be-sorted DESCENDING variable2-to-be-sorted;   /* optional (ASCENDING is the default order)*/\nRUN;\n\n\n\n\n\n\n\n\nThe \nNODUPKEY\n option deletes observations with duplicate \nBY\n values\n\n\nDUPOUT\n writes duplicate observations to a separate output data set\n\n\n\n\nEnhancing Reports\n\n\nTitles and footnotes\n\n\n1\n2\n3\n4\n5\n6\nTITLEline \ntext\n;       \nFOOTNOTEline \ntext\n;\n\nTITLE1 \ntext1\n;\nTITLE1 \ntext1_change\n;     /* Change title text and also cancels all footnotes with higher numbers */\nTITLE;                     /* Cancel (erase) all titles */\n\n\n\n\n\n\n\n\nThe \nlines\n specifies the line (1-10) on which the title/footnote will appear (line = 1 is the default value)\n\n\nThe title/footnote will remain until you \nchange\n it, \ncancel\n it or you \nend your SAS session\n\n\n\n\nTitles and footnotes inside AND outside a graph\n\n\nCOMPLETE\n\n\nLabels\n\n\nAssigning \ntemporary labels\n to display in the report instead of the variable names:\n\n\n1\n2\n3\n4\n5\nPROC PRINT DATA=SAS-data-set LABEL;            \n    LABEL variable1 = \nnew variable1 name\n \n          variable2 = \nnew variable2 name\n;\n    LABEL variable3 = \nnew variable3 name\n;\nRUN;\n\n\n\n\n\n\n\n\nYou need to add the \nLABEL\n option at the \nPROC PRINT\n definition to display the labels \n\n\nThe \nLABEL\n lengths can go up to 256 characters long\n\n\nYou can specify several labels in one \nLABEL\n statement or use a separate \nLABEL\n statement for each variable\n\n\n\n\nThe \nSPLIT\n option\n\n\n1\n2\n3\nPROC PRINT DATA=SAS-data-set SPLIT=\n*\n;             \n    LABEL variable1 = \nvariable label line 1*variable label line 1\n;   \nRUN;\n\n\n\n\n\n\n\n\nWhen you use \nSPLIT\n you no longer need to add the \nLABEL\n option to get the labels printed out\n\n\nThe \nSPLIT\n option introduces a line break at the label text whenever it finds the specified character (\n*\n)", 
            "title": "Producing Detailed Reports"
        }, 
        {
            "location": "/essentials/reporting/#subsetting-report-data", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 PROC PRINT DATA=SAS-data-set(OBS=3) NOOBS;  /* OBS=3 prints only 3 elements | NOOBS hides the  Obs  */\n    VAR variable1 variable2 variable3;      /* prints out only this variables in the report */\n    SUM variable1 variable2;                /* adds an extra line at the end with the total */\n    WHERE variable3 1000; variable3 1000;   /* operators:      =  = = ^= in + - / * **   | ~ ^ ? */\n    WHERE variable4 in ( Child , Elder );   /* only the last WHERE condition is applied */\n    WHERE variable1=20 AND variable4 CONTAINS  case-sensitive-substring ;  /* CONTAINS = ? */\n    IDWHERE ANYALPHA(variable) NE 0         /* only values containing at least a letter */\n    ID variable1                            /* replaces the  Obs  column by a selected variable values */\n    BY variable3variable3                   /* separate in different tables for different variable values (sort first) */\nRUN;   Special  WHERE  operators:   BETWEEN x AND y : an inclusive range  WHERE SAME AND : augment a previous where expression (both applied)  IS NULL : a missing value  IS MISSING : a missing value  LIKE : matches a pattern (% = any number of characters, _ = one character). E.g.:  'T_m%'  The  SOUNDS-LIKE (=\\*)  operator selects observations that contain a spelling variation of a specified word or words. This operator uses the  Soundex  algorithm to compare the variable value and the operand.  ANYVALUE  is an interesting function that searches a character string for an alphabetic character, and returns the first position at which the character is found    Note  To compare with a SAS date value you need to express is as a SAS date constant:  'DDMM \\YY YY'D .", 
            "title": "Subsetting Report Data"
        }, 
        {
            "location": "/essentials/reporting/#sorting-and-grouping-report-data", 
            "text": "1\n2\n3\n4\n5 PROC SORT DATA=SAS-data-set\n    OUT=new-SAS-data-set NODUPKEY;                                           /* optional */\n    DUPOUT=work.duplicates;                                                  /* optional */\n    BY ASCENDING variable1-to-be-sorted DESCENDING variable2-to-be-sorted;   /* optional (ASCENDING is the default order)*/\nRUN;    The  NODUPKEY  option deletes observations with duplicate  BY  values  DUPOUT  writes duplicate observations to a separate output data set", 
            "title": "Sorting and Grouping Report Data"
        }, 
        {
            "location": "/essentials/reporting/#enhancing-reports", 
            "text": "", 
            "title": "Enhancing Reports"
        }, 
        {
            "location": "/essentials/reporting/#titles-and-footnotes", 
            "text": "1\n2\n3\n4\n5\n6 TITLEline  text ;       \nFOOTNOTEline  text ;\n\nTITLE1  text1 ;\nTITLE1  text1_change ;     /* Change title text and also cancels all footnotes with higher numbers */\nTITLE;                     /* Cancel (erase) all titles */    The  lines  specifies the line (1-10) on which the title/footnote will appear (line = 1 is the default value)  The title/footnote will remain until you  change  it,  cancel  it or you  end your SAS session", 
            "title": "Titles and footnotes"
        }, 
        {
            "location": "/essentials/reporting/#titles-and-footnotes-inside-and-outside-a-graph", 
            "text": "COMPLETE", 
            "title": "Titles and footnotes inside AND outside a graph"
        }, 
        {
            "location": "/essentials/reporting/#labels", 
            "text": "Assigning  temporary labels  to display in the report instead of the variable names:  1\n2\n3\n4\n5 PROC PRINT DATA=SAS-data-set LABEL;            \n    LABEL variable1 =  new variable1 name  \n          variable2 =  new variable2 name ;\n    LABEL variable3 =  new variable3 name ;\nRUN;    You need to add the  LABEL  option at the  PROC PRINT  definition to display the labels   The  LABEL  lengths can go up to 256 characters long  You can specify several labels in one  LABEL  statement or use a separate  LABEL  statement for each variable", 
            "title": "Labels"
        }, 
        {
            "location": "/essentials/reporting/#the-split-option", 
            "text": "1\n2\n3 PROC PRINT DATA=SAS-data-set SPLIT= * ;             \n    LABEL variable1 =  variable label line 1*variable label line 1 ;   \nRUN;    When you use  SPLIT  you no longer need to add the  LABEL  option to get the labels printed out  The  SPLIT  option introduces a line break at the label text whenever it finds the specified character ( * )", 
            "title": "The SPLIT option"
        }, 
        {
            "location": "/essentials/formatting/", 
            "text": "Chapter summary in SAS\n\n\nUsing SAS Formats\n\n\n1\n2\n3\n4\nPROC PRINT DATA=SAS-data-base;\n    FORMAT variable1 variable2 format;\n    FORMAT variable3 format3 variable4 format4;\nRUN;\n\n\n\n\n\n\nFormat definition\n\n\n$\nformat\nw\n.\nd\n\n\n\n\n$\n = character format\n\n\nformat\n = format name\n\n\nw\n = total width (includes special characters, commas, decimal point and decimal places)\n\n\n.\n = required syntax (dot)\n\n\nd\n = decimal places (numeric format)\n\n\n\n\nSAS formats\n\n\nDictionary of formats\n\n\n\n\n$w.\n = writes standard character data\n\n\n$QUOTE.\n = writes a string in quotation marks \n\n\nw.d\n = writes standard numeric data\n\n\nCOMMAw.d\n = writes numeric values with a comma that separates every three digits and a period that separates the decimal fraction\n\n\nDOLLARw.d\n = writes numeric values with a leading dollar sign, a comma that separates every three digits and a period that separates the decimal fraction\n\n\nCOMMAXw.d\n = writes numeric values with a period that separates every three digits and a coma that separates the decimal fraction\n\n\nEUROXw.d\n = writes numeric values with a leading euro symbol, a period that separates every three digits and a comma that separates the decimal fraction\n\n\nDOSEF.\n = you can see the actual variable level values in the output rather than some indexes\n\n\n$UPCASE.\n = writes a string in uppercase\n\n\n\n\nIf you want to uppercase \nonly the first letter\n of words there is not a format but a function that you could use to transform your value:\n\n\n1\nvar_propercase = PROPCASE(var_uppercase);\n\n\n\n\n\n\nSAS date values\n\n\nMMDDYY\nw\n.\n | \nDDMMYY\nw\n.\n | \nMONYY\nw\n.\n | \nDATE\nw\n.\n | \nWEEKDATE.\n\n\n\n\nw = 6: only date numbers\n\n\nw = 8: date numbers with \n/\n separators (just the last 2 digits of year)\n\n\nw = 10: date numbers with \n/\n separators (full 4-digit year)\n\n\n\n\n\n\nNote\n\n\nDates before 01/01/1960 (0 value) will appear as negative numbers.\n\n\n\n\n\n\nWarning\n\n\nIf you ever have to deal with hours (4-character strings with the military hour) you better create an auxiliary character variable with a \n:\n in between hours and minutes or translate it into seconds (numeric) before applying an \nHOURw.d\n (time interval in hours and its fractions) or \nHHMMw.d\n (time in HH:MM appearance) format. \n\n\n\n\nCreating and Applying User-Defined Formats\n\n\nPROC FORMAT\n\n\n1\n2\n3\n4\nPROC FORMAT;\n    VALUE \n$\nformat-name value-or-range1=\nformatted-value1\n\n                         value-or-range2=\nformatted-value2\n;\nRUN;\n\n\n\n\n\n\n1\n2\n3\nPROC PRINT DATA=SAS-data-set;\n    FORMAT variable1 \n$\nformat-name.;\nRUN;\n\n\n\n\n\n\n\n\nA format name can have a maximum of \n32 characters\n\n\nThe name of a format that applies to \ncharacter values\n must begin with a \ndollar sign\n followed by a letter or underscore\n\n\nThe name of a format that applies to \nnumeric values\n must begin with a letter or underscore\n\n\nA format name cannot end in a number\n\n\nAll remaining characters can be letters, underscores or numbers\n\n\nA user defined format name cannot be the name of a SAS format\n\n\n\n\nEach \nvalue-range set\n has three parts:\n\n\n\n\nvalue-or-range\n: specifies one or more values to be formatted (it can be a value, a range or a list of values)\n\n\n=\n: equal sign\n\n\nformatted-value\n: the formatted value you want to display instead of the stored value/s (it is allways a character string no matter wheter the format applies to character values or numeric values)\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nPROC FORMAT LIBRARY = my-format-library;   /* To save the custom formats */\n    VALUE string \nA\n-\nH\n=\nFirst\n\n                 \nI\n,\nJ\n,\nK\n=\nMiddle\n\n                  OTHER = \nEnd\n;           /* Non-specified values */\n    VALUE tiers low-\n50000=\nTier1\n         /* 50000 not included */\n                50000-\n100000=\nTier2\n      /* 100000 not included */\n                100000-high=\nTier3\n\n                .=\nMissing value\n;\nRUN;\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nIf you omit the \nLIBRARY\n option, then formats and informats are stored in the \nwork.formats\n catalog.\n\n\nIf you do not include the keyword \nOTHER\n, then SAS applies the format only to values that match the value-range sets that you specify and the rest of values are displayed as they are stored in the data set.\n\n\nYou can only use the \n symbol to define a non-inclusive range.\n\n\n\n\n\n\n1\nOPTIONS FMTSEARCH = (libref1 libref2... librefn);\n\n\n\n\n\n\n\n\nThe \nFMTSEARCH\n system option controls the order in which format catalogs are searched until the desired member is found.\n\n\nThe \nWORK.FORMATS\n catalog is always searched first, unless it appears in the \nFMTSEARCH\n list.\n\n\n\n\n\n\nFormat maximum length\n\n\nThe maximum lenght of a custom format is defined by the length of its longer label.\nIf you need to increase it you can create a larger dummy element in the format or change the format attributes (see \nthis example\n). \n\n\n\n\nCreating a Format from a SAS Dataset\n\n\n1\n2\n3\n4\n5\n6\n7\n8\nDATA formatdataset;\n    SET originaldataset;\n    RETAIN fmtname \n$custom_format_name\n TYPE \nC\n;\n    RENAME index_variable=start label_variable=label;\nRUN;\n\nPROC FORMAT CNTLIN=formatdataset;\nRUN;\n\n\n\n\n\n\n\n\nCheck these websites\n\n\nCreating a Format from Raw Data or a SAS Dataset\n\n\n\n\nPROC FORMAT\n's \nPICTURE\n Statement\n\n\nLOW-HIGH\n ensures that all values are included in the range. The \nMULT=\n statement option specifies that each value is multiplied by 1.61. The \nPREFIX=\n statement adds a US dollar sign to any number that you format. The picture contains six digit selectors, five for the salary and one for the dollar sign prefix.\n\n\n1\n2\n3\n4\n5\n6\nPROC FORMAT;\n    PICTURE pct (round)   low-high =\n0009.9%)\n  (mult=10 prefix=\n(\n);\n    PICTURE pctl (round)  low-high =\n0000.00%)\n (mult=100 prefix=\n(\n);\n    PICTURE numero        low-high =\n0000000)\n  (prefix=\n(N=\n);\n    PICTURE uscurrency    low-high=\n000,000\n    (mult=1.61 prefix=\n$\n);\nRUN;\n\n\n\n\n\n\nExamples\n\n\nHow to Order Categorical Variables\n\n\n\n\nCheck these websites\n\n\n\n\nThere Is No \nAPPEND\n Option On \nPROC FORMAT\n. What Can You Do?\n\n\n\n\n\n\nYou first create a format that you will apply to an auxiliary variable: \n\n\n1\n2\n3\n4\n5\n6\nvalue SmFmt 1 = \nNon-smoker\n\n            2 = \nLight (1-5)\n\n            3 = \nModerate (6-15)\n\n            4 = \nHeavy (16-25)\n\n            5 = \nVery Heavy (\n 25)\n;\nrun;\n\n\n\n\n\n\nThen you create a data set view rather than a data set in order to save storage space (which might be important for large data sets) on which you define your auxiliary variable with the predefined format:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\ndata Heart / view=Heart;\n    format Smoking_Cat SmFmt.;\n    set sashelp.heart;\n    counter = _n_;\n    keep counter Status Sex AgeAtStart Height Weight Diastolic Systolic Smoking_Cat;\n\n    select (Smoking_Status);\n        when (\nNon-smoker\n)        Smoking_Cat=1;\n        when (\nLight (1-5)\n)       Smoking_Cat=2;\n        when (\nModerate (6-15)\n)   Smoking_Cat=3;\n        when (\nHeavy (16-25)\n)     Smoking_Cat=4;\n        when (\nVery Heavy (\n 25)\n) Smoking_Cat=5;\n        when (\n \n)                 Smoking_Cat=.;\n    end;\nrun;\n\n\n\n\n\n\nIf you then use a \nPROC REPORT\n to display your results, the order of appearance will be the numeric order of your auxiliary variable. By using this technique, you can specify any order for the categories of a contingency table.\n\n\nHow to Modify an Existing Format\n\n\nWe first load the existing format from the catalog into a data set:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n%LET fmtname = %QSYSFUNC(COMPRESS(\nfmtnamepoint.,%STR(%.)));\n\n\n\nPROC\n \nFORMAT\n \nLIBRARY\n=\nWORK\n \nCNTLOUT\n=\ntmpfmt1\n;\n\n    \nSELECT\n \nfmtname\n.;\n\n\nRUN\n;\n\n\n\nDATA\n \ntmpfmt2\n;\n\n    \nSET\n \ntmpfmt1\n;\n\n    \nKEEP\n \nSTART\n \nEND\n \nLABEL\n \nFMTNAME\n;\n\n\nRUN\n;\n\n\n\n\n\n\n\nWe create the new format entry and add it to the existing format list:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\nDATA updatefmt;\n    LENGTH FMTNAME $32. START $16. END $16. LABEL $102.;\n    FMTNAME = \nTIMEVARFORMAT.\n;\n    LABEL = \nFormat label\n;\n    END = \nend-valule\n;\n    START = \nstart-value\n;\nRUN;\n\nDATA newfmt;\n    SET updatefmt tmpfmt2;\n    END = TRIM(LEFT(END));\n    START = TRIM(LEFT(START));\nRUN;\n\n\n\n\n\n\nNow we load the updated format to the format catalog:\n\n\n1\n2\n3\n4\nODS SELECT NONE;\nPROC FORMAT LIBRARY=WORK CNTLIN=newfmt FMTLIB;\nRUN;\nODS SELECT ALL;\n\n\n\n\n\n\nIf you need to recover the original format after some operations you just load the original data set:\n\n\n1\n2\n3\n4\nODS SELECT NONE;\nPROC FORMAT LIBRARY=WORK CNTLIN=tmpfmt1 FMTLIB;\nRUN;\nODS SELECT ALL;", 
            "title": "Formatting Data Values"
        }, 
        {
            "location": "/essentials/formatting/#using-sas-formats", 
            "text": "1\n2\n3\n4 PROC PRINT DATA=SAS-data-base;\n    FORMAT variable1 variable2 format;\n    FORMAT variable3 format3 variable4 format4;\nRUN;", 
            "title": "Using SAS Formats"
        }, 
        {
            "location": "/essentials/formatting/#format-definition", 
            "text": "$ format w . d   $  = character format  format  = format name  w  = total width (includes special characters, commas, decimal point and decimal places)  .  = required syntax (dot)  d  = decimal places (numeric format)", 
            "title": "Format definition"
        }, 
        {
            "location": "/essentials/formatting/#sas-formats", 
            "text": "Dictionary of formats   $w.  = writes standard character data  $QUOTE.  = writes a string in quotation marks   w.d  = writes standard numeric data  COMMAw.d  = writes numeric values with a comma that separates every three digits and a period that separates the decimal fraction  DOLLARw.d  = writes numeric values with a leading dollar sign, a comma that separates every three digits and a period that separates the decimal fraction  COMMAXw.d  = writes numeric values with a period that separates every three digits and a coma that separates the decimal fraction  EUROXw.d  = writes numeric values with a leading euro symbol, a period that separates every three digits and a comma that separates the decimal fraction  DOSEF.  = you can see the actual variable level values in the output rather than some indexes  $UPCASE.  = writes a string in uppercase   If you want to uppercase  only the first letter  of words there is not a format but a function that you could use to transform your value:  1 var_propercase = PROPCASE(var_uppercase);", 
            "title": "SAS formats"
        }, 
        {
            "location": "/essentials/formatting/#sas-date-values", 
            "text": "MMDDYY w .  |  DDMMYY w .  |  MONYY w .  |  DATE w .  |  WEEKDATE.   w = 6: only date numbers  w = 8: date numbers with  /  separators (just the last 2 digits of year)  w = 10: date numbers with  /  separators (full 4-digit year)    Note  Dates before 01/01/1960 (0 value) will appear as negative numbers.    Warning  If you ever have to deal with hours (4-character strings with the military hour) you better create an auxiliary character variable with a  :  in between hours and minutes or translate it into seconds (numeric) before applying an  HOURw.d  (time interval in hours and its fractions) or  HHMMw.d  (time in HH:MM appearance) format.", 
            "title": "SAS date values"
        }, 
        {
            "location": "/essentials/formatting/#creating-and-applying-user-defined-formats", 
            "text": "", 
            "title": "Creating and Applying User-Defined Formats"
        }, 
        {
            "location": "/essentials/formatting/#proc-format", 
            "text": "1\n2\n3\n4 PROC FORMAT;\n    VALUE  $ format-name value-or-range1= formatted-value1 \n                         value-or-range2= formatted-value2 ;\nRUN;   1\n2\n3 PROC PRINT DATA=SAS-data-set;\n    FORMAT variable1  $ format-name.;\nRUN;    A format name can have a maximum of  32 characters  The name of a format that applies to  character values  must begin with a  dollar sign  followed by a letter or underscore  The name of a format that applies to  numeric values  must begin with a letter or underscore  A format name cannot end in a number  All remaining characters can be letters, underscores or numbers  A user defined format name cannot be the name of a SAS format   Each  value-range set  has three parts:   value-or-range : specifies one or more values to be formatted (it can be a value, a range or a list of values)  = : equal sign  formatted-value : the formatted value you want to display instead of the stored value/s (it is allways a character string no matter wheter the format applies to character values or numeric values)   1\n2\n3\n4\n5\n6\n7\n8\n9 PROC FORMAT LIBRARY = my-format-library;   /* To save the custom formats */\n    VALUE string  A - H = First \n                  I , J , K = Middle \n                  OTHER =  End ;           /* Non-specified values */\n    VALUE tiers low- 50000= Tier1          /* 50000 not included */\n                50000- 100000= Tier2       /* 100000 not included */\n                100000-high= Tier3 \n                .= Missing value ;\nRUN;    Note   If you omit the  LIBRARY  option, then formats and informats are stored in the  work.formats  catalog.  If you do not include the keyword  OTHER , then SAS applies the format only to values that match the value-range sets that you specify and the rest of values are displayed as they are stored in the data set.  You can only use the   symbol to define a non-inclusive range.    1 OPTIONS FMTSEARCH = (libref1 libref2... librefn);    The  FMTSEARCH  system option controls the order in which format catalogs are searched until the desired member is found.  The  WORK.FORMATS  catalog is always searched first, unless it appears in the  FMTSEARCH  list.    Format maximum length  The maximum lenght of a custom format is defined by the length of its longer label.\nIf you need to increase it you can create a larger dummy element in the format or change the format attributes (see  this example ).", 
            "title": "PROC FORMAT"
        }, 
        {
            "location": "/essentials/formatting/#creating-a-format-from-a-sas-dataset", 
            "text": "1\n2\n3\n4\n5\n6\n7\n8 DATA formatdataset;\n    SET originaldataset;\n    RETAIN fmtname  $custom_format_name  TYPE  C ;\n    RENAME index_variable=start label_variable=label;\nRUN;\n\nPROC FORMAT CNTLIN=formatdataset;\nRUN;    Check these websites  Creating a Format from Raw Data or a SAS Dataset", 
            "title": "Creating a Format from a SAS Dataset"
        }, 
        {
            "location": "/essentials/formatting/#proc-formats-picture-statement", 
            "text": "LOW-HIGH  ensures that all values are included in the range. The  MULT=  statement option specifies that each value is multiplied by 1.61. The  PREFIX=  statement adds a US dollar sign to any number that you format. The picture contains six digit selectors, five for the salary and one for the dollar sign prefix.  1\n2\n3\n4\n5\n6 PROC FORMAT;\n    PICTURE pct (round)   low-high = 0009.9%)   (mult=10 prefix= ( );\n    PICTURE pctl (round)  low-high = 0000.00%)  (mult=100 prefix= ( );\n    PICTURE numero        low-high = 0000000)   (prefix= (N= );\n    PICTURE uscurrency    low-high= 000,000     (mult=1.61 prefix= $ );\nRUN;", 
            "title": "PROC FORMAT's PICTURE Statement"
        }, 
        {
            "location": "/essentials/formatting/#examples", 
            "text": "", 
            "title": "Examples"
        }, 
        {
            "location": "/essentials/formatting/#how-to-order-categorical-variables", 
            "text": "Check these websites   There Is No  APPEND  Option On  PROC FORMAT . What Can You Do?    You first create a format that you will apply to an auxiliary variable:   1\n2\n3\n4\n5\n6 value SmFmt 1 =  Non-smoker \n            2 =  Light (1-5) \n            3 =  Moderate (6-15) \n            4 =  Heavy (16-25) \n            5 =  Very Heavy (  25) ;\nrun;   Then you create a data set view rather than a data set in order to save storage space (which might be important for large data sets) on which you define your auxiliary variable with the predefined format:   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15 data Heart / view=Heart;\n    format Smoking_Cat SmFmt.;\n    set sashelp.heart;\n    counter = _n_;\n    keep counter Status Sex AgeAtStart Height Weight Diastolic Systolic Smoking_Cat;\n\n    select (Smoking_Status);\n        when ( Non-smoker )        Smoking_Cat=1;\n        when ( Light (1-5) )       Smoking_Cat=2;\n        when ( Moderate (6-15) )   Smoking_Cat=3;\n        when ( Heavy (16-25) )     Smoking_Cat=4;\n        when ( Very Heavy (  25) ) Smoking_Cat=5;\n        when (   )                 Smoking_Cat=.;\n    end;\nrun;   If you then use a  PROC REPORT  to display your results, the order of appearance will be the numeric order of your auxiliary variable. By using this technique, you can specify any order for the categories of a contingency table.", 
            "title": "How to Order Categorical Variables"
        }, 
        {
            "location": "/essentials/formatting/#how-to-modify-an-existing-format", 
            "text": "We first load the existing format from the catalog into a data set:   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 %LET fmtname = %QSYSFUNC(COMPRESS( fmtnamepoint.,%STR(%.)));  PROC   FORMAT   LIBRARY = WORK   CNTLOUT = tmpfmt1 ; \n     SELECT   fmtname .;  RUN ;  DATA   tmpfmt2 ; \n     SET   tmpfmt1 ; \n     KEEP   START   END   LABEL   FMTNAME ;  RUN ;    We create the new format entry and add it to the existing format list:   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 DATA updatefmt;\n    LENGTH FMTNAME $32. START $16. END $16. LABEL $102.;\n    FMTNAME =  TIMEVARFORMAT. ;\n    LABEL =  Format label ;\n    END =  end-valule ;\n    START =  start-value ;\nRUN;\n\nDATA newfmt;\n    SET updatefmt tmpfmt2;\n    END = TRIM(LEFT(END));\n    START = TRIM(LEFT(START));\nRUN;   Now we load the updated format to the format catalog:  1\n2\n3\n4 ODS SELECT NONE;\nPROC FORMAT LIBRARY=WORK CNTLIN=newfmt FMTLIB;\nRUN;\nODS SELECT ALL;   If you need to recover the original format after some operations you just load the original data set:  1\n2\n3\n4 ODS SELECT NONE;\nPROC FORMAT LIBRARY=WORK CNTLIN=tmpfmt1 FMTLIB;\nRUN;\nODS SELECT ALL;", 
            "title": "How to Modify an Existing Format"
        }, 
        {
            "location": "/essentials/reading-sas/", 
            "text": "Chapter summary in SAS\n\n\nSubsetting using the \nWHERE\n statement\n\n\nTo create a new data set that is a subset of a previous data set:\n\n\n1\n2\n3\n4\n5\n6\nDATA output-SAS-data-set;\n    SET input-SAS-data-set;\n    WHERE where-expression;\n    variable_name   WHERE where-expression;\n    variable_name = expression;     /* new variable creation */\nRUN;\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nIf a missing value is involved in an arithmetic calculation the result will be a missing value too\n\n\nNew variables being created in the \nDATA\n step and not contained in the original data set cannot be used in a \nWHERE\n statement\n\n\n\n\n\n\nOther \nWHERE\n options\n\n\n\n\nHere's how to set a filter for \nWHERE\n a variable \nIS MISSING\n\n\n\n\nCustomizing a SAS Data Set\n\n\nHow to select a subset of the variables/observations of the original data set:\n\n\n1\n2\n3\n4\n5\nDATA output-SAS-data-set;\n    SET input-SAS-data-set;\n    DROP variable-list;        /* variables to exclude */\n    KEEP variable-list;        /* variables to include */\nRUN;\n\n\n\n\n\n\nHow SAS processes the \nDATA\n step\n\n\nCompilation phase\n\n\n\n\nSAS scan each DATA step statement for syntax errors and converts the program into machine code if everything's alright. \n\n\nSAS also creates the program data vector (\nPDV\n) in memory to hold the current observation.\n\n\n_N_\n: iteration number of the DATA step\n\n\n_ERROR_\n: its value is 0 is there are no errors (1 if there are some)\n\n\nSAS creates the descriptor portion of the new data set (takes the original one, adds the new variables and flags the variables to be dropped). \n\n\n\n\nExecution phase\n\n\n\n\nSAS initializes the PDV to missing\n\n\nSAS reads and processes the observations from the input data set \n\n\nSAS creates observations in the data portion of the output data set (an implicit output/implicit return loop over all the observations that continues until EOF)\n\n\n\n\nSubsetting using the \nIF\n statement\n\n\n1\n2\n3\n4\nDATA output-SAS-data-set;\n    SET input-SAS-data-set;\n    IF expression;\nRUN;\n\n\n\n\n\n\n\n\nWhen the expression is false, SAS excludes the observation from the output data set and continues processing\n\n\nWhile original values can be managed with a \nWHERE\n statement as well as an \nIF\n statement, for \nnew variable\n conditionals only \nIF\n can be used\n\n\nYou should subset as early as possible in your program for more efficient processing (a \nWHERE\n before an \nIF\n can make the processing more efficient).\n\n\nIn a \nPROC\n step \nIF\n statements are \nNOT allowed\n\n\nIF THEN\n analogue to \nCONTAINS\n:\n\n\n\n\n1\nIF FIND(variable_name,\npattern\n) THEN (...);\n\n\n\n\n\n\nSubsetting \nIF-THEN/DELETE\n statement\n\n\n1\n2\n3\n4\nDATA output-SAS-data-set;\n    SET input-SAS-data-set;\n    IF expression1 or expression2 THEN DELETE;\nRUN;\n\n\n\n\n\n\n\n\nThe \nIF-THEN/DELETE\n statement eliminates the observations where the \nconditions are not met\n (on the contrary of what the \nIF\n does)\n\n\nThe \nDELETE\n statement stops processing the current observation. It is often used in a \nTHEN\n clause of an \nIF-THEN\n statement or as part of a conditionally executed \nDO\n group.\n\n\n\n\n\n\nTip\n\n\nYou can remove all the observations with at least one missing value using this condition inside a \nDATA\n step:\n\nif cmiss(of _all_) then delete;\n\n\n\n\nCreate different data sets from one\n\n\n1\n2\n3\n4\n5\n6\nDATA data1 data2 data3;\n    SET original_data;\n    IF (condition1) THEN OUTPUT prueba1;\n    IF (condition2) THEN OUTPUT prueba2;\n    IF (condition3) THEN OUTPUT prueba3;\nRUN;\n\n\n\n\n\n\nAvailable operations\n\n\n\n\nAddition of several variables: \nTotal=sum(var1, var2, var3)\n\n\nCount of nonmissing values: \nNonmissing=n(var1, var2, var3)\n\n\n\n\nUsing \nPROC SQL\n to \nGROUP BY\n variables\n\n\nPROC SQL\n is a wonderful tool for \nsummarizing or aggregating\n data. When you use a \nGROUP BY\n clause, you also use an aggregate function in the \nSELECT\n clause or in a \nHAVING\n clause to instruct \nPROC SQL\n in how to summarize the data for each group:\n\n\n1\n2\n3\n4\n5\n6\n7\nPROC SQL;\n    select T, range(survival) as RangeSurvival, sqrt(sum(sdf_stderr**2)) as Squares, range(survival)/sqrt(sum(sdf_stderr**2)) as z,\n           probnorm(abs(range(survival)/sqrt(sum(sdf_stderr**2)))) as pz, 2 * (1-probnorm(abs(range(survival)/sqrt(sum(sdf_stderr**2))))) as pvalue\n    from BTM_param \n    where T \n 0\n    group by T;\nQUIT;\n\n\n\n\n\n\nAdding Permanent Attributes\n\n\nPermanent variable labels\n\n\n1\n2\n3\n4\n5\nDATA output-SAS-data-set;\n    SET input-SAS-data-set;\n    LABEL variable1=\nlabel1\n\n          variable2=\nlabel2\n;\nRUN;\n\n\n\n\n\n\n1\n2\nPROC PRINT DATA=output-SAS-data-set label;\nRUN;\n\n\n\n\n\n\n\n\nIf you use the \nLABEL\n statement in the \nPROC\n step the labels are \ntemporary\n while if you use it in the \nDATA\n step, SAS \npermanently\n associates the labels to the variables\n\n\nLabels and formats that you specify in \nPROC\n steps override the permanent labels in the current step. However, the permanent labels are not changed.\n\n\n\n\nPermanent variable formats\n\n\n1\n2\n3\n4\n5\nDATA output-SAS-data-set;\n    SET input-SAS-data-set;\n    FORMAT variable1 format1\n           variable2 format2;\nRUN;", 
            "title": "Reading SAS Data Sets"
        }, 
        {
            "location": "/essentials/reading-sas/#subsetting-using-the-where-statement", 
            "text": "To create a new data set that is a subset of a previous data set:  1\n2\n3\n4\n5\n6 DATA output-SAS-data-set;\n    SET input-SAS-data-set;\n    WHERE where-expression;\n    variable_name   WHERE where-expression;\n    variable_name = expression;     /* new variable creation */\nRUN;    Note   If a missing value is involved in an arithmetic calculation the result will be a missing value too  New variables being created in the  DATA  step and not contained in the original data set cannot be used in a  WHERE  statement", 
            "title": "Subsetting using the WHERE statement"
        }, 
        {
            "location": "/essentials/reading-sas/#other-where-options", 
            "text": "Here's how to set a filter for  WHERE  a variable  IS MISSING", 
            "title": "Other WHERE options"
        }, 
        {
            "location": "/essentials/reading-sas/#customizing-a-sas-data-set", 
            "text": "How to select a subset of the variables/observations of the original data set:  1\n2\n3\n4\n5 DATA output-SAS-data-set;\n    SET input-SAS-data-set;\n    DROP variable-list;        /* variables to exclude */\n    KEEP variable-list;        /* variables to include */\nRUN;", 
            "title": "Customizing a SAS Data Set"
        }, 
        {
            "location": "/essentials/reading-sas/#how-sas-processes-the-data-step", 
            "text": "Compilation phase   SAS scan each DATA step statement for syntax errors and converts the program into machine code if everything's alright.   SAS also creates the program data vector ( PDV ) in memory to hold the current observation.  _N_ : iteration number of the DATA step  _ERROR_ : its value is 0 is there are no errors (1 if there are some)  SAS creates the descriptor portion of the new data set (takes the original one, adds the new variables and flags the variables to be dropped).    Execution phase   SAS initializes the PDV to missing  SAS reads and processes the observations from the input data set   SAS creates observations in the data portion of the output data set (an implicit output/implicit return loop over all the observations that continues until EOF)", 
            "title": "How SAS processes the DATA step"
        }, 
        {
            "location": "/essentials/reading-sas/#subsetting-using-the-if-statement", 
            "text": "1\n2\n3\n4 DATA output-SAS-data-set;\n    SET input-SAS-data-set;\n    IF expression;\nRUN;    When the expression is false, SAS excludes the observation from the output data set and continues processing  While original values can be managed with a  WHERE  statement as well as an  IF  statement, for  new variable  conditionals only  IF  can be used  You should subset as early as possible in your program for more efficient processing (a  WHERE  before an  IF  can make the processing more efficient).  In a  PROC  step  IF  statements are  NOT allowed  IF THEN  analogue to  CONTAINS :   1 IF FIND(variable_name, pattern ) THEN (...);", 
            "title": "Subsetting using the IF statement"
        }, 
        {
            "location": "/essentials/reading-sas/#subsetting-if-thendelete-statement", 
            "text": "1\n2\n3\n4 DATA output-SAS-data-set;\n    SET input-SAS-data-set;\n    IF expression1 or expression2 THEN DELETE;\nRUN;    The  IF-THEN/DELETE  statement eliminates the observations where the  conditions are not met  (on the contrary of what the  IF  does)  The  DELETE  statement stops processing the current observation. It is often used in a  THEN  clause of an  IF-THEN  statement or as part of a conditionally executed  DO  group.    Tip  You can remove all the observations with at least one missing value using this condition inside a  DATA  step: if cmiss(of _all_) then delete;", 
            "title": "Subsetting IF-THEN/DELETE statement"
        }, 
        {
            "location": "/essentials/reading-sas/#create-different-data-sets-from-one", 
            "text": "1\n2\n3\n4\n5\n6 DATA data1 data2 data3;\n    SET original_data;\n    IF (condition1) THEN OUTPUT prueba1;\n    IF (condition2) THEN OUTPUT prueba2;\n    IF (condition3) THEN OUTPUT prueba3;\nRUN;", 
            "title": "Create different data sets from one"
        }, 
        {
            "location": "/essentials/reading-sas/#available-operations", 
            "text": "Addition of several variables:  Total=sum(var1, var2, var3)  Count of nonmissing values:  Nonmissing=n(var1, var2, var3)", 
            "title": "Available operations"
        }, 
        {
            "location": "/essentials/reading-sas/#using-proc-sql-to-group-by-variables", 
            "text": "PROC SQL  is a wonderful tool for  summarizing or aggregating  data. When you use a  GROUP BY  clause, you also use an aggregate function in the  SELECT  clause or in a  HAVING  clause to instruct  PROC SQL  in how to summarize the data for each group:  1\n2\n3\n4\n5\n6\n7 PROC SQL;\n    select T, range(survival) as RangeSurvival, sqrt(sum(sdf_stderr**2)) as Squares, range(survival)/sqrt(sum(sdf_stderr**2)) as z,\n           probnorm(abs(range(survival)/sqrt(sum(sdf_stderr**2)))) as pz, 2 * (1-probnorm(abs(range(survival)/sqrt(sum(sdf_stderr**2))))) as pvalue\n    from BTM_param \n    where T   0\n    group by T;\nQUIT;", 
            "title": "Using PROC SQL to GROUP BY variables"
        }, 
        {
            "location": "/essentials/reading-sas/#adding-permanent-attributes", 
            "text": "", 
            "title": "Adding Permanent Attributes"
        }, 
        {
            "location": "/essentials/reading-sas/#permanent-variable-labels", 
            "text": "1\n2\n3\n4\n5 DATA output-SAS-data-set;\n    SET input-SAS-data-set;\n    LABEL variable1= label1 \n          variable2= label2 ;\nRUN;   1\n2 PROC PRINT DATA=output-SAS-data-set label;\nRUN;    If you use the  LABEL  statement in the  PROC  step the labels are  temporary  while if you use it in the  DATA  step, SAS  permanently  associates the labels to the variables  Labels and formats that you specify in  PROC  steps override the permanent labels in the current step. However, the permanent labels are not changed.", 
            "title": "Permanent variable labels"
        }, 
        {
            "location": "/essentials/reading-sas/#permanent-variable-formats", 
            "text": "1\n2\n3\n4\n5 DATA output-SAS-data-set;\n    SET input-SAS-data-set;\n    FORMAT variable1 format1\n           variable2 format2;\nRUN;", 
            "title": "Permanent variable formats"
        }, 
        {
            "location": "/essentials/reading-spreadsheets/", 
            "text": "Chapter summary in SAS\n\n\nReading Spreadsheet Data\n\n\nSAS/ACCESS LIBNAME statement (read/write/update data):\n\n\n1\nLIBNAME libref \nengine\n \nPATH=\nworkbook-name\n \noptions\n;\n\n\n\n\n\n\nE.g.:\n\n\nDefault engine:\n \nLIBNAME orionx excel \"\npath/sales.xls\"\n\n\nPC Files server engine:\n \nLIBNAME orionx pcfiles PATH=\"\npath/sales.xls\"\n\n\n\n\n\\engine\n: excel (if both SAS and Office are 32/64 bits), pcfiles (if the value is different)\n\n\nThe icon of the library will be different (a globe) indicating that the data is outside SAS\n\n\nThe members whose name ends with a \n$\n are the \nspreadsheets\n while the others are named \nranges\n. In case it has the \n$\n, you need to refer to that Excel worksheet in a special way to account for that special character (SAS name literal): \nlibref.'worksheetname$'n\n\n\nYou can use the \nVALIDVARNAME = v7\n option in SAS Enterprise Guide to cause it to behave the same as in the SAS window environment\n\n\nIs important to disassociate the library: the workbook cannot be opened in Excel meanwhile (SAS puts a lock on the Excel file when the libref is assigned): \nLIBNAME libref CLEAR;\n\n\n\n\n\n\nImport the xls data:\n\n\n1\n2\n3\n4\n5\nPROC IMPORT DATAFILE=\n/folders/myfolders/reading_test.xlsx\n\n            OUT=work.myexcel\n            DBMS=xlsx \n            REPLACE;\nRUN;\n\n\n\n\n\n\nReading Database Data\n\n\n1\nLIBNAME libref engine \nSAS/ACCESS options\n;\n\n\n\n\n\n\n\n\nengine\n: oracle or BD2\n\n\nSAS/ACCESS options\n: USER, PASSWORD/PW, PATH (specifies the Oracle driver, node and database), SCHEMA (enables you to read database objects such as tables and views)", 
            "title": "Reading Spreadsheet and Database Data"
        }, 
        {
            "location": "/essentials/reading-spreadsheets/#reading-spreadsheet-data", 
            "text": "SAS/ACCESS LIBNAME statement (read/write/update data):  1 LIBNAME libref  engine   PATH= workbook-name   options ;   E.g.:  Default engine:   LIBNAME orionx excel \" path/sales.xls\"  PC Files server engine:   LIBNAME orionx pcfiles PATH=\" path/sales.xls\"   \\engine : excel (if both SAS and Office are 32/64 bits), pcfiles (if the value is different)  The icon of the library will be different (a globe) indicating that the data is outside SAS  The members whose name ends with a  $  are the  spreadsheets  while the others are named  ranges . In case it has the  $ , you need to refer to that Excel worksheet in a special way to account for that special character (SAS name literal):  libref.'worksheetname$'n  You can use the  VALIDVARNAME = v7  option in SAS Enterprise Guide to cause it to behave the same as in the SAS window environment  Is important to disassociate the library: the workbook cannot be opened in Excel meanwhile (SAS puts a lock on the Excel file when the libref is assigned):  LIBNAME libref CLEAR;    Import the xls data:  1\n2\n3\n4\n5 PROC IMPORT DATAFILE= /folders/myfolders/reading_test.xlsx \n            OUT=work.myexcel\n            DBMS=xlsx \n            REPLACE;\nRUN;", 
            "title": "Reading Spreadsheet Data"
        }, 
        {
            "location": "/essentials/reading-spreadsheets/#reading-database-data", 
            "text": "1 LIBNAME libref engine  SAS/ACCESS options ;    engine : oracle or BD2  SAS/ACCESS options : USER, PASSWORD/PW, PATH (specifies the Oracle driver, node and database), SCHEMA (enables you to read database objects such as tables and views)", 
            "title": "Reading Database Data"
        }, 
        {
            "location": "/essentials/reading-raw/", 
            "text": "Chapter summary in SAS\n\n\nIntroduction to Reading Raw Data Files\n\n\n\n\nRaw data files\n are not software specific\n\n\nA \ndelimited raw data file\n is an external text file in which the values are separated by spaces or other special characters.\n\n\nA \nlist input\n will be used to work with delimited raw data files that contain standard and/or nonstandard data\n\n\nStandard data\n is data that SAS can read without any special instructions\n\n\nNonstandard data\n includes values like dates or numeric values that include special characters like dollar signs (extra instructions needed)\n\n\nYou cannot use a \nWHERE\n statement when the input data is a raw data file instead of a SAS data set\n\n\n\n\nReading Standard Delimited Data\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\nDATA output-SAS-data-set-name;\n    LENGTH variable(s) \n$\n length;\n    INFILE \nraw-data-file-name\n DLM=\ndelimiter\n;  \n    INPUT variable1 \n$\n variable2 \n$\n ... variableN \n$\n;       \nRUN;\n\n/* Example */\n\nDATA work.sales1;\n    LENGTH First_Name Last_Name $ 12 Gender $ 1;\n    INFILE \npath/sales.csv\n DLM=\n,\n;  \n    INPUT Employee_ID Gender $ Salary $ Job_Title $ Country $; \nRUN;\n\n\n\n\n\n\n\n\nWith \nlist input\n, the default length for all variables is 8 bytes\n\n\nSAS uses an \ninput buffer\n only if the input data is a raw data file\n\n\nThe variable names will appear in the report as stated in the \nLENGTH\n statement (watch out the uppercase/lowercase)\n\n\nThe \nLENGTH\n statement must precede the \nINPUT\n statement in order to correctly set the length of the variable\n\n\nThe variables not specified in the \nLENGTH\n statement will appear at the end of the table. If you want to keep the original order you should include all variables even if you want them to have the defaul length (8)\n\n\n\n\nReading Nonstandard Delimited Data\n\n\nYou can use a \nmodified list input\n to read all of the fields from a raw data file (including nonstandard variables)\n\n\n\n\nInformats are similar to formats except that \nformats\n provide instruction on how to \nwrite\n a value while \ninformats\n provide instruction on how to \nread\n a value\n\n\nThe \ncolon format modifier (:)\n causes SAS to read up to the delimiter\n\n\n\n\n1\n2\n3\n4\n5\n6\nINPUT variable \n$\n variable \n:informat\n;\n\n/* Example */\n\n:date.\n:mmddyy.\n\n\n\n\n\n\n\n\nCOMMA./DOLLAR.\n: reads nonstandard numeric data and removes embedded commas, blanks, dollar sign, percent signs and dashes\n\n\nCOMMAX./DOLLARX.\n: reads nonstandard numeric data and removes embedded non-numeric characters; reverses the roles of the decima point and the comma\n\n\nEUROX.\n: reads nonstandard numeric data and removes embedded non-numeric characters in European currency\n\n\n$CHAR.\n: reads character values and preserves leading blanks\n\n\n$UPCASE.\n: reads character values and converts them to uppercase\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nDATA (...);\n    INFILE DATALINES DLM=\n,\n;   /* only if datalines are delimited */\n    INPUT (...);\n    DATALINES;\n    \ninstream data\n\n    ;\n    INPUT (...);\n    DATALINES;\n\ninstream data\n\n;\n\n\n\n\n\n\n\n\nThe null statement (\n;\n) indicates the end of the input data\n\n\nYou precede the instream data with the \nDATALINES\n statement and follow it with a null statement\n\n\nThe instream data should be the last part of the \nDATA step\n except for a null statement\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n/* Example */\n\n\nDATA\n \nwork\n.\nmanagers\n;\n\n\n   \ninfile\n \ndatalines\n \ndlm\n=\n/\n;\n\n\n   \ninput\n \nID\n \nFirst\n \n:\n$\n12\n.\n \nLast\n \n:\n$\n12\n.\n \nGender\n \n$\n \nSalary\n \n:\ncomma\n.\n \n\n\n            \nTitle\n \n:\n$\n25\n.\n \nHireDate\n \n:\ndate\n.\n;\n\n\n   \ndatalines\n;\n\n\n120102\n/\nTom\n/\nZhou\n/\nM\n/\n108\n,\n255\n/\nSales\n \nManager\n/\n01\nJun1993\n\n\n120103\n/\nWilson\n/\nDawes\n/\nM\n/\n87\n,\n975\n/\nSales\n \nManager\n/\n01\nJan1978\n\n\n120261\n/\nHarry\n/\nHighpoint\n/\nM\n/\n243\n,\n190\n/\nChief\n \nSales\n \nOfficer\n/\n01\nAug1991\n\n\n121143\n/\nLouis\n/\nFavaron\n/\nM\n/\n95\n,\n090\n/\nSenior\n \nSales\n \nManager\n/\n01\nJul2001\n\n\n121144\n/\nRenee\n/\nCapachietti\n/\nF\n/\n83\n,\n505\n/\nSales\n \nManager\n/\n01\nNov1995\n\n\n121145\n/\nDennis\n/\nLansberry\n/\nM\n/\n84\n,\n260\n/\nSales\n \nManager\n/\n01\nApr1980\n\n\n;\n\n\ntitle\n \nOrion Star Management Team\n;\n\nproc\n \nprint\n \ndata\n=\nwork\n.\nmanagers\n \nnoobs\n;\n\n\n   \nformat\n \nHireDate\n \nmmddyy10\n.\n;\n\nrun\n;\n\ntitle\n;\n\n\n\n\n\n\n\nValidating Data\n\n\nWhen SAS encounters a data error, it prints messages and a ruler in the log and assigns a missing value to the affected variable. Then SAS continues processing.\n\n\nMissing Values between Delimiters (Consecutive Delimiters)\n\n\n1\nINFILE \nraw-data-file-name\n \nDLM=\n DSD;\n\n\n\n\n\n\nThe \nDSD\n option sets the default delimiter to a comma, treats consecutive delimiters as missing values and enables SAS to read values with embedded delimiters if the value is surrounded by quotation marks\n\n\nMissing Values at the End of a Line\n\n\n1\nINFILE \nraw-data-file-name\n MISSOVER;\n\n\n\n\n\n\nWith the \nMISSOVER\n option, if SAS reaches the end of a record without finding values for all fields, variables without values are set to missing.", 
            "title": "Reading Raw Data Files"
        }, 
        {
            "location": "/essentials/reading-raw/#introduction-to-reading-raw-data-files", 
            "text": "Raw data files  are not software specific  A  delimited raw data file  is an external text file in which the values are separated by spaces or other special characters.  A  list input  will be used to work with delimited raw data files that contain standard and/or nonstandard data  Standard data  is data that SAS can read without any special instructions  Nonstandard data  includes values like dates or numeric values that include special characters like dollar signs (extra instructions needed)  You cannot use a  WHERE  statement when the input data is a raw data file instead of a SAS data set", 
            "title": "Introduction to Reading Raw Data Files"
        }, 
        {
            "location": "/essentials/reading-raw/#reading-standard-delimited-data", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 DATA output-SAS-data-set-name;\n    LENGTH variable(s)  $  length;\n    INFILE  raw-data-file-name  DLM= delimiter ;  \n    INPUT variable1  $  variable2  $  ... variableN  $ ;       \nRUN;\n\n/* Example */\n\nDATA work.sales1;\n    LENGTH First_Name Last_Name $ 12 Gender $ 1;\n    INFILE  path/sales.csv  DLM= , ;  \n    INPUT Employee_ID Gender $ Salary $ Job_Title $ Country $; \nRUN;    With  list input , the default length for all variables is 8 bytes  SAS uses an  input buffer  only if the input data is a raw data file  The variable names will appear in the report as stated in the  LENGTH  statement (watch out the uppercase/lowercase)  The  LENGTH  statement must precede the  INPUT  statement in order to correctly set the length of the variable  The variables not specified in the  LENGTH  statement will appear at the end of the table. If you want to keep the original order you should include all variables even if you want them to have the defaul length (8)", 
            "title": "Reading Standard Delimited Data"
        }, 
        {
            "location": "/essentials/reading-raw/#reading-nonstandard-delimited-data", 
            "text": "You can use a  modified list input  to read all of the fields from a raw data file (including nonstandard variables)   Informats are similar to formats except that  formats  provide instruction on how to  write  a value while  informats  provide instruction on how to  read  a value  The  colon format modifier (:)  causes SAS to read up to the delimiter   1\n2\n3\n4\n5\n6 INPUT variable  $  variable  :informat ;\n\n/* Example */\n\n:date.\n:mmddyy.    COMMA./DOLLAR. : reads nonstandard numeric data and removes embedded commas, blanks, dollar sign, percent signs and dashes  COMMAX./DOLLARX. : reads nonstandard numeric data and removes embedded non-numeric characters; reverses the roles of the decima point and the comma  EUROX. : reads nonstandard numeric data and removes embedded non-numeric characters in European currency  $CHAR. : reads character values and preserves leading blanks  $UPCASE. : reads character values and converts them to uppercase     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 DATA (...);\n    INFILE DATALINES DLM= , ;   /* only if datalines are delimited */\n    INPUT (...);\n    DATALINES;\n     instream data \n    ;\n    INPUT (...);\n    DATALINES; instream data \n;    The null statement ( ; ) indicates the end of the input data  You precede the instream data with the  DATALINES  statement and follow it with a null statement  The instream data should be the last part of the  DATA step  except for a null statement    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20 /* Example */ \n\nDATA   work . managers ;      infile   datalines   dlm = / ;      input   ID   First   : $ 12 .   Last   : $ 12 .   Gender   $   Salary   : comma .                 Title   : $ 25 .   HireDate   : date . ;      datalines ;  120102 / Tom / Zhou / M / 108 , 255 / Sales   Manager / 01 Jun1993  120103 / Wilson / Dawes / M / 87 , 975 / Sales   Manager / 01 Jan1978  120261 / Harry / Highpoint / M / 243 , 190 / Chief   Sales   Officer / 01 Aug1991  121143 / Louis / Favaron / M / 95 , 090 / Senior   Sales   Manager / 01 Jul2001  121144 / Renee / Capachietti / F / 83 , 505 / Sales   Manager / 01 Nov1995  121145 / Dennis / Lansberry / M / 84 , 260 / Sales   Manager / 01 Apr1980  ; \n\ntitle   Orion Star Management Team ; \nproc   print   data = work . managers   noobs ;      format   HireDate   mmddyy10 . ; \nrun ; \ntitle ;", 
            "title": "Reading Nonstandard Delimited Data"
        }, 
        {
            "location": "/essentials/reading-raw/#validating-data", 
            "text": "When SAS encounters a data error, it prints messages and a ruler in the log and assigns a missing value to the affected variable. Then SAS continues processing.", 
            "title": "Validating Data"
        }, 
        {
            "location": "/essentials/reading-raw/#missing-values-between-delimiters-consecutive-delimiters", 
            "text": "1 INFILE  raw-data-file-name   DLM=  DSD;   The  DSD  option sets the default delimiter to a comma, treats consecutive delimiters as missing values and enables SAS to read values with embedded delimiters if the value is surrounded by quotation marks", 
            "title": "Missing Values between Delimiters (Consecutive Delimiters)"
        }, 
        {
            "location": "/essentials/reading-raw/#missing-values-at-the-end-of-a-line", 
            "text": "1 INFILE  raw-data-file-name  MISSOVER;   With the  MISSOVER  option, if SAS reaches the end of a record without finding values for all fields, variables without values are set to missing.", 
            "title": "Missing Values at the End of a Line"
        }, 
        {
            "location": "/essentials/manipulation/", 
            "text": "Chapter summary in SAS\n\n\nUsing SAS Functions\n\n\nSUM\n Summation Function\n\n\n1\nSUM(argument1, argument2, ...)\n\n\n\n\n\n\n\n\nThe arguments must be numeric values\n\n\nThe \nSUM\n function ignores missing values, so if an argument has a missing value, the result of the \nSUM\n function is the sum of the nonmissing values\n\n\nIf you add two values by \n+\n, if one of them is missing, the result will be a missing value, which makes the \nSUM\n function a better choice\n\n\n\n\n\n\nDATE\n Function\n\n\n1\n2\n3\n4\n5\n6\n7\n8\nYEAR(SAS-date)     \nQTR(SAS-date)\nMONTH(SAS-date)\nDAY(SAS-date)\nWEEKDAY(SAS-date)\nTODAY()                /* Obtain the current date and convert to SAS-date (no argument) */\nDATE()                 /* Obtain the current date and convert to SAS-date (no argument) */\nMDY(month, day, year)\n\n\n\n\n\n\n\n\nThe arguments must be numeric values (except from \nTODAY()\n and \nDATE()\n functions)\n\n\nYou can subtract dates: \nAgein2012=(Bday2012-Birth_Date)/365.25;\n\n\n\n\n\n\nCATX\n Concatenation Function\n\n\n1\nCATX(\n \n, First_Name, Last_Name)\n\n\n\n\n\n\nThe \nCATX\n function removes leading and trailing blanks, inserts delimiters, and returns a concatenated character string. In the code, you first specify a character string that is used as a delimiter between concatenated items.\n\n\n\n\nINTCK\n Time Interval Function\n\n\n1\nINTCK(\nyear\n, Hire_Date, \n01JAN2012\nd)\n\n\n\n\n\n\nThe \nINTCK\n function returns the number of interval boundaries of a given kind that lie between the two dates, times, or datetime values. In the code, you first specify the interval value.\n\n\n\n\nWhat happens when you define a new variable from another that you are gonna \nDROP\n in this DATA statement?\n\n\nThe \nDROP\n statement is a compile-time-only statement. SAS sets a drop flag for the dropped variables, but the variables are in the PDV and, therefore, are available for processing.\n\n\n\n\nConditional Processing\n\n\nIF-THEN-ELSE\n Conditional Structures\n\n\n1\n2\n3\nIF expression THEN statement;\nELSE IF expression THEN statement;\nELSE statement;\n\n\n\n\n\n\nIn the conditional expressions involving strings watch out for possible mixed case values where the condition may not be met:  \ncountry = UPCASE(country);\n to avoid problems\n\n\n\n\nWhat do you mean 0.73 doesn't equal 0.73?\n\n\nWhen comparing numeric values, you may get unexpected results when the values of your variables seem to be the same (but actually they are not). The key to making sure these issues do not introduce erroneous results lies in understanding how numeric values are actually stored in data sets and in memory. This is referred to as \nnumeric representation and precision\n. Read this article for more information: \nNumeric Representation and Precision in SAS and Why it Matters\n.\nMain solutions to this problem:\n\n\n\n\nThe \nROUND\nfunction: \nA = round(A, 0.01);\n\n\nCharacter versions of numeric variables: \nCharA = put(A, best.);\n\n\nOptions in procedures: \nPROC COMPARE BASE=baseds COMPARE=compds CRITERION=0.0000001;\n\n\n\n\n\n\nExecuting Multiple Statements\n\n\n1\n2\n3\n4\n5\n6\n7\n8\nIF expression THEN\n    DO;\n        executable statements;\n    END;\nELSE IF expression THEN\n    DO;\n        executable statements;\n    END;\n\n\n\n\n\n\n\n\nIn the \nDATA\n step, the first reference to a variable determines its length. The first reference to a new variable can be in a \nLENGTH\n statement, an \nassignment\n statement, or \nanother\n statement such as an \nINPUT\n statement. After a variable is created in the PDV, the length of the variable's first value doesn't matter. \n\n\nTo avoid truncation in a variable defined inside a conditional structure you can:\n\n\n\n\nDefine the longer string as the first condition\n\n\nAdd some blanks at the end of shorter strings to fit the longer one\n\n\nDefine the length explicitly before any other reference to the variable\n\n\n\n\n\n\nSELECT\n Group\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\nSELECT(Gender);\n      WHEN(\nF\n) DO;\n         Gift1=\nScarf\n;\n         Gift2=\nPedometer\n;\n      END;\n      WHEN(\nM\n) DO;\n         Gift1=\nGloves\n;\n         Gift2=\nMoney Clip\n;\n      END;\n      OTHERWISE DO;\n         Gift1=\nCoffee\n;\n         Gift2=\nCalendar\n;\n      END;\nEND;\n\n\n\n\n\n\n\n\nThe \nSELECT\n statement executes one of several statements or groups of statements\n\n\nThe \nSELECT\n statement begins a \nSELECT\n group. They contain \nWHEN\n statements that identify SAS statements that are executed when a particular condition is true\n\n\nUse at least one \nWHEN\n statement in a \nSELECT\n group\n\n\nAn optional \nOTHERWISE\n statement specifies a statement to be executed if no \nWHEN\n condition is met\n\n\nAn \nEND\n statement ends a \nSELECT\n group\n\n\n\n\nAvoiding Duplicates\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n* Period has more than one register per patient and the calculation of time periods between each one and the reference;\nPROC SORT DATA=period;\n    BY pt periodmax periodvisit;\nRUN;\n\n* Keep only highest (last value after the sorting);\nDATA maxperiod;\n    SET period;\n    BY pt;\n    IF last.pt;\nRUN;", 
            "title": "Manipulating Data"
        }, 
        {
            "location": "/essentials/manipulation/#using-sas-functions", 
            "text": "", 
            "title": "Using SAS Functions"
        }, 
        {
            "location": "/essentials/manipulation/#sum-summation-function", 
            "text": "1 SUM(argument1, argument2, ...)    The arguments must be numeric values  The  SUM  function ignores missing values, so if an argument has a missing value, the result of the  SUM  function is the sum of the nonmissing values  If you add two values by  + , if one of them is missing, the result will be a missing value, which makes the  SUM  function a better choice", 
            "title": "SUM Summation Function"
        }, 
        {
            "location": "/essentials/manipulation/#date-function", 
            "text": "1\n2\n3\n4\n5\n6\n7\n8 YEAR(SAS-date)     \nQTR(SAS-date)\nMONTH(SAS-date)\nDAY(SAS-date)\nWEEKDAY(SAS-date)\nTODAY()                /* Obtain the current date and convert to SAS-date (no argument) */\nDATE()                 /* Obtain the current date and convert to SAS-date (no argument) */\nMDY(month, day, year)    The arguments must be numeric values (except from  TODAY()  and  DATE()  functions)  You can subtract dates:  Agein2012=(Bday2012-Birth_Date)/365.25;", 
            "title": "DATE Function"
        }, 
        {
            "location": "/essentials/manipulation/#catx-concatenation-function", 
            "text": "1 CATX(   , First_Name, Last_Name)   The  CATX  function removes leading and trailing blanks, inserts delimiters, and returns a concatenated character string. In the code, you first specify a character string that is used as a delimiter between concatenated items.", 
            "title": "CATX Concatenation Function"
        }, 
        {
            "location": "/essentials/manipulation/#intck-time-interval-function", 
            "text": "1 INTCK( year , Hire_Date,  01JAN2012 d)   The  INTCK  function returns the number of interval boundaries of a given kind that lie between the two dates, times, or datetime values. In the code, you first specify the interval value.   What happens when you define a new variable from another that you are gonna  DROP  in this DATA statement?  The  DROP  statement is a compile-time-only statement. SAS sets a drop flag for the dropped variables, but the variables are in the PDV and, therefore, are available for processing.", 
            "title": "INTCK Time Interval Function"
        }, 
        {
            "location": "/essentials/manipulation/#conditional-processing", 
            "text": "", 
            "title": "Conditional Processing"
        }, 
        {
            "location": "/essentials/manipulation/#if-then-else-conditional-structures", 
            "text": "1\n2\n3 IF expression THEN statement;\nELSE IF expression THEN statement;\nELSE statement;   In the conditional expressions involving strings watch out for possible mixed case values where the condition may not be met:   country = UPCASE(country);  to avoid problems   What do you mean 0.73 doesn't equal 0.73?  When comparing numeric values, you may get unexpected results when the values of your variables seem to be the same (but actually they are not). The key to making sure these issues do not introduce erroneous results lies in understanding how numeric values are actually stored in data sets and in memory. This is referred to as  numeric representation and precision . Read this article for more information:  Numeric Representation and Precision in SAS and Why it Matters .\nMain solutions to this problem:   The  ROUND function:  A = round(A, 0.01);  Character versions of numeric variables:  CharA = put(A, best.);  Options in procedures:  PROC COMPARE BASE=baseds COMPARE=compds CRITERION=0.0000001;", 
            "title": "IF-THEN-ELSE Conditional Structures"
        }, 
        {
            "location": "/essentials/manipulation/#executing-multiple-statements", 
            "text": "1\n2\n3\n4\n5\n6\n7\n8 IF expression THEN\n    DO;\n        executable statements;\n    END;\nELSE IF expression THEN\n    DO;\n        executable statements;\n    END;    In the  DATA  step, the first reference to a variable determines its length. The first reference to a new variable can be in a  LENGTH  statement, an  assignment  statement, or  another  statement such as an  INPUT  statement. After a variable is created in the PDV, the length of the variable's first value doesn't matter.   To avoid truncation in a variable defined inside a conditional structure you can:   Define the longer string as the first condition  Add some blanks at the end of shorter strings to fit the longer one  Define the length explicitly before any other reference to the variable", 
            "title": "Executing Multiple Statements"
        }, 
        {
            "location": "/essentials/manipulation/#select-group", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 SELECT(Gender);\n      WHEN( F ) DO;\n         Gift1= Scarf ;\n         Gift2= Pedometer ;\n      END;\n      WHEN( M ) DO;\n         Gift1= Gloves ;\n         Gift2= Money Clip ;\n      END;\n      OTHERWISE DO;\n         Gift1= Coffee ;\n         Gift2= Calendar ;\n      END;\nEND;    The  SELECT  statement executes one of several statements or groups of statements  The  SELECT  statement begins a  SELECT  group. They contain  WHEN  statements that identify SAS statements that are executed when a particular condition is true  Use at least one  WHEN  statement in a  SELECT  group  An optional  OTHERWISE  statement specifies a statement to be executed if no  WHEN  condition is met  An  END  statement ends a  SELECT  group", 
            "title": "SELECT Group"
        }, 
        {
            "location": "/essentials/manipulation/#avoiding-duplicates", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 * Period has more than one register per patient and the calculation of time periods between each one and the reference;\nPROC SORT DATA=period;\n    BY pt periodmax periodvisit;\nRUN;\n\n* Keep only highest (last value after the sorting);\nDATA maxperiod;\n    SET period;\n    BY pt;\n    IF last.pt;\nRUN;", 
            "title": "Avoiding Duplicates"
        }, 
        {
            "location": "/essentials/combination/", 
            "text": "Chapter summary in SAS\n\n\nConcatenating Data Sets (Vertical Combination)\n\n\n1\n2\n3\nDATA SAS-data-set;\n    SET SAS-data-set1 SAS-data-set2 ...;\nRUN;\n\n\n\n\n\n\nCombine two different variables that are actually the same one\n\n\n1\n2\n3\nDATA SAS-data-set;\n    SET SAS-data-set1 (RENAME=(old-name1 = new-name1 old-name2 = new-name2)) SAS-data-set2 ...;\nRUN;\n\n\n\n\n\n\n\n\nThe name change affects the PDV and the output data set, but has no effect on the input data set\n\n\nThe \nvariable attributes\n are assigned from the \nfirst data set\n in the \nSET\n statement\n\n\nYou will get an \nerror\n in the \nDATA\n step if a variable is defined with \ndifferent data types\n in the files that you are trying to concatenate\n\n\n\n\nMerging SAS Data Sets (Horizontal Combination)\n\n\n\n\nIn a \none-to-one\n relationship, a single observation in one data set is related to one, and only one, observation in another data set based on the values of one or more common variables\n\n\nIn a \none-to-many\n relationship, a single observation in one data set is related to one or more observations in another data set\n\n\nIn a \nmany-to-one\n relationship, multiple observations in one data set are related to one observation in another data set\n\n\nIn a \nmany-to-many\n relationship, multiple observations in one data set are related to multiple observations in another data set\n\n\nSometimes the data sets have \nnon-matches\n: at least one observation in one of the data sets is unrelated to any observation in another data set based on the values of one or more common variables\n\n\n\n\nMerging SAS Data Sets One-to-One\n\n\n\n\nThe \nmatch-merging\n is a process based on the values of common variables\n\n\nData sets are merged in the order that they appear in the MERGE statement\n\n\nYou may need to \nSORT\n the files by the \nBY-variable(s)\n before merging the files\n\n\n\n\n1\n2\n3\n4\n5\nDATA SAS-data-set;\n    MERGE SAS-data-set1 (RENAME=(old-name1 = new-name1 ...)) SAS-data-set2 ...;\n    BY \nDESCENDING\n BY-variable(s);\n    \nadditional SAS statements\n\nRUN;\n\n\n\n\n\n\nMerging SAS Data Sets One-to-Many\n\n\n1\n2\n3\n4\n5\nDATA SAS-data-set;\n    MERGE SAS-data-set1 SAS-data-set2 ...;\n    BY \nDESCENDING\n BY-variable(s);\n    \nadditional SAS statements\n\nRUN;\n\n\n\n\n\n\n\n\nPerforming a merge without a \nBY\n statement merges the observations based on their positions, this is almost never done intentionally and can lead to unexpected results\n\n\nMERGENOBY\n (\n= NOWARN (default) | WARN | ERROR\n) controls whether a message is issued when \nMERGE\n processing occurs without an associated \nBY\n statement\n\n\nWhen you reverse the order of the data sets in the \nMERGE\n statement, the results are the same, but the order of the variables is different. SAS performs a \nmany-to-one merge\n\n\n\n\n\n\nOne-line-to-Many Merge\n\n\nIf you have a data set that is just one line that you would like to joint to all the observations of a different data set you can do it in a single \nDATA\n step:\n\n\nDATA result-data-set;\nSET multiple-observations-data-set;\nIF _N_ EQ 1 THEN DO;\n    SET single-line-data-set;\nEND;\nRUN;\n\n\nIf you want to perform a cross join which is functionally the same as a Cartesian product join instead you can do it in a single \nPROC SQL\n:\n\n\nPROC SQL;\nCREATE TABLE result-data-set AS SELECT var1, var2 FROM data-set1 AS alias1 CROSS JOIN data-set2 AS alias2;\nQUIT;\n\n\n\n\nMerging SAS Data Sets that Have Non-Matches\n\n\n1\n2\n3\n4\n5\nDATA SAS-data-set;\n    MERGE SAS-data-set1 SAS-data-set2 ...;\n    BY \nDESCENDING\n BY-variable(s);\n    \nadditional SAS statements\n\nRUN;\n\n\n\n\n\n\n\n\nAfter the merging, the output data set contains \nboth matches and non-matches\n\n\nYou want the new data set to contain only the observations that match across the input data sets, and not those ones that are missing in one of the data sets that you are merging\n\n\n\n\n1\n2\n3\n4\n5\n6\nDATA SAS-data-set;\n    MERGE SAS-data-set1 (IN=variable1) \n          SAS-data-set2 (IN=variable2) ...;\n    BY \nDESCENDING\n BY-variable(s);\n    \nadditional SAS statements\n\nRUN;\n\n\n\n\n\n\nWhen you specify the \nIN\n option after an input data set in the \nMERGE\n statement, SAS creates a \ntemporary numeric variable\n that indicates whether the data set contributed data to the current observation (0 = it did not contribute to the current observation, 1 = it did contribute). These variables are only available \nduring execution\n.\n\n\n1\n2\n3\n4\n5\n6\n7\nDATA SAS-data-set;\n    MERGE SAS-data-set1 (IN=variable1) \n          SAS-data-set2 (IN=variable2) ...;\n    BY \nDESCENDING\n BY-variable(s);\n    IF variable1 = 1 and variable2 = 1;     /* write only matches */\n    \nadditional SAS statements\n\nRUN;\n\n\n\n\n\n\n\n\nMatches\n\n\n\n\n1\n2\nIF variable1 = 1 and variable2 = 1 \nIF variable1 and variable2\n\n\n\n\n\n\n\n\nNon-matches from either data set\n\n\n\n\n1\n2\nIF variable1 = 0 or not variable2 = 0\nIF not variable1 or not variable2\n\n\n\n\n\n\nE.g.:\n\n\n1\n2\n3\n4\n5\n6\n7\nDATA SAS-new-data-set1 SAS-new-data-set2;\n    MERGE SAS-data-set1 (in=var1) SAS-data-set2 (in=var2);\n    BY BY-variable(s);\n    IF var2 THEN OUTPUT SAS-new-data-set1;\n    ELSE IF var1 and not var2 THEN OUTPUT SAS-new-data-set2;\n    KEEP variable1 variable2 variable5 variable8;\nRUN;\n\n\n\n\n\n\nMerging SAS Data Sets Many-to-Many\n\n\nWith the macros \nmakewide.sas\n and \nmakelong.sas\n you can \n\n\n\n\nMake one of your data sets wide\n\n\nPerform a one-to-many merge with the other data set\n\n\nMake your resultant data set long to obtain the required result", 
            "title": "Combining SAS Data Sets"
        }, 
        {
            "location": "/essentials/combination/#concatenating-data-sets-vertical-combination", 
            "text": "1\n2\n3 DATA SAS-data-set;\n    SET SAS-data-set1 SAS-data-set2 ...;\nRUN;   Combine two different variables that are actually the same one  1\n2\n3 DATA SAS-data-set;\n    SET SAS-data-set1 (RENAME=(old-name1 = new-name1 old-name2 = new-name2)) SAS-data-set2 ...;\nRUN;    The name change affects the PDV and the output data set, but has no effect on the input data set  The  variable attributes  are assigned from the  first data set  in the  SET  statement  You will get an  error  in the  DATA  step if a variable is defined with  different data types  in the files that you are trying to concatenate", 
            "title": "Concatenating Data Sets (Vertical Combination)"
        }, 
        {
            "location": "/essentials/combination/#merging-sas-data-sets-horizontal-combination", 
            "text": "In a  one-to-one  relationship, a single observation in one data set is related to one, and only one, observation in another data set based on the values of one or more common variables  In a  one-to-many  relationship, a single observation in one data set is related to one or more observations in another data set  In a  many-to-one  relationship, multiple observations in one data set are related to one observation in another data set  In a  many-to-many  relationship, multiple observations in one data set are related to multiple observations in another data set  Sometimes the data sets have  non-matches : at least one observation in one of the data sets is unrelated to any observation in another data set based on the values of one or more common variables", 
            "title": "Merging SAS Data Sets (Horizontal Combination)"
        }, 
        {
            "location": "/essentials/combination/#merging-sas-data-sets-one-to-one", 
            "text": "The  match-merging  is a process based on the values of common variables  Data sets are merged in the order that they appear in the MERGE statement  You may need to  SORT  the files by the  BY-variable(s)  before merging the files   1\n2\n3\n4\n5 DATA SAS-data-set;\n    MERGE SAS-data-set1 (RENAME=(old-name1 = new-name1 ...)) SAS-data-set2 ...;\n    BY  DESCENDING  BY-variable(s);\n     additional SAS statements \nRUN;", 
            "title": "Merging SAS Data Sets One-to-One"
        }, 
        {
            "location": "/essentials/combination/#merging-sas-data-sets-one-to-many", 
            "text": "1\n2\n3\n4\n5 DATA SAS-data-set;\n    MERGE SAS-data-set1 SAS-data-set2 ...;\n    BY  DESCENDING  BY-variable(s);\n     additional SAS statements \nRUN;    Performing a merge without a  BY  statement merges the observations based on their positions, this is almost never done intentionally and can lead to unexpected results  MERGENOBY  ( = NOWARN (default) | WARN | ERROR ) controls whether a message is issued when  MERGE  processing occurs without an associated  BY  statement  When you reverse the order of the data sets in the  MERGE  statement, the results are the same, but the order of the variables is different. SAS performs a  many-to-one merge    One-line-to-Many Merge  If you have a data set that is just one line that you would like to joint to all the observations of a different data set you can do it in a single  DATA  step:  DATA result-data-set;\nSET multiple-observations-data-set;\nIF _N_ EQ 1 THEN DO;\n    SET single-line-data-set;\nEND;\nRUN;  If you want to perform a cross join which is functionally the same as a Cartesian product join instead you can do it in a single  PROC SQL :  PROC SQL;\nCREATE TABLE result-data-set AS SELECT var1, var2 FROM data-set1 AS alias1 CROSS JOIN data-set2 AS alias2;\nQUIT;", 
            "title": "Merging SAS Data Sets One-to-Many"
        }, 
        {
            "location": "/essentials/combination/#merging-sas-data-sets-that-have-non-matches", 
            "text": "1\n2\n3\n4\n5 DATA SAS-data-set;\n    MERGE SAS-data-set1 SAS-data-set2 ...;\n    BY  DESCENDING  BY-variable(s);\n     additional SAS statements \nRUN;    After the merging, the output data set contains  both matches and non-matches  You want the new data set to contain only the observations that match across the input data sets, and not those ones that are missing in one of the data sets that you are merging   1\n2\n3\n4\n5\n6 DATA SAS-data-set;\n    MERGE SAS-data-set1 (IN=variable1) \n          SAS-data-set2 (IN=variable2) ...;\n    BY  DESCENDING  BY-variable(s);\n     additional SAS statements \nRUN;   When you specify the  IN  option after an input data set in the  MERGE  statement, SAS creates a  temporary numeric variable  that indicates whether the data set contributed data to the current observation (0 = it did not contribute to the current observation, 1 = it did contribute). These variables are only available  during execution .  1\n2\n3\n4\n5\n6\n7 DATA SAS-data-set;\n    MERGE SAS-data-set1 (IN=variable1) \n          SAS-data-set2 (IN=variable2) ...;\n    BY  DESCENDING  BY-variable(s);\n    IF variable1 = 1 and variable2 = 1;     /* write only matches */\n     additional SAS statements \nRUN;    Matches   1\n2 IF variable1 = 1 and variable2 = 1 \nIF variable1 and variable2    Non-matches from either data set   1\n2 IF variable1 = 0 or not variable2 = 0\nIF not variable1 or not variable2   E.g.:  1\n2\n3\n4\n5\n6\n7 DATA SAS-new-data-set1 SAS-new-data-set2;\n    MERGE SAS-data-set1 (in=var1) SAS-data-set2 (in=var2);\n    BY BY-variable(s);\n    IF var2 THEN OUTPUT SAS-new-data-set1;\n    ELSE IF var1 and not var2 THEN OUTPUT SAS-new-data-set2;\n    KEEP variable1 variable2 variable5 variable8;\nRUN;", 
            "title": "Merging SAS Data Sets that Have Non-Matches"
        }, 
        {
            "location": "/essentials/combination/#merging-sas-data-sets-many-to-many", 
            "text": "With the macros  makewide.sas  and  makelong.sas  you can    Make one of your data sets wide  Perform a one-to-many merge with the other data set  Make your resultant data set long to obtain the required result", 
            "title": "Merging SAS Data Sets Many-to-Many"
        }, 
        {
            "location": "/essentials/summary/", 
            "text": "Chapter summary in SAS\n\n\nPROC FREQ\n to Create Summary Reports\n\n\n\n\nWhen you're summarizing data, there's no need to show a frequency distribution for variables that have a large number of distinct values\n\n\nFrequency distributions work best with variables whose values meet two criteria: variable with \ncategorical values\n and values are \nbest summarized by counts instead of averages\n\n\nVariables that have continuous numerical values, such as dollar amounts and dates, will need to be \ngrouped into categories\n by \napplying formats\n inside the \nPROC FREQ\n step (substitute an specific range of those values by a tag)\n\n\n\n\n1\n2\n3\n4\nPROC FREQ DATA=SAS-data-set \noption\n(s)\n;\n    TABLE(S) variable(s) \n/ option(s)\n;\n    \nadditional\n \nstatements\n\nRUN;\n\n\n\n\n\n\n\n\nPROC FREQ\n produces frequency tables that report the distribution of any or all variable values in a SAS data set\n\n\nIn the \nTABLE(S)\n statement you specify the frequency tables to produce \n\n\nEach unique variable's value displayed in the 1\nst\n column of the output is called a \nlevel of the variable\n\n\n\n\nOne-way Frequency Tables\n\n\nTo create \none-way\n frequency tables you specify one or more variable names separated by space.\n\n\n\n\nWarning\n\n\nIf you omit the \nTABLE\n statement, SAS produces a one-way tables for every variable in the data set, which could be very messy if you have a lot of variables.\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nPROC SORT DATA=SAS-data-set\n    OUT=SAS-data-set-sorted;\n    BY variable_sorted;\nRUN;\n\nPROC FREQ DATA=SAS-data-set-sorted ORDER=FREQ \noption(s)\n;\n    TABLES variable / NOCUM NOPERCENT OUT=custom-output-name;\n    BY variable_sorted;\n    \nadditional statements\n\nRUN;\n\n\n\n\n\n\n\n\nNOCUM\n option supresses the display of  the cummulative frequency and cummulative percent values \n\n\nNOPERCENT\n option supresses the display of all percentages\n\n\nORDER=FREQ\n option orders the output in descending frequency order\n\n\nOUT=\n option saves the output data set with a custom name\n\n\nBY\n option produces a frequency table for each value of \nvariable_sorted\n (the data set must be sorted by the variable named in the statement)\n\n\n\n\nCrosstabulation Tables\n\n\n\n\nSometimes it is useful to view a single table with statistics for each distintic combination of values of the selected variables\n\n\nThe simplest crosstabulation table is a \ntwo-way table\n\n\n\n\n1\n2\n3\nPROC FREQ DATA=SAS-data-set;\n    TABLES variable_rows * variable_columns / NOFREQ NOPERCENT NOROW NOCOL;\nRUN;\n\n\n\n\n\n\nInformation contained in crosstabulation tables (legend):\n\n\n\n\nFrequency\n: indicates the number of observations with the unique combination of values represented in that cell\n\n\nPercent\n: indicates the cell's percentage of the total frequency\n\n\nRow Pct\n: cell's percentage of the total frequency for its row\n\n\nCol Pct\n: cell's percentage of the total frequency for its column \n\n\n\nLIST\n option format: the first two columns specify each possible combination of the two variables; it displays the same statistics as the default \none-way frequency\n table\n\n\nCROSSLIST\n option format: it displays the same statistics as the default \ncrosstabulation\n table\n\n\n\n\nFormatting Variables in \nPROC FREQ\n\n\nThe \nFORMAT=\n option allows you to format the frequency value (to any SAS numeric format or a user-defined numeric format while its length is not more than 24) and to change the width of the column (e.g. to allow variable labels to fit in one line). \n\n\n1\n2\n3\n4\n5\nPROC FREQ DATA=SAS-data-set;\n    TABLES variable1 * variable2 /\n    FORMAT = \nw\n.;\n    FORMAT variable1 $format-name.;    \nRUN;\n\n\n\n\n\n\nThe \nFORMAT=\n option applies only to crosstabulation tables displayed in the default format. It doesn't apply to crosstabulation tables produced with the \nLIST\n/\nCROSSLIST\n option.\n\n\nUsing the \nMEANS\n and \nUNIVARIATE\n Procedures\n\n\nPROC MEANS\n produces summary reports with descriptive statistics and you can create statistics for groups of observations\n\n\n\n\nIt automatically displays output in a report and you can also save the output in a SAS data set\n\n\nIt reports the \nnumber of nonmissing values\n of the analysis variable (N), and the \nmean\n, the \nstandard deviation\n and \nminimum\n/\nmaximum values\n of every numeric variable in the data set\n\n\nThe variables in the \nCLASS\n statement are called \nclassification variables\n or \nclass variables\n (they typically have few discrete values)\n\n\nEach combination of class variable values is called a \nclass level\n\n\nThe data set \ndoesn't need to be sorted\n or indexed by the class variables\n\n\nN Obs\n reports the number of observations with each unique combination of class variables, whether or not there are missing values (if these \nN Obs\n are identical to \nN\n, there are no missing values in you data set)\n\n\n\n\n1\n2\n3\n4\nPROC MEANS DATA=SAS-data-set \nstatistic(s)\n;\n    VAR analysis-variable(s);\n    CLASS classification-variable(s);\nRUN;\n\n\n\n\n\n\nTo write the report in a new data set (including total addition):\n\n\n1\n2\n3\n4\n5\nPROC MEANS DATA=SAS-data-set NOPRINT NWAY;\n    OUTPUT OUT=SAS-new-data-set SUM=addition-new-variable;\n    VAR analysis-variable(s);\n    CLASS classification-variable(s);\nRUN;\n\n\n\n\n\n\nFormat options: \n\n\n\n\nMAXDEC=number\n (default format = \nBESTw.\n) \n\n\nNONOBS\n\n\nFW=number\n: specifies that the field width for all columns is \nnumber\n\n\nPRINTALLTYPES\n: displays statistics for all requested combination of class variables\n\n\n\n\n\n\n\n\n\n\nSums and counts by group\n\n\nODS EXCLUDE ALL;\nPROC MEANS DATA=SAS-data-set SUM;\n  CLASS classification-variable;\n  VAR var1 var2 var3 var4;\n  ODS OUTPUT SUMMARY=name-output-data-set;\nRUN;\nODS EXCLUDE NONE;\n\n\n\n\nHow to Use these Procedures for Data Validation\n\n\nPROC FREQ\n\n\nYou can use a \nPROC FREQ\n step with the \nTABLES\n statement to detect invalud numeric and character data by looking at distinct values. The \nFREQ\n procedure \nlists all discrete values\n for a variable and \nreports its missing values\n.\n\n\n1\n2\n3\nPROC FREQ DATA=SAS-data-set \nORDER=FREQ\n;\n    TABLES variable;\nRUN;\n\n\n\n\n\n\n\n\nYou can check for non-expected variable's values\n\n\nYou can check for missing values\n\n\nYou can find duplicated values\n\n\n\n\n\n\nThe table showing the \nNumber of Variable Levels\n can indicate whether a variable contains duplicate/missing/non-expected values:\n\n\n1\n2\n3\nPROC FREQ DATA=SAS-data-set NLEVELS;\n    TABLES variable / NOPRINT;\nRUN;\n\n\n\n\n\n\n\n\nYou can use a \nWHERE\n statement to print out only the invalid values to be checked:\n\n\n1\n2\n3\n4\n5\n6\nPROC PRINT DATA=SAS-data-set;\n    WHERE gender NOT IN (\nF\n,\nM\n) OR\n          job_title IS NULL OR\n          salary NOT BETWEEN 24000 AND 500000 OR\n          employee IS MISSING;\nRUN;\n\n\n\n\n\n\n\n\nYou can output the tables to a new data set instead of displaying it:\n\n\n1\n2\n3\nPROC FREQ DATA=SAS-data-set NOPRINT;\n   TABLE variable / OUT=SAS-new-data-set;\nRUN;\n\n\n\n\n\n\nPROC MEANS\n\n\n\n\nNMISS\n option displays the number of observations with missing values\n\n\nThe \nMIN\n/\nMAX\n values can be useful to check if the data is within a range\n\n\n\n\nTo get the \nMIN\n/\nMAX\n values per group and the number of observations per group follow the next example:\n\n\n1\n2\n3\n4\n5\nPROC MEANS DATA=SAS-data-set MIN MAX;\n    CLASS grouping-variable;\n    VAR evaluation-variable;\n    ODS OUTPUT SUMMARY=SAS-output-data-set;\nRUN;\n\n\n\n\n\n\nPROC UNIVARIATE\n\n\nPROC UNIVARIATE\n is a procedure that is useful for detecting data outliers that also produces summary reports of \ndescriptive statistics\n.\n\n\n1\n2\n3\n4\n5\n6\n7\nPROC UNIVARIATE DATA=SAS-data-set;\n    VAR variable(s);\n    ID variable_to_relate;\n    HISTOGRAM variables \n/options\n;\n    PROBPLOT variables \n/options\n;\n    INSET keywords \n/options\n;\nRUN;\n\n\n\n\n\n\n\n\nIf you omit the \nVAR\n statement, all numeric variables in the data set are analyzed\n\n\nThe \nExtreme Observations\n table contains useful information to locate outliers: it displays the 5 lowest/highest values by default along with the corresponding observation number. The \nID\n statement specifies that SAS will use this variable as a label in the table of extreme observations and as an identifier for any extreme.\n\n\nTo specify the number of listed observations you can use \nNEXTROBS=\n\n\nHISTOGRAM\n/\nPROBPLOT\n options: \nnormal(mu=est sigma=est)\n creates a normal curve overlay to the histogram using the estimates of the population mean and standard deviation\n\n\nINSET\n writes a legend for the graph. \n/ position=ne\n moves the \nINSET\n to the north-east corner of the graph.\n\n\n\n\nTo include in the report only one of the automatically produced tables:\n\n\n\n\nCheck the specific table name in the \nLOG information\n using \nODS TRACE ON\n:\n\n\n\n\n1\n2\n3\n4\n5\nODS TRACE ON;\nPROC UNIVARIATE DATA=SAS-data-set;\n    VAR variable(s);\nRUN;\nODS TRACE OFF;\n\n\n\n\n\n\n\n\nSelect the wanted table with \nODS SELECT\n:\n\n\n\n\n1\n2\n3\n4\nODS SELECT ExtremeObs;\nPROC UNIVARIATE DATA=SAS-data-set;\n    VAR variable(s);\nRUN;\n\n\n\n\n\n\nSummary of Validation Procedures\n\n\n\n\nUsing the SAS Output Delivery System (\nODS\n)\n\n\n1\n2\n3\nODS destination FILE=\nfilename\n \noptions\n;\n    \nSAS code to generate the report\n\nODS destination CLOSE;\n\n\n\n\n\n\n\n\nYou can have multiple destinations open and execute multiple procedures\n\n\nAll generated output will be sent to every open destination\n\n\nYou might not be able to view the file, or the most updated file, outside of SAS until you close the destination\n\n\n\n\nE.g.:\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nODS\n \npdf\n \nFILE\n=\nC:/output/test.pdf\n;\n\n\n(...)\n\n\nODS\n \npdf\n \nCLOSE\n;\n\n\n\nODS\n \ncsvall\n \nFILE\n=\nC:/output/test.cvs\n;\n\n\nODS\n \nrtf\n \nFILE\n=\nC:/output/test.rtf\n;\n\n\n(...)\n\n\nODS\n \ncsvall\n \nCLOSE\n;\n\n\nODS\n \nrtf\n \nCLOSE\n;\n\n\n\n\n\n\n\nAllowed File Formats and Their Corresponding Destinations\n\n\n\n\nYou can also export a database to a different format:\n\n\n\n\nExport to \n*.csv\n:\n\n\n\n\n1\n2\n3\n4\n5\nPROC EXPORT DATA=sashelp.class\n    OUTFILE=\nc:\\temp\\sashelp class.csv\n\n    DBMS=CSV\n    REPLACE;\nRUN;\n\n\n\n\n\n\n\n\nExport to \n*.dat\n:\n\n\n\n\n1\n2\n3\n4\n5\ndata\n \n_null_\n;\n\n    \nset\n \nlibrary\n.\nSAS-data-set\n;\n\n    \nfile\n \nC:\\your-custom-path\\your-file-name.dat\n;\n\n    \nput\n \nvariable1\n \nvariable2\n \nvariable3\n;\n\n\nrun\n;", 
            "title": "Creating Summary Reports"
        }, 
        {
            "location": "/essentials/summary/#proc-freq-to-create-summary-reports", 
            "text": "When you're summarizing data, there's no need to show a frequency distribution for variables that have a large number of distinct values  Frequency distributions work best with variables whose values meet two criteria: variable with  categorical values  and values are  best summarized by counts instead of averages  Variables that have continuous numerical values, such as dollar amounts and dates, will need to be  grouped into categories  by  applying formats  inside the  PROC FREQ  step (substitute an specific range of those values by a tag)   1\n2\n3\n4 PROC FREQ DATA=SAS-data-set  option (s) ;\n    TABLE(S) variable(s)  / option(s) ;\n     additional   statements \nRUN;    PROC FREQ  produces frequency tables that report the distribution of any or all variable values in a SAS data set  In the  TABLE(S)  statement you specify the frequency tables to produce   Each unique variable's value displayed in the 1 st  column of the output is called a  level of the variable", 
            "title": "PROC FREQ to Create Summary Reports"
        }, 
        {
            "location": "/essentials/summary/#one-way-frequency-tables", 
            "text": "To create  one-way  frequency tables you specify one or more variable names separated by space.   Warning  If you omit the  TABLE  statement, SAS produces a one-way tables for every variable in the data set, which could be very messy if you have a lot of variables.    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 PROC SORT DATA=SAS-data-set\n    OUT=SAS-data-set-sorted;\n    BY variable_sorted;\nRUN;\n\nPROC FREQ DATA=SAS-data-set-sorted ORDER=FREQ  option(s) ;\n    TABLES variable / NOCUM NOPERCENT OUT=custom-output-name;\n    BY variable_sorted;\n     additional statements \nRUN;    NOCUM  option supresses the display of  the cummulative frequency and cummulative percent values   NOPERCENT  option supresses the display of all percentages  ORDER=FREQ  option orders the output in descending frequency order  OUT=  option saves the output data set with a custom name  BY  option produces a frequency table for each value of  variable_sorted  (the data set must be sorted by the variable named in the statement)", 
            "title": "One-way Frequency Tables"
        }, 
        {
            "location": "/essentials/summary/#crosstabulation-tables", 
            "text": "Sometimes it is useful to view a single table with statistics for each distintic combination of values of the selected variables  The simplest crosstabulation table is a  two-way table   1\n2\n3 PROC FREQ DATA=SAS-data-set;\n    TABLES variable_rows * variable_columns / NOFREQ NOPERCENT NOROW NOCOL;\nRUN;   Information contained in crosstabulation tables (legend):   Frequency : indicates the number of observations with the unique combination of values represented in that cell  Percent : indicates the cell's percentage of the total frequency  Row Pct : cell's percentage of the total frequency for its row  Col Pct : cell's percentage of the total frequency for its column   LIST  option format: the first two columns specify each possible combination of the two variables; it displays the same statistics as the default  one-way frequency  table  CROSSLIST  option format: it displays the same statistics as the default  crosstabulation  table", 
            "title": "Crosstabulation Tables"
        }, 
        {
            "location": "/essentials/summary/#formatting-variables-in-proc-freq", 
            "text": "The  FORMAT=  option allows you to format the frequency value (to any SAS numeric format or a user-defined numeric format while its length is not more than 24) and to change the width of the column (e.g. to allow variable labels to fit in one line).   1\n2\n3\n4\n5 PROC FREQ DATA=SAS-data-set;\n    TABLES variable1 * variable2 /\n    FORMAT =  w .;\n    FORMAT variable1 $format-name.;    \nRUN;   The  FORMAT=  option applies only to crosstabulation tables displayed in the default format. It doesn't apply to crosstabulation tables produced with the  LIST / CROSSLIST  option.", 
            "title": "Formatting Variables in PROC FREQ"
        }, 
        {
            "location": "/essentials/summary/#using-the-means-and-univariate-procedures", 
            "text": "PROC MEANS  produces summary reports with descriptive statistics and you can create statistics for groups of observations   It automatically displays output in a report and you can also save the output in a SAS data set  It reports the  number of nonmissing values  of the analysis variable (N), and the  mean , the  standard deviation  and  minimum / maximum values  of every numeric variable in the data set  The variables in the  CLASS  statement are called  classification variables  or  class variables  (they typically have few discrete values)  Each combination of class variable values is called a  class level  The data set  doesn't need to be sorted  or indexed by the class variables  N Obs  reports the number of observations with each unique combination of class variables, whether or not there are missing values (if these  N Obs  are identical to  N , there are no missing values in you data set)   1\n2\n3\n4 PROC MEANS DATA=SAS-data-set  statistic(s) ;\n    VAR analysis-variable(s);\n    CLASS classification-variable(s);\nRUN;   To write the report in a new data set (including total addition):  1\n2\n3\n4\n5 PROC MEANS DATA=SAS-data-set NOPRINT NWAY;\n    OUTPUT OUT=SAS-new-data-set SUM=addition-new-variable;\n    VAR analysis-variable(s);\n    CLASS classification-variable(s);\nRUN;   Format options:    MAXDEC=number  (default format =  BESTw. )   NONOBS  FW=number : specifies that the field width for all columns is  number  PRINTALLTYPES : displays statistics for all requested combination of class variables      Sums and counts by group  ODS EXCLUDE ALL;\nPROC MEANS DATA=SAS-data-set SUM;\n  CLASS classification-variable;\n  VAR var1 var2 var3 var4;\n  ODS OUTPUT SUMMARY=name-output-data-set;\nRUN;\nODS EXCLUDE NONE;", 
            "title": "Using the MEANS and UNIVARIATE Procedures"
        }, 
        {
            "location": "/essentials/summary/#how-to-use-these-procedures-for-data-validation", 
            "text": "", 
            "title": "How to Use these Procedures for Data Validation"
        }, 
        {
            "location": "/essentials/summary/#proc-freq", 
            "text": "You can use a  PROC FREQ  step with the  TABLES  statement to detect invalud numeric and character data by looking at distinct values. The  FREQ  procedure  lists all discrete values  for a variable and  reports its missing values .  1\n2\n3 PROC FREQ DATA=SAS-data-set  ORDER=FREQ ;\n    TABLES variable;\nRUN;    You can check for non-expected variable's values  You can check for missing values  You can find duplicated values    The table showing the  Number of Variable Levels  can indicate whether a variable contains duplicate/missing/non-expected values:  1\n2\n3 PROC FREQ DATA=SAS-data-set NLEVELS;\n    TABLES variable / NOPRINT;\nRUN;    You can use a  WHERE  statement to print out only the invalid values to be checked:  1\n2\n3\n4\n5\n6 PROC PRINT DATA=SAS-data-set;\n    WHERE gender NOT IN ( F , M ) OR\n          job_title IS NULL OR\n          salary NOT BETWEEN 24000 AND 500000 OR\n          employee IS MISSING;\nRUN;    You can output the tables to a new data set instead of displaying it:  1\n2\n3 PROC FREQ DATA=SAS-data-set NOPRINT;\n   TABLE variable / OUT=SAS-new-data-set;\nRUN;", 
            "title": "PROC FREQ"
        }, 
        {
            "location": "/essentials/summary/#proc-means", 
            "text": "NMISS  option displays the number of observations with missing values  The  MIN / MAX  values can be useful to check if the data is within a range   To get the  MIN / MAX  values per group and the number of observations per group follow the next example:  1\n2\n3\n4\n5 PROC MEANS DATA=SAS-data-set MIN MAX;\n    CLASS grouping-variable;\n    VAR evaluation-variable;\n    ODS OUTPUT SUMMARY=SAS-output-data-set;\nRUN;", 
            "title": "PROC MEANS"
        }, 
        {
            "location": "/essentials/summary/#proc-univariate", 
            "text": "PROC UNIVARIATE  is a procedure that is useful for detecting data outliers that also produces summary reports of  descriptive statistics .  1\n2\n3\n4\n5\n6\n7 PROC UNIVARIATE DATA=SAS-data-set;\n    VAR variable(s);\n    ID variable_to_relate;\n    HISTOGRAM variables  /options ;\n    PROBPLOT variables  /options ;\n    INSET keywords  /options ;\nRUN;    If you omit the  VAR  statement, all numeric variables in the data set are analyzed  The  Extreme Observations  table contains useful information to locate outliers: it displays the 5 lowest/highest values by default along with the corresponding observation number. The  ID  statement specifies that SAS will use this variable as a label in the table of extreme observations and as an identifier for any extreme.  To specify the number of listed observations you can use  NEXTROBS=  HISTOGRAM / PROBPLOT  options:  normal(mu=est sigma=est)  creates a normal curve overlay to the histogram using the estimates of the population mean and standard deviation  INSET  writes a legend for the graph.  / position=ne  moves the  INSET  to the north-east corner of the graph.   To include in the report only one of the automatically produced tables:   Check the specific table name in the  LOG information  using  ODS TRACE ON :   1\n2\n3\n4\n5 ODS TRACE ON;\nPROC UNIVARIATE DATA=SAS-data-set;\n    VAR variable(s);\nRUN;\nODS TRACE OFF;    Select the wanted table with  ODS SELECT :   1\n2\n3\n4 ODS SELECT ExtremeObs;\nPROC UNIVARIATE DATA=SAS-data-set;\n    VAR variable(s);\nRUN;", 
            "title": "PROC UNIVARIATE"
        }, 
        {
            "location": "/essentials/summary/#summary-of-validation-procedures", 
            "text": "", 
            "title": "Summary of Validation Procedures"
        }, 
        {
            "location": "/essentials/summary/#using-the-sas-output-delivery-system-ods", 
            "text": "1\n2\n3 ODS destination FILE= filename   options ;\n     SAS code to generate the report \nODS destination CLOSE;    You can have multiple destinations open and execute multiple procedures  All generated output will be sent to every open destination  You might not be able to view the file, or the most updated file, outside of SAS until you close the destination   E.g.:  1\n2\n3\n4\n5\n6\n7\n8\n9 ODS   pdf   FILE = C:/output/test.pdf ;  (...)  ODS   pdf   CLOSE ;  ODS   csvall   FILE = C:/output/test.cvs ;  ODS   rtf   FILE = C:/output/test.rtf ;  (...)  ODS   csvall   CLOSE ;  ODS   rtf   CLOSE ;", 
            "title": "Using the SAS Output Delivery System (ODS)"
        }, 
        {
            "location": "/essentials/summary/#allowed-file-formats-and-their-corresponding-destinations", 
            "text": "You can also export a database to a different format:   Export to  *.csv :   1\n2\n3\n4\n5 PROC EXPORT DATA=sashelp.class\n    OUTFILE= c:\\temp\\sashelp class.csv \n    DBMS=CSV\n    REPLACE;\nRUN;    Export to  *.dat :   1\n2\n3\n4\n5 data   _null_ ; \n     set   library . SAS-data-set ; \n     file   C:\\your-custom-path\\your-file-name.dat ; \n     put   variable1   variable2   variable3 ;  run ;", 
            "title": "Allowed File Formats and Their Corresponding Destinations"
        }, 
        {
            "location": "/statistics/introduction/", 
            "text": "Chapter summary in SAS\n\n\nBasic Statistical Concepts\n\n\nDescriptive statistics (exploratory data analysis, EDA)\n\n\n\n\nExplore your data\n\n\n\n\nInferential statistics (explanatory modelling)\n\n\n\n\nHow is X related to Y?\n\n\nSample sizes are typically small and include few variables\n\n\nThe focus is on the parameters of the model\n\n\nTo assess the model, you use p-values and confidence intervals\n\n\n\n\nPredictive modelling\n\n\n\n\nIf you know X, can you predict Y?\n\n\nSample sizes are large and include many predictive (input) variables\n\n\nThe focus is on the predictions of observations rather than the parameters of the model\n\n\nTo assess a predictive model, you validate predictions using holdout sample data\n\n\n\n\n\n\n\n\nParameters\n: numerical values (typically unknown, you can't measure the entire population) that summarize characteristics of a population (greek letters)\n\n\nStatistics\n: summarizes characteristics of a sample (standard alphabet)\n\n\nYou use \nstatistics\n to estimate \nparameters\n\n\n\n\n\n\n\n\nIndependent variable\n: it can take different values, it affects or determines a \ndependent variable\n. It can be called predictor, explanatory, control or input variable.\n\n\nDependent variable\n: it can take different values in response to an \nindependent variable\n. Also known as response, outcome or target variable.\n\n\n\n\n\n\nScale of measurement\n: variable's classification\n\n\n\n\nQuantitative/numerical variables\n: counts or measurements, you can perform arithmetical operations with it\n\n\nDiscrete data\n: variables that can have only a countable number of values within a measurement range\n\n\nContinuous data\n: variables that are measured on a scale that has infinite number of values and has no breaks or jumps\n\n\nInterval scale data\n: it can be rank-ordered like ordinal data but also has a sensible spacing of observations such that differenes between measurements are meaningful but it lacks a true zero (ratios are meaningless)\n\n\nRatio scale data\n: it is rank-ordered with meaningful spacing and also includes a true zero point and can therefore accurately indicate the ratio difference between two spaces on the measurement scale\n\n\n\n\n\n\n\n\n\n\nCategorical/attribute variables\n: variables that denote groupings or labels\n\n\nNominal data (qualitative/classification variable)\n: exhibits no ordering within its observed levels, groups or categories\n\n\nOrdinal data\n: the observed labels can be ordered in some meaningful way that implies that the differences between the groups or categories are due to magnitude\n\n\n\n\n\n\n\n\n\n\n\n\nUnivariate analysis\n provides techniques for analyzing and describing a sigle variable. It reveals patterns in the data by looking at the \nrange\n of values, measures of \ndispersion\n, the \ncentral tendecy\n of the values and \nfrequency distribution\n.\n\n\nBivariate analysis\n describes and explains the relationships between two variables and how they change or covary together. It include techniques such as \ncorrelation analysis\n and \nchi-square tests of independance\n.\n\n\nMultivariate/Multivariable analysis\n examines two or more variables at the same time in order to understand the relationships among them. \n\n\nTechniques such as \nmutiple linear regression\n and n-way \nANOVA\n are typically called \nmultivariable\n analysis (only one response variable). \n\n\nTechniques such as \nfactora analysis\n and \nclustering\n are typically called \nmutivariate\n analysis (they consider more than one response variable).\n\n\n\n\n\n\n\n\n\n\nDescriptive Statistics\n\n\nMeasures of central tendencies\n: mean (affected by outliers), median (less sensitive to outliers), mode\n\n\n\n\n\n\n\n\nPercentile\n\n\nQuartile\n\n\n\n\n\n\n\n\n\n\n\n\n25th\n\n\n1st / lower / Q1\n\n\n\n\n\n\n\n\n50th\n\n\n2nd / middle / Q2\n\n\nMedian\n\n\n\n\n\n\n75th\n\n\n3rd / upper / Q3\n\n\n\n\n\n\n\n\n\n\nThe \ninterquartile range (IQR)\n is the difference between Q1 and Q3, it is a \nrobust estimate of the variability\n because changes in the upper/lower 25% of the data do not affect it. If there are \noutliers\n in the data, then the IQR is a more reliable measure of the spread than the overall range.\n\n\nThe \ncoefficient of variation (CV)\n is a measure of the standard deviation expressed as a percentage of the mean ($c_v = \\sigma / 100 \\mu$).\n\n\n\n\nNormal distribution\n\n\n\n\n\n\n\n\nIntervals\n\n\nPercentage contained\n\n\n\n\n\n\n\n\n\n\n$\\mu \\pm \\sigma$\n\n\n68%\n\n\n\n\n\n\n$\\mu \\pm 2 \\sigma$\n\n\n95%\n\n\n\n\n\n\n$\\mu \\pm 3 \\sigma$\n\n\n99%\n\n\n\n\n\n\n\n\nHow to check the normality of a sample?\n\n\n\n\nCompare the \nmean\n and the \nmedian\n: if they are nearly equal, that is an indicator of symmetry (requirement for normality)\n\n\nCheck that \nskewness\n and \nkurtosis\n are close to 0:\n\n\nIf both are greater than 1 or less than -1: data is not normal\n\n\nIf either is greater than 2 or less than -2: data is not normal\n\n\n\n\n\n\n\n\nStatistical summaries\n \n\n\nSkewness\n and \nkurtosis\n measure certain aspects of the shape of a distribution (they are \n0\n and \n3\n for a normal distribution, although SAS has standardized both to 0)\n\n\n\n\nSkewness\n measures the tendency of your data to be more spread out on one side of the mean than on the other (asymmetry of the distribution). \n\n\nYou can think of the direction of skewness as the direction the data is trailing off to. \n\n\nA \nright-skewed\n distribution tells us that the mean is \ngreater than the median\n.\n\n\n\n\n\n\nKurtosis\n measures the tendency of your data to be concentrated toward the center or toward the tails of the distribution (peakedness of the data, tail thickness). \n\n\nA \nnegative kurtosis (platykurtic distribution)\n means that the data has lighter tails than in a normal distribution. \n\n\nA \npositive kurtosis (leptokurtic/heavy-tailed/outlier-prone distribution)\n means that the data has heavier tails and is more concentrated around the mean than a normal distribution.\n\n\nRectangular, bimodal and multimodal distributions tend to have low values of kurtosis.\n\n\nAsymmetric distributions\n also tend to have nonzero kurtosis. In these cases, understanding kurtosis is considerably more complex and can be difficult to assess visually.\n\n\n\n\n\n\n\n\n\n\nPROC SURVEYSELECT\n\n\nHow to generate random (representative) samples (population subsets):\n\n\n1\n2\n3\n4\n5\n6\n7\nPROC SURVEYSELECT DATA=SAS-data-set \n                  OUT=name-of-output-data-set\n                  METHOD=method-of-random-sampling\n                  SEED=seed-value \n                  SAMPSIZE=number-of-observations-desired;\n     \nSTRATA stratification-variable(s);\n\nRUN;\n\n\n\n\n\n\n\n\nMETHOD\n specifies the random sampling method to be used. For simple random sampling without replacement, use \nMETHOD=SRS\n. For simple random sampling with replacement, use \nMETHOD=URS\n. For other selection methods and details on sampling algorithms, see the SAS online documentation for \nPROC SURVEYSELECT\n.\n\n\nSEED\n specifies the initial seed for random number generation. If no \nSEED\n option is specified, SAS uses the system time as its seed value. This creates a different random sample every time the procedure is run.\n\n\nSAMPSIZE\n indicates the number of observations to be included in the sample. To select a certain fraction of the original data set rather than a given number of observations, use the \nSAMPRATE\n option.\n\n\n\n\nPicturing Your Data\n\n\nPROC UNIVARIATE\n\n\nPlots that can be produced with this procedure:\n\n\n\n\nHistograms\n\n\nNormal probability plots\n: expected percentiles from standard normal vs actual data values\n\n\n\n\n\n\nPROC SGSCATTER\n\n\nPlots that can be produced with this procedure:\n\n\n\n\nScatter plots\n: you can create a \nsingle-cell\n (simple Y by X) scatter plot, a \nmulti-cell\n scatter plot with multiple independent scatter plots in a grid and a \nscatter plot matrix\n, which produces a matrix of scatter plots comparing multiple variables.\n\n\n\n\nPROC SGPLOT\n\n\nPlots that can be produced with this procedure:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\nPROC SGPLOT DATA=SAS-data-set \noptions\n;\n        DOT category-variable \n/options\n;\n        HBAR category-variable \n/options\n;\n        VBAR category-variable \n/options\n;\n        HBOX response-variable \n/options\n;\n        VBOX response-variable \n/options\n;\n        HISTOGRAM response-variable \n/options\n;\n        SCATTER X=variable Y=variable \n/options\n;\n        NEEDLE X=variable Y=numeric-variable \n/options\n;\n        REG X=numeric-variable Y=numeric-variable \n/options\n;\nRUN;\n\n\n\n\n\n\nAnywhere in the procedure you can add \nreference lines\n:\n\n\n1\n2\n3\n4\nREFLINE variable | value-1 \n...\n \nvalue-n\n \n/option(s)\n\n\n/* Example: */\nREFLINE 1200 / axis=y lineattrs=(color=blue);\n\n\n\n\n\n\n\n\nNote\n\n\nThe order on which you define the parts of the plot will the determined the order on which it is displayed (if you want to send a \nREFLINE\n to the back, define it first)\n\n\n\n\n\n\nScatter plots (\nSCATTER\n)\n\n\nLine graphs\n\n\nHistograms (\nHISTOGRAM\n)\n with overlaid distribution curves\n\n\nRegression lines (\nREG\n)\n with confidence and prediction bands\n\n\nDot plots (\nDOT\n)\n\n\nBox plots (\nHBOX\n/\nVBOX\n)\n: it makes it easy to see how spread out your data is and if there are any outliers. The box represents the middle 50% of your data (IQR). The lower/middle/upper \nline of the box\n represent Q1/Q2/Q3. The \ndiamond\n denotes the mean (easy to check how close the mean is to the median). The \nwhiskers\n extend as far as the data extends to a maximum length of 1.5 times the IQR above Q3. Any data points farther than this distance are considered possible outliers and are represented in this plot as \ncircles\n.\n\n\nBar charts (\nHBAR\n/\nVBAR\n)\n\n\nNeedle plot (\nNEEDLE\n)\n: creates a plot with needles connecting each point to the baseline\n\n\nYou can also \noverlay plots together\n to produce many different types of graphs\n\n\n\n\nPROC SGPANEL\n\n\nPlots that can be produced with this procedure:\n\n\n\n\nPanels of plots\n for different levels of a factor or several different time periods depending on the classification variable\n\n\nSide-by-side histograms\n which provide a visual comparison for your data\n\n\n\n\nPROC SGRENDER\n\n\n\n\nPlots from graphs templates you have modified or written yourself\n\n\n\n\n\n\nTo specify options for graphs you submit the \nODS GRAPHICS\n statement:\n\n\n1\nODS GRAPHICS ON \noptions\n;\n\n\n\n\n\n\n\n\nTo select/exclude specific test results, graphs or tables from you output, you can use \nODS SELECT\n and \nODS EXCLUDE\n statements.\n\n\nYou can use ODS templates to modify the layout and details of each graph\n\n\nYou can use ODS styles to control the general appearance and consistency of yous graphs and tables (by default \nHTMLBLUE\n).\n\n\n\n\nAnother way to control your output is to use the \nPLOTS\n option which is usually available in the procedure statement:\n\n\n1\nPROC UNIVARIATE DATA=SAS-data-set PLOTS=options;\n\n\n\n\n\n\nThis option enables you to specify which graphs SAS should create, either in addtion or instead of the default plots.\n\n\nConfidence Intervals for the Mean\n\n\n\n\nA \npoint estimator\n is a sample statistic used to estimate a population parameter\n\n\nAn estimator takes on different values from sample to sample, so it's important to know its variance\n\n\nA statistic that measures the variability of your estimator is the \nstandard error\n\n\nIt differs from the standard deviation: the \nstandard deviation\n deals with the variability of your data while \nstandard error\n deals with the variability of you sample statistic\n\n\n\n\nE.g.:\n $standard \\ error \\ of \\ the \\ mean = standard \\ deviation/ \\sqrt{sample \\ size}$\n\n\nThe \ndistribution of sample means\n is always less variable than the data.\n\n\n\n\nBecause we know that point estimators vary from sample to sample, it would be nice to have an estimator of the mean that directly accounts for this natural variability\n\n\nThe \ninterval estimator\n gives us a range of values that is likely to contain the population mean\n\n\nIt is calculated from the \nstandard error\n and a value that is determined by the \ndegree of certainty\n we require (\nsignificance level\n)\n\n\nConfidence intervals\n are a type of interval estimator used to estimate the population mean\n\n\nYou can make the confidence interval narrower by increasing the sample size and by decreasing the confidence level\n\n\n\n\n$CI = sample \\ mean \\pm quantile \\cdot standard \\ error$\n\n\n\n\nThe \nCLM\n option of \nPROC MEANS\n calculates the confidence limits for the mean, you can add \nALPHA=\n to change the default 0.05 value for a 95% confidence level\n\n\nThe \ncentral limit theorem\n states that the distribution of sample means is approximately normal regardless of the population distribution's shape, if the sample size is large enough (~30 observations)\n\n\n\n\nHypothesis Testing\n\n\n\n\nThe \nnull hypothesis\n ($H_0$) is what you assume to be true when you start your analysis\n\n\nThe \nalternative hypothesis\n ($H_a$ or $H_1$) is your initial research hypothesis, that is, your proposed explanation\n\n\n\n\nDecision-making process:\n\n\n\n\nDefine null and alternative hypothesis\n\n\nSpecify significance level (type I error rate)\n\n\nCollect data\n\n\nReject or fail to reject the null hypothesis\n\n\n\n\n\n\n\n\nThe type I and II errors are \ninversely related\n: as one type increases the other decreases \n\n\nThe \npower\n is the probability of a \ncorrect rejection\n = 1 - \n\n\nIt is the ability of the statistical test to detect a true difference\n\n\n\n\nIt is the ability to successfully reject a false null hypothesis\n\n\n\n\n\n\nA \np-value\n measures the probability of observing a value as extreme as the one observed\n\n\n\n\nThe p-value is used to determine \nstatistical significance\n\n\nIt helps you assess whether you should reject the null hypothesis\n\n\n\n\n\n\n\n\nThe \np-value\n is affected by:\n\n\n\n\nThe \neffect size\n: the difference between the observed statistic and the hypothesized value\n\n\nThe \nsample size\n: the larger the sample size, the more sure you are about the sample statistics, the lower the p-value is\n\n\n\n\n\n\n\n\nA reference distribution enables you to quantify the probability (p-value) of observing a particular outcome (the calculated sample statistic) or a more extreme outcome, if the nul hypothesis is true\n\n\n\n\nTwo common reference distributions for statistical hypothesis testing are the \nt distribution\n and the \nF distribution\n\n\nThese distributions are characterized by the \ndegrees of freedom\n associated with your data\n\n\nThe \nt distribution\n arises when you're making inferences about a population mean and the population standard deviation is unknown and has to be estimated from the data\n\n\nIt is \napproximately normal\n as the \nsample size grows larger\n\n\nThe t distribution is a \nsymmetric distribution\n like the normal distribution except that the t distribution has \nthicker tails\n\n\nThe \nt statistic\n is positive/negative when the sample is more/less than the hypothesized mean\n\n\nIf the data doesn't come from a normal distribution, then the t statistic approximately follows a t distribution as long as the sample size is large (\ncentral limit theorem\n)\n\n\n\n\n\n\n\n\nCalculation with \nPROC UNIVARIATE\n:\n\n\n1\n2\n3\n4\n5\n6\n7\n8\nODS SELECT TESTSFORLOCATION;\nPROC UNIVARIATE DATA=SAS-data-set MU0=number alpha=number;\n  VAR variable(s);\n  ID variable_to_relate;\n  HISTOGRAM variables \n/options\n;\n  PROBPLOT variables \n/options\n;\n  INSET keywords \n/options\n;\nRUN;\n\n\n\n\n\n\n\n\nTESTSFORLOCATION\n displays only the p-values calculation\n\n\nBy default \nMU0 = 0", 
            "title": "Introduction"
        }, 
        {
            "location": "/statistics/introduction/#basic-statistical-concepts", 
            "text": "Descriptive statistics (exploratory data analysis, EDA)   Explore your data   Inferential statistics (explanatory modelling)   How is X related to Y?  Sample sizes are typically small and include few variables  The focus is on the parameters of the model  To assess the model, you use p-values and confidence intervals   Predictive modelling   If you know X, can you predict Y?  Sample sizes are large and include many predictive (input) variables  The focus is on the predictions of observations rather than the parameters of the model  To assess a predictive model, you validate predictions using holdout sample data     Parameters : numerical values (typically unknown, you can't measure the entire population) that summarize characteristics of a population (greek letters)  Statistics : summarizes characteristics of a sample (standard alphabet)  You use  statistics  to estimate  parameters     Independent variable : it can take different values, it affects or determines a  dependent variable . It can be called predictor, explanatory, control or input variable.  Dependent variable : it can take different values in response to an  independent variable . Also known as response, outcome or target variable.    Scale of measurement : variable's classification   Quantitative/numerical variables : counts or measurements, you can perform arithmetical operations with it  Discrete data : variables that can have only a countable number of values within a measurement range  Continuous data : variables that are measured on a scale that has infinite number of values and has no breaks or jumps  Interval scale data : it can be rank-ordered like ordinal data but also has a sensible spacing of observations such that differenes between measurements are meaningful but it lacks a true zero (ratios are meaningless)  Ratio scale data : it is rank-ordered with meaningful spacing and also includes a true zero point and can therefore accurately indicate the ratio difference between two spaces on the measurement scale      Categorical/attribute variables : variables that denote groupings or labels  Nominal data (qualitative/classification variable) : exhibits no ordering within its observed levels, groups or categories  Ordinal data : the observed labels can be ordered in some meaningful way that implies that the differences between the groups or categories are due to magnitude       Univariate analysis  provides techniques for analyzing and describing a sigle variable. It reveals patterns in the data by looking at the  range  of values, measures of  dispersion , the  central tendecy  of the values and  frequency distribution .  Bivariate analysis  describes and explains the relationships between two variables and how they change or covary together. It include techniques such as  correlation analysis  and  chi-square tests of independance .  Multivariate/Multivariable analysis  examines two or more variables at the same time in order to understand the relationships among them.   Techniques such as  mutiple linear regression  and n-way  ANOVA  are typically called  multivariable  analysis (only one response variable).   Techniques such as  factora analysis  and  clustering  are typically called  mutivariate  analysis (they consider more than one response variable).      Descriptive Statistics  Measures of central tendencies : mean (affected by outliers), median (less sensitive to outliers), mode     Percentile  Quartile       25th  1st / lower / Q1     50th  2nd / middle / Q2  Median    75th  3rd / upper / Q3      The  interquartile range (IQR)  is the difference between Q1 and Q3, it is a  robust estimate of the variability  because changes in the upper/lower 25% of the data do not affect it. If there are  outliers  in the data, then the IQR is a more reliable measure of the spread than the overall range.  The  coefficient of variation (CV)  is a measure of the standard deviation expressed as a percentage of the mean ($c_v = \\sigma / 100 \\mu$).   Normal distribution     Intervals  Percentage contained      $\\mu \\pm \\sigma$  68%    $\\mu \\pm 2 \\sigma$  95%    $\\mu \\pm 3 \\sigma$  99%     How to check the normality of a sample?   Compare the  mean  and the  median : if they are nearly equal, that is an indicator of symmetry (requirement for normality)  Check that  skewness  and  kurtosis  are close to 0:  If both are greater than 1 or less than -1: data is not normal  If either is greater than 2 or less than -2: data is not normal     Statistical summaries    Skewness  and  kurtosis  measure certain aspects of the shape of a distribution (they are  0  and  3  for a normal distribution, although SAS has standardized both to 0)   Skewness  measures the tendency of your data to be more spread out on one side of the mean than on the other (asymmetry of the distribution).   You can think of the direction of skewness as the direction the data is trailing off to.   A  right-skewed  distribution tells us that the mean is  greater than the median .    Kurtosis  measures the tendency of your data to be concentrated toward the center or toward the tails of the distribution (peakedness of the data, tail thickness).   A  negative kurtosis (platykurtic distribution)  means that the data has lighter tails than in a normal distribution.   A  positive kurtosis (leptokurtic/heavy-tailed/outlier-prone distribution)  means that the data has heavier tails and is more concentrated around the mean than a normal distribution.  Rectangular, bimodal and multimodal distributions tend to have low values of kurtosis.  Asymmetric distributions  also tend to have nonzero kurtosis. In these cases, understanding kurtosis is considerably more complex and can be difficult to assess visually.", 
            "title": "Basic Statistical Concepts"
        }, 
        {
            "location": "/statistics/introduction/#proc-surveyselect", 
            "text": "How to generate random (representative) samples (population subsets):  1\n2\n3\n4\n5\n6\n7 PROC SURVEYSELECT DATA=SAS-data-set \n                  OUT=name-of-output-data-set\n                  METHOD=method-of-random-sampling\n                  SEED=seed-value \n                  SAMPSIZE=number-of-observations-desired;\n      STRATA stratification-variable(s); \nRUN;    METHOD  specifies the random sampling method to be used. For simple random sampling without replacement, use  METHOD=SRS . For simple random sampling with replacement, use  METHOD=URS . For other selection methods and details on sampling algorithms, see the SAS online documentation for  PROC SURVEYSELECT .  SEED  specifies the initial seed for random number generation. If no  SEED  option is specified, SAS uses the system time as its seed value. This creates a different random sample every time the procedure is run.  SAMPSIZE  indicates the number of observations to be included in the sample. To select a certain fraction of the original data set rather than a given number of observations, use the  SAMPRATE  option.", 
            "title": "PROC SURVEYSELECT"
        }, 
        {
            "location": "/statistics/introduction/#picturing-your-data", 
            "text": "", 
            "title": "Picturing Your Data"
        }, 
        {
            "location": "/statistics/introduction/#proc-univariate", 
            "text": "Plots that can be produced with this procedure:   Histograms  Normal probability plots : expected percentiles from standard normal vs actual data values", 
            "title": "PROC UNIVARIATE"
        }, 
        {
            "location": "/statistics/introduction/#proc-sgscatter", 
            "text": "Plots that can be produced with this procedure:   Scatter plots : you can create a  single-cell  (simple Y by X) scatter plot, a  multi-cell  scatter plot with multiple independent scatter plots in a grid and a  scatter plot matrix , which produces a matrix of scatter plots comparing multiple variables.", 
            "title": "PROC SGSCATTER"
        }, 
        {
            "location": "/statistics/introduction/#proc-sgplot", 
            "text": "Plots that can be produced with this procedure:   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 PROC SGPLOT DATA=SAS-data-set  options ;\n        DOT category-variable  /options ;\n        HBAR category-variable  /options ;\n        VBAR category-variable  /options ;\n        HBOX response-variable  /options ;\n        VBOX response-variable  /options ;\n        HISTOGRAM response-variable  /options ;\n        SCATTER X=variable Y=variable  /options ;\n        NEEDLE X=variable Y=numeric-variable  /options ;\n        REG X=numeric-variable Y=numeric-variable  /options ;\nRUN;   Anywhere in the procedure you can add  reference lines :  1\n2\n3\n4 REFLINE variable | value-1  ...   value-n   /option(s) \n\n/* Example: */\nREFLINE 1200 / axis=y lineattrs=(color=blue);    Note  The order on which you define the parts of the plot will the determined the order on which it is displayed (if you want to send a  REFLINE  to the back, define it first)    Scatter plots ( SCATTER )  Line graphs  Histograms ( HISTOGRAM )  with overlaid distribution curves  Regression lines ( REG )  with confidence and prediction bands  Dot plots ( DOT )  Box plots ( HBOX / VBOX ) : it makes it easy to see how spread out your data is and if there are any outliers. The box represents the middle 50% of your data (IQR). The lower/middle/upper  line of the box  represent Q1/Q2/Q3. The  diamond  denotes the mean (easy to check how close the mean is to the median). The  whiskers  extend as far as the data extends to a maximum length of 1.5 times the IQR above Q3. Any data points farther than this distance are considered possible outliers and are represented in this plot as  circles .  Bar charts ( HBAR / VBAR )  Needle plot ( NEEDLE ) : creates a plot with needles connecting each point to the baseline  You can also  overlay plots together  to produce many different types of graphs", 
            "title": "PROC SGPLOT"
        }, 
        {
            "location": "/statistics/introduction/#proc-sgpanel", 
            "text": "Plots that can be produced with this procedure:   Panels of plots  for different levels of a factor or several different time periods depending on the classification variable  Side-by-side histograms  which provide a visual comparison for your data", 
            "title": "PROC SGPANEL"
        }, 
        {
            "location": "/statistics/introduction/#proc-sgrender", 
            "text": "Plots from graphs templates you have modified or written yourself    To specify options for graphs you submit the  ODS GRAPHICS  statement:  1 ODS GRAPHICS ON  options ;    To select/exclude specific test results, graphs or tables from you output, you can use  ODS SELECT  and  ODS EXCLUDE  statements.  You can use ODS templates to modify the layout and details of each graph  You can use ODS styles to control the general appearance and consistency of yous graphs and tables (by default  HTMLBLUE ).   Another way to control your output is to use the  PLOTS  option which is usually available in the procedure statement:  1 PROC UNIVARIATE DATA=SAS-data-set PLOTS=options;   This option enables you to specify which graphs SAS should create, either in addtion or instead of the default plots.", 
            "title": "PROC SGRENDER"
        }, 
        {
            "location": "/statistics/introduction/#confidence-intervals-for-the-mean", 
            "text": "A  point estimator  is a sample statistic used to estimate a population parameter  An estimator takes on different values from sample to sample, so it's important to know its variance  A statistic that measures the variability of your estimator is the  standard error  It differs from the standard deviation: the  standard deviation  deals with the variability of your data while  standard error  deals with the variability of you sample statistic   E.g.:  $standard \\ error \\ of \\ the \\ mean = standard \\ deviation/ \\sqrt{sample \\ size}$  The  distribution of sample means  is always less variable than the data.   Because we know that point estimators vary from sample to sample, it would be nice to have an estimator of the mean that directly accounts for this natural variability  The  interval estimator  gives us a range of values that is likely to contain the population mean  It is calculated from the  standard error  and a value that is determined by the  degree of certainty  we require ( significance level )  Confidence intervals  are a type of interval estimator used to estimate the population mean  You can make the confidence interval narrower by increasing the sample size and by decreasing the confidence level   $CI = sample \\ mean \\pm quantile \\cdot standard \\ error$   The  CLM  option of  PROC MEANS  calculates the confidence limits for the mean, you can add  ALPHA=  to change the default 0.05 value for a 95% confidence level  The  central limit theorem  states that the distribution of sample means is approximately normal regardless of the population distribution's shape, if the sample size is large enough (~30 observations)", 
            "title": "Confidence Intervals for the Mean"
        }, 
        {
            "location": "/statistics/introduction/#hypothesis-testing", 
            "text": "The  null hypothesis  ($H_0$) is what you assume to be true when you start your analysis  The  alternative hypothesis  ($H_a$ or $H_1$) is your initial research hypothesis, that is, your proposed explanation   Decision-making process:   Define null and alternative hypothesis  Specify significance level (type I error rate)  Collect data  Reject or fail to reject the null hypothesis     The type I and II errors are  inversely related : as one type increases the other decreases   The  power  is the probability of a  correct rejection  = 1 -   It is the ability of the statistical test to detect a true difference   It is the ability to successfully reject a false null hypothesis    A  p-value  measures the probability of observing a value as extreme as the one observed   The p-value is used to determine  statistical significance  It helps you assess whether you should reject the null hypothesis     The  p-value  is affected by:   The  effect size : the difference between the observed statistic and the hypothesized value  The  sample size : the larger the sample size, the more sure you are about the sample statistics, the lower the p-value is     A reference distribution enables you to quantify the probability (p-value) of observing a particular outcome (the calculated sample statistic) or a more extreme outcome, if the nul hypothesis is true   Two common reference distributions for statistical hypothesis testing are the  t distribution  and the  F distribution  These distributions are characterized by the  degrees of freedom  associated with your data  The  t distribution  arises when you're making inferences about a population mean and the population standard deviation is unknown and has to be estimated from the data  It is  approximately normal  as the  sample size grows larger  The t distribution is a  symmetric distribution  like the normal distribution except that the t distribution has  thicker tails  The  t statistic  is positive/negative when the sample is more/less than the hypothesized mean  If the data doesn't come from a normal distribution, then the t statistic approximately follows a t distribution as long as the sample size is large ( central limit theorem )     Calculation with  PROC UNIVARIATE :  1\n2\n3\n4\n5\n6\n7\n8 ODS SELECT TESTSFORLOCATION;\nPROC UNIVARIATE DATA=SAS-data-set MU0=number alpha=number;\n  VAR variable(s);\n  ID variable_to_relate;\n  HISTOGRAM variables  /options ;\n  PROBPLOT variables  /options ;\n  INSET keywords  /options ;\nRUN;    TESTSFORLOCATION  displays only the p-values calculation  By default  MU0 = 0", 
            "title": "Hypothesis Testing"
        }, 
        {
            "location": "/statistics/anova/", 
            "text": "Chapter summary in SAS\n\n\n\n\nGraphical Analysis of Associations\n\n\n\n\nBefore analyzing your data, you need to have a general idea of any associations between \npredictor variables\n and \nresponse variables\n\n\nAn \nassociation\n exists between two variables when the expected value of one variable differs at different levels of the other variable\n\n\nOne method for doing this is to conduct a \ngraphical analysis\n of your data\n\n\nAssociations between \ncategorical\n predictor variable and a \ncontinuous\n response variable can be explored with \nSGPLOT\n to product \nbox plots\n (box-and-whisker plots) (\nX\n predictor variable vs \nY\n response variable)\n\n\nIf the \nregression line\n conecting the means of Y at each value of X is not horizontal \nthere might be an association\n between them\n\n\nIf the \nregression line\n is horizontal \nthere is no association\n: knowing the value of X doesn't tell you anything about the value of Y\n\n\n\n\nPROC SGPLOT\n\n\n1\n2\n3\n4\nPROC SGPLOT DATA=SAS-data-set;\n    VBOX response-variable / CATEGORY=predictor-variable\n    CONNECT=MEAN DATALABEL=outlier-ID-variable;\nRUN;\n\n\n\n\n\n\nTwo-Sample t-Tests\n\n\n\n\nYou can use a \none-sample t-test\n to determine if the mean of a population is equal to a particular value or not\n\n\nWhen you collect a random sample of independent observations from two different populations, you can perform a \ntwo-sample t-test\n\n\n\n\nWhen you compare the means of two populations using a \ntwo-sample t-test\n you make three assumptions:\n\n\n\n\nThe data contains independent observations\n\n\nThe distributions of the two populations are normal (check histograms and normal probability/Q-Q plots)\n\n\nThe variances in these normal distributions are equal (\nF-test\n is the formal way to verify this assumption)\n\n\n\n\n$F$ statistic: $F=max(s_1^2,s_2^2)/min(s_1^2,s_2^2) \\ge 1$\n\n\n$H_0$: \n$_1^2$ $=$  \n$_2^2\\rightarrow F \\approx 1$\n\n\n$H_a$: \n$_1^2$ $\\ne$  \n$_2^2\\rightarrow F\\gt 1$\n\n\nThe \nPr\nF\n value in the \nEquality of Variances\n table represents the \np-value\n of the F-test for equal variances\n\n\n\n\nTwo-sided Tests\n\n\nPROC TTEST\n\n\n\n\nPROC TTEST\n performs a two-sided two-sample t-test by default (confidence limits and ODS graphics included)\n\n\nIt \nautomatically test the assumption of equal variances\n and provides an exact two-sample t-test (\npooled\n) when the assumptions are met and an approximate t-test (\nscatterthwaite\n) when it is not met \n\n\nThe pooled and scatterthwaite t-tests are equal when the variances are equal\n\n\n\n\n1\n2\n3\n4\n5\nPROC TTEST DATA=SAS-data-set \noptions\n\n    plots(shownull)=interval;         \\* shownull = vertical reference line at the mean value of H0 *\\\n    CLASS variable;                   \\* Classification variable *\\\n    VAR variable(s);                  \\* Continuous response variables *\\\nRUN;\n\n\n\n\n\n\n\n\nOne-sided Tests\n\n\n\n\nIt \ncan increase the power\n of a statistical test, meaning that if you are right about the direction of the true difference, you will more likely detect a significant difference with a one-sided test than with a tow-sided test\n\n\nThe difference between the mean values for the null hypothesis will be defined by the alphabetical order of the classification variables (e.g.: female - male)\n\n\n\n\n1\n2\n3\n4\n5\nPROC TTEST DATA=SAS-data-set \n    plots(only shownull)=interval H0=0 SIDES=u;     \\* only = suppress the default plots; u/l = upper/lower-tailed t-test  *\\\n    CLASS variable;                                 \\* Classification variable *\\\n    VAR variable(s);                                \\* Continuous response variables *\\\nRUN;\n\n\n\n\n\n\nOne-Way ANOVA\n\n\nWhen you want to determine whether there are significant differences between the \nmeans of two or more populations\n, you can use analysis of variance (ANOVA).\n\n\n\n\nYou have a continuous dependent (\nresponse\n) variable and a categorical independent (\npredictor\n) variable\n\n\nYou can have \nmany levels of the predictor variable\n, but you can have \nonly one predictor variable\n\n\nThe \nsquared value of the t statistic\n for a two-sample t-test is equal to the \nF statistic\n of a one-way ANOVA with two populations\n\n\nWith ANOVA the $H_0$ is that all of the population means are equal and $H_a$ is that not all the population means are equal (at least one mean is different)\n\n\n\n\nTo perform an ANOVA test you make three assumptions:\n\n\n\n\nYou have a \ngood, random, representative sample\n\n\nThe \nerror terms are normally distributed\n\n\nThe \nresiduals\n (each observation minus its group mean) are estimates of the error term in the model so you verify this assumption by examining diagnostic plots of the residuals (if they are approximately normal, the error terms will be too)\n\n\nIf your sample sizes are reasonably large and approximately equal across groups, then only severe departures from normality are considered a problem\n\n\nResiduals always sum to 0, regardless of the number of observations.\n\n\n\n\n\n\nThe \nerror terms have equal variances\n across the predictor variable levels: you can conduct a formal test for equal variances and also plot the residuals vs predicted values as a way to graphically verify this assumption\n\n\n\n\nPROC GLM\n\n\nYou can use \nPROC GLM\n to verify the ANOVA assumptions and perform the ANOVA test. It fits a general linear model of which ANOVA is a special case and also displays the sums of squares associated with each hypothesis it tests.\n\n\n1\n2\n3\n4\n5\n6\n7\nPROC GLM DATA=SAS-data-set\n         PLOTS(ONLY)=DIAGNOSTICS(UNPACK);     /* print each plot on a separated page */\n    CLASS variable(s);\n    MODEL dependents=intependents \n/options\n;\n    MEANS effects / HOVTEST \n/options\n;    \nRUN;\nQUIT;\n\n\n\n\n\n\n\n\nHOVTEST\n: homogeneity of variance test option (Levene's test by default) + plot of residuals vs predicted values (means)\n\n\n\n\n\n\n\n\nIf the \nbetween-group variability\n is significantly larger than the \nwithin-group variability\n, you reject the null that all the group means are equal\n\n\nYou partition out the variability using sums of squares: \n\n\nBetween-group\n variation: also called Model Sum of Squares (SSM): $\\sum n_i (\\overline Y_i- \\overline {\\overline Y})^2$\n\n\nWithin-group\n variation: also called Error Sum of Squares (SSE): $\\sum \\sum (Y_{ij}- \\overline Y_i)^2$\n\n\nTotal\n variation: also called the Total Sum of Squares (SST): $\\sum \\sum (Y_{ij}- \\overline {\\overline Y})^2$\n\n\n\n\n\n\nSSM\n and \nSSE\n represent pieces of \nSST\n: the SSM is the variability explanied by the predictor variable levels and SSE the variability not explained by the predictor variable levels\n\n\nYou want the larger piece of the total to be better represented by what you can explain (SSM) vs what you can't explain (SSE) \n\n\n\n\nANOVA with Data from a Randomized Block Design\n\n\nIn an \nobservational study\n, you often examine what already occurred, and therefore have little control over factors contributing to the outcome. In a \ncontrolled experiment\n, you can manipulate the \nfactors of interest\n and can more reasonably claim causation.\n\n\n\n\nThe variation due to the \nnuisance factors\n (fundamental to the probabilistic model but are no longer of interest) is part of the random variation that the error sum of squares accounts for.\n\n\nIncluding a \nblocking variable\n in the model is in essence like adding a second predictor variable to the model in terms of the way you write it\n\n\nThe way you set up your experiment and data collection is what defines it as a blocking factor\n\n\nAlthough you're not specifically interested in its effect, \ncontrolling the blocking variable makes it easier to detect an effect of the factor of interest\n\n\nIn a model that does not include a blocking variable, its effects are lumped into the error term of the model (unaccounted for variation)\n\n\nWhen you include a blocking variable in your ANOVA model, any effects caused by the nuisance factors that are common within a sector are accounted for in the \nmodel sum of squares rather than the error sum of squares\n\n\n\n\nYou make two more assumptions when you include a blocking factor in the model:\n\n\n\n\nPrimary variable levels are \nrandomly assigned\n within each block\n\n\nThe effects of the primary variable are \nconstant across the levels\n of the blocking factor (the effects don't depend on the block they are in, there are \nno interactions\n with the blocking variable)\n\n\n\n\n\n\nNote\n\n\nLevene's test for homogeneity is \nonly available for one-way ANOVA models\n, so in this case, you have to use the Residuals by Predicted plot.\n\n\n\n\nPROC GLM\n\n\n1\n2\n3\n4\n5\n6\nPROC GLM DATA=SAS-data-set\n         PLOTS(ONLY)=DIAGNOSTICS(UNPACK);   /* print each plot on a separated page */\n    CLASS variable(s) blocking-factor(s);\n    MODEL dependents=intependents blocking-factor(s)\n/options\n;\nRUN;\nQUIT;\n\n\n\n\n\n\n\n\nRule of thumb\n: if the \nF-value is \n 1\n, then it helped to add the blocking factor in your model \n\n\nIf you compare the MSE (\nMean Square\n in the table) without and with including the blocking variable in the model,  there is a drop of its value meaning that \nyou have been able to account for a bit more of the unexplained variability due to the nuisance factors\n helping o have more precise estimates of the effect of your primary variable\n\n\nIt is also reflected in the \nR-Square\n value that is increased when a blocking factor is added to the model\n\n\nThanks to adding a blocking variable to your model you can get your primary variable to be significant\n\n\nThe \nType III SS\n at the bottom of the output tests for the difference due to each variable, controlling for or adjusting for the other variable\n\n\n\n\nANOVA Post Hoc Tests\n\n\nThis test is used to determine which means differ from other means and control the error rate using \nmultiple comparison method\n.\n\n\nAssuming the null hypothesis is true for your different comparisons, the probability that you conclude a difference exist at least one time when there really  isn't a difference increases with the more tests you perform. So \nthe chance that you make a Type I error increases each time you conduct a statistical test\n.\n\n\n\n\nThe \ncomparisonwise error rate (CER)\n is the probability of a Type I error on a single pairwise test (\n)\n\n\nThe \nexperimentwise error rate (EER)\n is the probability of making at least one Type I error when performing the whole set of comparisons. It takes into consideration the number of pairwise comparisons you make, so it increases as the number of tests increase: \n\n \n EER=1-(1-\\alpha)^{comparisons} \n \n\n\n\n\nTukey's Multiple Comparison Method\n\n\n\n\nThis method, which is also known as the \nHonestly Significant Difference\n test, is a popular multiple comparison test that \ncontrols the EER\n\n\nThis tests compares all possible pairs of means, so \nit can only be used when you make pairwise comparisons\n\n\nThis method controls $EER=\\alpha$ when \nall possible pairwise comparisons are considered\n and controls $EER\n\\alpha$ when fewer than all pairwise comparisons are considered\n\n\n\n\nDunnett's Multiple Comparison Method\n\n\n\n\nThis method is a specialised multiple comparison test that allows you to \ncompare a single control group to all other groups\n\n\nIt controls $EER \\le \\alpha$ when all groups are compared to the reference group (control)\n\n\nIt accounts for the correlation that exists between the comparisons and \nyou can conduct one-sided tests\n of hypothesis against the reference group\n\n\n\n\nPROC GLM\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nPROC GLM DATA=SAS-data-set;\n    CLASS variable(s);\n    MODEL dependents=intependents \n/options\n;\n    LSMEANS effects \n/options-test-1\n;  \n    LSMEANS effects \n/options-test-2\n;\n    [...]\n    LSMEANS effects \n/options-test-n\n;  \nRUN;\nQUIT;\n\n\n\n\n\n\n\n\nPDIFF=ALL\n requests p-values for the differences between \nALL\n the means and a \ndiffogram\n is produced automatically displaying all pairwise least square means differences and indicating which are significant\n\n\nIt can be undestood as a least squares mean by least squares mean plot\n\n\nThe point estimates for differences between the means for each pairwise comparison can be found at the intersections of the gray grid lines (intersection of appropriate indexes)\n\n\nThe red/blue diagonal lines show the \nconfidence intervals for the true differences of the means\n for each pairwise comparison\n\n\nThe grey 45$^{\\circ}$ reference line represents equality of the means (if the confidence interval crosses over it, then there is no significant difference between the two groups and the diagonal line for the pair will be \ndashed and red\n; if the difference is significant the line will be \nsolid and blue\n)\n\n\n\n\n\n\n\n\nThe \nADJUST=\n option specifies the adjustment method for multiple comparisons\n\n\nIf you don't specify an option SAS uses the \nTukey method by default\n, if you specify \nADJUST=Dunnett\n the GLM procedure produces multiple comparisons using \nDunnett's method\n and a \ncontrol plot\n \n\n\nThe control plot displays the least squares mean and confidence limits of each group compared to the reference group \n\n\nThe middle \nhorizontal line represents its least square mean value\n (you can see the arithmetic mean value un the \nupper right corner\n of the graph)\n\n\nThe \nshaded area\n goes from the \nlower decision limit (LDL)\n to the \nupper decision limit (UDL)\n\n\nThere is a vertical line for each group that you're comparing to the reference (control) group. If a \nvertical line extends past the shaded area\n, then the group represented by the line is \nsignificantly different\n (small p-value) than the reference group \n\n\n\n\n\n\n\n\n\n\n\n\nPDIFF=CONTROLU('value')\n specifies the control group for the Dunnett's case: the direction of the sign in Ha is the same as the direction you are testing, so this is a \none-sided upper-tailed t-test\n\n\nIf you specify \nADJUST=T\n SAS will make no adjustments for multiple comparisons: is not recommended as there's a tendency to find \nmore significant pairwise differences than might actually exist\n\n\n\n\nTwo-Way ANOVA with Interactions\n\n\nWhen you have a continuous response variable and \ntwo categorical predictor variables\n, you use the \ntwo-way ANOVA model\n\n\n\n\nEffect\n: the magnitude of the expected change in the response variable presumably caused by the change in value of a predictor variable in the model\n\n\nIn addition, the variables in a model can be referred to as effects or terms\n\n\nMain effect\n: is the effect of a single predictor variable\n\n\nInteraction effects\n: when the relationship of the response variable with a predictor changes with the changing of another predictor variable (the effect of one variable depends on the value of the other variable)\n\n\n\n\n\n\nWhen you consider an ANOVA with more than one predictor variable, it's called \nn-way ANOVA\n where \nn\n represents the number of predictor variables\n\n\n\n\nThe analysis in a \nrandomized block design\n is actually a \nspecial type of two-way ANOVA\n in which you have one factor of interest and one blocking factor\n\n\nWhen you analyze a two-way ANOVA with interactions, you first look at any tests for \ninteractions among the factors\n\n\nIf there is \nno interaction between the factors\n you can interpret the tests for the individual factor effects to determine their significance/non-significance\n\n\nIf an \ninteraction exists between any factors\n, the tests for the individual factor effects might be misleading due to masking of these effect by the interaction (this is specially true for unbalanced data with different number of observations for each combination of groups)\n\n\n\n\n\n\nWhen the interaction is not statistically significant \nyou can analyze the main effect with the model in its current form\n (generally the method you use when you analyze designed experiments)\n\n\nEven when you analyze designed experiments, some statisticians might suggest that if the interaction is not significant, \nyou can delete the interaction effect from your model, rerun the model and then just analyze the main effects\n increasing the power of the main effects test\n\n\nIf the \ninteraction term is significant\n, it is good practice to keep the main effect terms that make up the interaction in the model, whether they are significant or not (this preserves model hierarchy)\n\n\nYou have to make the \nsame three assumptions used in the ANOVA test\n\n\nThe interaction terms are also called \nproduct terms\n or \ncrossed effects\n\n\n\n\nPROC GLM\n\n\n1\n2\n3\n4\n5\n6\n7\nPROC GLM DATA=SAS-data-set;\n    CLASS independent1 independent2;\n    MODEL dependent = independent1 independent2 independent1*independent2;\n    or\n    MODEL dependent = independent1 | independent2;\nRUN;\nQUIT;\n\n\n\n\n\n\nThis program is \nfitting to this model\n:\n\n\n\n\nY_{ijk}=\\mu + \\alpha_i+\\beta_j+(\\alpha\\beta)_{ij}+\\epsilon_{ijk}\n\n\n\n\n dependent = overall mean + intependent1 + independent2 + interaction12 + unaccounted for variation \n\n\n\n\nIn \nmost situations\n you will want to use the \nType III SS\n\n\nThe \nType I SS (sequential)\n are the sums of squares you obtain from fitting the effects in the order you specify in the model \n\n\nThe \nType III SS (marginal)\n are the sums of squares you obtain from fitting each effect after all the other terms in the model, that is the sums of squares for each effect corrected for the other terms in the model\n\n\nWhen examining these results you first have to \nlook at the interaction term and if it's significant\n (p-value), the \nmain effects don't tell you the whole story\n. It that is the case, you don't need to worry all that much about the significance of the main effects at this point for two reasons:\n\n\nYou know that the effect of each variable1 level changes for the different variable2 levels\n\n\nYou want to include the main effects in the model, whether they are significant or not, to preserve model hierarchy\n\n\n\n\n\n\nYou can analyze the interaction between terms by looking at the \ninteraction plot\n that SAS produces by default when you include an interaction term in the model\n\n\nTo analyze and interpret the effect of one of the interacting variables you need to add the \nLSMEANS\n statement to your program\n\n\n\n\n1\n2\n3\n4\n5\n6\nPROC GLM DATA=SAS-data-set ORDER=INTERNAL PLOTS(ONLY)=INTPLOT;\n    CLASS independent1 independent2;\n    MODEL dependent = intependent1 independent2 independent1*independent2;\n    LSMEANS independent1*independent2 / SLICE= independent1;\nRUN;\nQUIT;\n\n\n\n\n\n\nSAS creates two types of mean plots when you use the \nLSMEANS\n statement with an interaction term:\n\n\n\n\nThe first plot displays the \nleast squares mean (LS-Mean) for every effect level\n \n\n\nThe second plot contains the same information rearranged so you can \nlook a little closer at the combination levels\n\n\n\n\n\n\nSTORE\n statement\n\n\nYou can add a \nSTORE\n statement to save your analysis results in an \nitem store\n (a binary file format that cannot be modified). This allows you to \nrun post-processing analysis\n on the stored results even if you no longer have access to the original data set. The \nSTORE\n statement applies to the following SAS/STAT procedures: \nGENMOD\n, \nGLIMMIX\n, \nGLM\n, \nGLMSELECT\n, \nLOGISTIC\n, \nMIXED\n, \nORTHOREG\n, \nPHREG\n, \nPROBIT\n, \nSURVEYLOGISTIC\n, \nSURVEYPHREG\n, and \nSURVEYREG\n.\n\n\n1\n2\nSTORE \nOUT\n=\nitem-store-name\n    \n/ LABEL=\nlabel\n;\n\n\n\n\n\n\n\n\nitem-store-name\n is a usual one- or two-level SAS name, similar to the names that are used for SAS data sets\n\n\nlabel\n identifies the estimate on the output (is optional)\n\n\n\n\n\n\nPROC PLM\n\n\nTo perform post-fitting statistical analysis and plotting for the contents of the store item, you use \nPROC PLM\n. The statements and options that are available vary depending upon which procedure you used to produce the item store.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nPROC PLM RESTORE=item-store-specification \noptions\n;\n    EFFECTPLOT INTERACTION(SLICEBY=variable) \nplot-type\n \n(plot-definition\n \noptions)\n / CLM \n/ options\n;\n    LSMEANS \nmodel-effects\n \n/ options\n;\n    LSMESTIMATE model-effect \nlabel\n values\n        \ndivisor\n=n\n,...\nlabel\n values\n        \ndivisor\n=n\n \n/ options\n;\n    SHOW options;\n    SLICE model-effect / SLICEBY=variable ADJUST=tukey \n/ options\n;\n    WHERE expression;\nRUN;\n\n\n\n\n\n\n\n\nRESTORE\n specifies the source item store for processing\n\n\nEFFECTPLOT\n produces a display of the fitted model and provides options for changing and enhancing the displays\n\n\nLSMEANS\n computes and compares least squares means (LS-means) of fixed effects\n\n\nLSMESTIMATE\n provides custom hypothesis tests among least squares means\n\n\nSHOW\n uses \nODS\n to display contents of the item store. This statement is useful for verifying that the contents of the item store apply to the analysis and for generating \nODS\n tables.\n\n\nSLICE\n provides a general mechanism for performing a partitioned analysis of the LS-means for an interaction (analysis of simple effects) and it uses the same options as the \nLSMEANS\n statement\n\n\nWHERE\n is used in the PLM procedure when the item store contains \nBY-variable\n information and you want to apply the \nPROC PLM\n statements to only a subset of the BY groups", 
            "title": "Analysis of Variance (ANOVA)"
        }, 
        {
            "location": "/statistics/anova/#graphical-analysis-of-associations", 
            "text": "Before analyzing your data, you need to have a general idea of any associations between  predictor variables  and  response variables  An  association  exists between two variables when the expected value of one variable differs at different levels of the other variable  One method for doing this is to conduct a  graphical analysis  of your data  Associations between  categorical  predictor variable and a  continuous  response variable can be explored with  SGPLOT  to product  box plots  (box-and-whisker plots) ( X  predictor variable vs  Y  response variable)  If the  regression line  conecting the means of Y at each value of X is not horizontal  there might be an association  between them  If the  regression line  is horizontal  there is no association : knowing the value of X doesn't tell you anything about the value of Y", 
            "title": "Graphical Analysis of Associations"
        }, 
        {
            "location": "/statistics/anova/#proc-sgplot", 
            "text": "1\n2\n3\n4 PROC SGPLOT DATA=SAS-data-set;\n    VBOX response-variable / CATEGORY=predictor-variable\n    CONNECT=MEAN DATALABEL=outlier-ID-variable;\nRUN;", 
            "title": "PROC SGPLOT"
        }, 
        {
            "location": "/statistics/anova/#two-sample-t-tests", 
            "text": "You can use a  one-sample t-test  to determine if the mean of a population is equal to a particular value or not  When you collect a random sample of independent observations from two different populations, you can perform a  two-sample t-test   When you compare the means of two populations using a  two-sample t-test  you make three assumptions:   The data contains independent observations  The distributions of the two populations are normal (check histograms and normal probability/Q-Q plots)  The variances in these normal distributions are equal ( F-test  is the formal way to verify this assumption)   $F$ statistic: $F=max(s_1^2,s_2^2)/min(s_1^2,s_2^2) \\ge 1$  $H_0$:  $_1^2$ $=$   $_2^2\\rightarrow F \\approx 1$  $H_a$:  $_1^2$ $\\ne$   $_2^2\\rightarrow F\\gt 1$  The  Pr F  value in the  Equality of Variances  table represents the  p-value  of the F-test for equal variances   Two-sided Tests", 
            "title": "Two-Sample t-Tests"
        }, 
        {
            "location": "/statistics/anova/#proc-ttest", 
            "text": "PROC TTEST  performs a two-sided two-sample t-test by default (confidence limits and ODS graphics included)  It  automatically test the assumption of equal variances  and provides an exact two-sample t-test ( pooled ) when the assumptions are met and an approximate t-test ( scatterthwaite ) when it is not met   The pooled and scatterthwaite t-tests are equal when the variances are equal   1\n2\n3\n4\n5 PROC TTEST DATA=SAS-data-set  options \n    plots(shownull)=interval;         \\* shownull = vertical reference line at the mean value of H0 *\\\n    CLASS variable;                   \\* Classification variable *\\\n    VAR variable(s);                  \\* Continuous response variables *\\\nRUN;    One-sided Tests   It  can increase the power  of a statistical test, meaning that if you are right about the direction of the true difference, you will more likely detect a significant difference with a one-sided test than with a tow-sided test  The difference between the mean values for the null hypothesis will be defined by the alphabetical order of the classification variables (e.g.: female - male)   1\n2\n3\n4\n5 PROC TTEST DATA=SAS-data-set \n    plots(only shownull)=interval H0=0 SIDES=u;     \\* only = suppress the default plots; u/l = upper/lower-tailed t-test  *\\\n    CLASS variable;                                 \\* Classification variable *\\\n    VAR variable(s);                                \\* Continuous response variables *\\\nRUN;", 
            "title": "PROC TTEST"
        }, 
        {
            "location": "/statistics/anova/#one-way-anova", 
            "text": "When you want to determine whether there are significant differences between the  means of two or more populations , you can use analysis of variance (ANOVA).   You have a continuous dependent ( response ) variable and a categorical independent ( predictor ) variable  You can have  many levels of the predictor variable , but you can have  only one predictor variable  The  squared value of the t statistic  for a two-sample t-test is equal to the  F statistic  of a one-way ANOVA with two populations  With ANOVA the $H_0$ is that all of the population means are equal and $H_a$ is that not all the population means are equal (at least one mean is different)   To perform an ANOVA test you make three assumptions:   You have a  good, random, representative sample  The  error terms are normally distributed  The  residuals  (each observation minus its group mean) are estimates of the error term in the model so you verify this assumption by examining diagnostic plots of the residuals (if they are approximately normal, the error terms will be too)  If your sample sizes are reasonably large and approximately equal across groups, then only severe departures from normality are considered a problem  Residuals always sum to 0, regardless of the number of observations.    The  error terms have equal variances  across the predictor variable levels: you can conduct a formal test for equal variances and also plot the residuals vs predicted values as a way to graphically verify this assumption", 
            "title": "One-Way ANOVA"
        }, 
        {
            "location": "/statistics/anova/#proc-glm", 
            "text": "You can use  PROC GLM  to verify the ANOVA assumptions and perform the ANOVA test. It fits a general linear model of which ANOVA is a special case and also displays the sums of squares associated with each hypothesis it tests.  1\n2\n3\n4\n5\n6\n7 PROC GLM DATA=SAS-data-set\n         PLOTS(ONLY)=DIAGNOSTICS(UNPACK);     /* print each plot on a separated page */\n    CLASS variable(s);\n    MODEL dependents=intependents  /options ;\n    MEANS effects / HOVTEST  /options ;    \nRUN;\nQUIT;    HOVTEST : homogeneity of variance test option (Levene's test by default) + plot of residuals vs predicted values (means)     If the  between-group variability  is significantly larger than the  within-group variability , you reject the null that all the group means are equal  You partition out the variability using sums of squares:   Between-group  variation: also called Model Sum of Squares (SSM): $\\sum n_i (\\overline Y_i- \\overline {\\overline Y})^2$  Within-group  variation: also called Error Sum of Squares (SSE): $\\sum \\sum (Y_{ij}- \\overline Y_i)^2$  Total  variation: also called the Total Sum of Squares (SST): $\\sum \\sum (Y_{ij}- \\overline {\\overline Y})^2$    SSM  and  SSE  represent pieces of  SST : the SSM is the variability explanied by the predictor variable levels and SSE the variability not explained by the predictor variable levels  You want the larger piece of the total to be better represented by what you can explain (SSM) vs what you can't explain (SSE)", 
            "title": "PROC GLM"
        }, 
        {
            "location": "/statistics/anova/#anova-with-data-from-a-randomized-block-design", 
            "text": "In an  observational study , you often examine what already occurred, and therefore have little control over factors contributing to the outcome. In a  controlled experiment , you can manipulate the  factors of interest  and can more reasonably claim causation.   The variation due to the  nuisance factors  (fundamental to the probabilistic model but are no longer of interest) is part of the random variation that the error sum of squares accounts for.  Including a  blocking variable  in the model is in essence like adding a second predictor variable to the model in terms of the way you write it  The way you set up your experiment and data collection is what defines it as a blocking factor  Although you're not specifically interested in its effect,  controlling the blocking variable makes it easier to detect an effect of the factor of interest  In a model that does not include a blocking variable, its effects are lumped into the error term of the model (unaccounted for variation)  When you include a blocking variable in your ANOVA model, any effects caused by the nuisance factors that are common within a sector are accounted for in the  model sum of squares rather than the error sum of squares   You make two more assumptions when you include a blocking factor in the model:   Primary variable levels are  randomly assigned  within each block  The effects of the primary variable are  constant across the levels  of the blocking factor (the effects don't depend on the block they are in, there are  no interactions  with the blocking variable)    Note  Levene's test for homogeneity is  only available for one-way ANOVA models , so in this case, you have to use the Residuals by Predicted plot.", 
            "title": "ANOVA with Data from a Randomized Block Design"
        }, 
        {
            "location": "/statistics/anova/#proc-glm_1", 
            "text": "1\n2\n3\n4\n5\n6 PROC GLM DATA=SAS-data-set\n         PLOTS(ONLY)=DIAGNOSTICS(UNPACK);   /* print each plot on a separated page */\n    CLASS variable(s) blocking-factor(s);\n    MODEL dependents=intependents blocking-factor(s) /options ;\nRUN;\nQUIT;    Rule of thumb : if the  F-value is   1 , then it helped to add the blocking factor in your model   If you compare the MSE ( Mean Square  in the table) without and with including the blocking variable in the model,  there is a drop of its value meaning that  you have been able to account for a bit more of the unexplained variability due to the nuisance factors  helping o have more precise estimates of the effect of your primary variable  It is also reflected in the  R-Square  value that is increased when a blocking factor is added to the model  Thanks to adding a blocking variable to your model you can get your primary variable to be significant  The  Type III SS  at the bottom of the output tests for the difference due to each variable, controlling for or adjusting for the other variable", 
            "title": "PROC GLM"
        }, 
        {
            "location": "/statistics/anova/#anova-post-hoc-tests", 
            "text": "This test is used to determine which means differ from other means and control the error rate using  multiple comparison method .  Assuming the null hypothesis is true for your different comparisons, the probability that you conclude a difference exist at least one time when there really  isn't a difference increases with the more tests you perform. So  the chance that you make a Type I error increases each time you conduct a statistical test .   The  comparisonwise error rate (CER)  is the probability of a Type I error on a single pairwise test ( )  The  experimentwise error rate (EER)  is the probability of making at least one Type I error when performing the whole set of comparisons. It takes into consideration the number of pairwise comparisons you make, so it increases as the number of tests increase:     EER=1-(1-\\alpha)^{comparisons}      Tukey's Multiple Comparison Method   This method, which is also known as the  Honestly Significant Difference  test, is a popular multiple comparison test that  controls the EER  This tests compares all possible pairs of means, so  it can only be used when you make pairwise comparisons  This method controls $EER=\\alpha$ when  all possible pairwise comparisons are considered  and controls $EER \\alpha$ when fewer than all pairwise comparisons are considered   Dunnett's Multiple Comparison Method   This method is a specialised multiple comparison test that allows you to  compare a single control group to all other groups  It controls $EER \\le \\alpha$ when all groups are compared to the reference group (control)  It accounts for the correlation that exists between the comparisons and  you can conduct one-sided tests  of hypothesis against the reference group", 
            "title": "ANOVA Post Hoc Tests"
        }, 
        {
            "location": "/statistics/anova/#proc-glm_2", 
            "text": "1\n2\n3\n4\n5\n6\n7\n8\n9 PROC GLM DATA=SAS-data-set;\n    CLASS variable(s);\n    MODEL dependents=intependents  /options ;\n    LSMEANS effects  /options-test-1 ;  \n    LSMEANS effects  /options-test-2 ;\n    [...]\n    LSMEANS effects  /options-test-n ;  \nRUN;\nQUIT;    PDIFF=ALL  requests p-values for the differences between  ALL  the means and a  diffogram  is produced automatically displaying all pairwise least square means differences and indicating which are significant  It can be undestood as a least squares mean by least squares mean plot  The point estimates for differences between the means for each pairwise comparison can be found at the intersections of the gray grid lines (intersection of appropriate indexes)  The red/blue diagonal lines show the  confidence intervals for the true differences of the means  for each pairwise comparison  The grey 45$^{\\circ}$ reference line represents equality of the means (if the confidence interval crosses over it, then there is no significant difference between the two groups and the diagonal line for the pair will be  dashed and red ; if the difference is significant the line will be  solid and blue )     The  ADJUST=  option specifies the adjustment method for multiple comparisons  If you don't specify an option SAS uses the  Tukey method by default , if you specify  ADJUST=Dunnett  the GLM procedure produces multiple comparisons using  Dunnett's method  and a  control plot    The control plot displays the least squares mean and confidence limits of each group compared to the reference group   The middle  horizontal line represents its least square mean value  (you can see the arithmetic mean value un the  upper right corner  of the graph)  The  shaded area  goes from the  lower decision limit (LDL)  to the  upper decision limit (UDL)  There is a vertical line for each group that you're comparing to the reference (control) group. If a  vertical line extends past the shaded area , then the group represented by the line is  significantly different  (small p-value) than the reference group        PDIFF=CONTROLU('value')  specifies the control group for the Dunnett's case: the direction of the sign in Ha is the same as the direction you are testing, so this is a  one-sided upper-tailed t-test  If you specify  ADJUST=T  SAS will make no adjustments for multiple comparisons: is not recommended as there's a tendency to find  more significant pairwise differences than might actually exist", 
            "title": "PROC GLM"
        }, 
        {
            "location": "/statistics/anova/#two-way-anova-with-interactions", 
            "text": "When you have a continuous response variable and  two categorical predictor variables , you use the  two-way ANOVA model   Effect : the magnitude of the expected change in the response variable presumably caused by the change in value of a predictor variable in the model  In addition, the variables in a model can be referred to as effects or terms  Main effect : is the effect of a single predictor variable  Interaction effects : when the relationship of the response variable with a predictor changes with the changing of another predictor variable (the effect of one variable depends on the value of the other variable)    When you consider an ANOVA with more than one predictor variable, it's called  n-way ANOVA  where  n  represents the number of predictor variables   The analysis in a  randomized block design  is actually a  special type of two-way ANOVA  in which you have one factor of interest and one blocking factor  When you analyze a two-way ANOVA with interactions, you first look at any tests for  interactions among the factors  If there is  no interaction between the factors  you can interpret the tests for the individual factor effects to determine their significance/non-significance  If an  interaction exists between any factors , the tests for the individual factor effects might be misleading due to masking of these effect by the interaction (this is specially true for unbalanced data with different number of observations for each combination of groups)    When the interaction is not statistically significant  you can analyze the main effect with the model in its current form  (generally the method you use when you analyze designed experiments)  Even when you analyze designed experiments, some statisticians might suggest that if the interaction is not significant,  you can delete the interaction effect from your model, rerun the model and then just analyze the main effects  increasing the power of the main effects test  If the  interaction term is significant , it is good practice to keep the main effect terms that make up the interaction in the model, whether they are significant or not (this preserves model hierarchy)  You have to make the  same three assumptions used in the ANOVA test  The interaction terms are also called  product terms  or  crossed effects", 
            "title": "Two-Way ANOVA with Interactions"
        }, 
        {
            "location": "/statistics/anova/#proc-glm_3", 
            "text": "1\n2\n3\n4\n5\n6\n7 PROC GLM DATA=SAS-data-set;\n    CLASS independent1 independent2;\n    MODEL dependent = independent1 independent2 independent1*independent2;\n    or\n    MODEL dependent = independent1 | independent2;\nRUN;\nQUIT;   This program is  fitting to this model :   Y_{ijk}=\\mu + \\alpha_i+\\beta_j+(\\alpha\\beta)_{ij}+\\epsilon_{ijk}    dependent = overall mean + intependent1 + independent2 + interaction12 + unaccounted for variation    In  most situations  you will want to use the  Type III SS  The  Type I SS (sequential)  are the sums of squares you obtain from fitting the effects in the order you specify in the model   The  Type III SS (marginal)  are the sums of squares you obtain from fitting each effect after all the other terms in the model, that is the sums of squares for each effect corrected for the other terms in the model  When examining these results you first have to  look at the interaction term and if it's significant  (p-value), the  main effects don't tell you the whole story . It that is the case, you don't need to worry all that much about the significance of the main effects at this point for two reasons:  You know that the effect of each variable1 level changes for the different variable2 levels  You want to include the main effects in the model, whether they are significant or not, to preserve model hierarchy    You can analyze the interaction between terms by looking at the  interaction plot  that SAS produces by default when you include an interaction term in the model  To analyze and interpret the effect of one of the interacting variables you need to add the  LSMEANS  statement to your program   1\n2\n3\n4\n5\n6 PROC GLM DATA=SAS-data-set ORDER=INTERNAL PLOTS(ONLY)=INTPLOT;\n    CLASS independent1 independent2;\n    MODEL dependent = intependent1 independent2 independent1*independent2;\n    LSMEANS independent1*independent2 / SLICE= independent1;\nRUN;\nQUIT;   SAS creates two types of mean plots when you use the  LSMEANS  statement with an interaction term:   The first plot displays the  least squares mean (LS-Mean) for every effect level    The second plot contains the same information rearranged so you can  look a little closer at the combination levels", 
            "title": "PROC GLM"
        }, 
        {
            "location": "/statistics/anova/#store-statement", 
            "text": "You can add a  STORE  statement to save your analysis results in an  item store  (a binary file format that cannot be modified). This allows you to  run post-processing analysis  on the stored results even if you no longer have access to the original data set. The  STORE  statement applies to the following SAS/STAT procedures:  GENMOD ,  GLIMMIX ,  GLM ,  GLMSELECT ,  LOGISTIC ,  MIXED ,  ORTHOREG ,  PHREG ,  PROBIT ,  SURVEYLOGISTIC ,  SURVEYPHREG , and  SURVEYREG .  1\n2 STORE  OUT = item-store-name\n     / LABEL= label ;    item-store-name  is a usual one- or two-level SAS name, similar to the names that are used for SAS data sets  label  identifies the estimate on the output (is optional)", 
            "title": "STORE statement"
        }, 
        {
            "location": "/statistics/anova/#proc-plm", 
            "text": "To perform post-fitting statistical analysis and plotting for the contents of the store item, you use  PROC PLM . The statements and options that are available vary depending upon which procedure you used to produce the item store.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 PROC PLM RESTORE=item-store-specification  options ;\n    EFFECTPLOT INTERACTION(SLICEBY=variable)  plot-type   (plot-definition   options)  / CLM  / options ;\n    LSMEANS  model-effects   / options ;\n    LSMESTIMATE model-effect  label  values\n         divisor =n ,... label  values\n         divisor =n   / options ;\n    SHOW options;\n    SLICE model-effect / SLICEBY=variable ADJUST=tukey  / options ;\n    WHERE expression;\nRUN;    RESTORE  specifies the source item store for processing  EFFECTPLOT  produces a display of the fitted model and provides options for changing and enhancing the displays  LSMEANS  computes and compares least squares means (LS-means) of fixed effects  LSMESTIMATE  provides custom hypothesis tests among least squares means  SHOW  uses  ODS  to display contents of the item store. This statement is useful for verifying that the contents of the item store apply to the analysis and for generating  ODS  tables.  SLICE  provides a general mechanism for performing a partitioned analysis of the LS-means for an interaction (analysis of simple effects) and it uses the same options as the  LSMEANS  statement  WHERE  is used in the PLM procedure when the item store contains  BY-variable  information and you want to apply the  PROC PLM  statements to only a subset of the BY groups", 
            "title": "PROC PLM"
        }, 
        {
            "location": "/statistics/regression/", 
            "text": "Chapter summary in SAS\n\n\n\n\nExploratory Data Analysis\n\n\nA useful set of techniques for investigating your data is known as \nexploratory data analysis\n.\n\n\nPROC SGCATTER\n: Scatter Plots\n\n\n1\n2\n3\nPROC SGSCATTER DATA=SAS-data-base;\n    PLOT variableY*(variableX1 variableX2) / REG;\nRUN;\n\n\n\n\n\n\n\n\nIf you have \nso many observations\n that the scatter plot of the whole data set is difficult to interpret, you might run \nPROC SGSCATTER\n on a \nrandom sample of observations\n\n\n\n\nPROC CORR\n: Correlation Analysis\n\n\nThere are two ways of calculating correlations: the \nPearson correlation coefficient\n (specially used for normal distributed data) and the \nSpearman's rank correlation coefficient\n (which is best fitted when your data presents outliers).\n\n\nThe Spearman correlation between two variables is equal to the Pearson correlation between the rank values of those two variables; while Pearson's correlation assesses linear relationships, Spearman's correlation assesses monotonic relationships (whether linear or not). Spearman's coefficient is appropriate for both continuous and discrete ordinal variables.\n\n\nThe closer the \nPearson\n correlation coefficient is to +1/-1, the stronger the positive/negative linear relationship is between the two variables. The closer the correlation coefficient is to 0, the weaker the linear relationship and if it is 0 variables are uncorrelated.\n\n\n\n\nWhen you interpret the correlation, be cautious about the effect of \nlarge sample sizes\n: even a correlation of 0.01 can be statistically significant with a large enough sample size and you would almost always reject the hypothesis $H_0$: $\\rho =0$, even if the value of your correlation is small for all practical purposes\n\n\nSome \ncommon errors\n on interpreting correlations are concluding a \ncause-and-effect relationship\n between the variables misinterpreting the kind of relationship between the variables and failing to recognize the influence of outliers on the correlation\n\n\nThe variables might be related but not causally\n\n\nCorrelation coefficients can be large because both variables are affected by other variables\n\n\nVariables might be strongly correlated by chance\n\n\n\n\n\n\nJust because the correlation coefficient is close to 0 doesn't mean that no relationship exists between the two variables: they might have a \nnon-linear relationship\n\n\nAnother common error is failing to recognize the \ninfluence of outliers\n on the correlation\n\n\nIf you have an outlier you should report both correlation coefficients (with and without the outlier) to report how influential the unusual data point is in your analysis\n\n\n\n\n\n\n\n\nThe \nPROC CORR\n also produces \nscatter plots\n or a \nscatter plot matrix\n.\n\n\n1\n2\n3\n4\n5\nPROC CORR DATA=SAS-data-set RANK|NOSIMPLE PLOTS(ONLY)=MATRIX(NVAR=ALL HISTOGRAM)|SCATTER(NVAR=ALL ELLIPSE=NONE) \noptions\n;\n    VAR variable(s)X;\n    WITH variable(s)Y;\n    ID variable4label;\nRUN;\n\n\n\n\n\n\nSimple Linear Regression\n\n\nYou use correlation analysis to determine the strength of the linear relationship between continuous response variables. Now you need to go a step further and \ndefine the linear relationship itself\n: $Y= \\beta_0+\\beta_1 \\cdot X+\\epsilon$\n\n\n\n\n$Y$ is the response variable \n\n\n$X$ is the predictor variable\n\n\n$\\beta_0$ is the intercept parameter\n\n\n$\\beta_1$ is the slope parameter\n\n\n$\\epsilon$ is the error term\n\n\n\n\nThe method of \nleast squares\n produces parameter estimates $\\hat \\beta_0$ and $\\hat \\beta_1$ with certain \noptimum properties\n which make them the Best Linear Unbiased Estimators (\nBLUE\n):\n\n\n\n\nThey are \nunbiased estimates\n of the population parameters\n\n\nThey have \nminimum variance\n\n\n\n\nTo find out how much better is the model that takes the predictor variable into account than a model that ignores the predictor variable, you can compare the \nsimple linear regression model\n to a \nbaseline model\n ($Y= \\bar Y$ independent of $X$). For your comparison, you calculate the \nexplained\n, \nunexplained\n and \ntotal variability\n in the simple linear regression model.\n\n\n\n\nThe \nexplained variability (SSM)\n is the difference between the regression line and the mean of the response variable: $\\sum(\\hat Y_i-\\bar Y)^2$\n\n\nThe \nunexplained variability (SSE)\n is the difference between the observed values and the regression line: $\\sum(Y_i-\\hat Y_i)^2$\n\n\nThe \ntotal variability\n is the difference between the observed values and the mean of the response variable: $\\sum(Y_i-\\bar Y)^2$\n\n\n\n\nIf we consider \nhypothesis testing\n for linear regression:\n\n\n\n\n$H_0$: the regression model does not fit the data better than the baseline model (slope $= 0$)\n\n\n$H_a$: the regression model does fit the data better than the baseline model (slope $= \\hat\\beta_1 \\ne 0$)\n\n\n\n\nThese \nassumptions\n underlie the hypothesis test for the regression model and have to be met for a simple linear regression analysis to be valid (last three assumptions are the same as for ANOVA):\n\n\n\n\nThe mean of the response variable is linearly related to the value of the predictor variable\n\n\nThe error terms are normally distributed with a mean of 0\n\n\nThe error terms have equal variances\n\n\nThe error terms are independent at each value of the predictor variable\n\n\n\n\nPROC REG\n\n\n1\n2\n3\n4\n5\nPROC REG DATA=SAS-data-set \noptions\n;\n    MODEL dependent=regressor / CLM CLI \n/options\n;\n    ID regressor;\nRUN;\nQUIT;\n\n\n\n\n\n\nTo asses the level of precision around the mean estimates you can produce \nconfidence intervals\n around the means. Confidence intervals become wider as you move away from the mean of the predictor variable. The wider the confidence interval the less precise it is. You might also want to construct \nprediction intervals\n for a single observation. A prediction interval is wider than a confidence interval because \nsingle observations have more variability than sample means\n.\n\n\nFor producing \npredicted values\n with \nPROC REG\n:\n\n\n\n\nCreate a data set containing the values of the independent variables for which you want to make predictions\n\n\nConcatenate the new data set with the original data set\n\n\nFit a simple linear regression model to the new data set and specify the \nP\n option in the \nMODEL\n statement\n\n\n\n\nBecause the concatenated observations contain \nmissing values\n for the response variable, \nPROC REG\n does not include these observations when fitting the regression model. However, \nPROC REG\n does \nproduce predicted values\n for these observations.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\nDATA SAS-predictions-data-set;\n    INPUT dependent @@;\n    DATALINES;\n[new values separated with blanks]\n;\nRUN;\n\nDATA SAS-new-data-set;\n    SET SAS-predictions-data-set SAS-original-data-set;\nRUN;\n\nPROC REG DATA=SAS-new-data-set;\n    MODEL dependent=regressor / P;\n    ID regressor;\nRUN;\nQUIT;\n\n\n\n\n\n\nWhen you use a model to predict future values of the response variable given certain values of the predictor variable, you must \nstay within (or near) the range of values for the predictor variable used to create the model\n. The relationship between the predictor variable and the response variable might be different beyond the range of the data.\n\n\nIf you have a large data set and have already fitted the regression model, you can predict values more efficiently by using \nPROC REG\n and \nPROC SCORE\n:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\nPROC REG DATA=SAS-original-data-set NOPRINT OUTEST=SAS-estimates-data-set;\n    MODEL dependent=regressor \n/options\n;\n    ID regressor;\nRUN;\nQUIT;\n\nPROC SCORE DATA=SAS-predictions-data-set\n        SCORE=SAS-estimates-data-set\n        OUT=SAS-scored-data-set\n        TYPE=PARMS\n        \noptions\n;\n    VAR variable(s);\nRUN;\nQUIT;\n\n\n\n\n\n\nMultiple Regression\n\n\nIn \nmultiple regression\n you can model the relationship between the response variable and \nmore than one predictor variable\n. It is a powerful tool for both \nanalytical or explanatory analysis and for prediction\n.\n\n\n$Y=\\beta_0+\\beta_1X_1+\\beta_2X_2+...+\\beta_kX_k+\\epsilon$ ($k+1$ parameters)\n\n\nAdvantages\n\n\n\n\nMultiple linear regression is a more powerful tool\n\n\nYou can determine whether a relationship exists between the response variable and more than one predictor variable at the same time\n\n\n\n\nDisadvantages\n\n\n\n\nYou need to perform a selection process to decide which model to use\n\n\nThe more predictors you have, the more complicated interpreting the model becomes\n\n\n\n\nIf we consider \nhypothesis testing\n for linear regression:\n\n\n\n\n$H_0$: the regression model does not fit the data better than the baseline model $(\\beta_1=\\beta_2=...=\\beta_k= 0)$\n\n\n$H_a$: the regression model does fit the data better than the baseline model (at least one $\\beta_i \\ne 0$)\n\n\n\n\nThese \nassumptions\n have to be met for a multiple linear regression analysis to be valid (last three assumptions are the same as for ANOVA):\n\n\n\n\nA linear function of the $X$s accurately models the mean of the $Y$s\n\n\nThe error terms are normally distributed with a mean of 0\n\n\nThe error terms have constant variances\n\n\nThe error terms are independent at each value of the predictor variable\n\n\n\n\nThe \nregular $R^2$\n values never decrease when you add more terms to the model, but the \nadjusted $R^2$\n value takes into account the number of terms in the model by including a penalty for the complexity of the model. The \nadjusted $R^2$\n value increases only if new terms that you add significantly improve the model enough to warrant increasing the complexity of the model. It enables proper comparison between models with different parameter counts. When an \nadjusted $R^2$ increases by removing a variable\n from the models, it strongly implies that the removed \nvariable was not necessary\n.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\nPROC REG DATA=SAS-data-set \noptions\n;\n    MODEL dependent=regressor1 regressor2 \n/options\n;\nRUN;\nQUIT;\n\nPROC GLM DATA=SAS-data-set\n    PLOTS(ONLY)=(CONTOURFIT);\n    MODEL dependent=regressor1 regressor2;\n    STORE OUT=SAS-multiple-data-set;\nRUN;\nQUIT;\n\nPROC PLM RESTORE=SAS-multiple-data-set PLOTS=ALL;\n    EFFECTPLOT CONTOUR (Y=regressor1 X=regressor2);\n    EFFECTPLOT SLICEFIT (X=regressor2 SLICEBY=regressor1=250 to 1000 by 250);\nRUN;\n\n\n\n\n\n\n\n\nIn \nPROC GLM\n, when you run a linear regression model with only two predictor variables, the output includes a contour fit plot by default. We specify \nCONTOURFIT\n to tell SAS to overlay the contour plot with a scatter plot of the observed data\n\n\n\n\n\n\nThe plot shows \npredicted values\n of the response variable as \ngradations of the background color\n from blue, representing low values, to red, representing high values. The \ndots\n, which are similarly coloured, represent the \nactual data\n. Observations that are perfectly fit would show the same color within the circle as outside the circle. The \nlines on the graph\n help you read the actual predictions at even intervals.\n\n\n\n\nThe \nCONTOUR\n option displays a contour plot of predicted values against two continuous covariates\n\n\nThe \nSLICEFIT\n option displys a curve of predicted values vs a continuous variable grouped by the levels of another effect\n\n\n\n\nClearly the \nPROC GLM\n contour fit plot is \nmore useful\n. However, if you do not have access to the original data set and can run \nPROC PLM\n only on the item store, this plot still gives you an idea of the relationship between the predictor variables and predicted values.\n\n\nModel Building and Interpretation\n\n\nThe brute force approach to find a good model is to start including all the predictor variables available and rerun the model \nremoving the least significant remaining term\n each time \nuntil\n you're left with a model where \nonly significant terms remain\n. With a small number of predictor variables a manual approach isn't too difficult but with a large number of predictor variables it's very tedious. Fortunately, if you specify the model selection technique to use, SAS finds good candidate models in an automatic way.\n\n\nAll-possible regression methods\n\n\nSAS computes all possible models and ranks the results. Then, to evaluate the models, you compare statistics side by side ($R^2$, adjusted $R^2$ and $C_p$ statistic).\n\n\n\n\n\n\nMallows' $C_p$\n statistic helps you detect model bias if you are underfitting/overfitting the model, it is a simple indicator of effective variable selection within a model\n\n\n\n\n\n\nTo select the best model for prediction (most accurate model for predicting future values of $Y$), you should use the \nMallows' criterion\n:  $C_p \\le p$, which is the \nnumber of parameters\n in the model including the intercept\n\n\n\n\nTo select the best model for parameter estimation (analytical or explanatory analysis), you should use \nHocking's criterion\n: $C_p\\le2p-p_{full}+1$\n\n\n\n\n1\n2\n3\n4\nPROC REG DATA=SASdata-set PLOTS(ONLY)=(CP) \noptions\n;\n    \nlabel:\n MODEL dependent=regressors  / SELECTION=CP RSQUARE ADJRSQ BEST=n \n/options\n;\nRUN;\nQUIT;\n\n\n\n\n\n\n\n\nBEST\n prints an specific number of the best candidate models according to a few different statistical criteria\n\n\nSELECTION\n option is used to specify the method used to select the model (\nCP\n, \nRSQUARE\n and \nADJRSQ\n to calculate with the all-possible regression model; the first statistic determines the sorting order)\n\n\nFor this all-possible regression model, we add the label \nALL_REG:\n\n\nWith \nPLOTS=(CP)\n we produce a plot:\n\n\n\n\n\n\nEach \nstar\n represents the \nbest model\n for a given number of parameters. The solid \nblue line\n represents \nMallows' criterion\n for $C_p$, so using this line helps us find a good candidate model for prediction. Because we want the \nsmallest model possible\n, we start at the left side of the graph, with the fewest number of parameters moving to the right until we find the \nfirst model that falls below the solid blue line\n. To find models for parameter estimation we have to look for models that falls below the \nred solid line\n which represent the \nHocking's criterion\n for $C_p$ parameter estimation. If we hover over the star, we can see which variables are included in this model.\n\n\nStepwise selection methods\n\n\nHere you choose a selection method (\nstepwise\n, \nforward\n or \nbackward\n approaches) and SAS constructs a model based on that method. When you have a \nlarge number of potential predictor variables\n, the stepwise regression methods might be a better option. You can use either the \nREG\n procedure or the \nGLMSELECT\n procedure to perform stepwise selection methods\n\n\n\n\nForward selection\n starts with no predictor variables in the model\n\n\nIt selects the best one-variable model\n\n\nIt selects the best two-variable model that includes the variable from the first model (after a variable is added to the model, it stays in even if it becomes insignificant later)\n\n\nIt keeps adding variables, one at a time, until no significant terms are left to add\n\n\n\n\n\n\nBackward selection/elimination\n starts with all predictor variables in the model\n\n\nIt removes variables one at a time, starting with the most non-significant variable (after a variable is removed from the model, it cannot reenter)\n\n\nIt stops when only significant terms are left in the model\n\n\n\n\n\n\nStepwise selection\n combines aspects of both forward and backward selection\n\n\nIt starts with no predictor variables in the model and starts adding variables, one at a time, as in forward selection\n\n\nHowever, as in backward selection, stepwise selection can drop non-significant variables, one at a time\n\n\nIt stops when everything in the model is currently significant and everything not in the model is not significant\n\n\n\n\n\n\n\n\nStatisticians in general agree on first using \nstepwise methods\n to identify several good candidates models and then applying your \nsubject matter expertise\n to choose the best model. Because the techniques for selecting or eliminating variables differ between the three selection methods, \nthey don't always produce the same final model\n. There is no one method that is best and \nyou need to be cautious\n when reporting statistical quantities produced by these methods:\n\n\n\n\nUsing automated model selections results in \nbiases in parameter estimates\n, \npredictions\n and \nstandard errors\n\n\nIncorrect\n calculation of \ndegrees of freedom\n\n\np-values\n that tend to err on the side of \noverestimating significance\n\n\n\n\nHow can you \navoid these issues\n?\n\n\n\n\nYou can hold out some of your data in order to perform an honest assessment of how well your model performs on a different sample of data (\nholdout/validation data\n) than you use to develop the model (\ntraining data\n)\n\n\nOther honest assessment approaches include \ncross-validation\n (if your data set is not large enough to split) or \nbootstraping\n (a resampling method that tries to approximate the distribution of the parameter estimates to estimate the standard error and p-values)\n\n\n\n\nUsing Lasso for Predictor Selection\n\n\nThe most widely researched and implemented modern method is the \nleast absolute shrinkage and selection operator (Lasso)\n, which fits within the broader \nleast angle regression framework (LARS)\n that can estimate Lasso with the computational complexity of ordinary least squares (OLS). Despite the overwhelming support for modern methods like Lasso in the statistical literature, \nmore traditional methods such as p values or automatic selection methods such as forward, backward, or stepwise selection are still widely used\n even though short-comings of these methods have been disseminated for over a decade.\n\n\nWhen fitting any type of regression model, random noise can become entangled with signal, especially with small or moderate sample sizes. This can lead to estimates that overstate the impact of particular predictor variables and attribute more predictive power to the model than is present in the population. When this occurs, the model is said to be \noverfit\n with the consequence that \nregression coefficients have inflated magnitude, standard errors are underestimated, p values are consequently too small, $R^2$ values are consequently too large compared to their population values\n, and the model is not parsimonious because extraneous predictors may be seen as important.\n\n\n\n\nTip\n\n\nA widely quoted rule is that \nyou need 10 or 15 observations per independent variable\n in a regression model.\nTo avoid over-fitting in a binary logistic regression model, you need to focus on the number of events per variable (EPV), not the total number of cases (i.e. events plus non-events). An event is defined as the outcome category (0 or 1) with the lower frequency. For example, if a sample of 200 patients are studied and 180 patients die during the study (so that 20 patients survive), only two pre-specified predictors can reliably be fitted to the total data. Similarly, if 120 patients die during the study (so that 80 patients survive), eight pre-specified predictors (based on the smallest of the two counts, being 80) can be fitted reliably. If more are fitted, overfitting is likely and the results will not predict well outside the training data. Sometimes this rule is too conservative and \ncan be relaxed\n.\nNotice that having a sample size large enough to avoid over-fitting is not the same thing as having a sample size large enough to ensure adequate power. That's a separate issue.\n\n\n\n\nPROC GLMSELECT\n\n\n1\n2\n3\n4\n5\nPROC GLMSELECT DATA=SAS-data-set \noptions\n;\n    CLASS variables;\n    \nlabel:\n MODEL dependent(s) = regressor(s) / \n/options\n;\nRUN;\nQUIT;\n\n\n\n\n\n\n\n\nThe \nSELECTION\n option specifies the method to be used to select the model (\nFORWARD\n | \nBACKWARD\n | \nSTEPWISE\n = default value)\n\n\nThe \nSELECT\n option specifies the criterion to be used to determine which variable to add/remove from the model (\nSL\n = significance level as the selection criterion)\n\n\nThe \nSLENTRY\n option determines the significance level for a variable to enter the model (default = 0.5 for forward and 0.15 for stepwise)\n\n\nThe \nSLSTAY\n option determines the significance level for a variable to stay in the model (default = 0.1 for backward and 0.15 for stepwise)\n\n\nYou can display p-values in the \nParameter Estimates\n table by including the \nSHOWPVALUES\n option int he MODEL statement\n\n\nThe \nDETAILS\n option specifies the level of detail produced (\nALL\n | \nSTEPS\n | \nSUMMARY\n)\n\n\n\n\n\n\nRecommendations to decide which model is best for your needs:\n\n\n\n\nRun all model selection methods\n\n\nLook for commonalities across the results \n\n\nNarrow down your choice of models by using your subject matter knowledge\n\n\n\n\nInformation Criterion and Other Selection Options\n\n\nThere are other selection criteria that you can use to select variables for a model as well as evaluate competing models. These statistics are collectively referred to as \ninformation criteria\n. Each information criterion searched for a model that minimizes the \nunexplained variability\n with as \nfew effects in the model as possible\n. The model with the \nsmaller information criterion is considered to be better\n. For types are available in \nPROC GLMSELECT\n:\n\n\n\n\nAkaike's information criterion (\nSELECT=AIC\n)\n\n\nCorrecterd Akaike's information criterion (\nSELECT=AICC\n)\n\n\nSawa Bayesian information criterion (\nSELECT=BIC\n)\n\n\nSchwarz Bayesian information criterion (\nSELECT=SBC\n, it could be called \nBIC\n in some other SAS procedures)\n\n\n\n\nThe calculations of all information criteria begin the same way:\n\n\n\n\nFirst you calculate $n\\cdot log(SSE/n)$ \n\n\nThen, each criterion adds a penalty that represents the complexity of the model (each type of information criterion invokes a different penalty component)\n\n\nAIC\n: $2p+n+2$\n\n\nAICC\n: $n(n+p)/(n-p-2)$\n\n\nBIC\n: $2(p+2)1-2q^2$\n\n\nSBC\n: $p\\cdot log(n)$", 
            "title": "Regression"
        }, 
        {
            "location": "/statistics/regression/#exploratory-data-analysis", 
            "text": "A useful set of techniques for investigating your data is known as  exploratory data analysis .", 
            "title": "Exploratory Data Analysis"
        }, 
        {
            "location": "/statistics/regression/#proc-sgcatter-scatter-plots", 
            "text": "1\n2\n3 PROC SGSCATTER DATA=SAS-data-base;\n    PLOT variableY*(variableX1 variableX2) / REG;\nRUN;    If you have  so many observations  that the scatter plot of the whole data set is difficult to interpret, you might run  PROC SGSCATTER  on a  random sample of observations", 
            "title": "PROC SGCATTER: Scatter Plots"
        }, 
        {
            "location": "/statistics/regression/#proc-corr-correlation-analysis", 
            "text": "There are two ways of calculating correlations: the  Pearson correlation coefficient  (specially used for normal distributed data) and the  Spearman's rank correlation coefficient  (which is best fitted when your data presents outliers).  The Spearman correlation between two variables is equal to the Pearson correlation between the rank values of those two variables; while Pearson's correlation assesses linear relationships, Spearman's correlation assesses monotonic relationships (whether linear or not). Spearman's coefficient is appropriate for both continuous and discrete ordinal variables.  The closer the  Pearson  correlation coefficient is to +1/-1, the stronger the positive/negative linear relationship is between the two variables. The closer the correlation coefficient is to 0, the weaker the linear relationship and if it is 0 variables are uncorrelated.   When you interpret the correlation, be cautious about the effect of  large sample sizes : even a correlation of 0.01 can be statistically significant with a large enough sample size and you would almost always reject the hypothesis $H_0$: $\\rho =0$, even if the value of your correlation is small for all practical purposes  Some  common errors  on interpreting correlations are concluding a  cause-and-effect relationship  between the variables misinterpreting the kind of relationship between the variables and failing to recognize the influence of outliers on the correlation  The variables might be related but not causally  Correlation coefficients can be large because both variables are affected by other variables  Variables might be strongly correlated by chance    Just because the correlation coefficient is close to 0 doesn't mean that no relationship exists between the two variables: they might have a  non-linear relationship  Another common error is failing to recognize the  influence of outliers  on the correlation  If you have an outlier you should report both correlation coefficients (with and without the outlier) to report how influential the unusual data point is in your analysis     The  PROC CORR  also produces  scatter plots  or a  scatter plot matrix .  1\n2\n3\n4\n5 PROC CORR DATA=SAS-data-set RANK|NOSIMPLE PLOTS(ONLY)=MATRIX(NVAR=ALL HISTOGRAM)|SCATTER(NVAR=ALL ELLIPSE=NONE)  options ;\n    VAR variable(s)X;\n    WITH variable(s)Y;\n    ID variable4label;\nRUN;", 
            "title": "PROC CORR: Correlation Analysis"
        }, 
        {
            "location": "/statistics/regression/#simple-linear-regression", 
            "text": "You use correlation analysis to determine the strength of the linear relationship between continuous response variables. Now you need to go a step further and  define the linear relationship itself : $Y= \\beta_0+\\beta_1 \\cdot X+\\epsilon$   $Y$ is the response variable   $X$ is the predictor variable  $\\beta_0$ is the intercept parameter  $\\beta_1$ is the slope parameter  $\\epsilon$ is the error term   The method of  least squares  produces parameter estimates $\\hat \\beta_0$ and $\\hat \\beta_1$ with certain  optimum properties  which make them the Best Linear Unbiased Estimators ( BLUE ):   They are  unbiased estimates  of the population parameters  They have  minimum variance   To find out how much better is the model that takes the predictor variable into account than a model that ignores the predictor variable, you can compare the  simple linear regression model  to a  baseline model  ($Y= \\bar Y$ independent of $X$). For your comparison, you calculate the  explained ,  unexplained  and  total variability  in the simple linear regression model.   The  explained variability (SSM)  is the difference between the regression line and the mean of the response variable: $\\sum(\\hat Y_i-\\bar Y)^2$  The  unexplained variability (SSE)  is the difference between the observed values and the regression line: $\\sum(Y_i-\\hat Y_i)^2$  The  total variability  is the difference between the observed values and the mean of the response variable: $\\sum(Y_i-\\bar Y)^2$   If we consider  hypothesis testing  for linear regression:   $H_0$: the regression model does not fit the data better than the baseline model (slope $= 0$)  $H_a$: the regression model does fit the data better than the baseline model (slope $= \\hat\\beta_1 \\ne 0$)   These  assumptions  underlie the hypothesis test for the regression model and have to be met for a simple linear regression analysis to be valid (last three assumptions are the same as for ANOVA):   The mean of the response variable is linearly related to the value of the predictor variable  The error terms are normally distributed with a mean of 0  The error terms have equal variances  The error terms are independent at each value of the predictor variable", 
            "title": "Simple Linear Regression"
        }, 
        {
            "location": "/statistics/regression/#proc-reg", 
            "text": "1\n2\n3\n4\n5 PROC REG DATA=SAS-data-set  options ;\n    MODEL dependent=regressor / CLM CLI  /options ;\n    ID regressor;\nRUN;\nQUIT;   To asses the level of precision around the mean estimates you can produce  confidence intervals  around the means. Confidence intervals become wider as you move away from the mean of the predictor variable. The wider the confidence interval the less precise it is. You might also want to construct  prediction intervals  for a single observation. A prediction interval is wider than a confidence interval because  single observations have more variability than sample means .  For producing  predicted values  with  PROC REG :   Create a data set containing the values of the independent variables for which you want to make predictions  Concatenate the new data set with the original data set  Fit a simple linear regression model to the new data set and specify the  P  option in the  MODEL  statement   Because the concatenated observations contain  missing values  for the response variable,  PROC REG  does not include these observations when fitting the regression model. However,  PROC REG  does  produce predicted values  for these observations.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16 DATA SAS-predictions-data-set;\n    INPUT dependent @@;\n    DATALINES;\n[new values separated with blanks]\n;\nRUN;\n\nDATA SAS-new-data-set;\n    SET SAS-predictions-data-set SAS-original-data-set;\nRUN;\n\nPROC REG DATA=SAS-new-data-set;\n    MODEL dependent=regressor / P;\n    ID regressor;\nRUN;\nQUIT;   When you use a model to predict future values of the response variable given certain values of the predictor variable, you must  stay within (or near) the range of values for the predictor variable used to create the model . The relationship between the predictor variable and the response variable might be different beyond the range of the data.  If you have a large data set and have already fitted the regression model, you can predict values more efficiently by using  PROC REG  and  PROC SCORE :   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 PROC REG DATA=SAS-original-data-set NOPRINT OUTEST=SAS-estimates-data-set;\n    MODEL dependent=regressor  /options ;\n    ID regressor;\nRUN;\nQUIT;\n\nPROC SCORE DATA=SAS-predictions-data-set\n        SCORE=SAS-estimates-data-set\n        OUT=SAS-scored-data-set\n        TYPE=PARMS\n         options ;\n    VAR variable(s);\nRUN;\nQUIT;", 
            "title": "PROC REG"
        }, 
        {
            "location": "/statistics/regression/#multiple-regression", 
            "text": "In  multiple regression  you can model the relationship between the response variable and  more than one predictor variable . It is a powerful tool for both  analytical or explanatory analysis and for prediction .  $Y=\\beta_0+\\beta_1X_1+\\beta_2X_2+...+\\beta_kX_k+\\epsilon$ ($k+1$ parameters)  Advantages   Multiple linear regression is a more powerful tool  You can determine whether a relationship exists between the response variable and more than one predictor variable at the same time   Disadvantages   You need to perform a selection process to decide which model to use  The more predictors you have, the more complicated interpreting the model becomes   If we consider  hypothesis testing  for linear regression:   $H_0$: the regression model does not fit the data better than the baseline model $(\\beta_1=\\beta_2=...=\\beta_k= 0)$  $H_a$: the regression model does fit the data better than the baseline model (at least one $\\beta_i \\ne 0$)   These  assumptions  have to be met for a multiple linear regression analysis to be valid (last three assumptions are the same as for ANOVA):   A linear function of the $X$s accurately models the mean of the $Y$s  The error terms are normally distributed with a mean of 0  The error terms have constant variances  The error terms are independent at each value of the predictor variable   The  regular $R^2$  values never decrease when you add more terms to the model, but the  adjusted $R^2$  value takes into account the number of terms in the model by including a penalty for the complexity of the model. The  adjusted $R^2$  value increases only if new terms that you add significantly improve the model enough to warrant increasing the complexity of the model. It enables proper comparison between models with different parameter counts. When an  adjusted $R^2$ increases by removing a variable  from the models, it strongly implies that the removed  variable was not necessary .   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16 PROC REG DATA=SAS-data-set  options ;\n    MODEL dependent=regressor1 regressor2  /options ;\nRUN;\nQUIT;\n\nPROC GLM DATA=SAS-data-set\n    PLOTS(ONLY)=(CONTOURFIT);\n    MODEL dependent=regressor1 regressor2;\n    STORE OUT=SAS-multiple-data-set;\nRUN;\nQUIT;\n\nPROC PLM RESTORE=SAS-multiple-data-set PLOTS=ALL;\n    EFFECTPLOT CONTOUR (Y=regressor1 X=regressor2);\n    EFFECTPLOT SLICEFIT (X=regressor2 SLICEBY=regressor1=250 to 1000 by 250);\nRUN;    In  PROC GLM , when you run a linear regression model with only two predictor variables, the output includes a contour fit plot by default. We specify  CONTOURFIT  to tell SAS to overlay the contour plot with a scatter plot of the observed data    The plot shows  predicted values  of the response variable as  gradations of the background color  from blue, representing low values, to red, representing high values. The  dots , which are similarly coloured, represent the  actual data . Observations that are perfectly fit would show the same color within the circle as outside the circle. The  lines on the graph  help you read the actual predictions at even intervals.   The  CONTOUR  option displays a contour plot of predicted values against two continuous covariates  The  SLICEFIT  option displys a curve of predicted values vs a continuous variable grouped by the levels of another effect   Clearly the  PROC GLM  contour fit plot is  more useful . However, if you do not have access to the original data set and can run  PROC PLM  only on the item store, this plot still gives you an idea of the relationship between the predictor variables and predicted values.", 
            "title": "Multiple Regression"
        }, 
        {
            "location": "/statistics/regression/#model-building-and-interpretation", 
            "text": "The brute force approach to find a good model is to start including all the predictor variables available and rerun the model  removing the least significant remaining term  each time  until  you're left with a model where  only significant terms remain . With a small number of predictor variables a manual approach isn't too difficult but with a large number of predictor variables it's very tedious. Fortunately, if you specify the model selection technique to use, SAS finds good candidate models in an automatic way.", 
            "title": "Model Building and Interpretation"
        }, 
        {
            "location": "/statistics/regression/#all-possible-regression-methods", 
            "text": "SAS computes all possible models and ranks the results. Then, to evaluate the models, you compare statistics side by side ($R^2$, adjusted $R^2$ and $C_p$ statistic).    Mallows' $C_p$  statistic helps you detect model bias if you are underfitting/overfitting the model, it is a simple indicator of effective variable selection within a model    To select the best model for prediction (most accurate model for predicting future values of $Y$), you should use the  Mallows' criterion :  $C_p \\le p$, which is the  number of parameters  in the model including the intercept   To select the best model for parameter estimation (analytical or explanatory analysis), you should use  Hocking's criterion : $C_p\\le2p-p_{full}+1$   1\n2\n3\n4 PROC REG DATA=SASdata-set PLOTS(ONLY)=(CP)  options ;\n     label:  MODEL dependent=regressors  / SELECTION=CP RSQUARE ADJRSQ BEST=n  /options ;\nRUN;\nQUIT;    BEST  prints an specific number of the best candidate models according to a few different statistical criteria  SELECTION  option is used to specify the method used to select the model ( CP ,  RSQUARE  and  ADJRSQ  to calculate with the all-possible regression model; the first statistic determines the sorting order)  For this all-possible regression model, we add the label  ALL_REG:  With  PLOTS=(CP)  we produce a plot:    Each  star  represents the  best model  for a given number of parameters. The solid  blue line  represents  Mallows' criterion  for $C_p$, so using this line helps us find a good candidate model for prediction. Because we want the  smallest model possible , we start at the left side of the graph, with the fewest number of parameters moving to the right until we find the  first model that falls below the solid blue line . To find models for parameter estimation we have to look for models that falls below the  red solid line  which represent the  Hocking's criterion  for $C_p$ parameter estimation. If we hover over the star, we can see which variables are included in this model.", 
            "title": "All-possible regression methods"
        }, 
        {
            "location": "/statistics/regression/#stepwise-selection-methods", 
            "text": "Here you choose a selection method ( stepwise ,  forward  or  backward  approaches) and SAS constructs a model based on that method. When you have a  large number of potential predictor variables , the stepwise regression methods might be a better option. You can use either the  REG  procedure or the  GLMSELECT  procedure to perform stepwise selection methods   Forward selection  starts with no predictor variables in the model  It selects the best one-variable model  It selects the best two-variable model that includes the variable from the first model (after a variable is added to the model, it stays in even if it becomes insignificant later)  It keeps adding variables, one at a time, until no significant terms are left to add    Backward selection/elimination  starts with all predictor variables in the model  It removes variables one at a time, starting with the most non-significant variable (after a variable is removed from the model, it cannot reenter)  It stops when only significant terms are left in the model    Stepwise selection  combines aspects of both forward and backward selection  It starts with no predictor variables in the model and starts adding variables, one at a time, as in forward selection  However, as in backward selection, stepwise selection can drop non-significant variables, one at a time  It stops when everything in the model is currently significant and everything not in the model is not significant     Statisticians in general agree on first using  stepwise methods  to identify several good candidates models and then applying your  subject matter expertise  to choose the best model. Because the techniques for selecting or eliminating variables differ between the three selection methods,  they don't always produce the same final model . There is no one method that is best and  you need to be cautious  when reporting statistical quantities produced by these methods:   Using automated model selections results in  biases in parameter estimates ,  predictions  and  standard errors  Incorrect  calculation of  degrees of freedom  p-values  that tend to err on the side of  overestimating significance   How can you  avoid these issues ?   You can hold out some of your data in order to perform an honest assessment of how well your model performs on a different sample of data ( holdout/validation data ) than you use to develop the model ( training data )  Other honest assessment approaches include  cross-validation  (if your data set is not large enough to split) or  bootstraping  (a resampling method that tries to approximate the distribution of the parameter estimates to estimate the standard error and p-values)", 
            "title": "Stepwise selection methods"
        }, 
        {
            "location": "/statistics/regression/#using-lasso-for-predictor-selection", 
            "text": "The most widely researched and implemented modern method is the  least absolute shrinkage and selection operator (Lasso) , which fits within the broader  least angle regression framework (LARS)  that can estimate Lasso with the computational complexity of ordinary least squares (OLS). Despite the overwhelming support for modern methods like Lasso in the statistical literature,  more traditional methods such as p values or automatic selection methods such as forward, backward, or stepwise selection are still widely used  even though short-comings of these methods have been disseminated for over a decade.  When fitting any type of regression model, random noise can become entangled with signal, especially with small or moderate sample sizes. This can lead to estimates that overstate the impact of particular predictor variables and attribute more predictive power to the model than is present in the population. When this occurs, the model is said to be  overfit  with the consequence that  regression coefficients have inflated magnitude, standard errors are underestimated, p values are consequently too small, $R^2$ values are consequently too large compared to their population values , and the model is not parsimonious because extraneous predictors may be seen as important.   Tip  A widely quoted rule is that  you need 10 or 15 observations per independent variable  in a regression model.\nTo avoid over-fitting in a binary logistic regression model, you need to focus on the number of events per variable (EPV), not the total number of cases (i.e. events plus non-events). An event is defined as the outcome category (0 or 1) with the lower frequency. For example, if a sample of 200 patients are studied and 180 patients die during the study (so that 20 patients survive), only two pre-specified predictors can reliably be fitted to the total data. Similarly, if 120 patients die during the study (so that 80 patients survive), eight pre-specified predictors (based on the smallest of the two counts, being 80) can be fitted reliably. If more are fitted, overfitting is likely and the results will not predict well outside the training data. Sometimes this rule is too conservative and  can be relaxed .\nNotice that having a sample size large enough to avoid over-fitting is not the same thing as having a sample size large enough to ensure adequate power. That's a separate issue.", 
            "title": "Using Lasso for Predictor Selection"
        }, 
        {
            "location": "/statistics/regression/#proc-glmselect", 
            "text": "1\n2\n3\n4\n5 PROC GLMSELECT DATA=SAS-data-set  options ;\n    CLASS variables;\n     label:  MODEL dependent(s) = regressor(s) /  /options ;\nRUN;\nQUIT;    The  SELECTION  option specifies the method to be used to select the model ( FORWARD  |  BACKWARD  |  STEPWISE  = default value)  The  SELECT  option specifies the criterion to be used to determine which variable to add/remove from the model ( SL  = significance level as the selection criterion)  The  SLENTRY  option determines the significance level for a variable to enter the model (default = 0.5 for forward and 0.15 for stepwise)  The  SLSTAY  option determines the significance level for a variable to stay in the model (default = 0.1 for backward and 0.15 for stepwise)  You can display p-values in the  Parameter Estimates  table by including the  SHOWPVALUES  option int he MODEL statement  The  DETAILS  option specifies the level of detail produced ( ALL  |  STEPS  |  SUMMARY )    Recommendations to decide which model is best for your needs:   Run all model selection methods  Look for commonalities across the results   Narrow down your choice of models by using your subject matter knowledge", 
            "title": "PROC GLMSELECT"
        }, 
        {
            "location": "/statistics/regression/#information-criterion-and-other-selection-options", 
            "text": "There are other selection criteria that you can use to select variables for a model as well as evaluate competing models. These statistics are collectively referred to as  information criteria . Each information criterion searched for a model that minimizes the  unexplained variability  with as  few effects in the model as possible . The model with the  smaller information criterion is considered to be better . For types are available in  PROC GLMSELECT :   Akaike's information criterion ( SELECT=AIC )  Correcterd Akaike's information criterion ( SELECT=AICC )  Sawa Bayesian information criterion ( SELECT=BIC )  Schwarz Bayesian information criterion ( SELECT=SBC , it could be called  BIC  in some other SAS procedures)   The calculations of all information criteria begin the same way:   First you calculate $n\\cdot log(SSE/n)$   Then, each criterion adds a penalty that represents the complexity of the model (each type of information criterion invokes a different penalty component)  AIC : $2p+n+2$  AICC : $n(n+p)/(n-p-2)$  BIC : $2(p+2)1-2q^2$  SBC : $p\\cdot log(n)$", 
            "title": "Information Criterion and Other Selection Options"
        }, 
        {
            "location": "/statistics/inference/", 
            "text": "Chapter summary in SAS\n\n\nHow to \nverify the assumptions\n and \ndiagnose problems\n that you encounter in \nlinear regression\n?\n\n\nExamining Residuals\n\n\nYou can use the \nresidual values\n (difference between each observed value of $Y$ and its predicted value) from the regression analysis to verify the \nassumptions of the linear regression\n. Residuals are estimates of the errors, so you can \nplot the residuals to check the assumptions of the errors\n.\n\n\n\n\nYou can plot residuals vs the predicted values to check for \nviolations of equal variances\n\n\nYou can also use this plot to check for \nviolations of linearity and independence\n\n\nYou can plot the residuals vs the values of the independent variables to \nfurther examine any violations of equal variances\n (you can see which predictor contributes to the violation of the assumption)\n\n\nYou can use a histogram or a normal probability plot of the residuals to determine whether or not the \nerrors are normally distributed\n\n\n\n\nYou want to see a \nrandom scatter of the residual values\n above and below the reference line at 0. If you see \npatterns or trends\n in the residual values, the assumptions might not be valid and the models might have problems.\n\n\n\n\n\n\nNote\n\n\nTo take autocorrelation (correlated over time) into account, you might need to use a regression procedure such as \nPROC AUTOREG\n\n\n\n\nYou can also use these plots to \ndetect outliers\n, which often reflect data errors or unusual circumstances. They can affect your regression results, so you want to know whether any outliers are present and causing problems and investigate if they result from \ndata entry error or some other problem\n that you can correct.\n\n\nPROC REG\n\n\n1\n2\n3\n4\n5\nPROC REG DATA=SAS-data-set PLOTS(ONLY)=(QQ RESIDUALBYPREDICTED RESIDUALS)\noptions\n;\n    \nlabel:\n MODEL dependent=regressor(s) \n/options\n;\n    ID variable4identification;\nRUN;\nQUIT;\n\n\n\n\n\n\n\n\nQQ\n requests a residual quantile-quantile plot to assess the normality of the residual error\n\n\nRESIDUALBYPREDICTED\n requests a plot of residuals by predicted values to verify the equal variance assumption, the independence assumption and model adequacy\n\n\nRESIDUALS\n requests a panel of plots of residuals by the predictor variables in the model: if any of the \nResidual by Regressors\n plots show signs of unequal variance, we can determine which predictor variable is involved in the problem\n\n\n\n\nIdentifying Influential Observations\n\n\nAn influential observation is different from an outlier. An \noutlier\n is an unusual observation that has a large residual compare to the rest of the points. An \ninfluential observation\n can sometimes have a large residual compared to the rest of the points, but it is an observation so far away from the rest of the data that it singlehandedly exerts influence on the slope of the regression line.\n\n\n\n\nUsing \nSTUDENT\n residuals to detect outliers\n\n\nAlso known as \nstudientized or standardized residuals\n, the \nSTUDENT\n residuals are calculated by dividing the \nresidual by their standard errors\n, so you can think of them as roughly equivalent to a z-score. \n\n\n\n\nFor \nrelatively small sample sizes\n, if the absolute value of the \nSTUDENT\n \nresidual is $\n2$\n, you can suspect that the corresponding observation is an outlier\n\n\nFor \nlarge sample sizes\n, it's very likely that even more \nSTUDENT\n \nresiduals greater than $\\pm2$\n will occur just by chance, so you should typically use a larger cutoff value of $\n3$\n\n\n\n\nUsing \nCOOKSD\n statistics to detect influential observations\n\n\nFor each observation, the Cook's D statistic is \ncalculated as if that observation weren't in the data set\n as well as the set of parameter estimates with all the observations in your regression analysis. \n\n\n\n\nIf any observation has a Cook's D \nstatistic $\n4/n$\n that observation is influential\n\n\nThe Cook's D statistic is most useful for identifying influential observations when the purpose of your model is \nparameter estimation\n\n\n\n\nUsing \nRSTUDENT\n residuals to detect influential observations\n\n\nRSTUDENT\n residuals are similar to \nSTUDENT\n residuals. For each observation, the \nRSTUDENT\n residual is the \nresidual divided by the standard error estimated with the current observation deleted\n.\n\n\n\n\nIf the RSTUDENT residual is different from the \nSTUDENT\n residual, the observation is probably influential\n\n\nIf the absolute value of the \nRSTUDENT\n residuals is $\n2$ or $\n3$, you've probably detected an influential observation\n\n\n\n\nUsing \nDFFITS\n statistics to detect influential observations\n\n\nDFFITS\n measures the impact that each observation has on its own predicted value. For each observation, \nDFFITS\n is \ncalculated using two predicted values\n:\n\n\n\n\nThe first predicted value is calculated from a model using the entire data set to estimate model parameters\n\n\nThe second predicted value is calculated from a model using the data set with that particular observation removed to estimate model parameters\n\n\nThe difference between the two predicted values is divided by the standard error of the predicted value, without the observation\n\n\n\n\nIf the \nstandardized difference\n between these predicted values \nis large\n, that particular observation has a \nlarge effect on the model fit\n.\n\n\n\n\nThe \ngeneral cutoff\n value is $2$\n\n\nThe more \nprecise cutoff\n is $2 \\cdot sqrt(p/n)$\n\n\nIf the absolute value of DFFITS for any observation is $\n$ cutoff value, you've detected an influential observation\n\n\nDFFITS\n is most useful for \npredictive models\n\n\n\n\nUsing \nDFBETAS\n statistics to explore the influenced predictor variable\n\n\nTo help identifying which parameter the observation might be influencing most you can use \nDFBETAS\n (difference in betas). It measure the change in each parameter estimate. \n\n\n\n\nOne \nDFBETAS\n is calculated per predictor variable per observation\n\n\nEach value is calculated by taking the estimated coefficient for that particular predictor variable \nusing all the data\n, subtracting the estimated coefficient for that particular predictor variable with the \ncurrent observation removed\n and dividing by its standard error\n\n\nLarge \nDFBETAS\n indicate observations that are influential in estimating a given parameter:\n\n\nThe \ngeneral cutoff\n value is $2$\n\n\nThe more \nprecise cutoff\n is $2 \\cdot sqrt(1/n)$\n\n\n\n\n\n\n\n\nPROC GLMSELECT\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\nPROC GLMSELECT DATA=SAS-data-set \noptions\n;\n    \nlabel:\n MODEL dependent(s) = regressor(s) / \n/options\n;\nRUN;\nQUIT;\n\nODS OUTPUT RSTUDENTBYPREDICTED=name-rstud-data-set\n           COOKSDPLOT=name-cooksd-data-set\n           DFFITSPLOT=name-dffits-data-set\n           DFBETASPANEL=name-dfbs-data-set;\n\nPROC REG DATA=SAS-data-set PLOTS(ONLY LABEL)=\n                                (RSTUDENTBYPREDICTED \n                                 COOKSD \n                                 DFFITS \n                                 DFBETAS) \noptions\n;\n    \nlabel:\n MODEL dependent=\n_GLSIND \n/options\n;\n    ID variable4identification;\nRUN;\nQUIT;\n\nDATA influential;\n    MERGE name-rstud-data-set\n          name-cooksd-data-set\n          name-dffits-data-set\n          name-dfbs-data-set;\n    BY observation;\n\n    IF (ABS(RSTUDENT)\n3) OR (COOKSDLABEL NE \n \n) OR DFFITSOUT THEN FLAG=1;\n    ARRAY DFBETAS{*} _DFBETASOUT: ;\n    DO I=2 TO DIM(DFBETAS);\n        IF DFBETAS{I} THEN FLAG=1;\n    END;\n\n    IF ABS(RSTUDENT)\n=3 THEN RSTUDENT=.;\n    IF COOKSDLABEL EQ \n \n THEN COOKSD=.;\n\n    IF FLAG=1;\n    DROP I FLAG;\nRUN;\n\nPROC PRINT DATA=influential;\n    ID observation;\n    VAR RSTUDENT COOKSD DFFITSOUT _DFBETASOUT: ;\nRUN;\n\n\n\n\n\n\n\n\nPROC GLMSELECT\n automatically creates the \n_GLSIND\n macro variable which stores the list of effects that are in the model whose variable order can be checked in the \nInfluence Diagnostics\n panel\n\n\nThe \nODS\n statement takes the data that creates each of the requested plots and saves it in the specified data set\n\n\nThe \nLABEL\n option includes a label for the extreme observations in the plot (labeled with the observation numbers if there is not ID specified)\n\n\n\n\nHaving \ninfluential observations doesn't violate regression assumptions\n, but it's a major nuisance that you need to address:\n\n\n\n\nRecheck\n for data entry errors\n\n\nIf the data appears to be valid, \nconsider whether you have an adequate model\n (a different model might fit the data better). Divide the number of influential observations you detect by the number of observations in you data set: if the result is \n$\n5\\%$ you probably have the wrong model\n.\n\n\nDetermine whether the influential observation is \nvalid but just unusual\n\n\nAs a general rule you should \nnot exclude data\n (some unusual observations contain important information)\n\n\nIf you choose to exclude some observations, include in your report a \ndescription of the types of observations that you excluded and why\n and discuss the limitation of the conclusions given the exclusions\n\n\n\n\nDetecting Collinearity\n\n\nCollinearity (or multicollinearity) is a problem that you face in multiple regression. It occurs when two or more \npredictor variables are highly correlated with each other\n (\nredundant information\n among them, the predictor variables explain much of the same variation in the response). Collinearity doesn't violate the assumptions of multiple regression.\n\n\n\n\nCollinearity can \nhide significant effects\n (if you include only one of the collinear variables in the model it is significant but when there are more than one included none of them are significant)\n\n\nCollinearity \nincreases the variance\n of the parameter estimates, making them \nunstable\n (the data points don't spread out enough in the space to provide stable support for the plane defined by the model) and, in turn, this \nincreases the prediction error\n of the model\n\n\n\n\nWhen an overall model is highly significant but the individual variables don't tell the same story, it's a \nwarning sign of collinearity\n. When the \nstandard error for an estimate is larger than the parameter estimate\n itself, it's not going to be statistically significant. The SE tells us how variable the corresponding parameter estimate is: when the standard errors are high, the \nmodel lacks stability\n.\n\n\n1\n2\n3\n4\nPROC REG DATA=SAS-data-set \noptions\n;\n    \nlabel:\n MODEL dependent = regressors / VIF \n/options\n;\nRUN;\nQUIT;\n\n\n\n\n\n\n\n\nThe \nVIF\n (variance inflation factor, $VIF_i=1/(1-R_i^2)$) option measures the magnitude of collinearity in a model (VIF$\n10$ for any predictor in the model, those predictors are probably involved in collinearity)\n\n\nOther options are \nCOLLIN\n (includes the intercept when analyzing collinearity and helps identify the predictors that are causing the problem) and \nCOLLINOINT\n (requests the same analysis as \nCOLLIN\n but excludes the intercept)\n\n\n\n\nEffective modeling cycle\n\n\n\n\nYou want to get to know your data by \nperforming preliminary analysis\n: \n\n\nPlot your data\n\n\nCalculate descriptive statistics \n\n\nPerform correlation analysis\n\n\n\n\n\n\nIdentify some \ngood candidate models\n using \nPROC REG\n: \n\n\nFirst check for collinearity \n\n\nUse all-possible regression or stepwise selection methods and subject matter knowledge to select model candidates\n\n\nIdentify the good ones with the Mallows' (prediction) or Hocking's (explanatory) criterion for $C_p$\n\n\n\n\n\n\n\n\nCheck and validate your assumtions\n by creating residual plots and conducting a few other statistical tests\n\n\n\n\n\n\nDeal with any \nproblems in your data\n: \n\n\n\n\nDetermine whether any influential observations might be throwing off your model calculations\n\n\nDetermine whether any variables are collinear\n\n\n\n\n\n\n\n\nRevise your model\n\n\n\n\n\n\nValidate your model\n with data not used to build the  model (prediction testing)", 
            "title": "Model Post-Fitting for Inference"
        }, 
        {
            "location": "/statistics/inference/#examining-residuals", 
            "text": "You can use the  residual values  (difference between each observed value of $Y$ and its predicted value) from the regression analysis to verify the  assumptions of the linear regression . Residuals are estimates of the errors, so you can  plot the residuals to check the assumptions of the errors .   You can plot residuals vs the predicted values to check for  violations of equal variances  You can also use this plot to check for  violations of linearity and independence  You can plot the residuals vs the values of the independent variables to  further examine any violations of equal variances  (you can see which predictor contributes to the violation of the assumption)  You can use a histogram or a normal probability plot of the residuals to determine whether or not the  errors are normally distributed   You want to see a  random scatter of the residual values  above and below the reference line at 0. If you see  patterns or trends  in the residual values, the assumptions might not be valid and the models might have problems.    Note  To take autocorrelation (correlated over time) into account, you might need to use a regression procedure such as  PROC AUTOREG   You can also use these plots to  detect outliers , which often reflect data errors or unusual circumstances. They can affect your regression results, so you want to know whether any outliers are present and causing problems and investigate if they result from  data entry error or some other problem  that you can correct.", 
            "title": "Examining Residuals"
        }, 
        {
            "location": "/statistics/inference/#proc-reg", 
            "text": "1\n2\n3\n4\n5 PROC REG DATA=SAS-data-set PLOTS(ONLY)=(QQ RESIDUALBYPREDICTED RESIDUALS) options ;\n     label:  MODEL dependent=regressor(s)  /options ;\n    ID variable4identification;\nRUN;\nQUIT;    QQ  requests a residual quantile-quantile plot to assess the normality of the residual error  RESIDUALBYPREDICTED  requests a plot of residuals by predicted values to verify the equal variance assumption, the independence assumption and model adequacy  RESIDUALS  requests a panel of plots of residuals by the predictor variables in the model: if any of the  Residual by Regressors  plots show signs of unequal variance, we can determine which predictor variable is involved in the problem", 
            "title": "PROC REG"
        }, 
        {
            "location": "/statistics/inference/#identifying-influential-observations", 
            "text": "An influential observation is different from an outlier. An  outlier  is an unusual observation that has a large residual compare to the rest of the points. An  influential observation  can sometimes have a large residual compared to the rest of the points, but it is an observation so far away from the rest of the data that it singlehandedly exerts influence on the slope of the regression line.", 
            "title": "Identifying Influential Observations"
        }, 
        {
            "location": "/statistics/inference/#using-student-residuals-to-detect-outliers", 
            "text": "Also known as  studientized or standardized residuals , the  STUDENT  residuals are calculated by dividing the  residual by their standard errors , so you can think of them as roughly equivalent to a z-score.    For  relatively small sample sizes , if the absolute value of the  STUDENT   residual is $ 2$ , you can suspect that the corresponding observation is an outlier  For  large sample sizes , it's very likely that even more  STUDENT   residuals greater than $\\pm2$  will occur just by chance, so you should typically use a larger cutoff value of $ 3$", 
            "title": "Using STUDENT residuals to detect outliers"
        }, 
        {
            "location": "/statistics/inference/#using-cooksd-statistics-to-detect-influential-observations", 
            "text": "For each observation, the Cook's D statistic is  calculated as if that observation weren't in the data set  as well as the set of parameter estimates with all the observations in your regression analysis.    If any observation has a Cook's D  statistic $ 4/n$  that observation is influential  The Cook's D statistic is most useful for identifying influential observations when the purpose of your model is  parameter estimation", 
            "title": "Using COOKSD statistics to detect influential observations"
        }, 
        {
            "location": "/statistics/inference/#using-rstudent-residuals-to-detect-influential-observations", 
            "text": "RSTUDENT  residuals are similar to  STUDENT  residuals. For each observation, the  RSTUDENT  residual is the  residual divided by the standard error estimated with the current observation deleted .   If the RSTUDENT residual is different from the  STUDENT  residual, the observation is probably influential  If the absolute value of the  RSTUDENT  residuals is $ 2$ or $ 3$, you've probably detected an influential observation", 
            "title": "Using RSTUDENT residuals to detect influential observations"
        }, 
        {
            "location": "/statistics/inference/#using-dffits-statistics-to-detect-influential-observations", 
            "text": "DFFITS  measures the impact that each observation has on its own predicted value. For each observation,  DFFITS  is  calculated using two predicted values :   The first predicted value is calculated from a model using the entire data set to estimate model parameters  The second predicted value is calculated from a model using the data set with that particular observation removed to estimate model parameters  The difference between the two predicted values is divided by the standard error of the predicted value, without the observation   If the  standardized difference  between these predicted values  is large , that particular observation has a  large effect on the model fit .   The  general cutoff  value is $2$  The more  precise cutoff  is $2 \\cdot sqrt(p/n)$  If the absolute value of DFFITS for any observation is $ $ cutoff value, you've detected an influential observation  DFFITS  is most useful for  predictive models", 
            "title": "Using DFFITS statistics to detect influential observations"
        }, 
        {
            "location": "/statistics/inference/#using-dfbetas-statistics-to-explore-the-influenced-predictor-variable", 
            "text": "To help identifying which parameter the observation might be influencing most you can use  DFBETAS  (difference in betas). It measure the change in each parameter estimate.    One  DFBETAS  is calculated per predictor variable per observation  Each value is calculated by taking the estimated coefficient for that particular predictor variable  using all the data , subtracting the estimated coefficient for that particular predictor variable with the  current observation removed  and dividing by its standard error  Large  DFBETAS  indicate observations that are influential in estimating a given parameter:  The  general cutoff  value is $2$  The more  precise cutoff  is $2 \\cdot sqrt(1/n)$", 
            "title": "Using DFBETAS statistics to explore the influenced predictor variable"
        }, 
        {
            "location": "/statistics/inference/#proc-glmselect", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44 PROC GLMSELECT DATA=SAS-data-set  options ;\n     label:  MODEL dependent(s) = regressor(s) /  /options ;\nRUN;\nQUIT;\n\nODS OUTPUT RSTUDENTBYPREDICTED=name-rstud-data-set\n           COOKSDPLOT=name-cooksd-data-set\n           DFFITSPLOT=name-dffits-data-set\n           DFBETASPANEL=name-dfbs-data-set;\n\nPROC REG DATA=SAS-data-set PLOTS(ONLY LABEL)=\n                                (RSTUDENTBYPREDICTED \n                                 COOKSD \n                                 DFFITS \n                                 DFBETAS)  options ;\n     label:  MODEL dependent= _GLSIND  /options ;\n    ID variable4identification;\nRUN;\nQUIT;\n\nDATA influential;\n    MERGE name-rstud-data-set\n          name-cooksd-data-set\n          name-dffits-data-set\n          name-dfbs-data-set;\n    BY observation;\n\n    IF (ABS(RSTUDENT) 3) OR (COOKSDLABEL NE    ) OR DFFITSOUT THEN FLAG=1;\n    ARRAY DFBETAS{*} _DFBETASOUT: ;\n    DO I=2 TO DIM(DFBETAS);\n        IF DFBETAS{I} THEN FLAG=1;\n    END;\n\n    IF ABS(RSTUDENT) =3 THEN RSTUDENT=.;\n    IF COOKSDLABEL EQ     THEN COOKSD=.;\n\n    IF FLAG=1;\n    DROP I FLAG;\nRUN;\n\nPROC PRINT DATA=influential;\n    ID observation;\n    VAR RSTUDENT COOKSD DFFITSOUT _DFBETASOUT: ;\nRUN;    PROC GLMSELECT  automatically creates the  _GLSIND  macro variable which stores the list of effects that are in the model whose variable order can be checked in the  Influence Diagnostics  panel  The  ODS  statement takes the data that creates each of the requested plots and saves it in the specified data set  The  LABEL  option includes a label for the extreme observations in the plot (labeled with the observation numbers if there is not ID specified)   Having  influential observations doesn't violate regression assumptions , but it's a major nuisance that you need to address:   Recheck  for data entry errors  If the data appears to be valid,  consider whether you have an adequate model  (a different model might fit the data better). Divide the number of influential observations you detect by the number of observations in you data set: if the result is  $ 5\\%$ you probably have the wrong model .  Determine whether the influential observation is  valid but just unusual  As a general rule you should  not exclude data  (some unusual observations contain important information)  If you choose to exclude some observations, include in your report a  description of the types of observations that you excluded and why  and discuss the limitation of the conclusions given the exclusions", 
            "title": "PROC GLMSELECT"
        }, 
        {
            "location": "/statistics/inference/#detecting-collinearity", 
            "text": "Collinearity (or multicollinearity) is a problem that you face in multiple regression. It occurs when two or more  predictor variables are highly correlated with each other  ( redundant information  among them, the predictor variables explain much of the same variation in the response). Collinearity doesn't violate the assumptions of multiple regression.   Collinearity can  hide significant effects  (if you include only one of the collinear variables in the model it is significant but when there are more than one included none of them are significant)  Collinearity  increases the variance  of the parameter estimates, making them  unstable  (the data points don't spread out enough in the space to provide stable support for the plane defined by the model) and, in turn, this  increases the prediction error  of the model   When an overall model is highly significant but the individual variables don't tell the same story, it's a  warning sign of collinearity . When the  standard error for an estimate is larger than the parameter estimate  itself, it's not going to be statistically significant. The SE tells us how variable the corresponding parameter estimate is: when the standard errors are high, the  model lacks stability .  1\n2\n3\n4 PROC REG DATA=SAS-data-set  options ;\n     label:  MODEL dependent = regressors / VIF  /options ;\nRUN;\nQUIT;    The  VIF  (variance inflation factor, $VIF_i=1/(1-R_i^2)$) option measures the magnitude of collinearity in a model (VIF$ 10$ for any predictor in the model, those predictors are probably involved in collinearity)  Other options are  COLLIN  (includes the intercept when analyzing collinearity and helps identify the predictors that are causing the problem) and  COLLINOINT  (requests the same analysis as  COLLIN  but excludes the intercept)", 
            "title": "Detecting Collinearity"
        }, 
        {
            "location": "/statistics/inference/#effective-modeling-cycle", 
            "text": "You want to get to know your data by  performing preliminary analysis :   Plot your data  Calculate descriptive statistics   Perform correlation analysis    Identify some  good candidate models  using  PROC REG :   First check for collinearity   Use all-possible regression or stepwise selection methods and subject matter knowledge to select model candidates  Identify the good ones with the Mallows' (prediction) or Hocking's (explanatory) criterion for $C_p$     Check and validate your assumtions  by creating residual plots and conducting a few other statistical tests    Deal with any  problems in your data :    Determine whether any influential observations might be throwing off your model calculations  Determine whether any variables are collinear     Revise your model    Validate your model  with data not used to build the  model (prediction testing)", 
            "title": "Effective modeling cycle"
        }, 
        {
            "location": "/statistics/categorical-data/", 
            "text": "Chapter summary in SAS\n\n\nWhen you response variable is categorical, you need to use a different kind of regression analysis: \nlogistic regression\n.\n\n\nDescribing Categorical Data\n\n\nWhen you examine the distribution of a \ncategorical variable\n, you want to know the \nvalues\n of the variable and the \nfrequency or count\n of each value in the data (\none-way frequency able\n).\n\n\n1\n2\n3\n4\nPROC FREQ DATA=SAS-data-set;\n    TABLES variable1 variable2 variable3 \n/options\n;\n    \nadditional statements\n\nRUN;\n\n\n\n\n\n\nTo look for a possible \nassociation\n between two or more categorical variables, you can create a \ncrosstabulation\n/\ncontingency table\n (when it displays statistics for two variables is also called \ntwo-way frequency able\n).\n\n\n1\n2\n3\n4\nPROC FREQ DATA=SAS-data-set;\n    TABLES variable-rows*variable-columns \n/options\n;\n    \nadditional statements\n\nRUN;\n\n\n\n\n\n\nTwo distribution plots are associated with a frequency or crosstabulation table: a \nfrequency plot\n, \nPLOTS=(FREQPLOT)\n, and a \ncumulative frequency plot\n.\n\n\nIn \nPROC FREQ\n output, the default order for character values is \nalphaumeric\n. To reorder the values of an ordinal variable in your \nPROC FREQ\n output you can:\n\n\n\n\nCreate a \nnew variable\n in which the values are stored in logical order\n\n\nApply a \ntemporary format\n to the original variable\n\n\nHow to \nreplace the variable's name with the variable's label in \nPROC FREQ\n output\n\n\n\n\n1\n2\n3\n4\noptions validvarname=any;\nPROC FREQ DATA=SAS-data-set (RENAME=(variable1=\nLabel variable 1\nn variable1=\nLabel variable 1\nn));\n    TABLES \nLabel variable 1\nn;\nRUN;\n\n\n\n\n\n\n\n\nCount the distinct values of a variable\n: The question of how to count distinct values of a \nCLASS\n or \nBY\n variable using either \nPROC MEANS\n or \nPROC SUMMARY\n is asked frequently. While neither of these procedures has this ability, \nPROC SQL\n can count these values using the \nDISTINCT\n option and \nPROC FREQ\n using the \nNLEVELS\n option.\n\n\n\n\nTests of Association\n\n\nPearson Chi-square Test\n\n\nTo perform a \nformal test of association\n between two categorical variables, you use the (Pearson) \nchi-square test\n which measures the difference between the observed cell frequencies and the cell frequencies that are expected if there is no association between variables ($H_0$ is true): \n$Expected=Row \\ total\\cdot Column\\ total/Total \\ sample \\ size$\n\n\n\n\nIf the \nsample size decreases\n, the \nchi-square value decreases\n and the \np-value for the chi-square statistic increases\n\n\nHypothesis testing: \n$H_0$\n: no association; \n$H_a$\n: association\n\n\n\n\nCramer's V statistic\n\n\nIt is one \nmeasure of strength of an association\n between two categorical variables:\n\n\n\n\nFor two-by-two tables, Cramer's V is in the range of -1 to 1\n\n\nFor larger tables, Cramer's V is in the range of 0 to 1 \n\n\nValues farther away from 0 indicate a relatively strong association between the variables\n\n\n\n\nTo measure the strength of the association between a binary predictor variable and a binary outcome variable, you can use an \nodds ratio\n: $Odds \\ Ratio=\\frac{Odds \\ of \\ Outcome \\ in \\ Group \\ B}{Odds \\ of \\ Outcome \\ in \\ Group \\ A}$; $Odds=p_{event}/(1-p_{event})$\n\n\n\n\nThe value of the odds ratio can range from 0 to $\\infty$; it cannot be negative\n\n\nWhen the odds ratio is 1 , there is no association between variables\n\n\nWhen the odds ratio \n1/\n1, the group in the numerator/denominator is more likely to have the outcome\n\n\nThe odds ratio is approximately the same \nregardless of the sample size\n\n\nTo estimate the true odds ratio while taking into account the variability of the sample statistic, you can calculate \nconfidence intervals\n\n\nYou can use an odds ratio to \ntest for significance\n between two categorical variables\n\n\nOdds ratio expressed as percent difference: $(odd \\ ratio -1) \\cdot 100$\n\n\n\n\n1\n2\n3\n4\nPROC FREQ DATA=SAS-data-set;\n    TABLES variable-rows*variable-columns / CHISQ EXPECTED \n/options\n;\n    \nadditional statements\n\nRUN;\n\n\n\n\n\n\n\n\nCHISQ\n produces the Pearson chi-square test of association, the likelihood-ratio chi-square and the Mantel-Haenszel: $\\sum \\frac{(obs. \\ freq. - exp. \\ freq.)^2}{exp. \\ freq.}$\n\n\nEXPECTED\n prints the expected cell frequencies\n\n\nCELLCHI2\n prints each cell's contribution to the total chi-square statistic: $ \\frac{(obs. \\ freq. - exp. \\ freq.)^2}{exp. \\ freq.}$\n\n\nNOCOL\n/\nNOROW\n suppresses the printing of the column/row percentages\n\n\nNOPERCENT\n supresses the printing of the cell percentages\n\n\nRELRISK\n (relative risk) prints a table that contains risk ratios (probability ratios) and odds ratios; \nPROC FREQ\n uses the \nclassification in the first column\n of the crosstabulation table as the \noutcome of interest\n and the first/second row in the numerator/denominator\n\n\n\n\nMantel-Haenszel chi-square test\n\n\nFor \nordinal associations\n, the \nMantel-Haenszel\n chi-square test is a more powerful test.\n\n\n\n\nThe levels must be in a \nlogical order\n for the test results to be meaningful\n\n\nHypothesis testing: \n$H_0$\n: no ordinal association; \n$H_a$\n: ordinal association\n\n\nSimilarly to the Pearson case, the Mantel-Haenszel chi-square statistic/p-value indicate whether an association exists but not its magnitude and they depend on and reflect the sample size\n\n\n\n\nTo measure the \nstrength of the association\n between two ordinal variables you can use the \nSpearman correlation\n statistic.\n\n\n\n\nYou should only use it if both variables are ordinal and are in logical order\n\n\nIs considered to be a rank correlation because it provides a degree of association between the ranks of the ordinal variables\n\n\nThis statistic has a \nrange between -1 and +1\n: values close to -1/+1 indicate that there is a relatively high degree of negative/positive correlation and values close to 0 indicate a weak correlation\n\n\nIt is \nnot affected by the sample size\n\n\n\n\n1\n2\n3\n4\nPROC FREQ DATA=SAS-data-set;\n    TABLES variable-rows*variable-columns / CHISQ EXPECTED \n/options\n;\n  \nadditional statements\n\nRUN;\n\n\n\n\n\n\n\n\nMEASURES\n produces the Spearman correlation statistic along with other measurement of association\n\n\nCL\n produces confidence bounds for the statistics that the MEASURES option requests\n\n\nThe confidence bounds are valid only if the sample size is large (\n25)\n\n\nThe asymptotic standard error (\nASE\n) is used for large samples and is used to calculate the confidence intervals for various measures of association (including the Spearman correlation coefficient)\n\n\n\n\nIntroduction to Logistic Regression\n\n\nLogistic Regression is a generalized linear model (like Linear Regression or ANOVA) that you can use to predict a categorical response/outcome based on one or more continuous/categorical predictor variables. There are three models:\n\n\n\n\nLinear vs Logistic\n\n\nLinear Regression Model\n\n\n\n\nAssumes that the expected value of the response continuous variable ($Y$) has a linear relationship with the predictor variable ($X$)\n\n\nThe conditional mean of the response hast the linear form $E(Y|X)=\\beta_0+\\beta_1X$ and it ranges $(-\\infty,+\\infty)$\n\n\n\n\n\n\nWhy not to use Linear Regression to model a binary response variable\n\n\nFollowing the Linear Regression Model scheme, the response variable is calculated as \n\n\n$Y_i=\\beta_0+\\beta_1\\cdot X_i+\\epsilon_i$, \n\n\nwhere $\\beta_0$ and $\\beta_1$ are obtained by the method of least squares. \n\n\n\n\nThis model \nassumes that the data is continuous\n, which is not true for the case of binary data\n\n\nThis model \nassumes that the mean of the response is $\\beta_0+\\beta_1\\cdot X$\n, while for binary data the mean of the response is the probability of a success\n\n\nIf the response variable has only two levels, you cannot \nassume the constant variance and normality\n that are required for linear regression\n\n\n\n\n\n\nBinary Logistic Regression Model\n\n\n\n\nThe predictor variable ($X$) is used to estimate the probability of a specific outcome ($p$) for which you need to use a nonlinear function\n\n\nThe mean of the response is a probability, which is between $(0, 1)$. \n\n\nThe \nInverse Logit Function\n binds the linear predictor between $0$ and $1$ is defined as $p_i=(1+e^{-(\\beta_0+\\beta_1 X_i)})^{-1}$\n\n\nThis model applies a \nLogit Transformation\n to the probabilities $logit(p_i)=ln\\left ( \\frac{p_i}{1-p_i} \\right ) = \\beta_0+\\beta_1X_i$, so that the transformed probabilities and predictor variables end up with a linear relationship\n\n\nThe logit is the \nnatural log of the odds\n (the probability of the event occurring divided by the probability of the event not occurring)\n\n\nWe make the \nassumption that the logit transformation of the probabilities results in a linear relationship with the predictor variables\n (we can use a linear function $X$ to model the logit in order to indireclty model the probability)\n\n\nThe logit of the probability transforms the probability into a linear function, which has no lower or upper bounds. So a \nlogit has no lower or upper bounds\n.\n\n\n\n\nPROC LOGISTIC\n\n\nTo model categorical data yu use the \nLOGISTIC\n procedure. Some of the most common statements of this procedure are shown here:\n\n\n1\n2\n3\n4\n5\n6\nPROC LOGISTIC DATA=SAS-data-set \noptions\n;\n\n    CLASS variable \n(variable_options)\n ... \n/ options\n;\n\n    MODEL response \n(variable_options)\n = predictors \n/ options\n;\n\n    UNITS independent1=list... \n/ options\n;\n    ODDSRATIO \nlabel\n variable \n/ options\n;\nRUN;\n\n\n\n\n\n\n\n\nCLASS\n is used to define the classification (categorical) predictor variables (if any); this statement must precede the \nMODEL\n statement\n\n\nCLODDS = PL\n (profile likelihood) | \nWALD\n (default) | \nBOTH\n is an example of a general option that you can specify in the \nMODEL\n statement which computes confidence intervals for the odds ratios of all predictor variables and also enables the production of the odds ratio plot\n\n\n\n\n\n\nExample\n\n\nPROC LOGISTIC DATA=statdata.sales_inc PLOTS(ONLY)=(EFFECT ODDSRATIO);\n    CLASS gender;\n    MODEL purchase(EVENT='1')=gender / CLODDS=PL;\nRUN;\n\n\n\n\nClassification Variables Parametrization\n\n\nWhen the predictor variable is categorical, the assumption of linearity cannot be met. To get past the obstacle of nonlinearity, the \nCLASS\n statement creates a set of one or more \ndesign variables\n (also called dummy variables). \nPROC LOGISTIC\n uses these variables, and not the original ones, in model calculations.\n\n\nDifferent parametrization methods for the classification variables will produce the same results regarding the significance of the categorical predictors, but understanding the parametrization method helps to interpret the results accurately.\n\n\nHere we present two of the most common methods of parameterizing (\nPARAM =\n) the classification variables. For both of them:\n\n\n\n\nThe default \nreference level\n is the level that has the highest ranked value (or the last value) when the levels are sorted in ascending alphanumeric order\n\n\nThe number of design variables (or $\\beta$) that are created are the number of levels of the classification variable -1\n\n\n\n\nEffect coding (default)\n\n\nAlso called \ndeviation from the mean coding\n, it compares the effect of each level of the variable to the \naverage effect of all levels\n. \n\n\n\n\nExample\n\n\nUsing this parametrization scheme the model will be described as follows\n\n\n$logit(p)=\\beta_0+\\beta_1\\cdot D_{Low \\ Income} + \\beta_2\\cdot D_{Medium \\ Income}$\n\n\n\n\n\n\n$\\beta_0$ is the average value of the logit across all income levels\n\n\n$\\beta_1$ is the difference between the logit for income level 1 and $\\beta_0$\n\n\n$\\beta_2$ is the difference between the logit for income level 2 and $\\beta_0$\n\n\n\n\nHere's the Analysis of Maximum Likelihood Estimates table that \nPROC LOGISTIC\n generates for this example on which the parameter estimates ($\\beta_0$, $\\beta_1$, $\\beta_2$) and p-values reflect differences from the overall mean value over all levels. \n\n\n\n\nThe p-values indicate whether each particular level is significant compared to the average effect of all levels. The p-values for $\\beta_1$ and $\\beta_2$ not significant meaning that the effect of those levels is not different than the average effect of low, medium and high income.\n\n\n\n\nReference cell coding\n\n\nIt compares the effect of each level of the predictor to the effect of another \nlevel that is the designated reference level\n.\n\n\n\n\nExample\n\n\nTo use this scheme the classification variable has to be defined in the following way\n\n\nCLASS gender (PARAM=REF REF='Male');\n\n\nUsing this parametrization scheme the model will be described as follows\n\n\n$logit(p)=\\beta_0+\\beta_1\\cdot D_{Low \\ Income} + \\beta_2\\cdot D_{Medium \\ Income}$\n\n\n\n\n\n\n$\\beta_0$ is the intercept, but not in terms of where you cross the $Y$ axis, instead is the value of the logit of the probability when income is high (or at the reference level)\n\n\n$\\beta_1$ is the difference between the logit of the probability for low and high income\n\n\n$\\beta_2$ is the difference between the logit of the probability for medium and high income\n\n\n\n\nHere's the Analysis of Maximum Likelihood Estimates table that \nPROC LOGISTIC\n generates for this example on which the parameter estimates ($\\beta_0$, $\\beta_1$, $\\beta_2$) and p-values reflect differences with respect to the reference level. \n\n\n\n\nThe p-values indicate whether each particular level is significant compared to the reference level. The p-value for $\\beta_1\n0.05$ is significant meaning that the effect of a low income is statistically different than the effect of a high income on the probability that people will spend at least $100\\$$. The same applies to $\\beta_2$.\n\n\n\n\nFitting a Binary Logistic Regression\n\n\n1\n2\n3\n4\n5\nPROC LOGISTIC DATA=statdata.sales_inc PLOTS(ONLY)=(EFFECT);\n    CLASS Gender (PARAM=REF REF=\nMale\n);\n    MODEL Purchase(event=\n1\n) = Gender;\n    title1 \nLOGISTIC MODEL (1): Purchase = Gender\n;\nRUN;\n\n\n\n\n\n\nWe look at the first few tables to make sure that the model is set up the way we want\n\n\n\n\nThe \nModel Information\n table describes the data set, the response variable, the number of response levels, the type of model, the algorithm used to obtain the parameter estimates, and the number of observations read and used.\n\n\nThe \nResponse Profile\n table shows the values of the response variable, listed according to their ordered value and frequency. By default, \nPROC LOGISTIC\n orders the values of the response variable alphanumercally and bases the logistic regression model on the probability of the lowest value. However, we set the \nEVENT=1\n, the highest value, so this model is based on the probability that \nPurchase=1\n.\n\n\nBelow this table, we see the probability that \nPROC LOGISTIC\n is modeling, as shown in the log.\n\n\nThe \nClass Level Information\n table displays the predictor variable in the \nCLASS\n statement \nGender\n (in the model we fixed \n'Male'\n as the reference level, so the design variable is 1 when \nGender='Female'\n and 0 when \nGender='Male'\n).\n\n\nThe \nModel Convergence Status\n simply indicates that the convergence criterion was met. There are a number of options to control the convergence criterion, but the default is the gradient convergence criterion with a default value of $10^{-8}$.\n\n\nThe \nModel Fit Statistic\n table reports the resuls of three tests (for the model with the intercept only and the model with the intercept and the predictor variables): AIC, SC and -2$\\cdot$Log(likelihood). AIC and SC are \ngoodness-of-fit measures\n that you can use to compare one model to another (lower values indicate more desirable model, although there is no standard for determining how much of a difference indicates an improvement) and are not dependent on the number of terms in the model. \n\n\nAkaike's Information Criterion (AIC)\n: it adjusts for the number of predictor valriables. It is the best statitstic to come up with the best \nexplanatory model\n.\n\n\nSchwarz's Bayesian Criterion (SC)\n: it adjusts for the number of predictor variables and the number of observations. This test uses a bigger penalty for extra variables and therefore favors more parsimonious models. It is the best statitstic to come up with the best \npredictive model\n.\n\n\n\n\n\n\nThe \nTesting Global Null Hypothesis: BETA=0\n table provides three statistics to test $H_0$ that all the regression coefficients in the model are 0. The \nLikelihood Ratio\n is the most reliable test, specially for small sample sizes. It is similar to the overall F test in linear regression.\n\n\nThe \nType 3 Analysis of Effects\n table is generated when \nCLASS\n specifies a categorical predictor variable. It shows the results of testing whether each individual parameter estimate is statistically different from 0 (\nPr\nChiSq\n$\n0.05$). The \nWald Chi-Square\n statistic tests the listed effects. When there is only one predictor variable in the model, the value listed in the table will be identical to the Wald test in the \nTesting Global Null Hypothesis\n table.\n\n\nThe \nAnalysis of Maximum Likelihood Estimates\n table lists the estimated model parameters (the betas), their standard errors, Wald test statistics and corresponding p-values. The parameter estimates are the estimated coefficients of the fitted logistic regression model. We can use these estimates to construct the logistic regression equation $logit(\\beta)=\\beta_0+\\beta_1 \\cdot Categorical \\ predictor$.\n\n\nThe \nOdds Ratio Estimates\n table (\nONLY for binary logistic regression\n) shows the OR ratio for the modeled event. Notice that \nPROC LOGISTIC\n calculates Wald confidence limits by default.\n\n\nThe \nAssociation of Predicted Probabilities and Observed Responses\n table lists several goodness-of-fit measures.\n\n\nThe \nOdds Ratio Estimates and Profile-Likelihood Confidence Intervals\n table (\nONLY for multiple logistic regression\n) contains the OR estimates and the profile-likelihood confidence limits that the \nCLODDS=\n option specified for the main effects. For multiple logistic regression, remember that \nPROC LOGISTIC\n calculates the adjusted odds ratios. For a continuous predictor, the odds ratio measures the change in odds associated with a one-unit difference of the predictor variable by default although it is specified otherwise in the \nUNITS\n statement.\n\n\nIn the \nOdds Ratios plot\n, the dots represent the OR estimates and the horizontal lines represent the confidence intervals for those estimates. There is a reference line at one to check if the confidence intervals cross this value meaning that it is not statistically different from 1.\n\n\nThe \nEffect plot\n shows the levels of the \nCLASS\n predictor variable vs the probability of the desired outcome. If you are performing a \nmultiple logistic regression\n analysis where there is a continuous predictor variable it will be represented in the horizontal axis. Moreover, the lines will represent all possible combinations of the different \nCLASS\n variables.\n\n\n\n\nIterpreting the Odds Ratio for a Categorical Predictor\n\n\nLet's see how to calculate the odds and the odds ratio from the logistic regression model. Here is the logistic regression model that predicts the logit of $p$:\n\n\n$logit(\\hat p)=ln(odds)=ln\\left ( \\frac{p_i}{1-p_i} \\right )=\\beta_0 + \\beta_1 \\cdot Gender$\n\n\nAccording to our example the variable \nGender\n is codified in a way that \nFemales=1\n and \nMales=0\n, so the OR can be written:\n\n\n$odds_{females}=e^{\\beta_0+\\beta_1}$\n\n\n$odds_{males}=e^{\\beta_0}$\n\n\n$odds \\ ratio = \\frac{e^{\\beta_0+\\beta_1}}{e^{\\beta_0}}=e^{\\beta_1}$\n\n\nIf the 95% confidence interval does not include 1, the OR is significant at the 0.05 level indicating an association between the predictor and response variables of your model.\n\n\nIterpreting the Odds Ratio for a Continuous Predictor\n\n\nFor a continuous predictor variable, the OR measures the \nincrease or decrease in odds associated with a one-unit difference\n of the predictor variable by default. $OR - 1 = %$ of greater odds for having one-unit of difference.\n\n\nComparing Pairs to Assess the Fit of a Logistic Regression Model\n\n\nPROC LOGISTIC\n calculates several different goodness-of-fit measures and displayed in the \nAssociation of Predicted Probabilities and Observed Responses\n table.\n\n\nOne of these goodness-of-fit methods is comparing pairs (\nPairs\n). To start, \nPROC LOGISTIC\n creates two groups of observations, one for each value of the response variable. Then, the procedure selects pairs of observations, one from each group, until no more pairs can be selected. \nPROC LOGISTIC\ndetermines whether each pair is concordant, discordant or tied.\n\n\n\n\nA pair is \nconcordant\n if the \nmodel predicts it correclty\n, i.e. if the observation with the desired outcome has a \nhigher predicted probability\n, based on the model, than the observation without the outcome.\n\n\nA pair is \ndiscordant\n if the \nmodel does not predict it correctly\n, i.e. if the observation with the desired outcome has a \nlower predicted probability\n, based on the model, than the observation without the outcome.\n\n\nA pair is \ntied\n if it is neither concordant not discordant, i.e. the \nprobabilities are the same\n and the model can not distinguished between them. Tied pairs aren't very common when there are continuous variables in the model.\n\n\n\n\nThe left column of the \nAssociation of Predicted Probabilities and Observed Responses\n table lists the percentage of pairs of each type. At the bottom is the total number of observation pairs on which the percentages are based, i.e. the number of pairs of observations with different outcome values $(N_{event=0} \\cdot N_{event=1})$.\n\n\nMore complex models have more than two predicted probabilities. However, regardless of the model's complexity, the same comparisons are made across all pairs of observations with different outcomes.\n\n\nYou can use these results as goodness-of-fit measures to compare one model to another. In general, higher percentage of concordant pairs and lower percentages of discordant and tied pairs indicate a more desirable model.\n\n\nThis table also shows the four rank correlation indices that are computed from the numbers of concordant $(n_c)$, discordant $(n_d)$ and tied $(n_t)$ pairs of observations:\n\n\n\n\nSomers' D\n (Gini coefficient)\n, defined as $(n_c-n_d)/(n_c+n_d+n_t)$\n\n\nGoodman-Kruskal \nGamma\n, defined as $(n_c-n_d)/(n_c+n_d)$\n\n\nKendall's \nTau-a\n, defined as $(n_c-n_d)/(0.5 \\cdot N(N-1))$, with $N$ being the sum of observation frequencies in the data\n\n\nThe concordance index \nc\n is the most commonly used of these values and estimates the probability of an observation with the desired outcome having a higher predicted probability than an observation without the desired outcome and is defined as $c=\\frac{n_c+0.5 \\cdot n_t}{n_c+n_d+n_t}$. Note that the concordance index, \nc\n, also gives an estimate of the \narea under the receiver operating characteristic (ROC) curve\n when the response is binary.\n\n\n\n\n\n\nNote\n\n\nMore information about these parameters \nhere\n.\n\n\n\n\nIn general, a model with higher values of these indices has better predictive ability than a model with lower values.\n\n\nMultiple Logistic Regression Model\n\n\nIntroduction\n\n\nSometimes you want to create a statistical model that explains the relationships among multiple predictors and a categorical response. You might want \n\n\n\n\nto examine the \neffect of each individual predictor\n on the response regardless of the levels of the other predictors \n\n\nto perform a more complex analysis that takes into account the \ninteractions between the predictors\n\n\n\n\nIn order to do this you will explore\n\n\n\n\nhow to define and explain the \nadjusted odds ratio\n\n\nhow to fit a multiple logistic regression model using the \nbackward elimination method\n\n\nhow to fit a multiple logistic regression \nmodel with interactions\n\n\n\n\nMultiple Logistic Regression\n\n\nAmultiple logistic regression model characterized the relationship between a categorical response variable and multiple predictor variables. The predictor variables can be continuous or categorical or both.\n\n\n$logit(p)=\\beta_0+\\beta_1 X_1 +...+\\beta_k X_k$\n\n\nThe goal of multiple logistic regression, like multiple linear regression, is to find the subset of variables that best explains the variability of the response variable. Models that are \nparsimonious or simple\n are more likely to be numerically stable and are also easier to generalize.\n\n\nThe Backward Elimination Method of Variable Selection\n\n\nThis method starts with a full model (a model that contains all of the main effects or predictor variables). Using an iterative process, the backward elimination method identifies and eliminates the nonsignificant predictor variables, one at a time. At each step, this method removes the least significant variable of the nonsignificant terms (the variable with the largest p-value).\n\n\nThe smaller your significance level, the more evidence you need to keep a predictor variable in the model. This results in a more parsimonious model.\n\n\nThe default significance level for a predictor to stay in the model is 0.05. You can change the significance level by adding \nSLSTAY=\nvalue\n or \nSLS=\nvalue\n in the \nMODEL\nstatement.\n\n\nAdjusted Odds Ratios\n\n\nOne major difference between a multiple logistic model and a logistic regression model with only one predictor variable is that the odds ratios are reported differently. Multiple logistic regression uses \nadjusted odds ratios\n. An adjusted odds ratio measures the effect of a single predictor variable on a response variable while holding all the other predictor variables constant. \n\n\nThe adjusted odds ratio \nassumes that the OR for a predictor variable is the same regardless of the level of the other predictor variables\n. If that assumption is not true, then you need to fit \na more complex model that also considers the interactions\n between predictor variables.\n\n\nThe \nMODEL\n statement can include the \nCLODDS=\n option which enables the production of the OR plot (\n\nPLOTS(ONLY)=(EFFECT ODDSRATIO)\n). This option can be set to \nPL\n so that the procedure calculates profile-likelihood confidence limits for the OR of all predictor variables. These limits are based on the log likelihood and are generally preferred, especially for \nsmaller sample sizes\n.\n\n\nSpecifiying the Variable Selection Method in the \nMODEL\n Statement\n\n\nTo specify the method that \nPROC LOGISTIC\n uses to select variable in a multiple logistic regression model, you add the \nSELECTION=\n option to the \nMODEL\n statement. The possible values of the \nSELECTION=\n statement are\n\n\n\n\nNONE | N\n (default): no selection method is used and the complete model is fitted\n\n\nBACKWARD | B\n: backward elimination\n\n\nFORWARD | F\n: forward selection\n\n\nSTEPWISE | S\n: stepwise selection\n\n\nSCORE\n: best subset selection\n\n\n\n\n1\n2\n3\n4\nPROC LOGISTIC DATA=SAS-data-set \noptions\n;\n    CLASS variable \n(variable_options)\n ... \n/ options\n;\n    MODEL response \n(variable_options)\n = predictors \n/ options SELECTION\n;\nRUN;\n\n\n\n\n\n\nBy default the procedure uses a $\\alpha=0.05$ significance level to determine which variables remain in the model. If you want to change the significance level, you can use the \nSLSTAY | SLS =\n option in the \nMODEL\n statement.\n\n\nThe \nUNITS\n Statement\n\n\nThe \nUNITS\nstatement enables you to obtain customized \nodds ratio estimates\n for a specified unit of change in one or more continuous predictor variables. For each continuous predictor (or independent variable) that you want to modify, you specify the variable name, an equal sign, and a list of \none or more units of change\n, separated by spaces, that are of interest for that variable. A unit of change can be a number, a standard deviation $(SD)$, or a number multiplied by the standard deviation $(n \\times SD)$. \n\n\nThe \nUNITS\n statement is optional. If you want to use the units to change that are reflected in the stored data values, you do not need to include the \nUNITS\n statement.\n\n\nSpecifying a Formatted Value as a Reference Level\n\n\nTo ease the interpretation of the classification variable levels, text labels can be defined through a format definition.\n\n\nAppliying a format requires a change in the \nPROC LOGISTIC\n step. In the \nCLASS\n statement when you use the \nREF=\n option with a variable that has either a temporary or a permanent format assigned to it, you must \nspecify the formatted value of the level instead of the stored value\n.\n\n\nInteraction between Variables\n\n\nWhen you fit a multiple logistic regression model, the simplest approach is to consider only the main effects (each predictor individually) on the response. In other words, this approach assumes that each variable has the same effect on the outcome regardless of the levels of the other variables. However, sometimes the effect of one variable on the outcome depends on the observed level of another variable. When this happens, we say that \nthere is an interaction\n.\n\n\nKeep in mind that \ninteractions that have more than two factors might be difficult to interpret\n.\n\n\nThe Backward Elimination Method with Interactions in the Model\n\n\nWhen you use the backward elimination method with interactions in the model, \nPROC LOGISTIC\n begins by fitting th full model with all the main effects and interactions.\n\n\nUsing an iterative process, \nPROC LOGISTIC\n eliminates nonsignificant interactions one at a time, starting with the least significant interaction (largest p-value). When only significant interactions remain, \nPROC LOGISTIC\n turns its attention to the main effects. \nPROC LOGISTIC\n eliminates, one at a time, the nonsignificant main effects that are not involved in any significant interactions. \n\n\nWhen eliminating main effects, \nPROC LOGISTIC\n must preserve the model hierarchy. According to this requirement, for any interaction that is included in the model, all main effects that the interaction contains must also be in the model, whether or not they are significant.\n\n\nThe \nfinal model\n has only significant interactions, the main effects involved in the interactions, and any other significant main effects.\n\n\nSpecifying Interactions in the \nMODEL\n Statement\n\n\nTo specify interactions concisely, you can place a bar operator \n|\n between the names of each two main effects. The bar tells \nPROC LOGISTIC\n to treat the terms on either side of it as effects and also to include their combinations as interactions.\n\n\nIf you want to limit the maximum number of variables that are involved in each interaction, you can specify \n@integer\n after the list of effects (e.g. \n@2\n to include only the two-way interactions).\n\n\nThe \nODDSRATIO\n Statement\n\n\nBy default, \nPROC LOGISTIC\n produces the OR only for variables that are not involved in an interaction. The OR for a main effect within an interaction would be misleading. It would only show the OR for that variable, holding constant the other variable at the value 0, which might not even be a valid value.\n\n\nTo tell \nPROC LOGISTIC\n to produce the OR for each value of a variable that is involved in an interaction, you can use the \nODDSRATIO\n statement. You specify a separate \nODDSRATIO\n statement for each variable. In this statement you can optionally specify a label for the variable. The variable name is required. At the end, you can specify options following a forward slash \n/\n.\n\n\nIn this example, there are three \nODDSRATIO\n statements, one for each main effect (they do not have to appear in the same order that the variables are listed in the \nMODEL\n statement).\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\nPROC\n \nLOGISTIC\n \nDATA\n=\nstatdata\n.\nsales_inc\n \n        \nPLOTS\n(\nONLY\n)\n=\n(\nEFFECT\n \nODDSRATIO\n);\n\n    \nCLASS\n \nGender\n \n(\nPARAM\n=\nREF\n \nREF\n=\nMale\n)\n\n        \nIncLevel\n \n(\nPARAM\n=\nREF\n \nREF\n=\n1\n);\n\n    \nUNITS\n \nAge\n=\n10\n;\n\n    \nMODEL\n \nPurchase\n(\nEVENT\n=\n1\n)\n=\nGender\n \n|\n \nAge\n \n|\n \nIncLevel\n \n@2\n \n/\n\n        \nSELECTION\n=\nBACKWARD\n \nCLODDS\n=\nPL\n;\n\n\n    \nODDSRATIO\n \nAge\n \n/\n \nCL\n=\nPL\n;\n\n\n    \nODDSRATIO\n \nGender\n \n/\n \nDIFF\n=\nREF\n \nAT\n \n(\nIncLevel\n=\nALL\n)\n \nCL\n=\nPL\n;\n\n\n    \nODDSRATIO\n \nIncLevel\n \n/\n \nDIFF\n=\nREF\n \nAT\n \n(\nGender\n=\nALL\n)\n \nCL\n=\nPL\n;\n\n\nRUN\n;\n\n\n\n\n\n\n\n\n\nThe \nCL = WALD (default) | PL | BOTH\n option enables you to specify the type of confidence limits you want to produce: Wald, profile-likelihood or both.\n\n\nThe \nDIFF = REF | ALL (default)\n option applies only to categorical variables. Using this option, you can specify whether \nPROC LOGISTIC\n computes the OR for a categorical variable against the reference level or against all of its levels.\n\n\nThe \nAT (covariate = value-list | REF | ALL (default))\n option specifies fixed levels of one or more interacting variables (also called covariates). \nPROC LOGISTIC\n computes OR at each of the specified levels. For each categorical variable, you can specify a list of one or more formatted levels of the variable, or the keyword \nREF\n to select the reference level or the keyword \nALL\n to select all levels of the variable. \n\n\n\n\n\n\nThis plot shows graphically what we saw with the calculated confidence intervals in the table above.\n\n\n\n\nComparing the Binary and Multiple Logistic Regression Models\n\n\n\n\n\n\nRemember that, in general, when comparing the \nAIC\n and \nSC\n statistics, smaller values mean a \nbetter model fit\n. Note that the \nSC\n value increased with the addition of the interaction. \nSC\n selects more parsimonious models by imposing a more severe penalty for increasing the number of parameters.\n\n\nWhen comparing the \nc\n statistic values, larger values indicate a \nbetter predictive model\n.\n\n\n\n\nInteraction Plots\n\n\nTo visualize the interaction terms you can produce an interaction plot. This plot explains more of the story behind the significant interaction in the output. \n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\nproc means data=statdata.sales_inc noprint nway;\n   class IncLevel Gender;\n   var Purchase;\n   output out=bins sum(Purchase)=Purchase n(Purchase)=BinSize;\nrun;\n\ndata bins;\n   set bins;\n      Logit=log((Purchase+1)/(BinSize-Purchase+1));\nrun;\n\nproc sgscatter data=bins;\n   plot Logit*IncLevel /group=Gender markerattrs=(size=15)\n                        join;\n   format IncLevel incfmt.;\n   label IncLevel=\nIncome Level\n;\n   title;\nrun;\nquit;\n\n\n\n\n\n\n\n\nIf there is no interaction between two variables, the slopes shold be relatively parallel.\n\n\nSaving Analysis Results with the \nSTORE\n Statement\n\n\nYou can use the \nSTORE\n statement with \nPROC LOGISTIC\n to save your analysis results as an item store for later processing.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\nODS SELECT NONE;\nPROC LOGISTIC DATA=statdata.ameshousing3;\n    CLASS Fireplaces (REF=\n0\n) Lot_Shape_2 (REF=\nRegular\n) / param=ref;\n    MODEL Bonus(EVENT=\n1\n)=Basement_Area | Lot_Shape_2 Fireplaces;\n    UNITS Basement_Area=100;\n\n    STORE OUT=isbonus;\n\nRUN;\nODS SELECT ALL;\n\nDATA newhouses;\n    LENGTH Lot_Shape_2 $9;\n    INPUT Fireplaces Lot_Shape_2 $ Basement_Area;\n    DATALINES;\n    0 Regular 1060\n    2 Regular 775\n    2 Irregular 1100\n    1 Irregular 975\n    1 Regular 800\n    ;\nRUN;\n\n\nPROC PLM RESTORE=isbonus;\n\n    SCORE DATA=newhouses OUT=scored_houses / ILINK;\n    TITLE \nPredictions using PROC PLM\n;\nRUN;\n\nPROC PRINT DATA=scored_houses;\nRUN;\n\n\n\n\n\n\n\n\nFollowing the keyword \nSTORE\n, you use the \nOUT=\n option to specify the name of your item store.\n\n\nIn the \nPROC PLM\n statement, the \nRESTORE\n option specifies that the predictions will be based on the analysis results saved in the item store.\n\n\nThe \nSCORE\n statement specifies that SAS will score the provided data set. The \nILINK\n option requests that SAS provide the predictions in the form of predicted probabilities in lieu of logits where covariate effects are additive.\n\n\n\n\n\n\nWarning\n\n\nBe sure that you generate predictions only for new data records that fall within the range of the training data. If not, predictions could be invalid due to extrapolation.\n\n\nWe assume that the modeled relationships between predictors and responses holds across the span of the observed data. We should not assume that this relationship holds everywhere.", 
            "title": "Categorical Data Analysis"
        }, 
        {
            "location": "/statistics/categorical-data/#describing-categorical-data", 
            "text": "When you examine the distribution of a  categorical variable , you want to know the  values  of the variable and the  frequency or count  of each value in the data ( one-way frequency able ).  1\n2\n3\n4 PROC FREQ DATA=SAS-data-set;\n    TABLES variable1 variable2 variable3  /options ;\n     additional statements \nRUN;   To look for a possible  association  between two or more categorical variables, you can create a  crosstabulation / contingency table  (when it displays statistics for two variables is also called  two-way frequency able ).  1\n2\n3\n4 PROC FREQ DATA=SAS-data-set;\n    TABLES variable-rows*variable-columns  /options ;\n     additional statements \nRUN;   Two distribution plots are associated with a frequency or crosstabulation table: a  frequency plot ,  PLOTS=(FREQPLOT) , and a  cumulative frequency plot .  In  PROC FREQ  output, the default order for character values is  alphaumeric . To reorder the values of an ordinal variable in your  PROC FREQ  output you can:   Create a  new variable  in which the values are stored in logical order  Apply a  temporary format  to the original variable  How to  replace the variable's name with the variable's label in  PROC FREQ  output   1\n2\n3\n4 options validvarname=any;\nPROC FREQ DATA=SAS-data-set (RENAME=(variable1= Label variable 1 n variable1= Label variable 1 n));\n    TABLES  Label variable 1 n;\nRUN;    Count the distinct values of a variable : The question of how to count distinct values of a  CLASS  or  BY  variable using either  PROC MEANS  or  PROC SUMMARY  is asked frequently. While neither of these procedures has this ability,  PROC SQL  can count these values using the  DISTINCT  option and  PROC FREQ  using the  NLEVELS  option.", 
            "title": "Describing Categorical Data"
        }, 
        {
            "location": "/statistics/categorical-data/#tests-of-association", 
            "text": "", 
            "title": "Tests of Association"
        }, 
        {
            "location": "/statistics/categorical-data/#pearson-chi-square-test", 
            "text": "To perform a  formal test of association  between two categorical variables, you use the (Pearson)  chi-square test  which measures the difference between the observed cell frequencies and the cell frequencies that are expected if there is no association between variables ($H_0$ is true): \n$Expected=Row \\ total\\cdot Column\\ total/Total \\ sample \\ size$   If the  sample size decreases , the  chi-square value decreases  and the  p-value for the chi-square statistic increases  Hypothesis testing:  $H_0$ : no association;  $H_a$ : association", 
            "title": "Pearson Chi-square Test"
        }, 
        {
            "location": "/statistics/categorical-data/#cramers-v-statistic", 
            "text": "It is one  measure of strength of an association  between two categorical variables:   For two-by-two tables, Cramer's V is in the range of -1 to 1  For larger tables, Cramer's V is in the range of 0 to 1   Values farther away from 0 indicate a relatively strong association between the variables   To measure the strength of the association between a binary predictor variable and a binary outcome variable, you can use an  odds ratio : $Odds \\ Ratio=\\frac{Odds \\ of \\ Outcome \\ in \\ Group \\ B}{Odds \\ of \\ Outcome \\ in \\ Group \\ A}$; $Odds=p_{event}/(1-p_{event})$   The value of the odds ratio can range from 0 to $\\infty$; it cannot be negative  When the odds ratio is 1 , there is no association between variables  When the odds ratio  1/ 1, the group in the numerator/denominator is more likely to have the outcome  The odds ratio is approximately the same  regardless of the sample size  To estimate the true odds ratio while taking into account the variability of the sample statistic, you can calculate  confidence intervals  You can use an odds ratio to  test for significance  between two categorical variables  Odds ratio expressed as percent difference: $(odd \\ ratio -1) \\cdot 100$   1\n2\n3\n4 PROC FREQ DATA=SAS-data-set;\n    TABLES variable-rows*variable-columns / CHISQ EXPECTED  /options ;\n     additional statements \nRUN;    CHISQ  produces the Pearson chi-square test of association, the likelihood-ratio chi-square and the Mantel-Haenszel: $\\sum \\frac{(obs. \\ freq. - exp. \\ freq.)^2}{exp. \\ freq.}$  EXPECTED  prints the expected cell frequencies  CELLCHI2  prints each cell's contribution to the total chi-square statistic: $ \\frac{(obs. \\ freq. - exp. \\ freq.)^2}{exp. \\ freq.}$  NOCOL / NOROW  suppresses the printing of the column/row percentages  NOPERCENT  supresses the printing of the cell percentages  RELRISK  (relative risk) prints a table that contains risk ratios (probability ratios) and odds ratios;  PROC FREQ  uses the  classification in the first column  of the crosstabulation table as the  outcome of interest  and the first/second row in the numerator/denominator", 
            "title": "Cramer's V statistic"
        }, 
        {
            "location": "/statistics/categorical-data/#mantel-haenszel-chi-square-test", 
            "text": "For  ordinal associations , the  Mantel-Haenszel  chi-square test is a more powerful test.   The levels must be in a  logical order  for the test results to be meaningful  Hypothesis testing:  $H_0$ : no ordinal association;  $H_a$ : ordinal association  Similarly to the Pearson case, the Mantel-Haenszel chi-square statistic/p-value indicate whether an association exists but not its magnitude and they depend on and reflect the sample size   To measure the  strength of the association  between two ordinal variables you can use the  Spearman correlation  statistic.   You should only use it if both variables are ordinal and are in logical order  Is considered to be a rank correlation because it provides a degree of association between the ranks of the ordinal variables  This statistic has a  range between -1 and +1 : values close to -1/+1 indicate that there is a relatively high degree of negative/positive correlation and values close to 0 indicate a weak correlation  It is  not affected by the sample size   1\n2\n3\n4 PROC FREQ DATA=SAS-data-set;\n    TABLES variable-rows*variable-columns / CHISQ EXPECTED  /options ;\n   additional statements \nRUN;    MEASURES  produces the Spearman correlation statistic along with other measurement of association  CL  produces confidence bounds for the statistics that the MEASURES option requests  The confidence bounds are valid only if the sample size is large ( 25)  The asymptotic standard error ( ASE ) is used for large samples and is used to calculate the confidence intervals for various measures of association (including the Spearman correlation coefficient)", 
            "title": "Mantel-Haenszel chi-square test"
        }, 
        {
            "location": "/statistics/categorical-data/#introduction-to-logistic-regression", 
            "text": "Logistic Regression is a generalized linear model (like Linear Regression or ANOVA) that you can use to predict a categorical response/outcome based on one or more continuous/categorical predictor variables. There are three models:", 
            "title": "Introduction to Logistic Regression"
        }, 
        {
            "location": "/statistics/categorical-data/#linear-vs-logistic", 
            "text": "", 
            "title": "Linear vs Logistic"
        }, 
        {
            "location": "/statistics/categorical-data/#linear-regression-model", 
            "text": "Assumes that the expected value of the response continuous variable ($Y$) has a linear relationship with the predictor variable ($X$)  The conditional mean of the response hast the linear form $E(Y|X)=\\beta_0+\\beta_1X$ and it ranges $(-\\infty,+\\infty)$    Why not to use Linear Regression to model a binary response variable  Following the Linear Regression Model scheme, the response variable is calculated as   $Y_i=\\beta_0+\\beta_1\\cdot X_i+\\epsilon_i$,   where $\\beta_0$ and $\\beta_1$ are obtained by the method of least squares.    This model  assumes that the data is continuous , which is not true for the case of binary data  This model  assumes that the mean of the response is $\\beta_0+\\beta_1\\cdot X$ , while for binary data the mean of the response is the probability of a success  If the response variable has only two levels, you cannot  assume the constant variance and normality  that are required for linear regression", 
            "title": "Linear Regression Model"
        }, 
        {
            "location": "/statistics/categorical-data/#binary-logistic-regression-model", 
            "text": "The predictor variable ($X$) is used to estimate the probability of a specific outcome ($p$) for which you need to use a nonlinear function  The mean of the response is a probability, which is between $(0, 1)$.   The  Inverse Logit Function  binds the linear predictor between $0$ and $1$ is defined as $p_i=(1+e^{-(\\beta_0+\\beta_1 X_i)})^{-1}$  This model applies a  Logit Transformation  to the probabilities $logit(p_i)=ln\\left ( \\frac{p_i}{1-p_i} \\right ) = \\beta_0+\\beta_1X_i$, so that the transformed probabilities and predictor variables end up with a linear relationship  The logit is the  natural log of the odds  (the probability of the event occurring divided by the probability of the event not occurring)  We make the  assumption that the logit transformation of the probabilities results in a linear relationship with the predictor variables  (we can use a linear function $X$ to model the logit in order to indireclty model the probability)  The logit of the probability transforms the probability into a linear function, which has no lower or upper bounds. So a  logit has no lower or upper bounds .", 
            "title": "Binary Logistic Regression Model"
        }, 
        {
            "location": "/statistics/categorical-data/#proc-logistic", 
            "text": "To model categorical data yu use the  LOGISTIC  procedure. Some of the most common statements of this procedure are shown here:  1\n2\n3\n4\n5\n6 PROC LOGISTIC DATA=SAS-data-set  options ;     CLASS variable  (variable_options)  ...  / options ;     MODEL response  (variable_options)  = predictors  / options ;     UNITS independent1=list...  / options ;\n    ODDSRATIO  label  variable  / options ;\nRUN;    CLASS  is used to define the classification (categorical) predictor variables (if any); this statement must precede the  MODEL  statement  CLODDS = PL  (profile likelihood) |  WALD  (default) |  BOTH  is an example of a general option that you can specify in the  MODEL  statement which computes confidence intervals for the odds ratios of all predictor variables and also enables the production of the odds ratio plot    Example  PROC LOGISTIC DATA=statdata.sales_inc PLOTS(ONLY)=(EFFECT ODDSRATIO);\n    CLASS gender;\n    MODEL purchase(EVENT='1')=gender / CLODDS=PL;\nRUN;", 
            "title": "PROC LOGISTIC"
        }, 
        {
            "location": "/statistics/categorical-data/#classification-variables-parametrization", 
            "text": "When the predictor variable is categorical, the assumption of linearity cannot be met. To get past the obstacle of nonlinearity, the  CLASS  statement creates a set of one or more  design variables  (also called dummy variables).  PROC LOGISTIC  uses these variables, and not the original ones, in model calculations.  Different parametrization methods for the classification variables will produce the same results regarding the significance of the categorical predictors, but understanding the parametrization method helps to interpret the results accurately.  Here we present two of the most common methods of parameterizing ( PARAM = ) the classification variables. For both of them:   The default  reference level  is the level that has the highest ranked value (or the last value) when the levels are sorted in ascending alphanumeric order  The number of design variables (or $\\beta$) that are created are the number of levels of the classification variable -1", 
            "title": "Classification Variables Parametrization"
        }, 
        {
            "location": "/statistics/categorical-data/#effect-coding-default", 
            "text": "Also called  deviation from the mean coding , it compares the effect of each level of the variable to the  average effect of all levels .    Example  Using this parametrization scheme the model will be described as follows  $logit(p)=\\beta_0+\\beta_1\\cdot D_{Low \\ Income} + \\beta_2\\cdot D_{Medium \\ Income}$    $\\beta_0$ is the average value of the logit across all income levels  $\\beta_1$ is the difference between the logit for income level 1 and $\\beta_0$  $\\beta_2$ is the difference between the logit for income level 2 and $\\beta_0$   Here's the Analysis of Maximum Likelihood Estimates table that  PROC LOGISTIC  generates for this example on which the parameter estimates ($\\beta_0$, $\\beta_1$, $\\beta_2$) and p-values reflect differences from the overall mean value over all levels.    The p-values indicate whether each particular level is significant compared to the average effect of all levels. The p-values for $\\beta_1$ and $\\beta_2$ not significant meaning that the effect of those levels is not different than the average effect of low, medium and high income.", 
            "title": "Effect coding (default)"
        }, 
        {
            "location": "/statistics/categorical-data/#reference-cell-coding", 
            "text": "It compares the effect of each level of the predictor to the effect of another  level that is the designated reference level .   Example  To use this scheme the classification variable has to be defined in the following way  CLASS gender (PARAM=REF REF='Male');  Using this parametrization scheme the model will be described as follows  $logit(p)=\\beta_0+\\beta_1\\cdot D_{Low \\ Income} + \\beta_2\\cdot D_{Medium \\ Income}$    $\\beta_0$ is the intercept, but not in terms of where you cross the $Y$ axis, instead is the value of the logit of the probability when income is high (or at the reference level)  $\\beta_1$ is the difference between the logit of the probability for low and high income  $\\beta_2$ is the difference between the logit of the probability for medium and high income   Here's the Analysis of Maximum Likelihood Estimates table that  PROC LOGISTIC  generates for this example on which the parameter estimates ($\\beta_0$, $\\beta_1$, $\\beta_2$) and p-values reflect differences with respect to the reference level.    The p-values indicate whether each particular level is significant compared to the reference level. The p-value for $\\beta_1 0.05$ is significant meaning that the effect of a low income is statistically different than the effect of a high income on the probability that people will spend at least $100\\$$. The same applies to $\\beta_2$.", 
            "title": "Reference cell coding"
        }, 
        {
            "location": "/statistics/categorical-data/#fitting-a-binary-logistic-regression", 
            "text": "1\n2\n3\n4\n5 PROC LOGISTIC DATA=statdata.sales_inc PLOTS(ONLY)=(EFFECT);\n    CLASS Gender (PARAM=REF REF= Male );\n    MODEL Purchase(event= 1 ) = Gender;\n    title1  LOGISTIC MODEL (1): Purchase = Gender ;\nRUN;   We look at the first few tables to make sure that the model is set up the way we want   The  Model Information  table describes the data set, the response variable, the number of response levels, the type of model, the algorithm used to obtain the parameter estimates, and the number of observations read and used.  The  Response Profile  table shows the values of the response variable, listed according to their ordered value and frequency. By default,  PROC LOGISTIC  orders the values of the response variable alphanumercally and bases the logistic regression model on the probability of the lowest value. However, we set the  EVENT=1 , the highest value, so this model is based on the probability that  Purchase=1 .  Below this table, we see the probability that  PROC LOGISTIC  is modeling, as shown in the log.  The  Class Level Information  table displays the predictor variable in the  CLASS  statement  Gender  (in the model we fixed  'Male'  as the reference level, so the design variable is 1 when  Gender='Female'  and 0 when  Gender='Male' ).  The  Model Convergence Status  simply indicates that the convergence criterion was met. There are a number of options to control the convergence criterion, but the default is the gradient convergence criterion with a default value of $10^{-8}$.  The  Model Fit Statistic  table reports the resuls of three tests (for the model with the intercept only and the model with the intercept and the predictor variables): AIC, SC and -2$\\cdot$Log(likelihood). AIC and SC are  goodness-of-fit measures  that you can use to compare one model to another (lower values indicate more desirable model, although there is no standard for determining how much of a difference indicates an improvement) and are not dependent on the number of terms in the model.   Akaike's Information Criterion (AIC) : it adjusts for the number of predictor valriables. It is the best statitstic to come up with the best  explanatory model .  Schwarz's Bayesian Criterion (SC) : it adjusts for the number of predictor variables and the number of observations. This test uses a bigger penalty for extra variables and therefore favors more parsimonious models. It is the best statitstic to come up with the best  predictive model .    The  Testing Global Null Hypothesis: BETA=0  table provides three statistics to test $H_0$ that all the regression coefficients in the model are 0. The  Likelihood Ratio  is the most reliable test, specially for small sample sizes. It is similar to the overall F test in linear regression.  The  Type 3 Analysis of Effects  table is generated when  CLASS  specifies a categorical predictor variable. It shows the results of testing whether each individual parameter estimate is statistically different from 0 ( Pr ChiSq $ 0.05$). The  Wald Chi-Square  statistic tests the listed effects. When there is only one predictor variable in the model, the value listed in the table will be identical to the Wald test in the  Testing Global Null Hypothesis  table.  The  Analysis of Maximum Likelihood Estimates  table lists the estimated model parameters (the betas), their standard errors, Wald test statistics and corresponding p-values. The parameter estimates are the estimated coefficients of the fitted logistic regression model. We can use these estimates to construct the logistic regression equation $logit(\\beta)=\\beta_0+\\beta_1 \\cdot Categorical \\ predictor$.  The  Odds Ratio Estimates  table ( ONLY for binary logistic regression ) shows the OR ratio for the modeled event. Notice that  PROC LOGISTIC  calculates Wald confidence limits by default.  The  Association of Predicted Probabilities and Observed Responses  table lists several goodness-of-fit measures.  The  Odds Ratio Estimates and Profile-Likelihood Confidence Intervals  table ( ONLY for multiple logistic regression ) contains the OR estimates and the profile-likelihood confidence limits that the  CLODDS=  option specified for the main effects. For multiple logistic regression, remember that  PROC LOGISTIC  calculates the adjusted odds ratios. For a continuous predictor, the odds ratio measures the change in odds associated with a one-unit difference of the predictor variable by default although it is specified otherwise in the  UNITS  statement.  In the  Odds Ratios plot , the dots represent the OR estimates and the horizontal lines represent the confidence intervals for those estimates. There is a reference line at one to check if the confidence intervals cross this value meaning that it is not statistically different from 1.  The  Effect plot  shows the levels of the  CLASS  predictor variable vs the probability of the desired outcome. If you are performing a  multiple logistic regression  analysis where there is a continuous predictor variable it will be represented in the horizontal axis. Moreover, the lines will represent all possible combinations of the different  CLASS  variables.", 
            "title": "Fitting a Binary Logistic Regression"
        }, 
        {
            "location": "/statistics/categorical-data/#iterpreting-the-odds-ratio-for-a-categorical-predictor", 
            "text": "Let's see how to calculate the odds and the odds ratio from the logistic regression model. Here is the logistic regression model that predicts the logit of $p$:  $logit(\\hat p)=ln(odds)=ln\\left ( \\frac{p_i}{1-p_i} \\right )=\\beta_0 + \\beta_1 \\cdot Gender$  According to our example the variable  Gender  is codified in a way that  Females=1  and  Males=0 , so the OR can be written:  $odds_{females}=e^{\\beta_0+\\beta_1}$  $odds_{males}=e^{\\beta_0}$  $odds \\ ratio = \\frac{e^{\\beta_0+\\beta_1}}{e^{\\beta_0}}=e^{\\beta_1}$  If the 95% confidence interval does not include 1, the OR is significant at the 0.05 level indicating an association between the predictor and response variables of your model.", 
            "title": "Iterpreting the Odds Ratio for a Categorical Predictor"
        }, 
        {
            "location": "/statistics/categorical-data/#iterpreting-the-odds-ratio-for-a-continuous-predictor", 
            "text": "For a continuous predictor variable, the OR measures the  increase or decrease in odds associated with a one-unit difference  of the predictor variable by default. $OR - 1 = %$ of greater odds for having one-unit of difference.", 
            "title": "Iterpreting the Odds Ratio for a Continuous Predictor"
        }, 
        {
            "location": "/statistics/categorical-data/#comparing-pairs-to-assess-the-fit-of-a-logistic-regression-model", 
            "text": "PROC LOGISTIC  calculates several different goodness-of-fit measures and displayed in the  Association of Predicted Probabilities and Observed Responses  table.  One of these goodness-of-fit methods is comparing pairs ( Pairs ). To start,  PROC LOGISTIC  creates two groups of observations, one for each value of the response variable. Then, the procedure selects pairs of observations, one from each group, until no more pairs can be selected.  PROC LOGISTIC determines whether each pair is concordant, discordant or tied.   A pair is  concordant  if the  model predicts it correclty , i.e. if the observation with the desired outcome has a  higher predicted probability , based on the model, than the observation without the outcome.  A pair is  discordant  if the  model does not predict it correctly , i.e. if the observation with the desired outcome has a  lower predicted probability , based on the model, than the observation without the outcome.  A pair is  tied  if it is neither concordant not discordant, i.e. the  probabilities are the same  and the model can not distinguished between them. Tied pairs aren't very common when there are continuous variables in the model.   The left column of the  Association of Predicted Probabilities and Observed Responses  table lists the percentage of pairs of each type. At the bottom is the total number of observation pairs on which the percentages are based, i.e. the number of pairs of observations with different outcome values $(N_{event=0} \\cdot N_{event=1})$.  More complex models have more than two predicted probabilities. However, regardless of the model's complexity, the same comparisons are made across all pairs of observations with different outcomes.  You can use these results as goodness-of-fit measures to compare one model to another. In general, higher percentage of concordant pairs and lower percentages of discordant and tied pairs indicate a more desirable model.  This table also shows the four rank correlation indices that are computed from the numbers of concordant $(n_c)$, discordant $(n_d)$ and tied $(n_t)$ pairs of observations:   Somers' D  (Gini coefficient) , defined as $(n_c-n_d)/(n_c+n_d+n_t)$  Goodman-Kruskal  Gamma , defined as $(n_c-n_d)/(n_c+n_d)$  Kendall's  Tau-a , defined as $(n_c-n_d)/(0.5 \\cdot N(N-1))$, with $N$ being the sum of observation frequencies in the data  The concordance index  c  is the most commonly used of these values and estimates the probability of an observation with the desired outcome having a higher predicted probability than an observation without the desired outcome and is defined as $c=\\frac{n_c+0.5 \\cdot n_t}{n_c+n_d+n_t}$. Note that the concordance index,  c , also gives an estimate of the  area under the receiver operating characteristic (ROC) curve  when the response is binary.    Note  More information about these parameters  here .   In general, a model with higher values of these indices has better predictive ability than a model with lower values.", 
            "title": "Comparing Pairs to Assess the Fit of a Logistic Regression Model"
        }, 
        {
            "location": "/statistics/categorical-data/#multiple-logistic-regression-model", 
            "text": "", 
            "title": "Multiple Logistic Regression Model"
        }, 
        {
            "location": "/statistics/categorical-data/#introduction", 
            "text": "Sometimes you want to create a statistical model that explains the relationships among multiple predictors and a categorical response. You might want    to examine the  effect of each individual predictor  on the response regardless of the levels of the other predictors   to perform a more complex analysis that takes into account the  interactions between the predictors   In order to do this you will explore   how to define and explain the  adjusted odds ratio  how to fit a multiple logistic regression model using the  backward elimination method  how to fit a multiple logistic regression  model with interactions", 
            "title": "Introduction"
        }, 
        {
            "location": "/statistics/categorical-data/#multiple-logistic-regression", 
            "text": "Amultiple logistic regression model characterized the relationship between a categorical response variable and multiple predictor variables. The predictor variables can be continuous or categorical or both.  $logit(p)=\\beta_0+\\beta_1 X_1 +...+\\beta_k X_k$  The goal of multiple logistic regression, like multiple linear regression, is to find the subset of variables that best explains the variability of the response variable. Models that are  parsimonious or simple  are more likely to be numerically stable and are also easier to generalize.", 
            "title": "Multiple Logistic Regression"
        }, 
        {
            "location": "/statistics/categorical-data/#the-backward-elimination-method-of-variable-selection", 
            "text": "This method starts with a full model (a model that contains all of the main effects or predictor variables). Using an iterative process, the backward elimination method identifies and eliminates the nonsignificant predictor variables, one at a time. At each step, this method removes the least significant variable of the nonsignificant terms (the variable with the largest p-value).  The smaller your significance level, the more evidence you need to keep a predictor variable in the model. This results in a more parsimonious model.  The default significance level for a predictor to stay in the model is 0.05. You can change the significance level by adding  SLSTAY= value  or  SLS= value  in the  MODEL statement.", 
            "title": "The Backward Elimination Method of Variable Selection"
        }, 
        {
            "location": "/statistics/categorical-data/#adjusted-odds-ratios", 
            "text": "One major difference between a multiple logistic model and a logistic regression model with only one predictor variable is that the odds ratios are reported differently. Multiple logistic regression uses  adjusted odds ratios . An adjusted odds ratio measures the effect of a single predictor variable on a response variable while holding all the other predictor variables constant.   The adjusted odds ratio  assumes that the OR for a predictor variable is the same regardless of the level of the other predictor variables . If that assumption is not true, then you need to fit  a more complex model that also considers the interactions  between predictor variables.  The  MODEL  statement can include the  CLODDS=  option which enables the production of the OR plot ( PLOTS(ONLY)=(EFFECT ODDSRATIO) ). This option can be set to  PL  so that the procedure calculates profile-likelihood confidence limits for the OR of all predictor variables. These limits are based on the log likelihood and are generally preferred, especially for  smaller sample sizes .", 
            "title": "Adjusted Odds Ratios"
        }, 
        {
            "location": "/statistics/categorical-data/#specifiying-the-variable-selection-method-in-the-model-statement", 
            "text": "To specify the method that  PROC LOGISTIC  uses to select variable in a multiple logistic regression model, you add the  SELECTION=  option to the  MODEL  statement. The possible values of the  SELECTION=  statement are   NONE | N  (default): no selection method is used and the complete model is fitted  BACKWARD | B : backward elimination  FORWARD | F : forward selection  STEPWISE | S : stepwise selection  SCORE : best subset selection   1\n2\n3\n4 PROC LOGISTIC DATA=SAS-data-set  options ;\n    CLASS variable  (variable_options)  ...  / options ;\n    MODEL response  (variable_options)  = predictors  / options SELECTION ;\nRUN;   By default the procedure uses a $\\alpha=0.05$ significance level to determine which variables remain in the model. If you want to change the significance level, you can use the  SLSTAY | SLS =  option in the  MODEL  statement.", 
            "title": "Specifiying the Variable Selection Method in the MODEL Statement"
        }, 
        {
            "location": "/statistics/categorical-data/#the-units-statement", 
            "text": "The  UNITS statement enables you to obtain customized  odds ratio estimates  for a specified unit of change in one or more continuous predictor variables. For each continuous predictor (or independent variable) that you want to modify, you specify the variable name, an equal sign, and a list of  one or more units of change , separated by spaces, that are of interest for that variable. A unit of change can be a number, a standard deviation $(SD)$, or a number multiplied by the standard deviation $(n \\times SD)$.   The  UNITS  statement is optional. If you want to use the units to change that are reflected in the stored data values, you do not need to include the  UNITS  statement.", 
            "title": "The UNITS Statement"
        }, 
        {
            "location": "/statistics/categorical-data/#specifying-a-formatted-value-as-a-reference-level", 
            "text": "To ease the interpretation of the classification variable levels, text labels can be defined through a format definition.  Appliying a format requires a change in the  PROC LOGISTIC  step. In the  CLASS  statement when you use the  REF=  option with a variable that has either a temporary or a permanent format assigned to it, you must  specify the formatted value of the level instead of the stored value .", 
            "title": "Specifying a Formatted Value as a Reference Level"
        }, 
        {
            "location": "/statistics/categorical-data/#interaction-between-variables", 
            "text": "When you fit a multiple logistic regression model, the simplest approach is to consider only the main effects (each predictor individually) on the response. In other words, this approach assumes that each variable has the same effect on the outcome regardless of the levels of the other variables. However, sometimes the effect of one variable on the outcome depends on the observed level of another variable. When this happens, we say that  there is an interaction .  Keep in mind that  interactions that have more than two factors might be difficult to interpret .", 
            "title": "Interaction between Variables"
        }, 
        {
            "location": "/statistics/categorical-data/#the-backward-elimination-method-with-interactions-in-the-model", 
            "text": "When you use the backward elimination method with interactions in the model,  PROC LOGISTIC  begins by fitting th full model with all the main effects and interactions.  Using an iterative process,  PROC LOGISTIC  eliminates nonsignificant interactions one at a time, starting with the least significant interaction (largest p-value). When only significant interactions remain,  PROC LOGISTIC  turns its attention to the main effects.  PROC LOGISTIC  eliminates, one at a time, the nonsignificant main effects that are not involved in any significant interactions.   When eliminating main effects,  PROC LOGISTIC  must preserve the model hierarchy. According to this requirement, for any interaction that is included in the model, all main effects that the interaction contains must also be in the model, whether or not they are significant.  The  final model  has only significant interactions, the main effects involved in the interactions, and any other significant main effects.", 
            "title": "The Backward Elimination Method with Interactions in the Model"
        }, 
        {
            "location": "/statistics/categorical-data/#specifying-interactions-in-the-model-statement", 
            "text": "To specify interactions concisely, you can place a bar operator  |  between the names of each two main effects. The bar tells  PROC LOGISTIC  to treat the terms on either side of it as effects and also to include their combinations as interactions.  If you want to limit the maximum number of variables that are involved in each interaction, you can specify  @integer  after the list of effects (e.g.  @2  to include only the two-way interactions).", 
            "title": "Specifying Interactions in the MODEL Statement"
        }, 
        {
            "location": "/statistics/categorical-data/#the-oddsratio-statement", 
            "text": "By default,  PROC LOGISTIC  produces the OR only for variables that are not involved in an interaction. The OR for a main effect within an interaction would be misleading. It would only show the OR for that variable, holding constant the other variable at the value 0, which might not even be a valid value.  To tell  PROC LOGISTIC  to produce the OR for each value of a variable that is involved in an interaction, you can use the  ODDSRATIO  statement. You specify a separate  ODDSRATIO  statement for each variable. In this statement you can optionally specify a label for the variable. The variable name is required. At the end, you can specify options following a forward slash  / .  In this example, there are three  ODDSRATIO  statements, one for each main effect (they do not have to appear in the same order that the variables are listed in the  MODEL  statement).   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 PROC   LOGISTIC   DATA = statdata . sales_inc  \n         PLOTS ( ONLY ) = ( EFFECT   ODDSRATIO ); \n     CLASS   Gender   ( PARAM = REF   REF = Male ) \n         IncLevel   ( PARAM = REF   REF = 1 ); \n     UNITS   Age = 10 ; \n     MODEL   Purchase ( EVENT = 1 ) = Gender   |   Age   |   IncLevel   @2   / \n         SELECTION = BACKWARD   CLODDS = PL ;       ODDSRATIO   Age   /   CL = PL ;       ODDSRATIO   Gender   /   DIFF = REF   AT   ( IncLevel = ALL )   CL = PL ;       ODDSRATIO   IncLevel   /   DIFF = REF   AT   ( Gender = ALL )   CL = PL ;  RUN ;     The  CL = WALD (default) | PL | BOTH  option enables you to specify the type of confidence limits you want to produce: Wald, profile-likelihood or both.  The  DIFF = REF | ALL (default)  option applies only to categorical variables. Using this option, you can specify whether  PROC LOGISTIC  computes the OR for a categorical variable against the reference level or against all of its levels.  The  AT (covariate = value-list | REF | ALL (default))  option specifies fixed levels of one or more interacting variables (also called covariates).  PROC LOGISTIC  computes OR at each of the specified levels. For each categorical variable, you can specify a list of one or more formatted levels of the variable, or the keyword  REF  to select the reference level or the keyword  ALL  to select all levels of the variable.     This plot shows graphically what we saw with the calculated confidence intervals in the table above.", 
            "title": "The ODDSRATIO Statement"
        }, 
        {
            "location": "/statistics/categorical-data/#comparing-the-binary-and-multiple-logistic-regression-models", 
            "text": "Remember that, in general, when comparing the  AIC  and  SC  statistics, smaller values mean a  better model fit . Note that the  SC  value increased with the addition of the interaction.  SC  selects more parsimonious models by imposing a more severe penalty for increasing the number of parameters.  When comparing the  c  statistic values, larger values indicate a  better predictive model .", 
            "title": "Comparing the Binary and Multiple Logistic Regression Models"
        }, 
        {
            "location": "/statistics/categorical-data/#interaction-plots", 
            "text": "To visualize the interaction terms you can produce an interaction plot. This plot explains more of the story behind the significant interaction in the output.    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19 proc means data=statdata.sales_inc noprint nway;\n   class IncLevel Gender;\n   var Purchase;\n   output out=bins sum(Purchase)=Purchase n(Purchase)=BinSize;\nrun;\n\ndata bins;\n   set bins;\n      Logit=log((Purchase+1)/(BinSize-Purchase+1));\nrun;\n\nproc sgscatter data=bins;\n   plot Logit*IncLevel /group=Gender markerattrs=(size=15)\n                        join;\n   format IncLevel incfmt.;\n   label IncLevel= Income Level ;\n   title;\nrun;\nquit;    If there is no interaction between two variables, the slopes shold be relatively parallel.", 
            "title": "Interaction Plots"
        }, 
        {
            "location": "/statistics/categorical-data/#saving-analysis-results-with-the-store-statement", 
            "text": "You can use the  STORE  statement with  PROC LOGISTIC  to save your analysis results as an item store for later processing.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28 ODS SELECT NONE;\nPROC LOGISTIC DATA=statdata.ameshousing3;\n    CLASS Fireplaces (REF= 0 ) Lot_Shape_2 (REF= Regular ) / param=ref;\n    MODEL Bonus(EVENT= 1 )=Basement_Area | Lot_Shape_2 Fireplaces;\n    UNITS Basement_Area=100;     STORE OUT=isbonus; RUN;\nODS SELECT ALL;\n\nDATA newhouses;\n    LENGTH Lot_Shape_2 $9;\n    INPUT Fireplaces Lot_Shape_2 $ Basement_Area;\n    DATALINES;\n    0 Regular 1060\n    2 Regular 775\n    2 Irregular 1100\n    1 Irregular 975\n    1 Regular 800\n    ;\nRUN; PROC PLM RESTORE=isbonus;     SCORE DATA=newhouses OUT=scored_houses / ILINK;\n    TITLE  Predictions using PROC PLM ;\nRUN;\n\nPROC PRINT DATA=scored_houses;\nRUN;    Following the keyword  STORE , you use the  OUT=  option to specify the name of your item store.  In the  PROC PLM  statement, the  RESTORE  option specifies that the predictions will be based on the analysis results saved in the item store.  The  SCORE  statement specifies that SAS will score the provided data set. The  ILINK  option requests that SAS provide the predictions in the form of predicted probabilities in lieu of logits where covariate effects are additive.    Warning  Be sure that you generate predictions only for new data records that fall within the range of the training data. If not, predictions could be invalid due to extrapolation.  We assume that the modeled relationships between predictors and responses holds across the span of the observed data. We should not assume that this relationship holds everywhere.", 
            "title": "Saving Analysis Results with the STORE Statement"
        }, 
        {
            "location": "/statistics/prediction/", 
            "text": "Chapter summary in SAS\n\n\nInferential models, such as ANOVA and linear regression, are used to test hypotheses about the data and characterize the relationships between various types of predictor variables and a response variable. But, what if you want to go beyond explaining the relationship and \npredict future values of the response variable\n?\n\n\nIn predictive modeling, a statistical model is used to predict future values of a response variable, based on the existing values of predictor variables. In predictive modeling, the terms used to refer to the variables are often different from the terms used in explanatory modeling:\n\n\n\n\nPredictors are often called inputs, features, explanatory variables or independent variables \n\n\nThe response variable is often called a target, an outcome or dependent variable\n\n\n\n\nIn predictive modeling, the inputs and the target can be continuous, categorical, binary or any combination of these types.\n\n\nIntroduction to Predictive Modeling\n\n\nBefore you can predict values, you must first build a predictive model. To build a predictive model, you can use \nPROC GLMSELECT\n.\n\n\n\n\nDescribe the goal and uses of predictive modeling\n\n\nExplain how data partitioning is used in the hones assessment method of model selection\n\n\nDescribe the relationship between model complexity and flexibility\n\n\nUse \nPROC GLMSELECT\n to build a predictive model\n\n\n\n\nWhat Is Predictive Modeling?\n\n\nPredictive modeling uses historical data to predict future outcomes. The process of building and scoring a predictive model has two main parts: \nbuilding the predictive model on existing data and then deploying the model to make predictions on new data\n (using a process call scoring).\n\n\nA predictive model consists of either a formula or rules (depending on the type of analysis that you use) based on a set of input variables that are moes likely to predict the values of a target variable. Here we will deal with predictive models based on \nregression models, which are parametric and have formulas\n. Predictive models can also be based on \nnon-parametric models such as decision trees, which have rules\n.\n\n\nModel-based predictions are often called \nfact-based predictions\n. In contrast, decisions that are based completely on people's business expertise are often referred to as \nintuition-based decisions\n. Prediction modeling takes the guesswork out of the prediction process.\n\n\nModel Complexity\n\n\nWhether you are doing predictive modeling or inferential modeling, you want to select a model that generalizes well, that is, the model that best fits the entire population.\n\n\nYou assume that a sample that is used to fit the model is representative of the population. However, any given sample typically has idiosyncracies that are not found in the population. The model that best fits a sample and the population is the model that has the right complexity.\n\n\nA naive modeler might assume that most complex model should always outperform the others, but this is not the case. An overly complex model might be \ntoo flexible\n. This leads to \noverfitting\n that is, accomodating nuances of the random noise (the chance relationships) in the particular sample. Overfitting leads to models that have \nhigher variance when they are applied to a population\n. For regression, including more terms in the model increases complexity.\n\n\nOn the other hand, an inssufficiently complex model might \nnot be flexible enough\n. This leads to \nunderfitting\n that is, systematically missing the signal (the true relationships). This leads to \nbiased inferences\n, which are inferences that are not true of the population.\n\n\nA model with just enough complexity, which also means \njust enough flexibility\n, gives the best generalization. The important thing to realize is that there is not one perfect model; there is always a balance between overfitting and underfitting.\n\n\n\n\nBuilding a Predictive Model\n\n\nThe first part of the predictive modeling process is building the model. There are two steps to building the model: fitting the model and then, assessing model performance in order to select the model that will be deployed.\n\n\nTo build a predictive model, a method called \nhonest assessment\n is commonly used to ensure that the best model is selected. Honest assessment means that the assessment is done on a different data set than the one that was used to build the model and thus, it involves \npartitioning the available data\n typically into two data sets: a training data set and a validation data set. Both data sets contain the inputs and the target. \n\n\n\n\nThe \ntraining data set\n is used to fit the model. In the training data set, an observation is called a \ntraining case\n. Other synonyms for \"observation\" are example, instance and record. \n\n\nThe \nvalidation data set\n is a holdout sample that is used to assess model performance and select the model that has the best generalization.\n\n\nSometimes there is a third partition of the data, the \ntest data set\n, that is used to perform a final test on the model before the model is used for scoring. This final test can be referred to as a final honest estimate of generalization. Like the validation data set, the test data set is also referred to as a holdout data set. \n\n\n\n\nIn practice, many analysts see no need for a final honest assessment of generalization based on a test data set. Instead, an optimal model is chosen using the validation data. The model assessment that is measured on the validation data, is reported as an upper bound on the performance that is expected when the model is deployed for scoring.\n\n\nPartitioning the data avoids overfitting problems. The classical example of overfitting is selecting linear regression models based on R square.\n\n\nIs it always best to partition the data set when you build a predictive model? This depends on the size of the data set. If you start with a small or medium-size data set, partitioning the data might not be efficient. The reduced sample size can severely degrade the fit of the model. In fact, computer-intensive methods, such as the \ncross-validation and bootstrap methods\n, were developed so that all the data can be used for both fitting and honest assessment. However, predictive modeling usually involves very large data sets, so partitioning the data is usually appropriate.\n\n\nLet's take a closer look at using honest assessment to build a predictive model. During model fitting, the training data is used to model the target. You can use one of several model selection methods.\n\n\nThe \nforward selection\n process generates a number of possible models, which increases in complexity as variables are added to the model. Variable continue to be added as long as they meet the criterion for inclusion. For example, if you use the \nAICC\n criterion, variables will be added as long as the criterion value continues to decrease and then the process stops.\n\n\nFrom a number of models of different complexity generated with the training data on which the validation data is used to assess their performance, the chosen model will be \nthe simplest model (more parsimonious) with the highest validation assessment or best performance on the validation data\n.\n\n\nUsing \nPROC GLMSELECT\n to Build a Predictive Model\n\n\nThis procedure can build a model in two ways. The method that you use depends on the state of your data before model building begins. If your data is already partitioned into a trining data set and a validation data set, you can simply reference both data sets in the procedure. If you start with a single data set, \nPROC GLMSELECT\n can partition the data for you.\n\n\nIf the validation data set already exists:\n\n\n1\n2\n3\n4\nPROC GLMSELECT DATA=training-data-set\n               VALDATA=validation-data-set;\n    MODEL target(s)=input(s) \n/ options\n;\nRUN;\n\n\n\n\n\n\nIf you start with a data set that is not yet partitioned:\n\n\n1\n2\n3\n4\n5\nPROC GLMSELECT DATA=training-data-set\n               \nSEED\n=number\n;\n    MODEL target(s)=input(s) \n/ options\n;\n    PARTITION FRACTION (\nTEST\n=fraction\nVALIDATE\n=fraction\n);\nRUN;\n\n\n\n\n\n\nThe sum of the specified fractions must be less than 1 and the remaining fraction of the cases in the inputa data set are assigned the training role.\n\n\nThe \nPARTITION\n statement requires a pseudo-random number generator to start the random selection process and a starting seed seed is needed which must be an integer. If you need to be able to reproduce your results in the future, you specify an integer that is greater than zero in the \nSEED=\n option in order to get the same results every time. If the \nSEED=\n value specifies an invalid value or no value, the seed is automatically generated from reading the time of day from the computer's clock. In most situations, it is recommended that you use the \nSEED=\n option and specify an integer greater than zero.\n\n\n\n\nBuilding a Predictive Model\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\nODS GRAPHICS;\n\nPROC GLMSELECT DATA=training-data-set\n               PLOTS=all\n               VALDATA=validation-data-set;\n    CLASS categorical1 categorical2 / PARAM=glm REF=first;\n    MODEL target = categorical1 categorical2 interval1 interval2 / \n            SELECTION=backward|forward|stepwise SELECT=sbc|aic CHOOSE=validate;\n    STORE OUT=output-data-set;\n    title \nSelecting the Best Model using Honest Assessment\n;\nRUN;\n\n\n\n\n\n\n\n\nThe \nPARAM=\n option specifies the parametrization method. In this case \nglm\n parametrization is used (same method used in \nPROC GLM\n). With \nglm\n parametrization, one design variable is produced per level of each \nCLASS\n variable.\n\n\nWe specify \nREF=first\n so that, for each design variable, we can compare the output at the current level with the first level. By default, the reference level is the last level.\n\n\nThe \nSELECTION=\n option specifies the \nbackward\n selection model of variable selection.\n\n\nSELECT=sbc\n indicates that the Schwarz-Bayesian criterion will be used to determine which variables remain in the model.\n\n\nCHOOSE=validate\n specifies that \nPROC GLMSELECT\n will select the best model based on the validation data (the one that has the smallest overall validation error, specifically the average squared error --ASE--, which is the sum of the squared differences between the observed value and the predicted value using the model).\n\n\nThe \nSTORE\n statement saves the context and results of the statistical analysis in a file called an item store. It is convenient to create an item store because you can reference it in later programs and avoid having to rebuild the model. For example, you can use the item store to score new data in the next step of predictive modeling.\n\n\n\n\nLet's describe now the typical obtained results:\n\n\n\n\nThe first table summarizes model information, including the data sets, variables, selction method and criteria that are used to select the variables and the final model.\n\n\nThen there are two \nObservation Profile for Analysis\n tables, one for the analysis (that is, training) data and the other for the validation data.\n\n\nThe \nClass Level Information\n table shows the number of categorical variables included in the analysis. The total degrees of freedom are much higher than this number because most categorical variables have more than one level, which results in more than one design variable. The number of non-redundant design variables or degrees of freedom of a categorical variable is the total number of levels minus one.\n\n\nIn the \nDimensions\n table, you can see the number of effects and parameters of the analysis. The number of parameters is higher than the number of effects because some of the effects have multiple parameters. \n\n\nThe \nnumber of effects\n = number of interval variables + number of categorical variables + intercept\n\n\nThe \nnumber of parameters\n = number of interval variables + number of degrees of freedom of categorical variables (addition of total number of levels - the number of categorical variables) + intercept\n\n\n\n\n\n\nIn the \nBackward Selection Summary\n table, the Step 0 row shows the number of effects and non-redundant parameters (smaller than the number of parameters that is shown in the \nDimensions\n table) that we start with. The difference is the number of variables because there is one redundant design variable for each categorical variable. The \nSBC\n is assessed on the training data and variables are removed from the model while this produces a reduction on it. In the last row, the SBC value is followed by an asterisk, that means \nOptimal Value of Criterion\n. Remember that the SBC is also the stopping criterion because we didn't specify a different criterion. So, based on the SBC for the training data, the model at the last step is the best model. The next column reports the training average squared error, but this is not used to select the model. It seems contradictory that, as you remove variables from the model, the training ASE tends to increase. However, this is not true for the validation data set, so the validation data set gives a better idea of how the model performs on data that was not used to build the model. Remember that the model is chosen based on the validation ASE. Going down the column for the validation ASE, the values continue to decrease until a value marked with an asterisk, meaning that this is the best model based on the validation data.\n\n\nThe \nStop Details\n table summarizes the information that we just discussed.\n\n\nIn the first plot, \nCoefficient Progression for [target]\n, the lower section shows the performance of all the models based on the validation ASE. In the top section you can compare the models and see how the parameters changed as variables were removed from the model. A vertical line extends up through the point for the selected model. The parameter estimates on this vertical line are of interest to us.\n\n\nIn the \nProgression of Average Squared Errors by Role for [target]\n plot, the ASE for the validation data is plotted on the top line and the training data on the bottom line. Looking at the training data plot, notice that the ASE cannot go down; it can only go up because the backwards elimination criterion is used to remove variables from the model. For the training data, the ASE can go down only if variables are added.\n\n\nIn the \nAnalysis of Variance\n table, we see some summary information. Remember that the number of degrees of freedom in the selected model is not the number of variables.\n\n\nThe \nParameter Estimates\n table. For each categorical variable, the selected levels are shown and the table and the lowest one (because the \nREF=first\n option was defined) is used as the reference level and is then set to zero with zero degrees of freedom, which makes it the redundant design variable. This table also shows t values. We could also get p-values, but that is not useful because this model was already selected. Notice that this table lists all the categorical variables before the continuous variables.\n\n\n\n\nScoring Predictive Models\n\n\nAfter you build a predictive model, you are ready to deploy the model. To score new data you can use \nPROC GLMSELECT\n and \nPROC PLM\n.\n\n\nPreparing for Scoring\n\n\nBefore you start using a newly-built model to score data, some preparation of the model, the data or both is usually required.\n\n\nIt is essential for the scoring data to be comparable to the training and validation data that were used to build the model. Before the model is build, modifications are often made to the training data, such as missing value imputation, transformations and derivation of inputs through standardization of the creation of composite variables from existing variables. The same modifications must be made to the validation data before validating the model and to the scoring data before scoring. Making the same modifications becomes more complex if the original modifications were based on parameters derived from the training data set, such as the mean or standard deviation.\n\n\nMethods of Scoring\n\n\nWhen you score, you do not rerun the algorithm that was used to build the model. Instead, you apply the score code that is, the equations obtained from the final model to the scoring data. Let's look at three methods of scoring your data.\n\n\n\n\nMethod 1\n: a \nSCORE\n statement is added to the \nPROC GLMSELECT\n step that is used to create the model. This method is useful because you can build and score a model in one step. However, this method is inefficient if you want to score more than once or use a large data set to build a model. With this method, the model must be built from the training data each time the program is run.\n\n\nMethod 2\n: a \nSTORE\n statement in the \nPROC GLMSELECT\n step and then a \nSCORE\n statement in \nPROC PLM\n. This method enables you to build the model only once, along with an item store, using \nPROC GLMSELECT\n. You can then use \nPROC PLM\n to score new data using the item store. Separating the code for model building and model scoring is especially helpful if your model is based on a very large training data set or if you want to scroe more than once. One potential problem with this method is that others might not be able to use this code with earlier versions of SAS or you might not want to share the entire item store.\n\n\nMethod 3\n: a \nSTORE\n statement in \nPROC GLMSELECT\n, a \nCODE\n statement in \nPROC PLM\n to output SAS code for scoring and then a DATA step to do the scoring. Some of the previous method's problems are solved by using \nPROC PLM\n to write detailed scoring code, based on the item store, that is compatible with earlier versions of SAS. You can provide this code to others without having to share other information that is in the item store. The \nDATA\n step is then used for scoring.\n\n\n\n\nScoring Data\n\n\nLet's use the item store that we created in the last example to score data. When you score data in your work environment, you'll be scoring data that was not used in either training or validation. Here we use code that scores the data in two different ways and then compares the output from the two methods.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\nPROC\n \nPLM\n \nRESTORE\n=\noutput\n-\ndata\n-\nset\n;\n\n    \nSCORE\n \nDATA\n=\ndata\n-\nset\n-\nto\n-\nscore\n \nOUT\n=\nscored\n-\ndata\n-\nset1\n;\n\n    \nCODE\n \nFILE\n=\n/folders/myfolders/scoring.sas\n;\n\n\nRUN\n;\n\n\n\nDATA\n \nscored\n-\ndata\n-\nset2\n;\n\n    \nSET\n \ndata\n-\nset\n-\nto\n-\nscore\n;\n\n    \n%include\n \n/folders/myfolders/scoring.sas\n;\n\n\nRUN\n;\n\n\n\nPROC\n \nCOMPARE\n \nBASE\n=\nscored\n-\ndata\n-\nset1\n \nCOMPARE\n=\nscored\n-\ndata\n-\nset2\n \nCRITERION\n=\n0.0001\n;\n\n    \nVAR\n \npredicted1\n;\n\n    \nWITH\n \npredicted2\n;\n\n\nRUN\n;\n\n\n\n\n\n\n\n\n\nThe \nPROC PLM\n uses the \nSCORE\n statement to score the data. By default the name \nPredicted\n is used for the scored variable. An statement of this kind can be used also in the \nPROC GLMSELECT\n to score data.\n\n\nThe second method generates scoring code by using the \nCODE\n statement in \nPROC PLM\n, and then uses a \nDATA\n step to do the scoring.\n\n\nThe \nPROC COMPARE\n step compares the values of the scored variable in the two output data sets. There's no need to do any preliminary matching or sorting in this case because the output data sets are based ont he same input data set; they have the same number of variables and observations, in the same order. By default, the criterion for judging the equality of the numeric values is 0.00001 which can be changed by specifying a different criterion using the \nCRITERION=\n option. The scored variable has a different name in the two data sets, so the two names are specified in the \nVAR\n (for the \nBASE=\n) and \nWITH\n (for the \nCOMPARE=\n) statements.\n\n\n\n\nIn the results, we want to look at the \nValues Comparison Summary\n to see whether the two methods produced similar predictions and check for values that are compared as unequal. You can find \nValues not EXACTLY Equal\n but, as the \nMaximum Difference Criterion Value\n indicates, the differences are too small to be important.", 
            "title": "Model Building and Scoring for Prediction"
        }, 
        {
            "location": "/statistics/prediction/#introduction-to-predictive-modeling", 
            "text": "Before you can predict values, you must first build a predictive model. To build a predictive model, you can use  PROC GLMSELECT .   Describe the goal and uses of predictive modeling  Explain how data partitioning is used in the hones assessment method of model selection  Describe the relationship between model complexity and flexibility  Use  PROC GLMSELECT  to build a predictive model", 
            "title": "Introduction to Predictive Modeling"
        }, 
        {
            "location": "/statistics/prediction/#what-is-predictive-modeling", 
            "text": "Predictive modeling uses historical data to predict future outcomes. The process of building and scoring a predictive model has two main parts:  building the predictive model on existing data and then deploying the model to make predictions on new data  (using a process call scoring).  A predictive model consists of either a formula or rules (depending on the type of analysis that you use) based on a set of input variables that are moes likely to predict the values of a target variable. Here we will deal with predictive models based on  regression models, which are parametric and have formulas . Predictive models can also be based on  non-parametric models such as decision trees, which have rules .  Model-based predictions are often called  fact-based predictions . In contrast, decisions that are based completely on people's business expertise are often referred to as  intuition-based decisions . Prediction modeling takes the guesswork out of the prediction process.", 
            "title": "What Is Predictive Modeling?"
        }, 
        {
            "location": "/statistics/prediction/#model-complexity", 
            "text": "Whether you are doing predictive modeling or inferential modeling, you want to select a model that generalizes well, that is, the model that best fits the entire population.  You assume that a sample that is used to fit the model is representative of the population. However, any given sample typically has idiosyncracies that are not found in the population. The model that best fits a sample and the population is the model that has the right complexity.  A naive modeler might assume that most complex model should always outperform the others, but this is not the case. An overly complex model might be  too flexible . This leads to  overfitting  that is, accomodating nuances of the random noise (the chance relationships) in the particular sample. Overfitting leads to models that have  higher variance when they are applied to a population . For regression, including more terms in the model increases complexity.  On the other hand, an inssufficiently complex model might  not be flexible enough . This leads to  underfitting  that is, systematically missing the signal (the true relationships). This leads to  biased inferences , which are inferences that are not true of the population.  A model with just enough complexity, which also means  just enough flexibility , gives the best generalization. The important thing to realize is that there is not one perfect model; there is always a balance between overfitting and underfitting.", 
            "title": "Model Complexity"
        }, 
        {
            "location": "/statistics/prediction/#building-a-predictive-model", 
            "text": "The first part of the predictive modeling process is building the model. There are two steps to building the model: fitting the model and then, assessing model performance in order to select the model that will be deployed.  To build a predictive model, a method called  honest assessment  is commonly used to ensure that the best model is selected. Honest assessment means that the assessment is done on a different data set than the one that was used to build the model and thus, it involves  partitioning the available data  typically into two data sets: a training data set and a validation data set. Both data sets contain the inputs and the target.    The  training data set  is used to fit the model. In the training data set, an observation is called a  training case . Other synonyms for \"observation\" are example, instance and record.   The  validation data set  is a holdout sample that is used to assess model performance and select the model that has the best generalization.  Sometimes there is a third partition of the data, the  test data set , that is used to perform a final test on the model before the model is used for scoring. This final test can be referred to as a final honest estimate of generalization. Like the validation data set, the test data set is also referred to as a holdout data set.    In practice, many analysts see no need for a final honest assessment of generalization based on a test data set. Instead, an optimal model is chosen using the validation data. The model assessment that is measured on the validation data, is reported as an upper bound on the performance that is expected when the model is deployed for scoring.  Partitioning the data avoids overfitting problems. The classical example of overfitting is selecting linear regression models based on R square.  Is it always best to partition the data set when you build a predictive model? This depends on the size of the data set. If you start with a small or medium-size data set, partitioning the data might not be efficient. The reduced sample size can severely degrade the fit of the model. In fact, computer-intensive methods, such as the  cross-validation and bootstrap methods , were developed so that all the data can be used for both fitting and honest assessment. However, predictive modeling usually involves very large data sets, so partitioning the data is usually appropriate.  Let's take a closer look at using honest assessment to build a predictive model. During model fitting, the training data is used to model the target. You can use one of several model selection methods.  The  forward selection  process generates a number of possible models, which increases in complexity as variables are added to the model. Variable continue to be added as long as they meet the criterion for inclusion. For example, if you use the  AICC  criterion, variables will be added as long as the criterion value continues to decrease and then the process stops.  From a number of models of different complexity generated with the training data on which the validation data is used to assess their performance, the chosen model will be  the simplest model (more parsimonious) with the highest validation assessment or best performance on the validation data .", 
            "title": "Building a Predictive Model"
        }, 
        {
            "location": "/statistics/prediction/#using-proc-glmselect-to-build-a-predictive-model", 
            "text": "This procedure can build a model in two ways. The method that you use depends on the state of your data before model building begins. If your data is already partitioned into a trining data set and a validation data set, you can simply reference both data sets in the procedure. If you start with a single data set,  PROC GLMSELECT  can partition the data for you.  If the validation data set already exists:  1\n2\n3\n4 PROC GLMSELECT DATA=training-data-set\n               VALDATA=validation-data-set;\n    MODEL target(s)=input(s)  / options ;\nRUN;   If you start with a data set that is not yet partitioned:  1\n2\n3\n4\n5 PROC GLMSELECT DATA=training-data-set\n                SEED =number ;\n    MODEL target(s)=input(s)  / options ;\n    PARTITION FRACTION ( TEST =fraction VALIDATE =fraction );\nRUN;   The sum of the specified fractions must be less than 1 and the remaining fraction of the cases in the inputa data set are assigned the training role.  The  PARTITION  statement requires a pseudo-random number generator to start the random selection process and a starting seed seed is needed which must be an integer. If you need to be able to reproduce your results in the future, you specify an integer that is greater than zero in the  SEED=  option in order to get the same results every time. If the  SEED=  value specifies an invalid value or no value, the seed is automatically generated from reading the time of day from the computer's clock. In most situations, it is recommended that you use the  SEED=  option and specify an integer greater than zero.", 
            "title": "Using PROC GLMSELECT to Build a Predictive Model"
        }, 
        {
            "location": "/statistics/prediction/#building-a-predictive-model_1", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 ODS GRAPHICS;\n\nPROC GLMSELECT DATA=training-data-set\n               PLOTS=all\n               VALDATA=validation-data-set;\n    CLASS categorical1 categorical2 / PARAM=glm REF=first;\n    MODEL target = categorical1 categorical2 interval1 interval2 / \n            SELECTION=backward|forward|stepwise SELECT=sbc|aic CHOOSE=validate;\n    STORE OUT=output-data-set;\n    title  Selecting the Best Model using Honest Assessment ;\nRUN;    The  PARAM=  option specifies the parametrization method. In this case  glm  parametrization is used (same method used in  PROC GLM ). With  glm  parametrization, one design variable is produced per level of each  CLASS  variable.  We specify  REF=first  so that, for each design variable, we can compare the output at the current level with the first level. By default, the reference level is the last level.  The  SELECTION=  option specifies the  backward  selection model of variable selection.  SELECT=sbc  indicates that the Schwarz-Bayesian criterion will be used to determine which variables remain in the model.  CHOOSE=validate  specifies that  PROC GLMSELECT  will select the best model based on the validation data (the one that has the smallest overall validation error, specifically the average squared error --ASE--, which is the sum of the squared differences between the observed value and the predicted value using the model).  The  STORE  statement saves the context and results of the statistical analysis in a file called an item store. It is convenient to create an item store because you can reference it in later programs and avoid having to rebuild the model. For example, you can use the item store to score new data in the next step of predictive modeling.   Let's describe now the typical obtained results:   The first table summarizes model information, including the data sets, variables, selction method and criteria that are used to select the variables and the final model.  Then there are two  Observation Profile for Analysis  tables, one for the analysis (that is, training) data and the other for the validation data.  The  Class Level Information  table shows the number of categorical variables included in the analysis. The total degrees of freedom are much higher than this number because most categorical variables have more than one level, which results in more than one design variable. The number of non-redundant design variables or degrees of freedom of a categorical variable is the total number of levels minus one.  In the  Dimensions  table, you can see the number of effects and parameters of the analysis. The number of parameters is higher than the number of effects because some of the effects have multiple parameters.   The  number of effects  = number of interval variables + number of categorical variables + intercept  The  number of parameters  = number of interval variables + number of degrees of freedom of categorical variables (addition of total number of levels - the number of categorical variables) + intercept    In the  Backward Selection Summary  table, the Step 0 row shows the number of effects and non-redundant parameters (smaller than the number of parameters that is shown in the  Dimensions  table) that we start with. The difference is the number of variables because there is one redundant design variable for each categorical variable. The  SBC  is assessed on the training data and variables are removed from the model while this produces a reduction on it. In the last row, the SBC value is followed by an asterisk, that means  Optimal Value of Criterion . Remember that the SBC is also the stopping criterion because we didn't specify a different criterion. So, based on the SBC for the training data, the model at the last step is the best model. The next column reports the training average squared error, but this is not used to select the model. It seems contradictory that, as you remove variables from the model, the training ASE tends to increase. However, this is not true for the validation data set, so the validation data set gives a better idea of how the model performs on data that was not used to build the model. Remember that the model is chosen based on the validation ASE. Going down the column for the validation ASE, the values continue to decrease until a value marked with an asterisk, meaning that this is the best model based on the validation data.  The  Stop Details  table summarizes the information that we just discussed.  In the first plot,  Coefficient Progression for [target] , the lower section shows the performance of all the models based on the validation ASE. In the top section you can compare the models and see how the parameters changed as variables were removed from the model. A vertical line extends up through the point for the selected model. The parameter estimates on this vertical line are of interest to us.  In the  Progression of Average Squared Errors by Role for [target]  plot, the ASE for the validation data is plotted on the top line and the training data on the bottom line. Looking at the training data plot, notice that the ASE cannot go down; it can only go up because the backwards elimination criterion is used to remove variables from the model. For the training data, the ASE can go down only if variables are added.  In the  Analysis of Variance  table, we see some summary information. Remember that the number of degrees of freedom in the selected model is not the number of variables.  The  Parameter Estimates  table. For each categorical variable, the selected levels are shown and the table and the lowest one (because the  REF=first  option was defined) is used as the reference level and is then set to zero with zero degrees of freedom, which makes it the redundant design variable. This table also shows t values. We could also get p-values, but that is not useful because this model was already selected. Notice that this table lists all the categorical variables before the continuous variables.", 
            "title": "Building a Predictive Model"
        }, 
        {
            "location": "/statistics/prediction/#scoring-predictive-models", 
            "text": "After you build a predictive model, you are ready to deploy the model. To score new data you can use  PROC GLMSELECT  and  PROC PLM .", 
            "title": "Scoring Predictive Models"
        }, 
        {
            "location": "/statistics/prediction/#preparing-for-scoring", 
            "text": "Before you start using a newly-built model to score data, some preparation of the model, the data or both is usually required.  It is essential for the scoring data to be comparable to the training and validation data that were used to build the model. Before the model is build, modifications are often made to the training data, such as missing value imputation, transformations and derivation of inputs through standardization of the creation of composite variables from existing variables. The same modifications must be made to the validation data before validating the model and to the scoring data before scoring. Making the same modifications becomes more complex if the original modifications were based on parameters derived from the training data set, such as the mean or standard deviation.", 
            "title": "Preparing for Scoring"
        }, 
        {
            "location": "/statistics/prediction/#methods-of-scoring", 
            "text": "When you score, you do not rerun the algorithm that was used to build the model. Instead, you apply the score code that is, the equations obtained from the final model to the scoring data. Let's look at three methods of scoring your data.   Method 1 : a  SCORE  statement is added to the  PROC GLMSELECT  step that is used to create the model. This method is useful because you can build and score a model in one step. However, this method is inefficient if you want to score more than once or use a large data set to build a model. With this method, the model must be built from the training data each time the program is run.  Method 2 : a  STORE  statement in the  PROC GLMSELECT  step and then a  SCORE  statement in  PROC PLM . This method enables you to build the model only once, along with an item store, using  PROC GLMSELECT . You can then use  PROC PLM  to score new data using the item store. Separating the code for model building and model scoring is especially helpful if your model is based on a very large training data set or if you want to scroe more than once. One potential problem with this method is that others might not be able to use this code with earlier versions of SAS or you might not want to share the entire item store.  Method 3 : a  STORE  statement in  PROC GLMSELECT , a  CODE  statement in  PROC PLM  to output SAS code for scoring and then a DATA step to do the scoring. Some of the previous method's problems are solved by using  PROC PLM  to write detailed scoring code, based on the item store, that is compatible with earlier versions of SAS. You can provide this code to others without having to share other information that is in the item store. The  DATA  step is then used for scoring.", 
            "title": "Methods of Scoring"
        }, 
        {
            "location": "/statistics/prediction/#scoring-data", 
            "text": "Let's use the item store that we created in the last example to score data. When you score data in your work environment, you'll be scoring data that was not used in either training or validation. Here we use code that scores the data in two different ways and then compares the output from the two methods.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 PROC   PLM   RESTORE = output - data - set ; \n     SCORE   DATA = data - set - to - score   OUT = scored - data - set1 ; \n     CODE   FILE = /folders/myfolders/scoring.sas ;  RUN ;  DATA   scored - data - set2 ; \n     SET   data - set - to - score ; \n     %include   /folders/myfolders/scoring.sas ;  RUN ;  PROC   COMPARE   BASE = scored - data - set1   COMPARE = scored - data - set2   CRITERION = 0.0001 ; \n     VAR   predicted1 ; \n     WITH   predicted2 ;  RUN ;     The  PROC PLM  uses the  SCORE  statement to score the data. By default the name  Predicted  is used for the scored variable. An statement of this kind can be used also in the  PROC GLMSELECT  to score data.  The second method generates scoring code by using the  CODE  statement in  PROC PLM , and then uses a  DATA  step to do the scoring.  The  PROC COMPARE  step compares the values of the scored variable in the two output data sets. There's no need to do any preliminary matching or sorting in this case because the output data sets are based ont he same input data set; they have the same number of variables and observations, in the same order. By default, the criterion for judging the equality of the numeric values is 0.00001 which can be changed by specifying a different criterion using the  CRITERION=  option. The scored variable has a different name in the two data sets, so the two names are specified in the  VAR  (for the  BASE= ) and  WITH  (for the  COMPARE= ) statements.   In the results, we want to look at the  Values Comparison Summary  to see whether the two methods produced similar predictions and check for values that are compared as unequal. You can find  Values not EXACTLY Equal  but, as the  Maximum Difference Criterion Value  indicates, the differences are too small to be important.", 
            "title": "Scoring Data"
        }, 
        {
            "location": "/statistics/miscellanea/", 
            "text": "One-sided vs Two-sided Tests: How do their p-values compare?\n\n\nThe default among statistical packages performing tests is to report two-tailed p-values.  Because the most commonly used test statistic distributions (standard normal, Student's t) are symmetric about zero, most one-tailed p-values can be derived from the two-tailed p-values.\n\n\nThe null hypothesis is that the difference in means is zero.  The two-sided alternative is that the difference in means is not zero. There are two one-sided alternatives that one could opt to test instead: that the difference is positive ($diff \n 0$) or that the difference is negative ($diff \n 0$). \n\n\nNote that the test statistic is the same for all of these tests. The two-tailed p-value is $P \n |t|$. This can be rewritten as $P(\nx)+P(\nx)$. \n\n\nBecause the t-distribution is symmetric about zero, these two probabilities are equal: $P \n |t| = 2 \\cdot  P(\nx)=1-(P\nx)$. \n\n\nSo, depending on the direction of the one-tailed hypothesis, its p-value is either $0.5 \\cdot$(two-tailed p-value) or $1-0.5 \\cdot$(two-tailed p-value) if the test statistic symmetrically distributed around zero. \n\n\nIn summary, to understand the connection between the results, you have to carefully review the $H_a$ for each case: the one-sided analysis (there is a difference with a certain sign) is more restrictive and demanding than the two-sided (there is a difference). Therefore, it follows that\n\n\n\n\nthe \none-tail p-value is half the two-tail p-value\n and\n\n\nthe \ntwo-tail p-value is twice the one-tail p-value\n (assuming you correctly predicted the direction of the difference).\n\n\n\n\n\n\nCheck these websites\n\n\n\n\nWhat are the differences between one-tailed and two-tailed tests?\n\n\nOne-tail vs two-tail p-values\n\n\nOne-sided and two-sided tests and p-values\n\n\n\n\n\n\nMcNemar's Test\n vs \nCohen's Kappa Coefficient\n\n\nMcNemar's test\n is a statistical test used on paired nominal data. It is applied to $2 \\times 2$ contingency tables with a dichotomous trait, with matched pairs of subjects, to determine whether the row and column marginal frequencies are equal (that is, whether there is \"marginal homogeneity\"). \n\n\nThe null hypothesis of marginal homogeneity states that the two marginal probabilities for each outcome are the same.\n\n\n1\n2\n3\n4\n5\n6\n7\nODS EXCLUDE ALL;\nPROC FREQ DATA=SAS-data-set;\n    TABLE variable1 * variable2;\n    EXACT MCNEM;\n    ODS OUTPUT MCNEMARSTEST=mcnemarresults;\nRUN;\nODS EXCLUDE NONE;\n\n\n\n\n\n\nCohen's kappa coefficient\n is a statistic which measures inter-rater agreement for qualitative (categorical) items. It is generally thought to be a more robust measure than simple percent agreement calculation, since $\\kappa$ takes into account the possibility of the agreement occurring by chance. There is controversy surrounding Cohen\u2019s Kappa due to the difficulty in interpreting indices of agreement. Some researchers have suggested that it is conceptually simpler to evaluate disagreement between items.\n\n\nIf the raters are in complete agreement then $\\kappa=1$. If there is no agreement among the raters other than what would be expected by chance (as given by pe), $\\kappa \\le 0$.\n\n\nNote that Cohen's kappa measures agreement between \ntwo raters only\n. The Fleiss kappa is a \nmulti-rater\n generalization of Scott's pi statistic. Kappa is also used to compare performance in machine learning but the directional version known as Informedness or Youden's J statistic is argued to be more appropriate for supervised learning.\n\n\n\n\nCheck these websites\n\n\n\n\nMcNemar vs. Cohen's Kappa\n\n\n\n\n\n\nYates' Correction for Continuity\n\n\nYates' correction for continuity (or Yates' chi-squared test) is used in certain situations when testing for independence in a contingency table. It is a correction made to account for the fact that both Pearson\u2019s chi-square test and McNemar\u2019s chi-square test are biased upwards for a 2 x 2 contingency table. An upwards bias tends to make results larger than they should be. If you are creating a 2 x 2 contingency table that uses either of these two tests, the Yates correction is usually recommended\n\n\nCochran-Mantel-Haenszel Test\n\n\nIn statistics, the Cochran\u2013Mantel\u2013Haenszel test (CMH) is a test used in the \nanalysis of stratified or matched categorical data\n. It allows an investigator to test the association between a binary predictor or treatment and a binary outcome such as case or control status while taking into account the stratification. It is often used in observational studies where random assignment of subjects to different treatments cannot be controlled, but confounding covariates can be measured.\n\n\nThe null hypothesis is that there is no association between the treatment and the outcome. More precisely, the null hypothesis is $H_{0}:R=1$ and the alternative hypothesis is $H_{1}:R\\neq 1$. It follows a $\\chi^{2}$ distribution asymptotically under $H_{0}$.\n\n\n\n\nThe CMH test is a \ngeneralization of the McNemar test\n as their test statistics are identical when the strata are pairs. Unlike the McNemar test which can only handle pairs, the CMH test \nhandles arbitrary strata size\n.\n\n\nConditional logistic regression\n is more general than the CMH test as it can handle continuous variable and perform multivariate analysis. When the CMH test can be applied, the CMH test statistic and the score test statistic of the conditional logistic regression are identical.\n\n\nThe CMH test supposes that the effect of the treatment is homogeneous in all strata. The \nBreslow-Day for homogeneous association test\n allows to test this assumption. It should be noted that this is not a concern if the strata are small e.g. pairs.\n\n\n\n\n\n\nWarning\n\n\nThe Mantel-Haenszel statistic has 1 degree of freedom and assumes that either exposure or disease are \nmeasured on an ordinal (or interval) scales\n when you have \nmore than two levels\n.\n\n\n\n\n\n\nCheck these websites\n\n\n\n\nExample of CMH vs Fisher\n\n\n\n\n\n\nChi-Square test\n vs \nT-test\n\n\nCharacteristics\n\n\nA \nt-test\n can be either one-sided or two-sided.\n\n\nThe \nchi-square\n is The chi-squared test is essentially always a one-sided test. Here is a loose way to think about it: the chi-squared test is basically a \ngoodness of fit\n test. Sometimes it is explicitly referred to as such, but even when it's not, it is still often in essence a goodness of fit. \n\n\nWhen the realized chi-squared value is way out on the right tail of it's distribution, it indicates a \npoor fit\n, and if it is far enough, relative to some pre-specified threshold, we might conclude that it is so poor that we don't believe the data are from that reference distribution. If we were to use the \nchi-squared test as a two-sided test\n, we would also be worried if the statistic were too far into the left side of the chi-squared distribution. This would mean that we are worried the \nfit might be too good\n. This is simply not something we are typically worried about. As a historical side-note, this is related to the controversy of whether Mendel fudged his data. The idea was that his data were too good to be true. See \nhere\n for more info if you're curious.\n\n\nIn summary, the $\\chi^2$ is a two-sided test from which we are usually interested in only one of the tails of the distribution, indicating more disagreement, rather than less disagreement than one expects by chance.\n\n\nNull Hypothesis Tested\n\n\nA \nt-test\n tests a null hypothesis about two \nmeans\n; most often, it tests the hypothesis that two means are equal, or that the difference between them is zero. \n\n\nA \nchi-square\n test tests a null hypothesis about the \nrelationship between two variables\n (even with more than two levels). \n\n\nTypes of Data\n\n\nA \nt-test\n requires two variables; \none must be categorical and have exactly two levels, and the other must be quantitative and be estimable by a mean\n. \n\n\nA \nchi-square\n test requires \ncategorical variables\n, usually only two, but each may have \nany number of levels\n.\n\n\nRelationship Between These Tests\n\n\nYou can refer $z$ to the standard normal table to get one-sided or two-sided $P-$values. Equivalently, for the two-sided alternative $H_0:\\beta \\ne \\beta_0$, $z^2$ has a chi-squared distribution with $df = 1$. The $P-$value is then the right-tail chi-squared probability above the observed value. The \ntwo-tail probability\n beyond $\\pm z$ for the \nstandard normal distribution\n equals the \nright-tail probability\n above $z^2$ for the \nchi-squared distribution with $df = 1$\n. For example, the two-tail standard normal probability of $0.05$ that falls below $\u22121.96$ and above $1.96$ equals the right-tail chi-squared probability above $(1.96)^2 = 3.84$ when $df = 1$.\n\n\nThis relationship between normal and chi-square distributions can be extended to the relationship between \nt-test distribution and chi-square\n ones.\n\n\n\n\nCheck these websites\n\n\n\n\nThe difference between a t-test \n a chi-square\n\n\nIntroduction to Categorical Data Analysis, A. Agresti, 2007 (2nd ed.) page 11\n\n\nTest internal link Agresti 2007", 
            "title": "Miscellanea"
        }, 
        {
            "location": "/statistics/miscellanea/#one-sided-vs-two-sided-tests-how-do-their-p-values-compare", 
            "text": "The default among statistical packages performing tests is to report two-tailed p-values.  Because the most commonly used test statistic distributions (standard normal, Student's t) are symmetric about zero, most one-tailed p-values can be derived from the two-tailed p-values.  The null hypothesis is that the difference in means is zero.  The two-sided alternative is that the difference in means is not zero. There are two one-sided alternatives that one could opt to test instead: that the difference is positive ($diff   0$) or that the difference is negative ($diff   0$).   Note that the test statistic is the same for all of these tests. The two-tailed p-value is $P   |t|$. This can be rewritten as $P( x)+P( x)$.   Because the t-distribution is symmetric about zero, these two probabilities are equal: $P   |t| = 2 \\cdot  P( x)=1-(P x)$.   So, depending on the direction of the one-tailed hypothesis, its p-value is either $0.5 \\cdot$(two-tailed p-value) or $1-0.5 \\cdot$(two-tailed p-value) if the test statistic symmetrically distributed around zero.   In summary, to understand the connection between the results, you have to carefully review the $H_a$ for each case: the one-sided analysis (there is a difference with a certain sign) is more restrictive and demanding than the two-sided (there is a difference). Therefore, it follows that   the  one-tail p-value is half the two-tail p-value  and  the  two-tail p-value is twice the one-tail p-value  (assuming you correctly predicted the direction of the difference).    Check these websites   What are the differences between one-tailed and two-tailed tests?  One-tail vs two-tail p-values  One-sided and two-sided tests and p-values", 
            "title": "One-sided vs Two-sided Tests: How do their p-values compare?"
        }, 
        {
            "location": "/statistics/miscellanea/#mcnemars-test-vs-cohens-kappa-coefficient", 
            "text": "McNemar's test  is a statistical test used on paired nominal data. It is applied to $2 \\times 2$ contingency tables with a dichotomous trait, with matched pairs of subjects, to determine whether the row and column marginal frequencies are equal (that is, whether there is \"marginal homogeneity\").   The null hypothesis of marginal homogeneity states that the two marginal probabilities for each outcome are the same.  1\n2\n3\n4\n5\n6\n7 ODS EXCLUDE ALL;\nPROC FREQ DATA=SAS-data-set;\n    TABLE variable1 * variable2;\n    EXACT MCNEM;\n    ODS OUTPUT MCNEMARSTEST=mcnemarresults;\nRUN;\nODS EXCLUDE NONE;   Cohen's kappa coefficient  is a statistic which measures inter-rater agreement for qualitative (categorical) items. It is generally thought to be a more robust measure than simple percent agreement calculation, since $\\kappa$ takes into account the possibility of the agreement occurring by chance. There is controversy surrounding Cohen\u2019s Kappa due to the difficulty in interpreting indices of agreement. Some researchers have suggested that it is conceptually simpler to evaluate disagreement between items.  If the raters are in complete agreement then $\\kappa=1$. If there is no agreement among the raters other than what would be expected by chance (as given by pe), $\\kappa \\le 0$.  Note that Cohen's kappa measures agreement between  two raters only . The Fleiss kappa is a  multi-rater  generalization of Scott's pi statistic. Kappa is also used to compare performance in machine learning but the directional version known as Informedness or Youden's J statistic is argued to be more appropriate for supervised learning.   Check these websites   McNemar vs. Cohen's Kappa", 
            "title": "McNemar's Test vs Cohen's Kappa Coefficient"
        }, 
        {
            "location": "/statistics/miscellanea/#yates-correction-for-continuity", 
            "text": "Yates' correction for continuity (or Yates' chi-squared test) is used in certain situations when testing for independence in a contingency table. It is a correction made to account for the fact that both Pearson\u2019s chi-square test and McNemar\u2019s chi-square test are biased upwards for a 2 x 2 contingency table. An upwards bias tends to make results larger than they should be. If you are creating a 2 x 2 contingency table that uses either of these two tests, the Yates correction is usually recommended", 
            "title": "Yates' Correction for Continuity"
        }, 
        {
            "location": "/statistics/miscellanea/#cochran-mantel-haenszel-test", 
            "text": "In statistics, the Cochran\u2013Mantel\u2013Haenszel test (CMH) is a test used in the  analysis of stratified or matched categorical data . It allows an investigator to test the association between a binary predictor or treatment and a binary outcome such as case or control status while taking into account the stratification. It is often used in observational studies where random assignment of subjects to different treatments cannot be controlled, but confounding covariates can be measured.  The null hypothesis is that there is no association between the treatment and the outcome. More precisely, the null hypothesis is $H_{0}:R=1$ and the alternative hypothesis is $H_{1}:R\\neq 1$. It follows a $\\chi^{2}$ distribution asymptotically under $H_{0}$.   The CMH test is a  generalization of the McNemar test  as their test statistics are identical when the strata are pairs. Unlike the McNemar test which can only handle pairs, the CMH test  handles arbitrary strata size .  Conditional logistic regression  is more general than the CMH test as it can handle continuous variable and perform multivariate analysis. When the CMH test can be applied, the CMH test statistic and the score test statistic of the conditional logistic regression are identical.  The CMH test supposes that the effect of the treatment is homogeneous in all strata. The  Breslow-Day for homogeneous association test  allows to test this assumption. It should be noted that this is not a concern if the strata are small e.g. pairs.    Warning  The Mantel-Haenszel statistic has 1 degree of freedom and assumes that either exposure or disease are  measured on an ordinal (or interval) scales  when you have  more than two levels .    Check these websites   Example of CMH vs Fisher", 
            "title": "Cochran-Mantel-Haenszel Test"
        }, 
        {
            "location": "/statistics/miscellanea/#chi-square-test-vs-t-test", 
            "text": "", 
            "title": "Chi-Square test vs T-test"
        }, 
        {
            "location": "/statistics/miscellanea/#characteristics", 
            "text": "A  t-test  can be either one-sided or two-sided.  The  chi-square  is The chi-squared test is essentially always a one-sided test. Here is a loose way to think about it: the chi-squared test is basically a  goodness of fit  test. Sometimes it is explicitly referred to as such, but even when it's not, it is still often in essence a goodness of fit.   When the realized chi-squared value is way out on the right tail of it's distribution, it indicates a  poor fit , and if it is far enough, relative to some pre-specified threshold, we might conclude that it is so poor that we don't believe the data are from that reference distribution. If we were to use the  chi-squared test as a two-sided test , we would also be worried if the statistic were too far into the left side of the chi-squared distribution. This would mean that we are worried the  fit might be too good . This is simply not something we are typically worried about. As a historical side-note, this is related to the controversy of whether Mendel fudged his data. The idea was that his data were too good to be true. See  here  for more info if you're curious.  In summary, the $\\chi^2$ is a two-sided test from which we are usually interested in only one of the tails of the distribution, indicating more disagreement, rather than less disagreement than one expects by chance.", 
            "title": "Characteristics"
        }, 
        {
            "location": "/statistics/miscellanea/#null-hypothesis-tested", 
            "text": "A  t-test  tests a null hypothesis about two  means ; most often, it tests the hypothesis that two means are equal, or that the difference between them is zero.   A  chi-square  test tests a null hypothesis about the  relationship between two variables  (even with more than two levels).", 
            "title": "Null Hypothesis Tested"
        }, 
        {
            "location": "/statistics/miscellanea/#types-of-data", 
            "text": "A  t-test  requires two variables;  one must be categorical and have exactly two levels, and the other must be quantitative and be estimable by a mean .   A  chi-square  test requires  categorical variables , usually only two, but each may have  any number of levels .", 
            "title": "Types of Data"
        }, 
        {
            "location": "/statistics/miscellanea/#relationship-between-these-tests", 
            "text": "You can refer $z$ to the standard normal table to get one-sided or two-sided $P-$values. Equivalently, for the two-sided alternative $H_0:\\beta \\ne \\beta_0$, $z^2$ has a chi-squared distribution with $df = 1$. The $P-$value is then the right-tail chi-squared probability above the observed value. The  two-tail probability  beyond $\\pm z$ for the  standard normal distribution  equals the  right-tail probability  above $z^2$ for the  chi-squared distribution with $df = 1$ . For example, the two-tail standard normal probability of $0.05$ that falls below $\u22121.96$ and above $1.96$ equals the right-tail chi-squared probability above $(1.96)^2 = 3.84$ when $df = 1$.  This relationship between normal and chi-square distributions can be extended to the relationship between  t-test distribution and chi-square  ones.   Check these websites   The difference between a t-test   a chi-square  Introduction to Categorical Data Analysis, A. Agresti, 2007 (2nd ed.) page 11  Test internal link Agresti 2007", 
            "title": "Relationship Between These Tests"
        }, 
        {
            "location": "/other-analysis/model-selection/", 
            "text": "Check these websites\n\n\n\n\nThe GLMSELECT Procedure\n\n\nIntroducing the GLMSELECT Procedure for Model Selection\n\n\nPenalized Regression Methods for Linear Models in SAS\n\n\n\n\n\n\nTraditional Model Selection Algoritms\n\n\nThe \nGLMSELECT\n procedure extends the familiar forward, backward, and stepwise methods as implemented in the \nREG\n procedure to GLM-type models. Quite simply, \nFORWARD\n selection adds parameters one at a time, \nBACKWARD\n elimination deletes them, and \nSTEPWISE\n selection switches between adding and deleting them.\n\n\nForward Method (\nFORWARD\n)\n\n\nIt is important to keep in mind that forward selection bases the decision about what effect to add at any step by considering models that \ndiffer by one effect from the current model\n. This search paradigm cannot guarantee reaching a \"best\" subset model. Furthermore, \nthe add decision is greedy\n in the sense that the effect deemed most significant is the effect that is added. However, if your goal is to find a model that is best in terms of some selection criterion other than the significance level of the entering effect, then even this one step choice might not be optimal. For example, the effect you would add to get a model with the smallest value of the \nPRESS\n statistic at the next step is not necessarily the same effect that has the most significant entry \nF\n statistic. \n\n\nNote that in the case where all effects are variables (that is, effects with one degree of freedom and no hierarchy), using \nADJRSQ\n, \nAIC\n, \nAICC\n, \nBIC\n, \nCP\n, \nRSQUARE\n, or \nSBC\n as the selection criterion for forward selection produces the same sequence of additions. However, if the degrees of freedom contributed by different effects are not constant, or if an out-of-sample prediction-based criterion is used, then different sequences of additions might be obtained.\n\n\nBackward Method (\nBACKWARD\n)\n\n\nThe backward elimination technique starts from the full model including all independent effects. Then effects are deleted one by one until a stopping condition is satisfied. At each step, the effect showing the smallest contribution to the model is deleted. In traditional implementations of backward elimination, the contribution of an effect to the model is assessed by using an \nF\n statistic. At any step, the predictor producing the least significant \nF\n statistic is dropped and the process continues until all effects remaining in the model have \nF\n statistics significant at a stay significance level (\nSLS\n).\n\n\nStepwise Method (\nSTEPWISE\n)\n\n\nIn the traditional implementation of stepwise selection method, the same entry and removal F statistics for the forward selection and backward elimination methods are used to assess contributions of effects as they are added to or removed from a model. If at a step of the stepwise method, any effect in the model is not significant at the SLSTAY= level, then the least significant of these effects is removed from the model and the algorithm proceeds to the next step. This ensures that no effect can be added to a model while some effect currently in the model is not deemed significant. Only after all necessary deletions have been accomplished can another effect be added to the model.\n\n\nFor selection criteria other than significance level, \nPROC GLMSELECT\n optionally supports a further modification in the stepwise method. In the standard stepwise method, no effect can enter the model if removing any effect currently in the model would yield an improved value of the selection criterion. In the modification, you can use the \nDROP=COMPETITIVE\n option to specify that addition and deletion of effects should be treated competitively. The selection criterion is evaluated for all models obtained by deleting an effect from the current model or by adding an effect to this model. The action that most improves the selection criterion is the action taken.\n\n\nCode Examples\n\n\n1\nselection=forward\n\n\n\n\n\n\nadds effects that at each step give the lowest value of the SBC statistic and stops at the step where adding any\neffect would increase the SBC statistic.\n\n\n1\nselection=forward(select=SL)\n\n\n\n\n\n\nadds effects based on significance level and stops when all candidate effects for entry at a step have a\nsignificance level greater than the default entry significance level of 0.50.\n\n\n1\nselection=forward(select=ADJRSQ stop=SL SLE=0.2)\n\n\n\n\n\n\nadds effects that at each step give the largest value of the adjusted R-square statistic and stops at the step\nwhere the significance level corresponding to the addition of this effect is greater than 0.2.\n\n\n1\nselection=forward(select=SL stop=AIC)\n\n\n\n\n\n\nterminates at the step where the effect to be added at the next step would produce a model with an \nAIC\n statistic larger than the \nAIC\n statistic of the current model.\n\n\nProvided that the entry significance level is large enough that the local extremum of the named criterion occurs before the final step, specifying any of these options the same model is selected, but more steps are done in the second case:\n\n\n1\n2\nselection=forward(select=SL choose=CRITERION)\nselection=forward(select=SL stop=CRITERION)\n\n\n\n\n\n\nIn some cases there might be a better local extremum that cannot be reached if you specify the \nSTOP=\n option but can be found if you use the \nCHOOSE=\n option. Also, you can use the \nCHOOSE=\n option in preference to the \nSTOP=\n option if you want examine how the named criterion behaves as you move beyond the step where the first local minimum of this criterion occurs.\n\n\nNote that you can specify both the \nCHOOSE=\n and \nSTOP=\n options. You might want to consider models generated by forward selection that have at most some fixed number of effects but select from within this set based on a criterion you specify. The following example requests that forward selection continue until there are 20 effects in the final model and chooses among the sequence of models the one that has the largest value of the adjusted R-square statistic. \n\n\n1\nselection=forward(stop=20 choose=ADJRSQ)\n\n\n\n\n\n\nYou can also combine these options to select a model where one of two conditions is met. The following example chooses whatever occurs first between a local minimum of the predicted residual sum of squares (\nPRESS\n) and a local minimum of corrected Akaike\u2019s information criterion (\nAICC\n).\n\n\n1\nselection=forward(stop=AICC choose=PRESS)\n\n\n\n\n\n\nPROC GLMSELECT\n enables you to specify the criterion to optimize at each step by using the \nSELECT=\n option. For example, the following example requests that at each step the effect that is added be the one that gives a model with the smallest value of the  Mallows\u2019 $C_p$ statistic.\n\n\n1\nselection=forward(select=CP)\n\n\n\n\n\n\nYou can use \nSELECT=\n together with \nCHOOSE=\n and \nSTOP=\n. If you specify only the \nSELECT=\n criterion, then this criterion is also used as the stopping criterion. In the previous example where only the selection criterion is specified, not only do effects enter based on the Mallows\u2019 $C_p$ statistic, but the selection terminates when the $C_p$ statistic first increases.\n\n\n1\nselection=backward\n\n\n\n\n\n\nremoves effects that at each step produce the largest value of the Schwarz Bayesian information criterion (\nSBC\n) statistic and stops at the step where removing any effect increases the \nSBC\n statistic.\n\n\n1\nselection=backward(stop=press)\n\n\n\n\n\n\nremoves effects based on the \nSBC\n statistic and stops at the step where removing any effect increases the predicted residual sum of squares (\nPRESS\n).\n\n\n1\nselection=backward(select=SL)\n\n\n\n\n\n\nremoves effects based on significance level and stops when all candidate effects for removal at a step have a significance level less than the default stay significance level of 0.10.\n\n\n1\nselection=backward(select=SL choose=validate SLS=0.1)\n\n\n\n\n\n\nremoves effects based on significance level and stops when all effects in the model are significant at the 0.1 level. Finally, from the sequence of models generated, choose the one that gives the smallest average square error when scored on the validation data.\n\n\nPenalized Regression Methods for Linear Models\n\n\nLeast Absolute Selection Shrinkage Operator (\nLASSO\n)\n\n\nLasso regression is what is called the Penalized regression method, often used in machine learning to select the subset of variables. It is a supervised machine learning method. Specifically, \nLASSO\n is a \nShrinkage and Variable Selection method for linear regression models\n. \n\n\nThe \nLASSO\n algorithm \nimposes a constraint on the sum of the absolute values of the model parameters\n, where the sum has a specified constant as an upper bound. This constraint \ncauses regression coefficients for some variables to shrink towards zero\n allowing for a better interpretation of the model and to identifiying the variables most strongly associated with the response variable by obtaining the subset of predictors that minimizes prediction error. \n\n\nSo why use Lasso instead of just using ordinary least squares (OLS) multiple regression? \n\n\n\n\nIt can provide \ngreater prediction accuracy\n. If the true relationship between the response variable and the predictors is approximately linear and you have a large number of observations, then OLS regression parameter estimates will have low bias and low variance. However, if you have \na relatively small number of observations and a large number of predictors, then the variance of the OLS parameter estimates will be higher\n. In this case, \nLASSO\n regression is useful because shrinking the regression coefficients can reduce variance without a substantial increase in bias. \n\n\nLASSO\n regression can \nincrease model interpretability\n. Often times, at least \nsome of the explanatory variables in an OLS multiple regression analysis are not really associated with the response variable\n resulting in overfitted models which are more difficult to interpret. With Lasso Regression, the \nregression coefficients for unimportant variables are reduced to zero\n which effectively removes them from the model and produces a simpler one. \n\n\n\n\nIn Lasso Regression, a tuning parameter, $\\lambda$, is included in the model to control the strength of the penalty. As $\\lambda$ increases, more coefficients are reduced to zero, that is fewer predictors are selected and there is more shrinkage of the non-zero coefficient. With Lasso Regression when $\\lambda=0$ we have an OLS regression analysis. Bias increases and variance decreases as $\\lambda$ increases. \n\n\nAlthough \nLASSO\n regression models can handle categorical variables with more than two levels, you can also create a serie of auxiliary binary categorical varaibles in order to improve the interpretability of the selected model. Binary substitutes variables for measure with individual questions. \n\n\nLeast Angle Regression (\nLAR\n)\n\n\nThe \nLAR\n algorithm starts with no predictors in the model and adds a predictor at each step. It first adds a predictor that is most correlated with the response variable and moves it towards least square estimate, until there is another predictor that is equally correlated with the model residual. It adds this predictor to the model and starts the least square estimation process over again, with both variables. The \nLAR\n algorithm continues with this process until it has tested all the predictors. Parameter estimates at any step are shrunk and predictors with coefficients that are shrunk to zero are removed from the model so the process starts all over again. \n\n\nCode Examples\n\n\n1\n2\n3\n4\n5\n* LASSO multiple regression with LARS algorithm k=10 fold validation;\nPROG GLMSELECT DATA=SAS-data-set PLOTS=ALL SEED=12345;\n  PARTITION ROLE=selected(train=\n1\n test=\n0\n);\n  MODEL response=predictor1 predictor2 ... predictorN / SELECTION=LAR(CHOOSE=CV STOP=NONE) CVMETHOD=RANDOM(10);\nRUN;\n\n\n\n\n\n\n\n\n\n\nThe \nPARTITION\n statement assigns each observation a role, based on the variable called selected, to indicate whether the observation is a training or test observation. \n\n\n\n\n\n\nAfter the slash of the \nMODEL\n statement, we specify the options we want to use to test the model. \n\n\n\n\nThe \nSELECTION\n option tells us which method to use to compute the parameters for variable selection (\nLAR\n algorithm in this example). \n\n\nThe \nCHOOSE=CV\n option, asks SAS to use cross validation to choose the final statistical model. \n\n\nSTOP=NONE\n ensures that the model doesn't stop running until each of the candidate predictor variables is tested. \n\n\nCVMETHOD=RANDOM(10)\n, specifies that a K-fold cross-validation method with ten randomly selected folds will be used. \n\n\n\n\n\n\n\n\nThe model with the lowest average means square error is selected by SAS as the best model. \n\n\n\n\nTip\n\n\nIn \nLASSO\n regression, the penalty term is not fair if the predictive variables are not on the same scale. Meaning that not all the predictors will get the same penalty. The SAS \nGLMSELECT\n procedure handles this by automatically standardizing the predictor variables, so that they all have a mean equal to zero and a standard deviation equal to one, which places them all on the same scale. \n\n\n\n\nLASSO\n Regression Limitations\n\n\nAs with any statistical methods, the \nLASSO\n regression has some limitations. \n\n\n\n\nSelection of variables is 100% statistically driven. The \nLASSO\n selection process does not think like a human being, who take into account theory and other factors in deciding which predictors to include. There might be a good rational for including a predictor, even if it appears to have no association with response variable. \n\n\nIf \npredictors are strongly correlated with each other, the Lasso will arbitrarily select one of them\n. You may have different ideas about which of the predictors you would choose include or whether it's important to keep more than one. \n\n\nEstimating \np-values for \nLASSO\n regression is not so straightforward\n, although methods to calculate p-values have been developed. \n\n\nDifferent selection methods and even different software packages can produce different results. \n\n\nThere's no guarantee that the model selected by the \nLASSO\n regression will not be overfitted or the best model. \n\n\n\n\nIf you find yourself in a position which you have a large number of potential predictors of a response variable, what do you do? The best solution may be \na combination of machine learning, human intervention, and independent application\n. \n\n\nModel Selection Algorithms Pros and Cons\n\n\nNote that while the model selection question seems reasonable, trying to answer it for real data can lead to problematic pitfalls, including\n\n\n\n\nThe selected model is not guaranteed to be the \"best\"; there may be other, more parsimonious or more intuitively reasonable models that may provide nearly as good or even better models, but which the particular heuristic method employed does not find\n\n\nModel selection may be unduly affected by outliers\n\n\nThere is a \"selection bias\" because a parameter is more likely to be selected if it is above its expected value than if it is below its expected value\n\n\nStandard methods of inference for the final model are invalid in the model selection context\n\n\n\n\nHowever, certain features of \nGLMSELECT\n, in particular the procedure\u2019s \nextensive capabilities for customizing the selection and its flexibility and power in specifying complex potential effects\n, can partially mitigate these problems.", 
            "title": "Model Selection"
        }, 
        {
            "location": "/other-analysis/model-selection/#traditional-model-selection-algoritms", 
            "text": "The  GLMSELECT  procedure extends the familiar forward, backward, and stepwise methods as implemented in the  REG  procedure to GLM-type models. Quite simply,  FORWARD  selection adds parameters one at a time,  BACKWARD  elimination deletes them, and  STEPWISE  selection switches between adding and deleting them.", 
            "title": "Traditional Model Selection Algoritms"
        }, 
        {
            "location": "/other-analysis/model-selection/#forward-method-forward", 
            "text": "It is important to keep in mind that forward selection bases the decision about what effect to add at any step by considering models that  differ by one effect from the current model . This search paradigm cannot guarantee reaching a \"best\" subset model. Furthermore,  the add decision is greedy  in the sense that the effect deemed most significant is the effect that is added. However, if your goal is to find a model that is best in terms of some selection criterion other than the significance level of the entering effect, then even this one step choice might not be optimal. For example, the effect you would add to get a model with the smallest value of the  PRESS  statistic at the next step is not necessarily the same effect that has the most significant entry  F  statistic.   Note that in the case where all effects are variables (that is, effects with one degree of freedom and no hierarchy), using  ADJRSQ ,  AIC ,  AICC ,  BIC ,  CP ,  RSQUARE , or  SBC  as the selection criterion for forward selection produces the same sequence of additions. However, if the degrees of freedom contributed by different effects are not constant, or if an out-of-sample prediction-based criterion is used, then different sequences of additions might be obtained.", 
            "title": "Forward Method (FORWARD)"
        }, 
        {
            "location": "/other-analysis/model-selection/#backward-method-backward", 
            "text": "The backward elimination technique starts from the full model including all independent effects. Then effects are deleted one by one until a stopping condition is satisfied. At each step, the effect showing the smallest contribution to the model is deleted. In traditional implementations of backward elimination, the contribution of an effect to the model is assessed by using an  F  statistic. At any step, the predictor producing the least significant  F  statistic is dropped and the process continues until all effects remaining in the model have  F  statistics significant at a stay significance level ( SLS ).", 
            "title": "Backward Method (BACKWARD)"
        }, 
        {
            "location": "/other-analysis/model-selection/#stepwise-method-stepwise", 
            "text": "In the traditional implementation of stepwise selection method, the same entry and removal F statistics for the forward selection and backward elimination methods are used to assess contributions of effects as they are added to or removed from a model. If at a step of the stepwise method, any effect in the model is not significant at the SLSTAY= level, then the least significant of these effects is removed from the model and the algorithm proceeds to the next step. This ensures that no effect can be added to a model while some effect currently in the model is not deemed significant. Only after all necessary deletions have been accomplished can another effect be added to the model.  For selection criteria other than significance level,  PROC GLMSELECT  optionally supports a further modification in the stepwise method. In the standard stepwise method, no effect can enter the model if removing any effect currently in the model would yield an improved value of the selection criterion. In the modification, you can use the  DROP=COMPETITIVE  option to specify that addition and deletion of effects should be treated competitively. The selection criterion is evaluated for all models obtained by deleting an effect from the current model or by adding an effect to this model. The action that most improves the selection criterion is the action taken.", 
            "title": "Stepwise Method (STEPWISE)"
        }, 
        {
            "location": "/other-analysis/model-selection/#code-examples", 
            "text": "1 selection=forward   adds effects that at each step give the lowest value of the SBC statistic and stops at the step where adding any\neffect would increase the SBC statistic.  1 selection=forward(select=SL)   adds effects based on significance level and stops when all candidate effects for entry at a step have a\nsignificance level greater than the default entry significance level of 0.50.  1 selection=forward(select=ADJRSQ stop=SL SLE=0.2)   adds effects that at each step give the largest value of the adjusted R-square statistic and stops at the step\nwhere the significance level corresponding to the addition of this effect is greater than 0.2.  1 selection=forward(select=SL stop=AIC)   terminates at the step where the effect to be added at the next step would produce a model with an  AIC  statistic larger than the  AIC  statistic of the current model.  Provided that the entry significance level is large enough that the local extremum of the named criterion occurs before the final step, specifying any of these options the same model is selected, but more steps are done in the second case:  1\n2 selection=forward(select=SL choose=CRITERION)\nselection=forward(select=SL stop=CRITERION)   In some cases there might be a better local extremum that cannot be reached if you specify the  STOP=  option but can be found if you use the  CHOOSE=  option. Also, you can use the  CHOOSE=  option in preference to the  STOP=  option if you want examine how the named criterion behaves as you move beyond the step where the first local minimum of this criterion occurs.  Note that you can specify both the  CHOOSE=  and  STOP=  options. You might want to consider models generated by forward selection that have at most some fixed number of effects but select from within this set based on a criterion you specify. The following example requests that forward selection continue until there are 20 effects in the final model and chooses among the sequence of models the one that has the largest value of the adjusted R-square statistic.   1 selection=forward(stop=20 choose=ADJRSQ)   You can also combine these options to select a model where one of two conditions is met. The following example chooses whatever occurs first between a local minimum of the predicted residual sum of squares ( PRESS ) and a local minimum of corrected Akaike\u2019s information criterion ( AICC ).  1 selection=forward(stop=AICC choose=PRESS)   PROC GLMSELECT  enables you to specify the criterion to optimize at each step by using the  SELECT=  option. For example, the following example requests that at each step the effect that is added be the one that gives a model with the smallest value of the  Mallows\u2019 $C_p$ statistic.  1 selection=forward(select=CP)   You can use  SELECT=  together with  CHOOSE=  and  STOP= . If you specify only the  SELECT=  criterion, then this criterion is also used as the stopping criterion. In the previous example where only the selection criterion is specified, not only do effects enter based on the Mallows\u2019 $C_p$ statistic, but the selection terminates when the $C_p$ statistic first increases.  1 selection=backward   removes effects that at each step produce the largest value of the Schwarz Bayesian information criterion ( SBC ) statistic and stops at the step where removing any effect increases the  SBC  statistic.  1 selection=backward(stop=press)   removes effects based on the  SBC  statistic and stops at the step where removing any effect increases the predicted residual sum of squares ( PRESS ).  1 selection=backward(select=SL)   removes effects based on significance level and stops when all candidate effects for removal at a step have a significance level less than the default stay significance level of 0.10.  1 selection=backward(select=SL choose=validate SLS=0.1)   removes effects based on significance level and stops when all effects in the model are significant at the 0.1 level. Finally, from the sequence of models generated, choose the one that gives the smallest average square error when scored on the validation data.", 
            "title": "Code Examples"
        }, 
        {
            "location": "/other-analysis/model-selection/#penalized-regression-methods-for-linear-models", 
            "text": "", 
            "title": "Penalized Regression Methods for Linear Models"
        }, 
        {
            "location": "/other-analysis/model-selection/#least-absolute-selection-shrinkage-operator-lasso", 
            "text": "Lasso regression is what is called the Penalized regression method, often used in machine learning to select the subset of variables. It is a supervised machine learning method. Specifically,  LASSO  is a  Shrinkage and Variable Selection method for linear regression models .   The  LASSO  algorithm  imposes a constraint on the sum of the absolute values of the model parameters , where the sum has a specified constant as an upper bound. This constraint  causes regression coefficients for some variables to shrink towards zero  allowing for a better interpretation of the model and to identifiying the variables most strongly associated with the response variable by obtaining the subset of predictors that minimizes prediction error.   So why use Lasso instead of just using ordinary least squares (OLS) multiple regression?    It can provide  greater prediction accuracy . If the true relationship between the response variable and the predictors is approximately linear and you have a large number of observations, then OLS regression parameter estimates will have low bias and low variance. However, if you have  a relatively small number of observations and a large number of predictors, then the variance of the OLS parameter estimates will be higher . In this case,  LASSO  regression is useful because shrinking the regression coefficients can reduce variance without a substantial increase in bias.   LASSO  regression can  increase model interpretability . Often times, at least  some of the explanatory variables in an OLS multiple regression analysis are not really associated with the response variable  resulting in overfitted models which are more difficult to interpret. With Lasso Regression, the  regression coefficients for unimportant variables are reduced to zero  which effectively removes them from the model and produces a simpler one.    In Lasso Regression, a tuning parameter, $\\lambda$, is included in the model to control the strength of the penalty. As $\\lambda$ increases, more coefficients are reduced to zero, that is fewer predictors are selected and there is more shrinkage of the non-zero coefficient. With Lasso Regression when $\\lambda=0$ we have an OLS regression analysis. Bias increases and variance decreases as $\\lambda$ increases.   Although  LASSO  regression models can handle categorical variables with more than two levels, you can also create a serie of auxiliary binary categorical varaibles in order to improve the interpretability of the selected model. Binary substitutes variables for measure with individual questions.", 
            "title": "Least Absolute Selection Shrinkage Operator (LASSO)"
        }, 
        {
            "location": "/other-analysis/model-selection/#least-angle-regression-lar", 
            "text": "The  LAR  algorithm starts with no predictors in the model and adds a predictor at each step. It first adds a predictor that is most correlated with the response variable and moves it towards least square estimate, until there is another predictor that is equally correlated with the model residual. It adds this predictor to the model and starts the least square estimation process over again, with both variables. The  LAR  algorithm continues with this process until it has tested all the predictors. Parameter estimates at any step are shrunk and predictors with coefficients that are shrunk to zero are removed from the model so the process starts all over again.", 
            "title": "Least Angle Regression (LAR)"
        }, 
        {
            "location": "/other-analysis/model-selection/#code-examples_1", 
            "text": "1\n2\n3\n4\n5 * LASSO multiple regression with LARS algorithm k=10 fold validation;\nPROG GLMSELECT DATA=SAS-data-set PLOTS=ALL SEED=12345;\n  PARTITION ROLE=selected(train= 1  test= 0 );\n  MODEL response=predictor1 predictor2 ... predictorN / SELECTION=LAR(CHOOSE=CV STOP=NONE) CVMETHOD=RANDOM(10);\nRUN;     The  PARTITION  statement assigns each observation a role, based on the variable called selected, to indicate whether the observation is a training or test observation.     After the slash of the  MODEL  statement, we specify the options we want to use to test the model.    The  SELECTION  option tells us which method to use to compute the parameters for variable selection ( LAR  algorithm in this example).   The  CHOOSE=CV  option, asks SAS to use cross validation to choose the final statistical model.   STOP=NONE  ensures that the model doesn't stop running until each of the candidate predictor variables is tested.   CVMETHOD=RANDOM(10) , specifies that a K-fold cross-validation method with ten randomly selected folds will be used.      The model with the lowest average means square error is selected by SAS as the best model.    Tip  In  LASSO  regression, the penalty term is not fair if the predictive variables are not on the same scale. Meaning that not all the predictors will get the same penalty. The SAS  GLMSELECT  procedure handles this by automatically standardizing the predictor variables, so that they all have a mean equal to zero and a standard deviation equal to one, which places them all on the same scale.", 
            "title": "Code Examples"
        }, 
        {
            "location": "/other-analysis/model-selection/#lasso-regression-limitations", 
            "text": "As with any statistical methods, the  LASSO  regression has some limitations.    Selection of variables is 100% statistically driven. The  LASSO  selection process does not think like a human being, who take into account theory and other factors in deciding which predictors to include. There might be a good rational for including a predictor, even if it appears to have no association with response variable.   If  predictors are strongly correlated with each other, the Lasso will arbitrarily select one of them . You may have different ideas about which of the predictors you would choose include or whether it's important to keep more than one.   Estimating  p-values for  LASSO  regression is not so straightforward , although methods to calculate p-values have been developed.   Different selection methods and even different software packages can produce different results.   There's no guarantee that the model selected by the  LASSO  regression will not be overfitted or the best model.    If you find yourself in a position which you have a large number of potential predictors of a response variable, what do you do? The best solution may be  a combination of machine learning, human intervention, and independent application .", 
            "title": "LASSO Regression Limitations"
        }, 
        {
            "location": "/other-analysis/model-selection/#model-selection-algorithms-pros-and-cons", 
            "text": "Note that while the model selection question seems reasonable, trying to answer it for real data can lead to problematic pitfalls, including   The selected model is not guaranteed to be the \"best\"; there may be other, more parsimonious or more intuitively reasonable models that may provide nearly as good or even better models, but which the particular heuristic method employed does not find  Model selection may be unduly affected by outliers  There is a \"selection bias\" because a parameter is more likely to be selected if it is above its expected value than if it is below its expected value  Standard methods of inference for the final model are invalid in the model selection context   However, certain features of  GLMSELECT , in particular the procedure\u2019s  extensive capabilities for customizing the selection and its flexibility and power in specifying complex potential effects , can partially mitigate these problems.", 
            "title": "Model Selection Algorithms Pros and Cons"
        }, 
        {
            "location": "/other-analysis/missing-data/", 
            "text": "Check these websites\n\n\n\n\nCheck out \nthis review\n on techniques for treating missing data.\n\n\nA Conceptual Strategy and Macro Approach for Partial Date Handling in Data De-Identification\n\n\n\n\n\n\n\n\nDid you know?\n\n\nMissing values can be \"formatted\". \n.A\n thru \n.Z\n are all valid missing values just as \n.\n.\n\n\n\n\nMissing Data Mechanisms and Patterns\n\n\nTo use the more appropriate method to deal with your missing data, you should consider the missing data mechanism of your data which describes the process that is believed to have generated the missing values. According to \nRubin (1976)\n, there are three mechanisms under which missing data can occur:\n\n\n\n\nMissing completely at random (MCAR)\n:  neither the variables in the dataset nor the unobserved value of the variable itself predict whether a value will be missing\n\n\nMissing at random (MAR)\n: other variables (but not the variable itself) in the dataset can be used to predict missingness on a given variable\n\n\nMissing not at random (MNAR)\n: value of the unobserved variable itself predicts missingness\n\n\n\n\n\n\nCheck these websites\n\n\nCheck out the formal description of each missing mechanism in the \"Missing data mechanisms\" section of \nthis\n paper.\n\n\n\n\nObjetives of Imputation\n\n\nDepending on the \ntype of data and model\n you will be using, techniques such as \nmultiple imputation\n or \ndirect maximum likelihood\n may better serve your needs. The main goals of statistical analysis with missing data are:\n\n\n\n\nMinimize bias\n\n\nMaximize use of available information\n\n\nObtain appropriate estimates of uncertainty\n\n\n\n\nImputed values are \nnot\n equivalent to observed values and serve only to help estimate the covariances between variables needed for inference.\n\n\n\n\nAutomated imputations generally fall into one of six categories: \n\n\n\n\nDeterministic imputation \n\n\nModel based imputation \n\n\nDeck imputation \n\n\nMixed imputation \n\n\nExpert Systems \n\n\nNeural networks \n\n\n\n\nDeletion procedures\n\n\n\n\nComplete case analysis (listwise deletion)\n:  deleting cases in a particular dataset that are missing data on any variable of interest (for MCAR cases the power is reduced but it does not add any bias). It is a common technique because it is easy to implement and works with any type of analysis.\n\n\nAvailable case analysis (pairwise deletion)\n:  deleting cases where a variable required for a particular analysis is missing, but including those cases in analyses for which all required variables are present. One of the main drawbacks of this method is no consistent sample size because depending on the pairwise comparison examined, the sample size will change based on the amount of missing present in one or both variables. This method became popular because the loss of power due to missing information is not as substantial as with complete case analysis. Unless the mechanism of missing data is MCAR, this method will introduce bias into the parameter estimates. Therefore, this method is not recommended.\n\n\n\n\nReplacement procedures\n\n\nData replacement does not compensate for a badly designed instrument or for poor data collection. Overall, replacement procedures can be used in certain cases, as long as the researcher has a good reason for replacing.\n\n\nThe most important advantages of these procedures are the retention of the sample size (statistical power). To a greater or lesser extent, all replacement procedures are biased if there is a non-random distribution of missing values. In assessing the effectiveness of these procedures, both the accuracy of estimating the value of missing data and the accuracy of estimating the statistical effects have to be considered.\n\n\nMany different missing data replacement procedures have been developed over the years. In general, the differences between the various methods decrease with: (a) larger sample size, (b) a smaller percentage of missing values, (c) fewer missing variables and (d) a decrease in the level of the correlations between the variables. \n\n\nSingle Imputation Methods\n\n\nSingle imputation denotes that the missing value is replaced by a value. However, the imputed values are assumed to be the real values that would have been observed when the data would have been complete. When we have missing data, this is never the case. We can \nnever be completely certain about imputed values\n. \n\n\n\n\nUnconditional Mean Imputation / Mean Substitution\n: replacing the missing values for an individual variable wih it's overall estimated mean from the available cases. Its more important problem is that it will result in an artificial reduction in variability due to the fact you are imputing values at the center of the variable's distribution. This also has the unintended consequence of changing the magnitude of correlations between the imputed variable and other variables.\n\n\nRegression Imputation\n: This is a two-step approach: first, the relationships among variables are estimated, and then the regression coefficients are used to estimate the missing value. The underlying assumption of regression imputation is the existence of a linear relationship between the predictors and the missing variable. The technique also assumes that values are missing at random (i.e., a missing value is not related to the value of the predictors).\n\n\nStochastic Regression Imputation\n: In recognition of the problems with regression imputation and the reduced variability associated with this approach, researchers developed a technique to incorporate or \u201cadd back\u201d lost variability. A residual term, that is randomly drawn from a normal distribution with mean zero and variance equal to the residual variance from the regression model, is added to the predicted scores from the regression imputation thus restoring some of the lost variability. This method is superior to the previous methods as it will produce unbiased coefficient estimates under MAR. However, the standard errors produced during regression estimation while less biased then the single imputation approach, will still be attenuated.\n\n\n\n\n\n\n\n\n\n\nThe deterministic imputations are exactly at the regression predictions and ignore predictive uncertainty. In contrast, the random imputations are more variable and better capture the range of the data.\n\n\n\n\nHot-deck Imputation\n: According to this technique, the researcher should replace a missing value with the actual score from a similar case in the dataset. One form of hot-deck imputation is called \"last observation carried forward\" (LOCF), which involves sorting a dataset according to any of a number of variables, thus creating an ordered dataset. The technique then finds the first missing value and uses the cell value immediately prior to the data that are missing to impute the missing value. This method is known to increase risk of increasing bias and potentially false conclusions. For this reason LOCF is not recommended for use.\n\n\n\n\nCold-deck Imputation\n: This method replaces a missing value of an item with a constant value from an external source such as a value from a previous survey.\n\n\n\n\n\n\nSingle Imputation\n:\n\n\n\n\n\n\nMultiple imputation\n\n\n\n\nCheck these websites\n\n\nVisit \nthis website\n for more information.\n\n\n\n\nBayesian Methods for Completing Data\n are simply methods based on conditional probability.\n\n\nMultiple Imputation is always superior to any of the single imputation methods because:\n\n\n\n\nA single imputed value is never used\n\n\nThe variance estimates reflect the appropriate amount of uncertainty surrounding parameter estimates\n\n\n\n\nThere are several decisions to be made before performing a multiple imputation including \ndistribution\n, \nauxiliary variables\n and \nnumber of imputations\n that can affect the quality of the imputation.\n\n\n\n\nImputation phase (\nPROC MI\n)\n:  the user specifies the imputation model to be used and the number of imputed datasets to be created\n\n\nAnalysis phase (\nPROG GLM\n/\nPROC GENMOD\n)\n: runs the analytic model of interest within each of the imputed datasets\n\n\nPooling phase (\nPROC MIANALYZE\n)\n: combines all the estimates across all the imputed datasets and outputs one set of parameter estimates for the model of interest\n\n\n\n\nMVN\n vs \nFCS\n\n\nAuxiliary variables\n\n\n\n\nThey can can help improve the likelihood of meeting the MAR assumption \n\n\nThey help yield more accurate and stable estimates and thus reduce the estimated standard errors in analytic models \n\n\nIncluding them can also help to increase power\n\n\n\n\nNumber of imputations (m)\n\n\n\n\nEstimates of coefficients stabilize at much lower values of \nm\n than estimates of variances and covariances of error terms \n\n\nA larger number of imputations may also allow hypothesis tests with less restrictive assumptions (i.e., that do not assume equal fractions of missing information for all coefficients)\n\n\nMultiple runs of m imputations are recommended to assess the stability of the parameter estimates\n\n\nRecommendations: \n\n\nFor low fractions of missing information (and relatively simple analysis techniques) 5-20 imputations and 50 or more when the proportion of missing data is relatively high\n\n\nThe number of imputations should equal the percentage of incomplete cases (\nm\n=max(FMI%)), this way the error associated with estimating the regression coefficients, standard errors and the resulting p-values is considerably reduced and results in an adequate level of reproducibility\n\n\n\n\nMore comments\n\n\n\n\nYou should include the dependent variable (DV) in the imputation model unless you would like to impute independent variables (IVs) assuming they are uncorrelated with your DV\n\n\nAlthough MI can perform well up to 50% missing observations,  the larger the amount the higher the chance of finding estimation problems during the imputation process and the lower the chance of meeting the MAR assumption\n\n\n\n\nModel-based Procedures\n\n\nDirect Maximum Likelihood\n\n\nThis approach to analyzing missing data has many different forms. In its simplest form, it assumes that the observed data are a sample drawn from a multivariate normal distribution. The parameters are estimated by available data, and then missing scores are estimated based on the parameters just estimated. Contrary to the techniques discussed above, maximum likelihood procedures allow explicit modeling of missing data that is open to scientific analysis and critique. \n\n\nExpectation Maximization\n\n\nThis algorithm is an iterative process. The first iteration estimates missing data and then parameters using maximum likelihood. The second iteration re-estimates the missing data based on the new parameter estimates and then recalculates the new parameters estimates\nbased on actual and re-estimated missing data. The approach continues until there is convergence in the parameter estimates.\n\n\nSummary\n\n\nThe best technique to deal with your missing data depends on:\n\n\n\n\nThe amount of missing data (what percentage of data is missing)\n\n\nType of missing data (MAR, MCAR, NMAR)\n\n\n\n\nAccording to \nthis nice review\n, if more than 10% data is missing, the best solution is:\n\n\n\n\nMaximum likelihood imputation if data are NMAR (non-missing at random)\n\n\nMaximum likelihood and hot-deck if data are MAR (missing at random)\n\n\nPairwise deletion, hot-deck or regression if data are MCAR (missing completely at random)\n\n\n\n\nMoreover, \nmultiple imputation\n by chained equations is regarded the best imputation method by many researchers.", 
            "title": "Dealing with Missing Data"
        }, 
        {
            "location": "/other-analysis/missing-data/#missing-data-mechanisms-and-patterns", 
            "text": "To use the more appropriate method to deal with your missing data, you should consider the missing data mechanism of your data which describes the process that is believed to have generated the missing values. According to  Rubin (1976) , there are three mechanisms under which missing data can occur:   Missing completely at random (MCAR) :  neither the variables in the dataset nor the unobserved value of the variable itself predict whether a value will be missing  Missing at random (MAR) : other variables (but not the variable itself) in the dataset can be used to predict missingness on a given variable  Missing not at random (MNAR) : value of the unobserved variable itself predicts missingness    Check these websites  Check out the formal description of each missing mechanism in the \"Missing data mechanisms\" section of  this  paper.", 
            "title": "Missing Data Mechanisms and Patterns"
        }, 
        {
            "location": "/other-analysis/missing-data/#objetives-of-imputation", 
            "text": "Depending on the  type of data and model  you will be using, techniques such as  multiple imputation  or  direct maximum likelihood  may better serve your needs. The main goals of statistical analysis with missing data are:   Minimize bias  Maximize use of available information  Obtain appropriate estimates of uncertainty   Imputed values are  not  equivalent to observed values and serve only to help estimate the covariances between variables needed for inference.   Automated imputations generally fall into one of six categories:    Deterministic imputation   Model based imputation   Deck imputation   Mixed imputation   Expert Systems   Neural networks", 
            "title": "Objetives of Imputation"
        }, 
        {
            "location": "/other-analysis/missing-data/#deletion-procedures", 
            "text": "Complete case analysis (listwise deletion) :  deleting cases in a particular dataset that are missing data on any variable of interest (for MCAR cases the power is reduced but it does not add any bias). It is a common technique because it is easy to implement and works with any type of analysis.  Available case analysis (pairwise deletion) :  deleting cases where a variable required for a particular analysis is missing, but including those cases in analyses for which all required variables are present. One of the main drawbacks of this method is no consistent sample size because depending on the pairwise comparison examined, the sample size will change based on the amount of missing present in one or both variables. This method became popular because the loss of power due to missing information is not as substantial as with complete case analysis. Unless the mechanism of missing data is MCAR, this method will introduce bias into the parameter estimates. Therefore, this method is not recommended.", 
            "title": "Deletion procedures"
        }, 
        {
            "location": "/other-analysis/missing-data/#replacement-procedures", 
            "text": "Data replacement does not compensate for a badly designed instrument or for poor data collection. Overall, replacement procedures can be used in certain cases, as long as the researcher has a good reason for replacing.  The most important advantages of these procedures are the retention of the sample size (statistical power). To a greater or lesser extent, all replacement procedures are biased if there is a non-random distribution of missing values. In assessing the effectiveness of these procedures, both the accuracy of estimating the value of missing data and the accuracy of estimating the statistical effects have to be considered.  Many different missing data replacement procedures have been developed over the years. In general, the differences between the various methods decrease with: (a) larger sample size, (b) a smaller percentage of missing values, (c) fewer missing variables and (d) a decrease in the level of the correlations between the variables.", 
            "title": "Replacement procedures"
        }, 
        {
            "location": "/other-analysis/missing-data/#single-imputation-methods", 
            "text": "Single imputation denotes that the missing value is replaced by a value. However, the imputed values are assumed to be the real values that would have been observed when the data would have been complete. When we have missing data, this is never the case. We can  never be completely certain about imputed values .    Unconditional Mean Imputation / Mean Substitution : replacing the missing values for an individual variable wih it's overall estimated mean from the available cases. Its more important problem is that it will result in an artificial reduction in variability due to the fact you are imputing values at the center of the variable's distribution. This also has the unintended consequence of changing the magnitude of correlations between the imputed variable and other variables.  Regression Imputation : This is a two-step approach: first, the relationships among variables are estimated, and then the regression coefficients are used to estimate the missing value. The underlying assumption of regression imputation is the existence of a linear relationship between the predictors and the missing variable. The technique also assumes that values are missing at random (i.e., a missing value is not related to the value of the predictors).  Stochastic Regression Imputation : In recognition of the problems with regression imputation and the reduced variability associated with this approach, researchers developed a technique to incorporate or \u201cadd back\u201d lost variability. A residual term, that is randomly drawn from a normal distribution with mean zero and variance equal to the residual variance from the regression model, is added to the predicted scores from the regression imputation thus restoring some of the lost variability. This method is superior to the previous methods as it will produce unbiased coefficient estimates under MAR. However, the standard errors produced during regression estimation while less biased then the single imputation approach, will still be attenuated.      The deterministic imputations are exactly at the regression predictions and ignore predictive uncertainty. In contrast, the random imputations are more variable and better capture the range of the data.   Hot-deck Imputation : According to this technique, the researcher should replace a missing value with the actual score from a similar case in the dataset. One form of hot-deck imputation is called \"last observation carried forward\" (LOCF), which involves sorting a dataset according to any of a number of variables, thus creating an ordered dataset. The technique then finds the first missing value and uses the cell value immediately prior to the data that are missing to impute the missing value. This method is known to increase risk of increasing bias and potentially false conclusions. For this reason LOCF is not recommended for use.   Cold-deck Imputation : This method replaces a missing value of an item with a constant value from an external source such as a value from a previous survey.    Single Imputation :", 
            "title": "Single Imputation Methods"
        }, 
        {
            "location": "/other-analysis/missing-data/#multiple-imputation", 
            "text": "Check these websites  Visit  this website  for more information.   Bayesian Methods for Completing Data  are simply methods based on conditional probability.  Multiple Imputation is always superior to any of the single imputation methods because:   A single imputed value is never used  The variance estimates reflect the appropriate amount of uncertainty surrounding parameter estimates   There are several decisions to be made before performing a multiple imputation including  distribution ,  auxiliary variables  and  number of imputations  that can affect the quality of the imputation.   Imputation phase ( PROC MI ) :  the user specifies the imputation model to be used and the number of imputed datasets to be created  Analysis phase ( PROG GLM / PROC GENMOD ) : runs the analytic model of interest within each of the imputed datasets  Pooling phase ( PROC MIANALYZE ) : combines all the estimates across all the imputed datasets and outputs one set of parameter estimates for the model of interest", 
            "title": "Multiple imputation"
        }, 
        {
            "location": "/other-analysis/missing-data/#mvn-vs-fcs", 
            "text": "", 
            "title": "MVN vs FCS"
        }, 
        {
            "location": "/other-analysis/missing-data/#auxiliary-variables", 
            "text": "They can can help improve the likelihood of meeting the MAR assumption   They help yield more accurate and stable estimates and thus reduce the estimated standard errors in analytic models   Including them can also help to increase power", 
            "title": "Auxiliary variables"
        }, 
        {
            "location": "/other-analysis/missing-data/#number-of-imputations-m", 
            "text": "Estimates of coefficients stabilize at much lower values of  m  than estimates of variances and covariances of error terms   A larger number of imputations may also allow hypothesis tests with less restrictive assumptions (i.e., that do not assume equal fractions of missing information for all coefficients)  Multiple runs of m imputations are recommended to assess the stability of the parameter estimates  Recommendations:   For low fractions of missing information (and relatively simple analysis techniques) 5-20 imputations and 50 or more when the proportion of missing data is relatively high  The number of imputations should equal the percentage of incomplete cases ( m =max(FMI%)), this way the error associated with estimating the regression coefficients, standard errors and the resulting p-values is considerably reduced and results in an adequate level of reproducibility", 
            "title": "Number of imputations (m)"
        }, 
        {
            "location": "/other-analysis/missing-data/#more-comments", 
            "text": "You should include the dependent variable (DV) in the imputation model unless you would like to impute independent variables (IVs) assuming they are uncorrelated with your DV  Although MI can perform well up to 50% missing observations,  the larger the amount the higher the chance of finding estimation problems during the imputation process and the lower the chance of meeting the MAR assumption", 
            "title": "More comments"
        }, 
        {
            "location": "/other-analysis/missing-data/#model-based-procedures", 
            "text": "", 
            "title": "Model-based Procedures"
        }, 
        {
            "location": "/other-analysis/missing-data/#direct-maximum-likelihood", 
            "text": "This approach to analyzing missing data has many different forms. In its simplest form, it assumes that the observed data are a sample drawn from a multivariate normal distribution. The parameters are estimated by available data, and then missing scores are estimated based on the parameters just estimated. Contrary to the techniques discussed above, maximum likelihood procedures allow explicit modeling of missing data that is open to scientific analysis and critique.", 
            "title": "Direct Maximum Likelihood"
        }, 
        {
            "location": "/other-analysis/missing-data/#expectation-maximization", 
            "text": "This algorithm is an iterative process. The first iteration estimates missing data and then parameters using maximum likelihood. The second iteration re-estimates the missing data based on the new parameter estimates and then recalculates the new parameters estimates\nbased on actual and re-estimated missing data. The approach continues until there is convergence in the parameter estimates.", 
            "title": "Expectation Maximization"
        }, 
        {
            "location": "/other-analysis/missing-data/#summary", 
            "text": "The best technique to deal with your missing data depends on:   The amount of missing data (what percentage of data is missing)  Type of missing data (MAR, MCAR, NMAR)   According to  this nice review , if more than 10% data is missing, the best solution is:   Maximum likelihood imputation if data are NMAR (non-missing at random)  Maximum likelihood and hot-deck if data are MAR (missing at random)  Pairwise deletion, hot-deck or regression if data are MCAR (missing completely at random)   Moreover,  multiple imputation  by chained equations is regarded the best imputation method by many researchers.", 
            "title": "Summary"
        }, 
        {
            "location": "/other-analysis/interim-analysis/", 
            "text": "The purpose of the \nSEQDESIGN\n procedure is to design interim analyses for clinical trials. \n\n\nIn a \nfixed-sample trial\n, data about all individuals are first collected and then examined at the end of the study. In contrast, a \ngroup sequential trial\n provides for interim analyses before the completion of the trial while maintaining the specified overall Type I and Type II error probabilities.\n\n\nA group sequential trial is most useful in situations where it is important to monitor the trial to prevent unnecessary exposure of patients to an unsafe new drug, or alternatively to a placebo treatment if the new drug shows significant improvement. In most cases, if a group sequential trial \nstops early for safety concerns\n, fewer patients are exposed to the new treatment than in the fixed-sample trial. If a trial \nstops early for efficacy reasons\n, the new treatment is available sooner than it would be in a fixed-sample trial. Early stopping can also save time and resources.\n\n\nA \ngroup sequential design\n provides detailed specifications for a group sequential trial. In addition to the usual specification for a fixed-sample design, it provides the total number of stages (the number of interim stages plus a final stage) and a stopping criterion to reject, to accept, or to either reject or accept the null hypothesis at each interim stage. It also provides critical values and the sample size at each stage for the trial.\n\n\n\n\nPROC SEQDESIGN\n\n\nYou use the \nSEQDESIGN\n procedure compute the initial boundary values and required sample sizes for the trial. \n\n\nPROC SEQTEST\n\n\nYou use the \nSEQTEST\n procedure to compare the test statistic with its boundary values.\n\n\nhttp://support.sas.com/documentation/cdl/en/statug/68162/HTML/default/viewer.htm#statug_seqdesign_overview01.htm\nhttps://support.sas.com/resources/papers/proceedings09/311-2009.pdf\nhttps://support.sas.com/documentation/cdl/en/statug/63033/HTML/default/viewer.htm#statug_seqdesign_sect035.htm", 
            "title": "Interim Analysis Design"
        }, 
        {
            "location": "/other-analysis/interim-analysis/#proc-seqdesign", 
            "text": "You use the  SEQDESIGN  procedure compute the initial boundary values and required sample sizes for the trial.", 
            "title": "PROC SEQDESIGN"
        }, 
        {
            "location": "/other-analysis/interim-analysis/#proc-seqtest", 
            "text": "You use the  SEQTEST  procedure to compare the test statistic with its boundary values.  http://support.sas.com/documentation/cdl/en/statug/68162/HTML/default/viewer.htm#statug_seqdesign_overview01.htm\nhttps://support.sas.com/resources/papers/proceedings09/311-2009.pdf\nhttps://support.sas.com/documentation/cdl/en/statug/63033/HTML/default/viewer.htm#statug_seqdesign_sect035.htm", 
            "title": "PROC SEQTEST"
        }, 
        {
            "location": "/other-analysis/normality-tests/", 
            "text": "Saphiro-Wilk normality test\n\n\nIn statistics, the \nShapiro-Wilk test\n is used to test the normality of a data set. It is considered one of the most powerful tests for normality contrast, especially for small samples ($n\n50$).\n\n\nMonte Carlo simulations have found that Shapiro\u2013Wilk has \nthe best power for a given significance\n, followed closely by Anderson\u2013Darling when comparing to the Kolmogorov\u2013Smirnov, Lilliefors, and Anderson\u2013Darling tests.\n\n\nThe null-hypothesis of this test is that the population is normally distributed. Thus,\n\n\n\n\nif the p-value is less than the chosen alpha level, then the null \nhypothesis is rejected\n and there is evidence that the data tested are \nnot from a normally distributed population\n\n\nif the p-value is greater than the chosen alpha level, then the null \nhypothesis cannot be rejected\n \n\n\n\n\nHowever, since the \ntest is biased by sample size\n, it may be statistically significant from a normal distribution in any \nlarge samples\n and a \nQ\u2013Q plot would be required for verification\n in addition to the test.\n\n\nPROC UNIVARIATE\n\n\nThe \nUNIVARIATE\n procedure provides a variety of descriptive statistics, and draws Q-Q, stem-and-leaf, normal probability, and box plots. This procedure also conducts Shapiro-Wilk, Kolmogorov-Smirnov, Anderson-Darling and Cramer-von Misers tests. \n\n\n\n\nNote\n\n\nThe Shapiro-Wilk \nW\n will be reported only if $N\n2000$. \n\n\n\n\n1\n2\n3\n4\n5\nPROC UNIVARIATE DATA=SAS-data-set NORMAL PLOT;\n  VAR variable(s);\n  QQPLOT variable /NORMAL(MU=EST SIGMA=EST COLOR=RED L=1);\n  OUTPUT OUT=normality PROBN=probn;\nRUN;\n\n\n\n\n\n\n\n\nNORMAL\n performs normality tests\n\n\nPLOT\n draws a stem-and-leaf and a box plots\n\n\nQQPLOT\n draws a Q-Q plot\n\n\n\n\n\n\nNote\n\n\nYou must provide a \nVAR\n statement when you use an \nOUTPUT\n statement. To store the same statistic for several analysis variables in the \nOUT=\n data set, you specify a list of names in the \nOUTPUT\n statement. \nPROC UNIVARIATE\n makes a one-to-one correspondence between the order of the analysis variables in the \nVAR\n statement and the list of names that follow a statistic keyword.   \n\n\n\n\nPROC CAPABILITY\n\n\nLike \nUNIVARIATE\n, the \nCAPABILITY\n procedure also produces various descriptive statistics and plots. \nCAPABILITY\n can draw a P-P plot using the \nPPPLOT\n option but does not support stem-and-leaf, box, and normal probability plots (it does not have the \nPLOT\n option). \n\n\n1\n2\n3\n4\n5\n6\n7\nPROC CAPABILITY DATA=SAS-data-set NORMAL;\n  VAR variable(s);\n  QQPLOT variable /NORMAL(MU=EST SIGMA=EST COLOR=RED L=1);\n  PPPLOT variable /NORMAL(MU=EST SIGMA=EST COLOR=RED L=1);\n  HISTOGRAM /NORMAL(COLOR=MAROON W=4) CFILL = BLUE CFRAME = LIGR;\n  INSET MEAN STD /CFILL=BLANK FORMAT=5.2 ;\nRUN; \n\n\n\n\n\n\n\n\nNORMAL\n performs normality tests\n\n\nQQPLOT\n, \nPPPLOT\n and \nHISTOGRAM\n statements respectively draw a Q-Q plot, a P-P plot, and a histogram\n\n\nINSET\n statement adds summary statistics to graphs such as a histogram and a Q-Q plot", 
            "title": "Normality Tests"
        }, 
        {
            "location": "/other-analysis/normality-tests/#saphiro-wilk-normality-test", 
            "text": "In statistics, the  Shapiro-Wilk test  is used to test the normality of a data set. It is considered one of the most powerful tests for normality contrast, especially for small samples ($n 50$).  Monte Carlo simulations have found that Shapiro\u2013Wilk has  the best power for a given significance , followed closely by Anderson\u2013Darling when comparing to the Kolmogorov\u2013Smirnov, Lilliefors, and Anderson\u2013Darling tests.  The null-hypothesis of this test is that the population is normally distributed. Thus,   if the p-value is less than the chosen alpha level, then the null  hypothesis is rejected  and there is evidence that the data tested are  not from a normally distributed population  if the p-value is greater than the chosen alpha level, then the null  hypothesis cannot be rejected     However, since the  test is biased by sample size , it may be statistically significant from a normal distribution in any  large samples  and a  Q\u2013Q plot would be required for verification  in addition to the test.", 
            "title": "Saphiro-Wilk normality test"
        }, 
        {
            "location": "/other-analysis/normality-tests/#proc-univariate", 
            "text": "The  UNIVARIATE  procedure provides a variety of descriptive statistics, and draws Q-Q, stem-and-leaf, normal probability, and box plots. This procedure also conducts Shapiro-Wilk, Kolmogorov-Smirnov, Anderson-Darling and Cramer-von Misers tests.    Note  The Shapiro-Wilk  W  will be reported only if $N 2000$.    1\n2\n3\n4\n5 PROC UNIVARIATE DATA=SAS-data-set NORMAL PLOT;\n  VAR variable(s);\n  QQPLOT variable /NORMAL(MU=EST SIGMA=EST COLOR=RED L=1);\n  OUTPUT OUT=normality PROBN=probn;\nRUN;    NORMAL  performs normality tests  PLOT  draws a stem-and-leaf and a box plots  QQPLOT  draws a Q-Q plot    Note  You must provide a  VAR  statement when you use an  OUTPUT  statement. To store the same statistic for several analysis variables in the  OUT=  data set, you specify a list of names in the  OUTPUT  statement.  PROC UNIVARIATE  makes a one-to-one correspondence between the order of the analysis variables in the  VAR  statement and the list of names that follow a statistic keyword.", 
            "title": "PROC UNIVARIATE"
        }, 
        {
            "location": "/other-analysis/normality-tests/#proc-capability", 
            "text": "Like  UNIVARIATE , the  CAPABILITY  procedure also produces various descriptive statistics and plots.  CAPABILITY  can draw a P-P plot using the  PPPLOT  option but does not support stem-and-leaf, box, and normal probability plots (it does not have the  PLOT  option).   1\n2\n3\n4\n5\n6\n7 PROC CAPABILITY DATA=SAS-data-set NORMAL;\n  VAR variable(s);\n  QQPLOT variable /NORMAL(MU=EST SIGMA=EST COLOR=RED L=1);\n  PPPLOT variable /NORMAL(MU=EST SIGMA=EST COLOR=RED L=1);\n  HISTOGRAM /NORMAL(COLOR=MAROON W=4) CFILL = BLUE CFRAME = LIGR;\n  INSET MEAN STD /CFILL=BLANK FORMAT=5.2 ;\nRUN;     NORMAL  performs normality tests  QQPLOT ,  PPPLOT  and  HISTOGRAM  statements respectively draw a Q-Q plot, a P-P plot, and a histogram  INSET  statement adds summary statistics to graphs such as a histogram and a Q-Q plot", 
            "title": "PROC CAPABILITY"
        }, 
        {
            "location": "/other-analysis/effect-size/", 
            "text": "The effect size will be larger if\n\n\n\n\nThe absolute difference between the averages is higher, or\n\n\nthe responses are consistently close to the average values and not widely spread out (the standard deviation is low).\n\n\n\n\nT-Test\n\n\nA T-Test's effect size indicates whether or not the difference between two groups' averages is large enough to have practical meaning, whether or not it is statistically significant.\n\n\nCohen's d\n\n\nCohen's d is defined as the difference between two means divided by a standard deviation for the data, i.e.\n\n\n$d = \\frac{\\bar{x}_1-\\bar{x}_2}{s}=\\frac{\\mu_1-\\mu_2}{s}$.\n\n\nBy default SPSS and SAS compute the SD as an inferential statistic (i.e., S) rather than as the population parameter (i.e., $\\sigma$) by using N-1 in the denominator of the SD equation rather than N. In order to obtain Cohen's d rather than Hedge's g, the inferential statistic S will be transformed to $\\sigma$.\n\n\n\n\n\n\n\n\nEffect size\n\n\nd\n\n\nReference\n\n\n\n\n\n\n\n\n\n\nVery small\n\n\n0.01\n\n\nSawilowsky, 2009\n\n\n\n\n\n\nSmall\n\n\n0.20\n\n\nCohen, 1988\n\n\n\n\n\n\nMedium\n\n\n0.50\n\n\nCohen, 1988\n\n\n\n\n\n\nLarge\n\n\n0.80\n\n\nCohen, 1988\n\n\n\n\n\n\nVery large\n\n\n1.20\n\n\nSawilowsky, 2009\n\n\n\n\n\n\nHuge\n\n\n2.00\n\n\nSawilowsky, 2009\n\n\n\n\n\n\n\n\nMacro Example\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n%macro\n \neffectsize\n(\nscorelist\n=\n,\nscorelablist\n=\n);\n\n    \n%let\n \nnscore\n \n=\n \n%eval\n(\n%sysfunc\n(\ncount\n(\nscorelist\n,\n$\n))\n \n+\n \n1\n);\n\n    \n%do\n \niscore\n=\n1\n \n%to\n \nnscore\n;\n\n        \n%let\n \nscore\niscore\n=\n%scan\n(\nscorelist\n.,\niscore\n.,\n$\n);\n\n        \n%let\n \nscorelab\niscore\n=\n%scan\n(\nscorelablist\n.,\niscore\n.,\n$\n);\n\n\n        \nods\n \nexclude\n \nall\n;\n\n        \nproc\n \nttest\n \ndata\n=\nobjetivos_cambio\n;\n\n            \npaired\n \nscore\niscore\n..\n_fin\n*\nscore\niscore\n..\n_bs\n;\n\n            \nods\n \noutput\n \nStatistics\n=\nstat_\niscore\n.\n \nTTests\n=\nttest_\niscore\n.;\n\n        \nrun\n;\n\n\n        \nproc\n \nmeans\n \ndata\n=\nobjetivos_cambio\n \nmean\n \nstddev\n \nn\n \nskewness\n \nkurtosis\n \nt\n \nprt\n \nCLM\n;\n \n            \nvar\n \ncambio\nscore\niscore\n..;\n \n            \nods\n \noutput\n \nSummary\n=\nmeans_\niscore\n.;\n\n        \nrun\n;\n\n\n        \ndata\n \ncohensd_\niscore\n.;\n\n            \nset\n \nmeans_\niscore\n.;\n\n            \nt\n \n=\n \ncambio\nscore\niscore\n..\n_t\n;\n\n            \nn\n \n=\n \ncambio\nscore\niscore\n..\n_n\n;\n\n            \ndf\n \n=\n \nn\n-\n1\n;\n\n            \nd\n \n=\n \nt\n/\nsqrt\n(\nn\n);\n\n            \nncp_lower\n \n=\n \nTNONCT\n(\nt\n,\ndf\n,\n.975\n);\n\n            \nncp_upper\n \n=\n \nTNONCT\n(\nt\n,\ndf\n,\n.025\n);\n\n            \nd_lower\n \n=\n \nncp_lower\n/\nsqrt\n(\nn\n);\n\n            \nd_upper\n \n=\n \nncp_upper\n/\nsqrt\n(\nn\n);\n\n            \nd_ic\n \n=\n \n(\n||\ntrim\n(\nleft\n(\nput\n(\nd_lower\n,\n6.2\n)))\n||\n,\n||\ntrim\n(\nleft\n(\nput\n(\nd_upper\n,\n6.2\n)))\n||\n)\n;\n\n            \nVariable1\n \n=\n \nscore\niscore.._fin\n;\n\n            \nVariable2\n \n=\n \nscore\niscore.._bs\n;\n\n            \noutput\n;\n \n        \nrun\n;\n \n        \nods\n \nexclude\n \nnone\n;\n\n\n        \ndata\n \ntab_\niscore\n.;\n\n            \nlength\n \nlabel\n \nVariable1\n \nVariable2\n \nDifference\n \n$50\n;\n\n            \nmerge\n \nstat_\niscore\n.\n \nttest_\niscore\n.\n \ncohensd_\niscore\n.(\nkeep\n=\nVariable1\n \nVariable2\n \nd\n \nd_ic\n);\n\n            \nby\n \nVariable1\n \nVariable2\n;\n\n            \nlabel\n \n=\n \nscorelab\niscore\n;\n\n        \nrun\n;\n\n    \n%end\n;\n\n\n    \ndata\n \nfinal_tab\n;\n\n        \nset\n \n        \n%do\n \niscore\n=\n1\n \n%to\n \nnscore\n;\n\n            \ntab_\niscore\n.\n \n        \n%end\n;\n\n        \n;\n\n        \nmeansd\n \n=\n \ntrim\n(\nleft\n(\nput\n(\nmean\n,\n6.2\n)))\n||\n \n(\n||\ntrim\n(\nleft\n(\nput\n(\nstddev\n,\n6.2\n)))\n||\n)\n;\n\n        \ncimean\n \n=\n \n(\n||\ntrim\n(\nleft\n(\nput\n(\nLowerCLMean\n,\n6.2\n)))\n||\n,\n||\ntrim\n(\nleft\n(\nput\n(\nUpperCLMean\n,\n6.2\n)))\n||\n)\n;\n\n        \nminmax\n \n=\n \n(\n||\ntrim\n(\nleft\n(\nput\n(\nMinimum\n,\n6.2\n)))\n||\n,\n||\ntrim\n(\nleft\n(\nput\n(\nmaximum\n,\n6.2\n)))\n||\n)\n;\n\n    \nrun\n;\n\n\n    \nproc\n \nreport\n \ndata\n=\nfinal_tab\n \nnowd\n \nheadline\n \nstyle\n(\nheader\n)\n=\n{\nbackground\n=\nvery\n \nlight\n \ngrey\n \nfontsize\n=\n8\npt\n}\n \nmissing\n \n    \nstyle\n(\ncolumn\n)\n=\n{\nfontsize\n=\n8\npt\n \n}\n \nsplit\n=\n*\n;\n\n        \ncolumn\n  \n(\nTama\u00f1o del efecto para muestras pareadas de objetivos primario y secundarios\n \n(\nlabel\n \nn\n \nmeansd\n \ncimean\n \nminmax\n \nprobt\n \nd\n \nd_ic\n));\n\n        \ndefine\n \nlabel\n \n/\n \ndisplay\n \nCuestionario\n \nflow\n;\n\n        \ndefine\n \nn\n \n/\n \ndisplay\n \nN\n \nflow\n;\n\n        \ndefine\n \nmeansd\n \n/\n \ndisplay\n \nDiff\n.\n \nMedia\n \n(\nDE\n)\n \nflow\n;\n\n        \ndefine\n \ncimean\n \n/\n \ndisplay\n \nIC\n \nDiff\n.\n \nMedia\n \n(\n95\n%\n)\n \nflow\n;\n\n        \ndefine\n \nminmax\n \n/\n \ndisplay\n \n(\nM\n\u00ed\nnimo\n,\n \nM\n\u00e1\nximo\n)\n \nflow\n;\n\n        \ndefine\n \nprobt\n \n/\n \ndisplay\n \np\n-\nvalor\n \nflow\n;\n\n        \ndefine\n \nd\n \n/\n \ndisplay\n \nf\n=\n5.2\n \nd\n \nde\n \nCohen\n \nflow\n;\n\n        \ndefine\n \nd_ic\n \n/\n \ndisplay\n \nIC\n \nd\n \nde\n \nCohen\n \n(\n95\n%\n)\n \nflow\n;\n\n    \nrun\n;\n\n\n    \nproc\n \ndatasets\n \nlib\n=\nwork\n \nnowarn\n \nnolist\n \nnodetails\n;\n \n        \ndelete\n \ntab_\n:\n \nstat_\n:\n \nttest_\n:\n \nmeans_\n:\n \ncohensd_\n:;\n\n    \nrun\n;\n \n    \nquit\n;\n \n\n\n%mend\n;\n\n\n\n*\n%effectsize\n(\nscorelist\n=\nvarname1$varname2$varname3\n,\n\n         \nscorelablist\n=\nVariable\n \nlabel\n \n1\n$Variable\n \nlabel\n \n2\n$Variable\n \nlabel\n \n3\n);", 
            "title": "Effect Size"
        }, 
        {
            "location": "/other-analysis/effect-size/#t-test", 
            "text": "A T-Test's effect size indicates whether or not the difference between two groups' averages is large enough to have practical meaning, whether or not it is statistically significant.", 
            "title": "T-Test"
        }, 
        {
            "location": "/other-analysis/effect-size/#cohens-d", 
            "text": "Cohen's d is defined as the difference between two means divided by a standard deviation for the data, i.e.  $d = \\frac{\\bar{x}_1-\\bar{x}_2}{s}=\\frac{\\mu_1-\\mu_2}{s}$.  By default SPSS and SAS compute the SD as an inferential statistic (i.e., S) rather than as the population parameter (i.e., $\\sigma$) by using N-1 in the denominator of the SD equation rather than N. In order to obtain Cohen's d rather than Hedge's g, the inferential statistic S will be transformed to $\\sigma$.     Effect size  d  Reference      Very small  0.01  Sawilowsky, 2009    Small  0.20  Cohen, 1988    Medium  0.50  Cohen, 1988    Large  0.80  Cohen, 1988    Very large  1.20  Sawilowsky, 2009    Huge  2.00  Sawilowsky, 2009", 
            "title": "Cohen's d"
        }, 
        {
            "location": "/other-analysis/effect-size/#macro-example", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75 %macro   effectsize ( scorelist = , scorelablist = ); \n     %let   nscore   =   %eval ( %sysfunc ( count ( scorelist , $ ))   +   1 ); \n     %do   iscore = 1   %to   nscore ; \n         %let   score iscore = %scan ( scorelist ., iscore ., $ ); \n         %let   scorelab iscore = %scan ( scorelablist ., iscore ., $ ); \n\n         ods   exclude   all ; \n         proc   ttest   data = objetivos_cambio ; \n             paired   score iscore .. _fin * score iscore .. _bs ; \n             ods   output   Statistics = stat_ iscore .   TTests = ttest_ iscore .; \n         run ; \n\n         proc   means   data = objetivos_cambio   mean   stddev   n   skewness   kurtosis   t   prt   CLM ;  \n             var   cambio score iscore ..;  \n             ods   output   Summary = means_ iscore .; \n         run ; \n\n         data   cohensd_ iscore .; \n             set   means_ iscore .; \n             t   =   cambio score iscore .. _t ; \n             n   =   cambio score iscore .. _n ; \n             df   =   n - 1 ; \n             d   =   t / sqrt ( n ); \n             ncp_lower   =   TNONCT ( t , df , .975 ); \n             ncp_upper   =   TNONCT ( t , df , .025 ); \n             d_lower   =   ncp_lower / sqrt ( n ); \n             d_upper   =   ncp_upper / sqrt ( n ); \n             d_ic   =   ( || trim ( left ( put ( d_lower , 6.2 ))) || , || trim ( left ( put ( d_upper , 6.2 ))) || ) ; \n             Variable1   =   score iscore.._fin ; \n             Variable2   =   score iscore.._bs ; \n             output ;  \n         run ;  \n         ods   exclude   none ; \n\n         data   tab_ iscore .; \n             length   label   Variable1   Variable2   Difference   $50 ; \n             merge   stat_ iscore .   ttest_ iscore .   cohensd_ iscore .( keep = Variable1   Variable2   d   d_ic ); \n             by   Variable1   Variable2 ; \n             label   =   scorelab iscore ; \n         run ; \n     %end ; \n\n     data   final_tab ; \n         set  \n         %do   iscore = 1   %to   nscore ; \n             tab_ iscore .  \n         %end ; \n         ; \n         meansd   =   trim ( left ( put ( mean , 6.2 ))) ||   ( || trim ( left ( put ( stddev , 6.2 ))) || ) ; \n         cimean   =   ( || trim ( left ( put ( LowerCLMean , 6.2 ))) || , || trim ( left ( put ( UpperCLMean , 6.2 ))) || ) ; \n         minmax   =   ( || trim ( left ( put ( Minimum , 6.2 ))) || , || trim ( left ( put ( maximum , 6.2 ))) || ) ; \n     run ; \n\n     proc   report   data = final_tab   nowd   headline   style ( header ) = { background = very   light   grey   fontsize = 8 pt }   missing  \n     style ( column ) = { fontsize = 8 pt   }   split = * ; \n         column    ( Tama\u00f1o del efecto para muestras pareadas de objetivos primario y secundarios   ( label   n   meansd   cimean   minmax   probt   d   d_ic )); \n         define   label   /   display   Cuestionario   flow ; \n         define   n   /   display   N   flow ; \n         define   meansd   /   display   Diff .   Media   ( DE )   flow ; \n         define   cimean   /   display   IC   Diff .   Media   ( 95 % )   flow ; \n         define   minmax   /   display   ( M \u00ed nimo ,   M \u00e1 ximo )   flow ; \n         define   probt   /   display   p - valor   flow ; \n         define   d   /   display   f = 5.2   d   de   Cohen   flow ; \n         define   d_ic   /   display   IC   d   de   Cohen   ( 95 % )   flow ; \n     run ; \n\n     proc   datasets   lib = work   nowarn   nolist   nodetails ;  \n         delete   tab_ :   stat_ :   ttest_ :   means_ :   cohensd_ :; \n     run ;  \n     quit ;   %mend ;  * %effectsize ( scorelist = varname1$varname2$varname3 , \n          scorelablist = Variable   label   1 $Variable   label   2 $Variable   label   3 );", 
            "title": "Macro Example"
        }, 
        {
            "location": "/other-analysis/propensity-score/", 
            "text": "Propensity Score\n\n\nWhat is it?\n\n\nThe objective of randomization in statistics is to obtain groups that are comparable in terms of both observed and unobserved characteristics. When randomization is not possible, causal inference is complicated by the fact that a group that received a treatment or experienced an event maybe very different from another group that did not experience the event or receive the treatment. Thus, it is \nnot clear whether a difference in certain outcome of interest is due to the treatment or is the product of prior differences among groups\n. Propensity score methods were developed to facilitate the \ncreation of comparison groups that are similar in terms of the distribution of observed characteristics\n.\n\n\nThe first step involves estimating the likelihood (the propensity score) that a person would have received the treatment given certain characteristics. More formally, the propensity score is the \nconditional probability of assignment to a particular treatment given a vector of observed covariates\n. Two key assumptions of propensity scores are that \nboth the outcome of interest and the treatment assignment do not depend on unobservable characteristics\n.\n\n\nComputing the Propensity Score\n\n\nIn order to program the corresponding model in SAS, the response variable is the group/arm to which the patient belongs and the predictors are all the baseline variables which could affect the assignment to a certain group. While the model is fitted the propensity score value can be computed and kept in an output:\n\n\n1\n2\n3\n4\n5\nPROC LOGISTIC DATA=input-SAS-data-set;\n    CLASS sex site(param=ref);\n    MODEL arm = sex age weight site serologytests / FIRTH;\n    OUTPUT OUT=output-SAS-data-set PROB=ps-variable-customized-name;\nRUN;\n\n\n\n\n\n\nThis kind of analysis are commonly used in observacional studies on which the patient is not randomized to a certain group but it belongs to it due to a certain diagnostic. In order to correct the possible effect of unbalanced population groups, \nthe propensity score value can be included in the model\n as a way to isolate the effects due to the treatment from the baseline characteristics.\n\n\nPropensity Score Matching\n\n\nAfter estimating the propensity scores, they are used to group observations that are close to each other. One way of\naccomplishing this is to classify treated and untreated observations into subgroups and then separately compare the outcome\nfor each subgroup. This method is usually referred as \nsubclassification on the propensity scores\n (Rosenbaum and Rubin\n1984). The other way is to match one treated unit to one or more untreated controls, which is usually referred as \nmatching on\nthe propensity score\n (Rosenbaum and Rubin 1983).\n\n\nKey in the implementation of matching using propensity scores is to \ndecide what metric to use when evaluating the distance between scores\n (usually the absolute value or the Mahalanobis metric) and \nwhat type of algorithm to implement\n (local or global optimal).\n\n\nPair-matching Methods\n\n\nThe most common implementation is 1:1 (1 to 1) or pair-matching in which pairs of treated and untreated subjects are formed which allows to estimate for average treatment effect in the treated (ATT).\n\n\nMethods without Replacement\n\n\nWe match each untreated subject to at most one treated subject. Once an untreated subject has been matched to a treated subject, that untreated subject is no longer eligible for consideration as a match for other treated subjects.\n\n\n\n\nGlobal optimal matching: forms matched pairs so as to minimize the average within-pair difference in propensity scores\n\n\nLocal optimal, greedy or nearest available neighbor matching: selects a treated subject and then selects as a matched control subject the one whose propensity score is closest to that of the treated subject (if multiple untreated subjects are equally close to the treated subject, one of these untreated subjects is selected at random). In each iteration, the best (optimal) control is chosen, but this process does not guarantee that the total distance between propensity scores is minimized.\n\n\n\n\nFour different approaches:\n\n\n\n\nSequentially treated subjects from highest to lowest propensity score\n\n\nSequentially treated subjects from lowest to highest propensity score\n\n\nSequentially treated subjects in the order of the best possible match (the first selected treated subject is that treated subject who is closest to an untreated subject and so on)\n\n\nTreated subjects in a random order\n\n\nLocal optimal, greedy or nearest available neighbour matching within specified caliper widths: we can match treated and untreated subjects only if the absolute difference in their propensity scores is within a prespecified maximal distance (the caliper distance, defined as a proportion of the standard deviation of the logit of the propensity score)\n\n\n\n\n\n\nNote\n\n\nAlthough the propensity score is the natural metric to use, when using caliper matching, a reduction in bias due to the use of different caliper widths has been described when matching on the logit of the propensity score.\nAlthough it is difficult to know beforehand the optimal choice of caliper width, some researchers (Rosenbuam \n Rubin, 1985; Austin, 2011) have recommended using a caliper width that is equal to 0.2 of the standard deviation of the logit of the propensity score, i.e., $0.2\\cdot\\sqrt\\left ( \\sigma^2_1+\\sigma^2_2 \\right )/2$.\n\n\n\n\n\n\nSequentially treated subjects from highest to lowest propensity score\n\n\nSequentially treated subjects from lowest to highest propensity score\n\n\nSequentially treated subjects in the order of the best possible match (the first selected treated subject is that treated subject who is closest to an untreated subject and so on)\n\n\nTreated subjects in a random order\n\n\n\n\n\n\nNote\n\n\nOptimal matching and greedy nearest neighbor matching on the propensity score will result in all treated subjects being matched to an untreated subject (assuming that the number of untreated subjects is at least as large as the number of treated subjects). However, greedy nearest neighbor matching within specified caliper widths may not result in all treated subjects being matched to an untreated subject, because for some treated subjects, there may not be any untreated subjects who are unmatched and whose propensity score lies within the specified caliper distance of that of the treated subject. The objective of the caliper matching is to avoid bad matches.\n\n\n\n\nMethods with Replacement\n\n\nPermits the same untreated subject to be matched to multiple treated subjects (because untreated subjects are recycled or allowed to be included in multiple matched sets, the order in which the treated subjects are selected has no effect on the formation of matched pairs). Matching with replacement minimizes the propensity score distance between the matched units since each treated unit is matched to the closest control, even if the control has been selected before.\n\n\n\n\nNearest neighbor matching with replacement: matches each treated subject to the nearest untreated subject\n\n\nNearest neighbor matching within specified caliper widths with replacement: matches each treated subject to the nearest untreated subject (subject to possible caliper restrictions)\n\n\n\n\n1 to N Matching Methods\n\n\nThey include matching each treated unit to more than one control match. This can be done by creating N replicas of each treated unit and proceeding as described above. \n\n\nRadius Matching\n\n\nAll the control units within a certain distance are chosen (Dehejia and Wahba 1999).\n\n\nMahalanobis Metric Matching\n\n\nIn this type of matching, the definition of distance is changed. The similarity between the propensity score of treated and untreated units is evaluated using the multidimensional Mahalanobis metric matching:\n\n\n$D_{ij}=\\sqrt{\\left ( x_i-y_j \\right )^TS^{-1}\\left ( x_i-y_j \\right )}$\n\n\nwhere $S^{-1}$ is the pooled variance-covariance matrix and x and y are multivariate vectors. Note that if the variance-covariance\nmatrix is an identity matrix the Mahalanobis metric is reduced to the familiar Euclidean metric. Usually the Mahalanobis metric\nmatching includes the propensity score and other covariates that are considered to be important and are hoped to be balanced\n(Rosenbaum and Rubin 1985).\n\n\nPSMatching Macro\n\n\nWith the macro \nPSMatching.sas\n (Coca-Perraillon, 2006) different methods can be applied to calculate the matching once you have the propensity score. First you have to prepare the following input:\n\n\n\n\nTreatment\n data set containing the treatment cases along with the patient number \nidT\n and the corresponding propensity score \npscoreT\n\n\nControl\n data set containing the control cases along with the patient number \nidC\n and the corresponding propensity score \npscoreC\n\n\n\n\nThe parameters \ndatatreatment\n and \ndatacontrol\n refer to the Treatment and Control datasets and they do not need to be sorted.\nThe method parameter can be \nNN\n (nearest available neighbor), \ncaliper\n or \nradius\n. Caliper can be any number indicating the\nsize of the \ncaliper\n and the parameter \nreplacement\n takes the values of yes or no. All the parameters are case insensitive. If the\nmethod is \nNN\n, the caliper parameter is ignored.\n\n\n1\n%PSMatching(datatreatment=treatment, datacontrol=control, method=caliper, numberofcontrols=1, caliper=0.2, replacement=no);\n\n\n\n\n\n\n\nHere is the macro code:\n\n\n  1\n  2\n  3\n  4\n  5\n  6\n  7\n  8\n  9\n 10\n 11\n 12\n 13\n 14\n 15\n 16\n 17\n 18\n 19\n 20\n 21\n 22\n 23\n 24\n 25\n 26\n 27\n 28\n 29\n 30\n 31\n 32\n 33\n 34\n 35\n 36\n 37\n 38\n 39\n 40\n 41\n 42\n 43\n 44\n 45\n 46\n 47\n 48\n 49\n 50\n 51\n 52\n 53\n 54\n 55\n 56\n 57\n 58\n 59\n 60\n 61\n 62\n 63\n 64\n 65\n 66\n 67\n 68\n 69\n 70\n 71\n 72\n 73\n 74\n 75\n 76\n 77\n 78\n 79\n 80\n 81\n 82\n 83\n 84\n 85\n 86\n 87\n 88\n 89\n 90\n 91\n 92\n 93\n 94\n 95\n 96\n 97\n 98\n 99\n100\n%macro\n \nPSMatching\n(\ndatatreatment\n=\n,\n \ndatacontrol\n=\n,\n \nmethod\n=\n,\n \nnumberofcontrols\n=\n,\n \ncaliper\n=\n,\nreplacement\n=\n);\n\n\n\n/* Create copies of the treated units if N \n 1 */\n;\n\n\ndata\n \n_Treatment0\n(\ndrop\n=\n \ni\n);\n\n\nset\n \nTreatment\n;\n\n\ndo\n \ni\n=\n \n1\n \nto\n \nnumberofcontrols\n;\n\n\nRandomNumber\n=\n \nranuni\n(\n12345\n);\n\n\noutput\n;\n\n\nend\n;\n\n\nrun\n;\n\n\n\n/* Randomly sort both datasets */\n\n\nproc\n \nsort\n \ndata\n=\n \n_Treatment0\n \nout\n=\n \n_Treatment\n(\ndrop\n=\n \nRandomNumber\n);\n\n\nby\n \nRandomNumber\n;\n\n\nrun\n;\n\n\ndata\n \n_Control0\n;\n\n\nset\n \nControl\n;\n\n\nRandomNumber\n=\n \nranuni\n(\n45678\n);\n\n\nrun\n;\n\n\nproc\n \nsort\n \ndata\n=\n \n_Control0\n \nout\n=\n \n_Control\n(\ndrop\n=\n \nRandomNumber\n);\n\n\nby\n \nRandomNumber\n;\n\n\nrun\n;\n\n\ndata\n \nMatched\n(\nkeep\n \n=\n \nIdSelectedControl\n \nMatchedToTreatID\n);\n\n\nlength\n \npscoreC\n \n8\n;\n\n\nlength\n \nidC\n \n8\n;\n\n\n\n/* Load Control dataset into the hash object */\n\n\nif\n \n_N_\n=\n \n1\n \nthen\n \ndo\n;\n\n\ndeclare\n \nhash\n \nh\n(\ndataset\n:\n \n_Control\n,\n \nordered\n:\n \nno\n);\n\n\ndeclare\n \nhiter\n \niter\n(\nh\n);\n\n\nh\n.\ndefineKey\n(\nidC\n);\n\n\nh\n.\ndefineData\n(\npscoreC\n,\n \nidC\n);\n\n\nh\n.\ndefineDone\n();\n\n\ncall\n \nmissing\n(\nidC\n,\n \npscoreC\n);\n\n\nend\n;\n\n\n\n/* Open the treatment */\n\n\nset\n \n_Treatment\n;\n\n\n%if\n \n%upcase\n(\nmethod\n)\n \n~=\n \nRADIUS\n \n%then\n \n%do\n;\n\n\nretain\n \nBestDistance\n \n99\n;\n\n\n%end\n;\n\n\n\n/* Iterate over the hash */\n\n\nrc\n=\n \niter\n.\nfirst\n();\n\n\nif\n \n(\nrc\n=\n0\n)\n \nthen\n \nBestDistance\n=\n \n99\n;\n\n\ndo\n \nwhile\n \n(\nrc\n \n=\n \n0\n);\n\n\n\n/* Caliper */\n\n\n%if\n \n%upcase\n(\nmethod\n)\n \n=\n \nCALIPER\n \n%then\n \n%do\n;\n\n\nif\n \n(\npscoreT\n \n-\n \ncaliper\n)\n \n=\n \npscoreC\n \n=\n \n(\npscoreT\n \n+\n \ncaliper\n)\n \nthen\n \ndo\n;\n\n\nScoreDistance\n \n=\n \nabs\n(\npscoreT\n \n-\n \npscoreC\n);\n\n\nif\n \nScoreDistance\n \n \nBestDistance\n \nthen\n \ndo\n;\n\n\nBestDistance\n \n=\n \nScoreDistance\n;\n\n\nIdSelectedControl\n \n=\n \nidC\n;\n\n\nMatchedToTreatID\n \n=\n \nidT\n;\n\n\nend\n;\n\n\nend\n;\n\n\n%end\n;\n\n\n\n/* NN */\n\n\n%if\n \n%upcase\n(\nmethod\n)\n \n=\n \nNN\n \n%then\n \n%do\n;\n\n\nScoreDistance\n \n=\n \nabs\n(\npscoreT\n \n-\n \npscoreC\n);\n\n\nif\n \nScoreDistance\n \n \nBestDistance\n \nthen\n \ndo\n;\n\n\nBestDistance\n \n=\n \nScoreDistance\n;\n\n\nIdSelectedControl\n \n=\n \nidC\n;\n\n\nMatchedToTreatID\n \n=\n \nidT\n;\n\n\nend\n;\n\n\n%end\n;\n\n\n%if\n \n%upcase\n(\nmethod\n)\n \n=\n \nNN\n \nor\n \n%upcase\n(\nmethod\n)\n \n=\n \nCALIPER\n \n%then\n \n%do\n;\n\n\nrc\n \n=\n \niter\n.\nnext\n();\n\n\n\n/* Output the best control and remove it */\n\n\nif\n \n(\nrc\n \n~=\n \n0\n)\n \nand\n \nBestDistance\n \n~=\n99\n \nthen\n \ndo\n;\n\n\noutput\n;\n\n\n%if\n \n%upcase\n(\nreplacement\n)\n \n=\n \nNO\n \n%then\n \n%do\n;\n\n\nrc1\n \n=\n \nh\n.\nremove\n(\nkey\n:\n \nIdSelectedControl\n);\n\n\n%end\n;\n\n\nend\n;\n\n\n%end\n;\n\n\n\n/* Radius */\n\n\n%if\n \n%upcase\n(\nmethod\n)\n \n=\n \nRADIUS\n \n%then\n \n%do\n;\n\n\nif\n \n(\npscoreT\n \n-\n \ncaliper\n)\n \n=\n \npscoreC\n \n=\n \n(\npscoreT\n \n+\n \ncaliper\n)\n \nthen\n \ndo\n;\n\n\nIdSelectedControl\n \n=\n \nidC\n;\n\n\nMatchedToTreatID\n \n=\n \nidT\n;\n\n\noutput\n;\n\n\nend\n;\n\n\nrc\n \n=\n \niter\n.\nnext\n();\n\n\n%end\n;\n\n\nend\n;\n\n\nrun\n;\n\n\n\n/* Delete temporary tables. Quote for debugging */\n\n\nods\n \nselect\n \nnone\n;\n\n\nproc\n \ndatasets\n;\n\n\ndelete\n \n_\n:(\ngennum\n=\nall\n);\n\n\nrun\n;\n\n\nods\n \nselect\n \nall\n;\n\n\n\n%mend\n \nPSMatching\n;\n\n\n\n\n\n\n\nPROC PSMATCH\n\n\nHere\n and \nhere\n you can find the documentation of this procedure. \n\n\nStratification on the Propensity Score\n\n\nStratification on the propensity score involves \nstratifying subjects into mutually exclusive subsets based on their estimated propensity score\n. Subjects are ranked according to their estimated propensity score. Subjects are then stratified into subsets based on previously defined thresholds of the estimated propensity score. A common approach is to \ndivide subjects into five equal-size groups using the quintiles of the estimated propensity score\n. Rosenbaum and Rubin (1984) stated that stratifying on the quintiles of the propensity score eliminates approximately 90% of the bias due to measured confounders when estimating a linear treatment effect. Within each propensity score stratum, treated and untreated subjects will have roughly similar values of the propensity score. Therefore, when the propensity score has been correctly specified, the distribution of measured baseline covariates will be approximately similar between treated and untreated subjects within the same stratum.\n\n\nMore info on this topic \nhere\n.", 
            "title": "Propensity Score"
        }, 
        {
            "location": "/other-analysis/propensity-score/#propensity-score", 
            "text": "", 
            "title": "Propensity Score"
        }, 
        {
            "location": "/other-analysis/propensity-score/#what-is-it", 
            "text": "The objective of randomization in statistics is to obtain groups that are comparable in terms of both observed and unobserved characteristics. When randomization is not possible, causal inference is complicated by the fact that a group that received a treatment or experienced an event maybe very different from another group that did not experience the event or receive the treatment. Thus, it is  not clear whether a difference in certain outcome of interest is due to the treatment or is the product of prior differences among groups . Propensity score methods were developed to facilitate the  creation of comparison groups that are similar in terms of the distribution of observed characteristics .  The first step involves estimating the likelihood (the propensity score) that a person would have received the treatment given certain characteristics. More formally, the propensity score is the  conditional probability of assignment to a particular treatment given a vector of observed covariates . Two key assumptions of propensity scores are that  both the outcome of interest and the treatment assignment do not depend on unobservable characteristics .", 
            "title": "What is it?"
        }, 
        {
            "location": "/other-analysis/propensity-score/#computing-the-propensity-score", 
            "text": "In order to program the corresponding model in SAS, the response variable is the group/arm to which the patient belongs and the predictors are all the baseline variables which could affect the assignment to a certain group. While the model is fitted the propensity score value can be computed and kept in an output:  1\n2\n3\n4\n5 PROC LOGISTIC DATA=input-SAS-data-set;\n    CLASS sex site(param=ref);\n    MODEL arm = sex age weight site serologytests / FIRTH;\n    OUTPUT OUT=output-SAS-data-set PROB=ps-variable-customized-name;\nRUN;   This kind of analysis are commonly used in observacional studies on which the patient is not randomized to a certain group but it belongs to it due to a certain diagnostic. In order to correct the possible effect of unbalanced population groups,  the propensity score value can be included in the model  as a way to isolate the effects due to the treatment from the baseline characteristics.", 
            "title": "Computing the Propensity Score"
        }, 
        {
            "location": "/other-analysis/propensity-score/#propensity-score-matching", 
            "text": "After estimating the propensity scores, they are used to group observations that are close to each other. One way of\naccomplishing this is to classify treated and untreated observations into subgroups and then separately compare the outcome\nfor each subgroup. This method is usually referred as  subclassification on the propensity scores  (Rosenbaum and Rubin\n1984). The other way is to match one treated unit to one or more untreated controls, which is usually referred as  matching on\nthe propensity score  (Rosenbaum and Rubin 1983).  Key in the implementation of matching using propensity scores is to  decide what metric to use when evaluating the distance between scores  (usually the absolute value or the Mahalanobis metric) and  what type of algorithm to implement  (local or global optimal).", 
            "title": "Propensity Score Matching"
        }, 
        {
            "location": "/other-analysis/propensity-score/#pair-matching-methods", 
            "text": "The most common implementation is 1:1 (1 to 1) or pair-matching in which pairs of treated and untreated subjects are formed which allows to estimate for average treatment effect in the treated (ATT).", 
            "title": "Pair-matching Methods"
        }, 
        {
            "location": "/other-analysis/propensity-score/#methods-without-replacement", 
            "text": "We match each untreated subject to at most one treated subject. Once an untreated subject has been matched to a treated subject, that untreated subject is no longer eligible for consideration as a match for other treated subjects.   Global optimal matching: forms matched pairs so as to minimize the average within-pair difference in propensity scores  Local optimal, greedy or nearest available neighbor matching: selects a treated subject and then selects as a matched control subject the one whose propensity score is closest to that of the treated subject (if multiple untreated subjects are equally close to the treated subject, one of these untreated subjects is selected at random). In each iteration, the best (optimal) control is chosen, but this process does not guarantee that the total distance between propensity scores is minimized.   Four different approaches:   Sequentially treated subjects from highest to lowest propensity score  Sequentially treated subjects from lowest to highest propensity score  Sequentially treated subjects in the order of the best possible match (the first selected treated subject is that treated subject who is closest to an untreated subject and so on)  Treated subjects in a random order  Local optimal, greedy or nearest available neighbour matching within specified caliper widths: we can match treated and untreated subjects only if the absolute difference in their propensity scores is within a prespecified maximal distance (the caliper distance, defined as a proportion of the standard deviation of the logit of the propensity score)    Note  Although the propensity score is the natural metric to use, when using caliper matching, a reduction in bias due to the use of different caliper widths has been described when matching on the logit of the propensity score.\nAlthough it is difficult to know beforehand the optimal choice of caliper width, some researchers (Rosenbuam   Rubin, 1985; Austin, 2011) have recommended using a caliper width that is equal to 0.2 of the standard deviation of the logit of the propensity score, i.e., $0.2\\cdot\\sqrt\\left ( \\sigma^2_1+\\sigma^2_2 \\right )/2$.    Sequentially treated subjects from highest to lowest propensity score  Sequentially treated subjects from lowest to highest propensity score  Sequentially treated subjects in the order of the best possible match (the first selected treated subject is that treated subject who is closest to an untreated subject and so on)  Treated subjects in a random order    Note  Optimal matching and greedy nearest neighbor matching on the propensity score will result in all treated subjects being matched to an untreated subject (assuming that the number of untreated subjects is at least as large as the number of treated subjects). However, greedy nearest neighbor matching within specified caliper widths may not result in all treated subjects being matched to an untreated subject, because for some treated subjects, there may not be any untreated subjects who are unmatched and whose propensity score lies within the specified caliper distance of that of the treated subject. The objective of the caliper matching is to avoid bad matches.", 
            "title": "Methods without Replacement"
        }, 
        {
            "location": "/other-analysis/propensity-score/#methods-with-replacement", 
            "text": "Permits the same untreated subject to be matched to multiple treated subjects (because untreated subjects are recycled or allowed to be included in multiple matched sets, the order in which the treated subjects are selected has no effect on the formation of matched pairs). Matching with replacement minimizes the propensity score distance between the matched units since each treated unit is matched to the closest control, even if the control has been selected before.   Nearest neighbor matching with replacement: matches each treated subject to the nearest untreated subject  Nearest neighbor matching within specified caliper widths with replacement: matches each treated subject to the nearest untreated subject (subject to possible caliper restrictions)", 
            "title": "Methods with Replacement"
        }, 
        {
            "location": "/other-analysis/propensity-score/#1-to-n-matching-methods", 
            "text": "They include matching each treated unit to more than one control match. This can be done by creating N replicas of each treated unit and proceeding as described above.", 
            "title": "1 to N Matching Methods"
        }, 
        {
            "location": "/other-analysis/propensity-score/#radius-matching", 
            "text": "All the control units within a certain distance are chosen (Dehejia and Wahba 1999).", 
            "title": "Radius Matching"
        }, 
        {
            "location": "/other-analysis/propensity-score/#mahalanobis-metric-matching", 
            "text": "In this type of matching, the definition of distance is changed. The similarity between the propensity score of treated and untreated units is evaluated using the multidimensional Mahalanobis metric matching:  $D_{ij}=\\sqrt{\\left ( x_i-y_j \\right )^TS^{-1}\\left ( x_i-y_j \\right )}$  where $S^{-1}$ is the pooled variance-covariance matrix and x and y are multivariate vectors. Note that if the variance-covariance\nmatrix is an identity matrix the Mahalanobis metric is reduced to the familiar Euclidean metric. Usually the Mahalanobis metric\nmatching includes the propensity score and other covariates that are considered to be important and are hoped to be balanced\n(Rosenbaum and Rubin 1985).", 
            "title": "Mahalanobis Metric Matching"
        }, 
        {
            "location": "/other-analysis/propensity-score/#psmatching-macro", 
            "text": "With the macro  PSMatching.sas  (Coca-Perraillon, 2006) different methods can be applied to calculate the matching once you have the propensity score. First you have to prepare the following input:   Treatment  data set containing the treatment cases along with the patient number  idT  and the corresponding propensity score  pscoreT  Control  data set containing the control cases along with the patient number  idC  and the corresponding propensity score  pscoreC   The parameters  datatreatment  and  datacontrol  refer to the Treatment and Control datasets and they do not need to be sorted.\nThe method parameter can be  NN  (nearest available neighbor),  caliper  or  radius . Caliper can be any number indicating the\nsize of the  caliper  and the parameter  replacement  takes the values of yes or no. All the parameters are case insensitive. If the\nmethod is  NN , the caliper parameter is ignored.  1 %PSMatching(datatreatment=treatment, datacontrol=control, method=caliper, numberofcontrols=1, caliper=0.2, replacement=no);    Here is the macro code:    1\n  2\n  3\n  4\n  5\n  6\n  7\n  8\n  9\n 10\n 11\n 12\n 13\n 14\n 15\n 16\n 17\n 18\n 19\n 20\n 21\n 22\n 23\n 24\n 25\n 26\n 27\n 28\n 29\n 30\n 31\n 32\n 33\n 34\n 35\n 36\n 37\n 38\n 39\n 40\n 41\n 42\n 43\n 44\n 45\n 46\n 47\n 48\n 49\n 50\n 51\n 52\n 53\n 54\n 55\n 56\n 57\n 58\n 59\n 60\n 61\n 62\n 63\n 64\n 65\n 66\n 67\n 68\n 69\n 70\n 71\n 72\n 73\n 74\n 75\n 76\n 77\n 78\n 79\n 80\n 81\n 82\n 83\n 84\n 85\n 86\n 87\n 88\n 89\n 90\n 91\n 92\n 93\n 94\n 95\n 96\n 97\n 98\n 99\n100 %macro   PSMatching ( datatreatment = ,   datacontrol = ,   method = ,   numberofcontrols = ,   caliper = , replacement = );  /* Create copies of the treated units if N   1 */ ;  data   _Treatment0 ( drop =   i );  set   Treatment ;  do   i =   1   to   numberofcontrols ;  RandomNumber =   ranuni ( 12345 );  output ;  end ;  run ;  /* Randomly sort both datasets */  proc   sort   data =   _Treatment0   out =   _Treatment ( drop =   RandomNumber );  by   RandomNumber ;  run ;  data   _Control0 ;  set   Control ;  RandomNumber =   ranuni ( 45678 );  run ;  proc   sort   data =   _Control0   out =   _Control ( drop =   RandomNumber );  by   RandomNumber ;  run ;  data   Matched ( keep   =   IdSelectedControl   MatchedToTreatID );  length   pscoreC   8 ;  length   idC   8 ;  /* Load Control dataset into the hash object */  if   _N_ =   1   then   do ;  declare   hash   h ( dataset :   _Control ,   ordered :   no );  declare   hiter   iter ( h );  h . defineKey ( idC );  h . defineData ( pscoreC ,   idC );  h . defineDone ();  call   missing ( idC ,   pscoreC );  end ;  /* Open the treatment */  set   _Treatment ;  %if   %upcase ( method )   ~=   RADIUS   %then   %do ;  retain   BestDistance   99 ;  %end ;  /* Iterate over the hash */  rc =   iter . first ();  if   ( rc = 0 )   then   BestDistance =   99 ;  do   while   ( rc   =   0 );  /* Caliper */  %if   %upcase ( method )   =   CALIPER   %then   %do ;  if   ( pscoreT   -   caliper )   =   pscoreC   =   ( pscoreT   +   caliper )   then   do ;  ScoreDistance   =   abs ( pscoreT   -   pscoreC );  if   ScoreDistance     BestDistance   then   do ;  BestDistance   =   ScoreDistance ;  IdSelectedControl   =   idC ;  MatchedToTreatID   =   idT ;  end ;  end ;  %end ;  /* NN */  %if   %upcase ( method )   =   NN   %then   %do ;  ScoreDistance   =   abs ( pscoreT   -   pscoreC );  if   ScoreDistance     BestDistance   then   do ;  BestDistance   =   ScoreDistance ;  IdSelectedControl   =   idC ;  MatchedToTreatID   =   idT ;  end ;  %end ;  %if   %upcase ( method )   =   NN   or   %upcase ( method )   =   CALIPER   %then   %do ;  rc   =   iter . next ();  /* Output the best control and remove it */  if   ( rc   ~=   0 )   and   BestDistance   ~= 99   then   do ;  output ;  %if   %upcase ( replacement )   =   NO   %then   %do ;  rc1   =   h . remove ( key :   IdSelectedControl );  %end ;  end ;  %end ;  /* Radius */  %if   %upcase ( method )   =   RADIUS   %then   %do ;  if   ( pscoreT   -   caliper )   =   pscoreC   =   ( pscoreT   +   caliper )   then   do ;  IdSelectedControl   =   idC ;  MatchedToTreatID   =   idT ;  output ;  end ;  rc   =   iter . next ();  %end ;  end ;  run ;  /* Delete temporary tables. Quote for debugging */  ods   select   none ;  proc   datasets ;  delete   _ :( gennum = all );  run ;  ods   select   all ;  %mend   PSMatching ;", 
            "title": "PSMatching Macro"
        }, 
        {
            "location": "/other-analysis/propensity-score/#proc-psmatch", 
            "text": "Here  and  here  you can find the documentation of this procedure.", 
            "title": "PROC PSMATCH"
        }, 
        {
            "location": "/other-analysis/propensity-score/#stratification-on-the-propensity-score", 
            "text": "Stratification on the propensity score involves  stratifying subjects into mutually exclusive subsets based on their estimated propensity score . Subjects are ranked according to their estimated propensity score. Subjects are then stratified into subsets based on previously defined thresholds of the estimated propensity score. A common approach is to  divide subjects into five equal-size groups using the quintiles of the estimated propensity score . Rosenbaum and Rubin (1984) stated that stratifying on the quintiles of the propensity score eliminates approximately 90% of the bias due to measured confounders when estimating a linear treatment effect. Within each propensity score stratum, treated and untreated subjects will have roughly similar values of the propensity score. Therefore, when the propensity score has been correctly specified, the distribution of measured baseline covariates will be approximately similar between treated and untreated subjects within the same stratum.  More info on this topic  here .", 
            "title": "Stratification on the Propensity Score"
        }, 
        {
            "location": "/other-analysis/roc-curve/", 
            "text": "Check these websites\n\n\n\n\nGenerating Receiver Operating Characteristic (ROC) curve using SAS Macros\n\n\n\n\n\n\nIn statistics, a \nreceiver operating characteristic curve (ROC curve)\n, is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied. The ROC curve is created by plotting the \ntrue positive rate (TPR) against the false positive rate (FPR)\n at various threshold settings. The true-positive rate is also known as \nsensitivity\n. The false-positive rate is also known as the fall-out and can be calculated as \n(1 \u2212 specificity)\n.\n\n\nROC analysis provides tools to \nselect possibly optimal models and to discard suboptimal ones\n independently from (and prior to specifying) the cost context or the class distribution.\n\n\nMacro \nROCPLOT\n\n\nProduce a plot of the \nReceiver Operating Characteristic (ROC)\n curve associated with a fitted binary-response model. Label points on the ROC curve using statistic or input variable values. Identify optimal cutpoints on the ROC curve using several optimality criteria such as correct classification, efficiency, cost, and others. Plot optimality criteria against a selected variable.\n\n\nWhen only an ROC plot with labeled points is needed, you can often produce the desired plot in \nPROC LOGISTIC\n without this macro. Using  \nODS graphics\n, \nPROC LOGISTIC\n can plot the ROC curve of a model whether applied to the data used to fit the model or to additional data scored using the fitted model. Specify the \nPLOTS=ROC\n option in the \nPROC LOGISTIC\n statement, or specify the \nOUTROC=\n option in the \nMODEL\n and/or \nSCORE\n statements.\n\n\n1\n2\n3\nPROC LOGISTIC (\u2026) plots(only)=roc(id=obs);\n    MODEL (\u2026) / OUTROC=ROC_data;\nRUN;\n\n\n\n\n\n\nMore information on this macro \nhere\n.", 
            "title": "ROC Curve"
        }, 
        {
            "location": "/other-analysis/roc-curve/#macro-rocplot", 
            "text": "Produce a plot of the  Receiver Operating Characteristic (ROC)  curve associated with a fitted binary-response model. Label points on the ROC curve using statistic or input variable values. Identify optimal cutpoints on the ROC curve using several optimality criteria such as correct classification, efficiency, cost, and others. Plot optimality criteria against a selected variable.  When only an ROC plot with labeled points is needed, you can often produce the desired plot in  PROC LOGISTIC  without this macro. Using   ODS graphics ,  PROC LOGISTIC  can plot the ROC curve of a model whether applied to the data used to fit the model or to additional data scored using the fitted model. Specify the  PLOTS=ROC  option in the  PROC LOGISTIC  statement, or specify the  OUTROC=  option in the  MODEL  and/or  SCORE  statements.  1\n2\n3 PROC LOGISTIC (\u2026) plots(only)=roc(id=obs);\n    MODEL (\u2026) / OUTROC=ROC_data;\nRUN;   More information on this macro  here .", 
            "title": "Macro ROCPLOT"
        }, 
        {
            "location": "/other-analysis/sequence-design/", 
            "text": "Theory of Hypothesis-Based Adaptive Design\n\n\nAn adaptive design is a design that allows adaptations or modifcations to some aspects of a trial after its initiation without undermining the validity and integrity of the trial. The adaptations may include, but are not limited to, sample-size reestimation, early stopping for effcacy or futility, response-adaptive randomization, and dropping inferior treatment groups. Adaptive designs usually require unblinding data and invoke a dependent sampling procedure. Therefore, theory behind adaptive design is much more complicated than that behind classical design.\n\n\nMany interesting methods for adaptive design have been developed. Virtually all methods can be viewed as some combination of stagewise p-values. The stagewise p-values are obtained based on the subsample from each stage; therefore, they are mutually independent and uniformly distributed over [0,1] under the null hypothesis. \n\n\nThe first method uses the same stopping boundaries as a classical group sequential design (\nO'Brien and Fleming\n, 1979; \nPocock\n, 1977) and allows stopping for early efficacy or futility. Lan and DeMets (1983) proposed the \nerror spending method (ESM)\n, in which the timing and number of analyses can be changed based on a prespecified error-spending function. ESM is derived from Brownian motion. The method has been extended to allow for sample-size reestimation (SSR) (Cui, Hung, and Wang, 1999). It can be viewed as a fixed-weight method (i.e., using fixed weights for z-scores from the first and second stages regardless of sample-size change). Lehmacher and Wassmer (1999) further degeneralized this weight method by using the inverse-normal method, in which the z-score is not necessarily taken from a normal endpoint, but from the inverse-normal function of stagewise p-values. Hence, the method can be used for any type of endpoint.\n\n\nO'Brien-Flemming\n\n\nLet us start considering a two-arm trial. The usual settings for randomized two-arm clinical trials are:\n\n\n\n\nResponse is dichotomous and immediate\n\n\nThey are single-phase trials, with sample sizes fixed in advance\n\n\nAt the end of a trial, compare success rates (i.e. proportions) using a formal test of significance based on the usual Pearson, is chi-squared test.\n\n\n\n\nThe aim is to form a multiple testing procedure that provides investigators with an opportunity to conduct periodic reviews of the data as they accumulate and thereby offers the chance for early termination should one treatment prove superior to the other early on while continuing to use essentially the single-phase decision rule should early termination not occur. The following is a brief description of the \nO'Brien-Flemming procedure\n:\n\n\n\n\nInvestigators plan to test $k$ times, including the final comparison at the end of the trial.\n\n\nData are reviewd periodically, with $m_1$ subjects receiving treatment 1 and $m_2$ subjects receiving treatment 2, between successive tests; there are a total of $k\\cdot (m_1+m_2)$ subjects.\n\n\nThe constraint is to maintain an overall size $\\alpha$, say, $\\alpha = 0.05$.\n\n\nRule: After the $n$th test, $1 \\le n \\le k$, the study is terminated and $H_0$ is rejected if $(n/k)X^2 \\ge P(k,\\alpha)$ where $X^2$ is the usual Pearson's chi-squared statistic.\n\n\n\n\nUsing the theory of Brownian motion, O'Brien and Fleming (1979) obtained the values for $P(k,\\alpha)$ but, more importantly, they concluded that they are approximately the $(1-\\alpha)th$ percentile of the chi-squared distribution with 1 degree of freedom -- almost independent of $k$.\n\n\nSAS code\n\n\nWe first calculate the boundaries for our study design according to the prerequisites.\n\n\nOne-sided example\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\nPROC SEQDESIGN ALTREF=0.25 BOUNDARYSCALE=PVALUE PLOTS=BOUNDARY(HSCALE=SAMPLESIZE) ERRSPEND;\nONESIDEDOBRIENFLEMING:\n        DESIGN \n        NSTAGES=3\n        METHOD=OBF\n        STOP=BOTH\n        ALT=UPPER\n        ALPHA=0.1 \n        BETA=0.2 \n        INFO=EQUAL;\n    SAMPLESIZE MODEL= TWOSAMPLEFREQ(NULLPROP=0.15 TEST=PROP WEIGHT=2);\n    ODS OUTPUT BOUNDARY=output-boundaries-values;\nRUN;\n\n\n\n\n\n\nTwo-sided example\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\nPROC SEQDESIGN ALTREF=0.25 BOUNDARYSCALE=PVALUE PLOTS=BOUNDARY(HSCALE=SAMPLESIZE) ERRSPEND;\nONESIDEDOBRIENFLEMING:\n        DESIGN \n        NSTAGES=3\n        METHOD=OBF\n        STOP=BOTH\n        ALT=TWOSIDED\n        ALPHA=0.2 \n        BETA=0.2 \n        INFO=EQUAL;\n    SAMPLESIZE MODEL= TWOSAMPLEFREQ(NULLPROP=0.15 TEST=PROP WEIGHT=2);\n    ODS OUTPUT BOUNDARY=output-boundaries-values;\nRUN;\n\n\n\n\n\n\nPROC SEQDESIGN\n parameters:\n\n \nALTREF =\n specifies alternative reference, 0.25 for a difference of 25%\n\n \nBOUNDARYSCALE = MLE | SCORE | STDZ | PVALUE\n specifies statistic scale for the boundary\n* \nERRSPEND\n displays cumulative error spending at each stage\n\n\nDESIGN\n statement parameters:\n\n \nNSTAGES =\n is the number of stages in the design (including the final stage)\n\n \nMETHOD =\n specifies methods for boundary values (\nOBF\n specifies the O'Brien-Fleming method)\n\n \nSTOP = ACCEPT | REJECT | BOTH\n specifies the condition of early stopping for the design\n\n \nALT = LOWER | UPPER | TWOSIDED\n specifies type of alternative hypothesis\n\n \nALPHA =\n and \nBETA=\n specify the Type I error probability level $\\alpha$ and the Type II error probability level $\\beta$\n\n \nINFO =\n specifies information levels (\nEQUAL\n for equally spaced information levels and \nCUM\n for cumulative relative information levels)\n\n\nSAMPLESIZE\n statement parameters:\n* \nMODEL = TWOSAMPLEFREQ \n ( options ) \n specifies the two-sample test for binomial proportions. The available options are as follows:\n    * The \nNULLPROP =\n option specifies proportions $p_a=p_{0a}$ and $p_b=p_{0b}$ in groups A and B, respectively, under the null hypothesis\n    * The \nTEST =\n option specifies the null hypothesis $H_0:\\theta=0$ in the test (\nPROP\n option uses the difference in proportions $\\theta=(p_a-p_b)-(p_{0a}-p_{0b})$, \nLOGOR\n option uses the log odds-ratio test and the \nLOGRR\n option uses the log relative risk test) \n    * The \nWEIGHT=\n option specifies the sample size allocation weights for the two groups", 
            "title": "Sequence Design"
        }, 
        {
            "location": "/other-analysis/sequence-design/#theory-of-hypothesis-based-adaptive-design", 
            "text": "An adaptive design is a design that allows adaptations or modifcations to some aspects of a trial after its initiation without undermining the validity and integrity of the trial. The adaptations may include, but are not limited to, sample-size reestimation, early stopping for effcacy or futility, response-adaptive randomization, and dropping inferior treatment groups. Adaptive designs usually require unblinding data and invoke a dependent sampling procedure. Therefore, theory behind adaptive design is much more complicated than that behind classical design.  Many interesting methods for adaptive design have been developed. Virtually all methods can be viewed as some combination of stagewise p-values. The stagewise p-values are obtained based on the subsample from each stage; therefore, they are mutually independent and uniformly distributed over [0,1] under the null hypothesis.   The first method uses the same stopping boundaries as a classical group sequential design ( O'Brien and Fleming , 1979;  Pocock , 1977) and allows stopping for early efficacy or futility. Lan and DeMets (1983) proposed the  error spending method (ESM) , in which the timing and number of analyses can be changed based on a prespecified error-spending function. ESM is derived from Brownian motion. The method has been extended to allow for sample-size reestimation (SSR) (Cui, Hung, and Wang, 1999). It can be viewed as a fixed-weight method (i.e., using fixed weights for z-scores from the first and second stages regardless of sample-size change). Lehmacher and Wassmer (1999) further degeneralized this weight method by using the inverse-normal method, in which the z-score is not necessarily taken from a normal endpoint, but from the inverse-normal function of stagewise p-values. Hence, the method can be used for any type of endpoint.", 
            "title": "Theory of Hypothesis-Based Adaptive Design"
        }, 
        {
            "location": "/other-analysis/sequence-design/#obrien-flemming", 
            "text": "Let us start considering a two-arm trial. The usual settings for randomized two-arm clinical trials are:   Response is dichotomous and immediate  They are single-phase trials, with sample sizes fixed in advance  At the end of a trial, compare success rates (i.e. proportions) using a formal test of significance based on the usual Pearson, is chi-squared test.   The aim is to form a multiple testing procedure that provides investigators with an opportunity to conduct periodic reviews of the data as they accumulate and thereby offers the chance for early termination should one treatment prove superior to the other early on while continuing to use essentially the single-phase decision rule should early termination not occur. The following is a brief description of the  O'Brien-Flemming procedure :   Investigators plan to test $k$ times, including the final comparison at the end of the trial.  Data are reviewd periodically, with $m_1$ subjects receiving treatment 1 and $m_2$ subjects receiving treatment 2, between successive tests; there are a total of $k\\cdot (m_1+m_2)$ subjects.  The constraint is to maintain an overall size $\\alpha$, say, $\\alpha = 0.05$.  Rule: After the $n$th test, $1 \\le n \\le k$, the study is terminated and $H_0$ is rejected if $(n/k)X^2 \\ge P(k,\\alpha)$ where $X^2$ is the usual Pearson's chi-squared statistic.   Using the theory of Brownian motion, O'Brien and Fleming (1979) obtained the values for $P(k,\\alpha)$ but, more importantly, they concluded that they are approximately the $(1-\\alpha)th$ percentile of the chi-squared distribution with 1 degree of freedom -- almost independent of $k$.", 
            "title": "O'Brien-Flemming"
        }, 
        {
            "location": "/other-analysis/sequence-design/#sas-code", 
            "text": "We first calculate the boundaries for our study design according to the prerequisites.  One-sided example   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 PROC SEQDESIGN ALTREF=0.25 BOUNDARYSCALE=PVALUE PLOTS=BOUNDARY(HSCALE=SAMPLESIZE) ERRSPEND;\nONESIDEDOBRIENFLEMING:\n        DESIGN \n        NSTAGES=3\n        METHOD=OBF\n        STOP=BOTH\n        ALT=UPPER\n        ALPHA=0.1 \n        BETA=0.2 \n        INFO=EQUAL;\n    SAMPLESIZE MODEL= TWOSAMPLEFREQ(NULLPROP=0.15 TEST=PROP WEIGHT=2);\n    ODS OUTPUT BOUNDARY=output-boundaries-values;\nRUN;   Two-sided example   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 PROC SEQDESIGN ALTREF=0.25 BOUNDARYSCALE=PVALUE PLOTS=BOUNDARY(HSCALE=SAMPLESIZE) ERRSPEND;\nONESIDEDOBRIENFLEMING:\n        DESIGN \n        NSTAGES=3\n        METHOD=OBF\n        STOP=BOTH\n        ALT=TWOSIDED\n        ALPHA=0.2 \n        BETA=0.2 \n        INFO=EQUAL;\n    SAMPLESIZE MODEL= TWOSAMPLEFREQ(NULLPROP=0.15 TEST=PROP WEIGHT=2);\n    ODS OUTPUT BOUNDARY=output-boundaries-values;\nRUN;   PROC SEQDESIGN  parameters:   ALTREF =  specifies alternative reference, 0.25 for a difference of 25%   BOUNDARYSCALE = MLE | SCORE | STDZ | PVALUE  specifies statistic scale for the boundary\n*  ERRSPEND  displays cumulative error spending at each stage  DESIGN  statement parameters:   NSTAGES =  is the number of stages in the design (including the final stage)   METHOD =  specifies methods for boundary values ( OBF  specifies the O'Brien-Fleming method)   STOP = ACCEPT | REJECT | BOTH  specifies the condition of early stopping for the design   ALT = LOWER | UPPER | TWOSIDED  specifies type of alternative hypothesis   ALPHA =  and  BETA=  specify the Type I error probability level $\\alpha$ and the Type II error probability level $\\beta$   INFO =  specifies information levels ( EQUAL  for equally spaced information levels and  CUM  for cumulative relative information levels)  SAMPLESIZE  statement parameters:\n*  MODEL = TWOSAMPLEFREQ   ( options )   specifies the two-sample test for binomial proportions. The available options are as follows:\n    * The  NULLPROP =  option specifies proportions $p_a=p_{0a}$ and $p_b=p_{0b}$ in groups A and B, respectively, under the null hypothesis\n    * The  TEST =  option specifies the null hypothesis $H_0:\\theta=0$ in the test ( PROP  option uses the difference in proportions $\\theta=(p_a-p_b)-(p_{0a}-p_{0b})$,  LOGOR  option uses the log odds-ratio test and the  LOGRR  option uses the log relative risk test) \n    * The  WEIGHT=  option specifies the sample size allocation weights for the two groups", 
            "title": "SAS code"
        }, 
        {
            "location": "/other-analysis/survival-analysis/", 
            "text": "What Is Survival Analysis?\n\n\n\n\nCheck these websites\n\n\n\n\nAn\u00e1lisis de supervivencia\n\n\nIntroduction to survival analysis in SAS\n\n\nA SAS Macro to Generate Information Rich Kaplan-Meier Plots\n\n\nKaplan-Meier Survival Plotting Macro %NEWSURV\n\n\nApplication of Survival Analysis in Multiple Events Using SAS\n\n\n\n\n\n\nIn many cancer studies, the main outcome under assessment is the time to an event of interest. The generic name for the time is\n\nsurvival time\n, although it may be applied to the time \u2018survived\u2019 from complete remission to relapse or progression as equally as to\nthe time from diagnosis to death. If the \nevent occurred in all individuals, many methods of analysis would be applicable\n. However,\nit is usual that at the end of follow-up some of the individuals \nhave not had the event of interest\n, and thus their true time to event is unknown. Further, survival data are \nrarely Normally distributed\n, but are skewed and comprise typically of many early events and relatively few late ones. It is these features of the data that make the special methods called survival analysis necessary.\n\n\nNonparametric methods\n (\nPROC LIFETEST\n) provide simple and quick looks at the survival experience and the \nCox proportional hazards regression model\n (\nPROC PHREG\n) remains the dominant analysis method.\n\n\nEndpoints in Clinical Trials Related to Survival Analysis\n\n\n\n\nCheck these websites\n\n\n\n\n\n\nEfficacy Endpoints in Oncology Clinical Trials\n\n\n\n\nOverall Survival\n\n\nOS is the \ngold standard for demonstrating clinical benefit\n. Defined as the time from \nrandomization to death\n, this endpoint is unambiguous and is not subject to investigator interpretation. Survival is a direct clinical benefit to patients, and assessment can be calculated to the day. Patient benefit can be described as superior survival or noninferior survival after consideration of toxicity and the magnitude of benefit. A noninferiority analysis ensures that a survival advantage associated with an approved drug will not be lost with a new agent.\n\n\nSurvival analysis requires a large sample size and may require long follow-up\n. Survival analysis may be confounded because of subsequent therapies administered after a study drug is discontinued. OS should be evaluated in randomized, controlled trials.\n\n\nTime to Tumor Progression and Progression-Free Survival\n\n\nTime to tumor progression (TTP)\n, is defined as the time from \nrandomization to time of progressive disease\n. The \nprogression-free survival (PFS)\n duration is defined as the time from \nrandomization to objective tumor progression or death\n. Compared with TTP, PFS may be a preferred regulatory endpoint because it includes death and may correlate better with OS. In TTP analysis, deaths are censored either at the time of death or at an earlier visit representing informative censoring (nonrandom pattern of loss from\nthe study). PFS assumes patient deaths are randomly related to tumor progression. However, in situations where the majority of deaths are unrelated to cancer, TTP can be an acceptable endpoint.\n\n\nAssessment of either PFS or TTP needs to be conducted in randomized trials. Because of the subjectivity that may be introduced in endpoint assessment, blinding of trials or the use of an external blinded review committee is recommended. In assessing TTP or PFS, patients must be evaluated on a regular basis in all treatment arms, and an assessment of all disease sites should be performed. To reduce bias, the same assessment technique should be used at each follow-up, and the same evaluation schedule should be consistently used.\n\n\nTime to Treatment Failure\n\n\nTime to treatment failure (TTF)\n is defined as the time from \nrandomization to treatment discontinuation for any reason\n, including disease progression, treatment toxicity, patient preference, or death. From a regulatory point of view, TTF is generally not accepted as a valid endpoint. TTF is a composite endpoint influenced by factors unrelated to efficacy. Discontinuation may be a result of toxicity, patient preference, or a physician's reluctance to continue therapy. These factors are not a direct assessment of the effectiveness of a drug.\n\n\nDuration of Response\n\n\nDuration of response (DoR)\n is usually measured as the time from documentation of tumor response to disease progression or death. Patients whose first documented response assessment is disease progression will be exluded from the DoR analysis.\n\n\nIt can be assessed in single-arm trials, requires a smaller population and can be assessed earlier (compared to survival trials) and its effect is attributable directly to the drug, not the natural history of the disease. However, is not a comprehensive measure of drug activity.\n\n\nUnderstanding the Basis of Survival Analysis\n\n\nSurvival data are generally described and modelled in terms of two related probabilities, namely \nsurvival\n and \nhazard\n. \n\n\n\n\nThe \nsurvival\n probability (which is also called the survivor function) $S(t)$ is the probability that an individual survives from the time origin to a specified future time $t$. It is fundamental to a survival analysis because survival probabilities for different values of $t$ provide crucial summary information from time to event data. These values describe directly the survival experience of a study cohort.\n\n\nThe \nhazard\n is usually denoted by $h(t)$ or $l(t)$ and is the probability that an individual who is under observation at a time $t$ has an event at that time. Put another way, it represents the instantaneous event rate for an individual who has already survived to time $t$. Note that, in contrast to the survivor function, which focuses on not having an event, the hazard function focuses on the event occurring. It is of interest because it provides insight into the conditional failure rates and provides a vehicle for specifying a survival model. \n\n\n\n\nIn summary, the hazard relates to the incident (current) event rate, while survival reflects the cumulative non-occurrence.\n\n\nUnderstanding the mechanics behind survival analysis is aided by facility with the distributions used, which can be derived from the \nprobability density function\n and \ncumulative density functions\n of survival times.\n\n\nThe Probability Density Function\n\n\nImagine we have a random variable, $Time$, which records survival times. The function that describes likelihood of observing $Time$ at time $t$ relative to all other survival times is known as the probability density function (\npdf\n), or $f(t)$. Integrating the pdf over a range of survival times gives the probability of observing a survival time within that interval. For example, if the survival times were known to be exponentially distributed, then the probability of observing a survival time within the interval $\\left [ a,b \\right ]$ is \n\n\n$Pr\\left ( a \\leq Time \\leq b \\right )=\\int_{a}^{b}f(t)dt=\\int_{a}^{b} \\lambda e^{-\\lambda t}dt$, \n\n\nwhere $\\lambda$ is the rate parameter of the exponential distribution and is equal to the reciprocal of the mean survival time.\n\n\nMost of the time we will not know \na priori\n the distribution generating our observed survival times, but we can get and idea of what it looks like using nonparametric methods in SAS with \nPROC UNIVARIATE\n. \n\n\n1\n2\n3\n4\nPROC UNIVARIATE DATA=SAS-data-set(WHERE=(censoring-variable=1));\n    VAR survival-time-variable;\n    HISTOGRAM survival-time-variable / KERNEL;\nRUN;\n\n\n\n\n\n\nIn the graph above we see the correspondence between pdfs and histograms. Density functions are essentially histograms comprised of bins of vanishingly small widths. Technically, because there are no times less than $0$, there should be no graph to the left $Time=0$.\n\n\nThe Cumulative Distribution Function\n\n\nThe cumulative distribution function (\ncdf\n), $F(t)$, describes the probability of observing $Time$ less than or equal to some time $t$, or $Pr(Time \\le t)$. Above we described that integrating the pdf over some range yields the probability of observing $Time$ in that range. Thus, we define the cumulative distribution function as:\n\n\n$F(t)=\\int_{0}^{t}f(t)dt$\n\n\nThe above relationship between the cdf and pdf also implies:\n\n\n$f(t)=\\frac{dF(t)}{dt}$\n\n\nIn SAS, we can graph an estimate of the cdf using \nPROC UNIVARIATE\n.\n\n\n1\n2\n3\n4\nPROC UNIVARIATE DATA=SAS-data-set(WHERE=(censoring-variable=1));\n    VAR survival-time-variable;\n    CDFPLOT survival-time-variable;\nRUN;\n\n\n\n\n\n\nIn the graph produced with the code above we can check the probability of surviving a number of days. In intervals where event times are more probable, the cdf will increase faster.\n\n\nThe Survival Function\n\n\nA simple transformation of the cumulative distribution function produces the survival function, $S(t)$:\n\n\n$S(t) = 1 - F(T)$.\n\n\nThe survivor function, $S(t)$, describes the probability of surviving past time $t$, or $Pr(Time \n t)$. If we were to plot the estimate of $S(t)$, we would see that it is a reflection of $F(t)$ (about $y=0$ and shifted up by $1$). We can use \nPROC LIFETEST\n to graph $S(t)$.\n\n\n1\n2\n3\nPROC LIFETEST DATA=SAS-data-set(WHERE=(censoring-variable=1)) PLOTS=SURVIVAL(ATRISK);\n    TIME survival-time-variable*censoring-variable(0);\nRUN; \n\n\n\n\n\n\nThe probability of surviving beyond a number of days according to this survival plot can be confirmed by the cdf produced above, where the probability of surviving a number of days or fewer is equivalent.\n\n\nThe Hazard Function\n\n\nThe primary focus of survival analysis is typically to \nmodel the hazard rate\n, which has the following relationship with the $f(t)$ and $S(t)$:\n\n\n$h(t)=\\frac{f(t)}{S(t)}$\n\n\nThe hazard function, then, describes the relative likelihood of the event occurring at time $t(f(t))$, conditional on the subject\u2019s survival up to that time $t(S(t))$. The hazard rate thus describes the instantaneous rate of failure at time $t$ and ignores the accumulation of hazard up to time $t$ (unlike $F(t)$ and $S(t)$). We can estimate the hazard function is SAS as well using \nPROC LIFETEST\n:\n\n\n1\n2\n3\nPROC LIFETEST DATA=SAS-data-set(WHERE=(censoring-variable=1)) PLOTS=HAZARD(BW=200); /* BW = Bandwidth*/\n    TIME survival-time-variable*censoring-variable(0);\nRUN; \n\n\n\n\n\n\nThe plot shows the Estimated Hazard Rate which is the expected number of failures per time unit (per day in our example).\n\n\nThe Cumulative Hazard Function\n\n\nAlso useful to understand is the cumulative hazard function, which as the name implies, cumulates hazards over time. It is calculated by integrating the hazard function over an interval of time:\n\n\n$H(t)=\\int_{0}^{t}h(u)du$\n\n\nLet us again think of the hazard function, $h(t)$, as the rate at which failures occur at time $t$. Let us further suppose, for illustrative purposes, that the hazard rate stays constant at $\\frac{x}{t}$ ($x$ number of failures per unit time $t$) over the interval $[0,t]$. Summing over the entire interval, then, we would expect to observe $x$ failures, as $\\frac{x}{t} t = x$, (assuming repeated failures are possible, such that failing does not remove one from observation). One interpretation of the cumulative hazard function is thus the expected number of failures over time interval $[0,t]$. It is not at all necessary that the hazard function stay constant for the above interpretation of the cumulative hazard function to hold, but for illustrative purposes it is easier to calculate the expected number of failures since integration is not needed. Expressing the above relationship as $\\frac{d}{dt}H(t)=h(t)$, we see that the hazard function describes the rate at which hazards are accumulated over time.\n\n\nUsing the equations, $h(t)=\\frac{f(t)}{S(t)}$ and $f(t)=\u2212\\frac{dS}{dt}$, we can derive the following relationships between the cumulative hazard function and the other survival functions:\n\n\n$S(t)=exp(-H(t))$\n\n\n$F(t)=1-exp(-H(t))$\n\n\n$f(t)=h(t) \\cdot exp(-H(t))$\n\n\nFrom these equations we can see that the cumulative hazard function $H(t)$ and the survival function $S(t)$ have a simple monotonic relationship, such that when the Survival function is at its maximum at the beginning of analysis time, the cumulative hazard function is at its minimum. As time progresses, the Survival function proceeds towards it minimum, while the cumulative hazard function proceeds to its maximum. From these equations we can also see that we would expect the pdf, $f(t)$, to be high when $h(t)$ the hazard rate is high (its location depends on the study) and when the cumulative hazard $H(t)$ is low (the beginning, for all studies). In other words, we would expect to find a lot of failure times in a given time interval if \n\n\n\n\nthe hazard rate is high and \n\n\nthere are still a lot of subjects at-risk.\n\n\n\n\nWe can estimate the cumulative hazard function using \nPROC LIFETEST\n, the results of which we send to \nPROC SGPLOT\n for plotting. \n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nODS OUTPUT ProductLimitEstimates=PLE;\n\nPROC LIFETEST DATA=SAS-data-set whas500(WHERE=(censoring-variable=1)) NELSON OUTS=output-name;\n    TIME survival-time-variable*censoring-variable(0);\nRUN;\n\nPROC SGPLOT DATA=PLE;\n    SERIES x = survival-time-variable y = CumHaz;\nRUN;\n\n\n\n\n\n\nData Preparation and Exploration\n\n\nStructure of the Data\n\n\nTo work with \nPROC LIFETEST\n and \nPROC PHREG\n for survival analysis, data can be structured in one of these 2 ways:\n\n\n\n\nOne row of data per subject\n, with one outcome variable representing the time to event, one variable that codes for whether the event occurred or not (censored), and explanatory variables of interest, each with fixed values across follow up time. Both \nPROC LIFETEST\n and \nPROC PHREG\n will accept data structured this way.\n\n\nMultiple rows of data per subject\n (only accepted by \nPROC PHREG\n) following the \"counting process\" style of input. For each subject, the whole follow up period is partitioned into intervals, each defined by a \"start\" and \"stop\" time. Covariates are permitted to change value between intervals. Additionally, another variable counts the number of events occurring in each interval (either 0 or 1 in Cox regression, same as the censoring variable). This structuring allows the modeling of time-varying covariates, or explanatory variables whose values change across follow-up time. \n\n\n\n\nData that are structured in the first, single-row way can be modified to be structured like the second, multi-row way, but the reverse is typically not true.\n\n\nData Exploration with \nPROC UNIVARIATE\n and \nPROC CORR\n\n\nAny serious endeavor into data analysis should begin with data exploration, in which the researcher becomes familiar with the distributions and typical values of each variable individually, as well as relationships between pairs or sets of variables. Within SAS, \nPROC UNIVARIATE\n provides easy, quick looks into the distributions of each variable, whereas \nPROC CORR\n can be used to examine bivariate relationships.\n\n\nNonparametric Methods (\nPROC LIFETEST\n)\n\n\n1\n2\n3\n4\n5\n6\n7\n8\noms = (lastcontact - starttreatment + 1) / 30.45\nif exdate = . then censor = 1;\nelse censor = 0;\n\nPROC LIFETEST DATA=SAS-data-set plots=(s);\n  TIME osm * censor(1);   \n    STRATA alg              /* aleatorization group, (= . they didn\nt get to randomization) */\nRUN;\n\n\n\n\n\n\n\n\nIn the \nTIME\n statement, only patients that haven't been censored are analysed \n\n\nThe \nSTRATA\n statement includes only non-missing data points (no \nWHERE\n filtering is needed) \n\n\n\n\n\n\nTip\n\n\nIf you are performing a survival analysis which only applies to part of your population but you need the probabilities to be referred to the total population just define the excluded subpopulation as having event at time = 0. This way your plot will not start at 1/0. This applies, for example, to \nlocoregional control time\n plots on which patients without complete response are excluded (event at time = 0) and \n\n\n\n\nP-value Calculation\n\n\nWe select only 2 groups from the test data set (High and Low risk):\n\n\n1\n2\n3\n4\nDATA bmt_small;\n    SET SASHELP.bmt;\n    WHERE group IN (2, 3);\nRUN;\n\n\n\n\n\n\nWe generate the \nOUTSURV\n data set:\n\n\n1\n2\n3\n4\nPROC LIFETEST DATA=bmt_small PLOTS=SURVIVAL(CL CB=HW STRATA=PANEL) METHOD=LT INTERVALS=(0 to 800 by 100) OUTSURV=bmt_param STDERR;\n   TIME t * status(0);\n   STRATA group / ORDER=INTERNAL; \nrun;\n\n\n\n\n\n\nWe calculate the p-values from this data:\n\n\n1\n2\n3\n4\n5\n6\n7\nPROC SQL;\n    SELECT t, range(survival) AS RangeSurvival, sqrt(sum(sdf_stderr**2)) AS Squares, range(survival)/sqrt(sum(sdf_stderr**2)) AS z,\n           probnorm(abs(range(survival)/sqrt(sum(sdf_stderr**2)))) AS pz, 2 * (1-probnorm(abs(range(survival)/sqrt(sum(sdf_stderr**2))))) AS pvalue\n    FROM btm_param \n    WHERE t \n 0\n    GROUP BY t;\nQUIT;\n\n\n\n\n\n\nOther method:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\nDATA bmt600 ;\n  brazo=\nA\n; valor=1; contador=32; OUTPUT;\n  brazo=\nA\n; valor=2; contador=13; OUTPUT;\n  brazo=\nB\n; valor=1; contador=18; OUTPUT;\n  brazo=\nB\n; valor=2; contador=36; OUTPUT;\nRUN;\n\nPROC FREQ DATA=bmt600;\n    TABLES brazo*valor / CHISQ MEASURES RISKDIFF PLOTS=(FREQPLOT(TWOWAY=GROUPVERTICAL SCALE=PERCENT));\n    WEIGHT contador;\nRUN;\n\n\n\n\n\n\nCox Proportional Hazards Regression Model (\nPROC PHREG\n)\n\n\nCensoring\n\n\n\n\nRight-censoring\n: for some subjects we do not know when they died after the issue, but we do know at least how many days they survived.\n\n\n\n\nInformative Censoring\n\n\n\n\nCheck these papers\n\n\n\n\nCensoring in survival analysis: Potential for bias\n\n\nImpact of Informative Censoring on the Kaplan-Meier Estimate of Progression-Free Survival in Phase II Clinical Trials", 
            "title": "Survival Analysis"
        }, 
        {
            "location": "/other-analysis/survival-analysis/#what-is-survival-analysis", 
            "text": "Check these websites   An\u00e1lisis de supervivencia  Introduction to survival analysis in SAS  A SAS Macro to Generate Information Rich Kaplan-Meier Plots  Kaplan-Meier Survival Plotting Macro %NEWSURV  Application of Survival Analysis in Multiple Events Using SAS    In many cancer studies, the main outcome under assessment is the time to an event of interest. The generic name for the time is survival time , although it may be applied to the time \u2018survived\u2019 from complete remission to relapse or progression as equally as to\nthe time from diagnosis to death. If the  event occurred in all individuals, many methods of analysis would be applicable . However,\nit is usual that at the end of follow-up some of the individuals  have not had the event of interest , and thus their true time to event is unknown. Further, survival data are  rarely Normally distributed , but are skewed and comprise typically of many early events and relatively few late ones. It is these features of the data that make the special methods called survival analysis necessary.  Nonparametric methods  ( PROC LIFETEST ) provide simple and quick looks at the survival experience and the  Cox proportional hazards regression model  ( PROC PHREG ) remains the dominant analysis method.", 
            "title": "What Is Survival Analysis?"
        }, 
        {
            "location": "/other-analysis/survival-analysis/#endpoints-in-clinical-trials-related-to-survival-analysis", 
            "text": "Check these websites    Efficacy Endpoints in Oncology Clinical Trials", 
            "title": "Endpoints in Clinical Trials Related to Survival Analysis"
        }, 
        {
            "location": "/other-analysis/survival-analysis/#overall-survival", 
            "text": "OS is the  gold standard for demonstrating clinical benefit . Defined as the time from  randomization to death , this endpoint is unambiguous and is not subject to investigator interpretation. Survival is a direct clinical benefit to patients, and assessment can be calculated to the day. Patient benefit can be described as superior survival or noninferior survival after consideration of toxicity and the magnitude of benefit. A noninferiority analysis ensures that a survival advantage associated with an approved drug will not be lost with a new agent.  Survival analysis requires a large sample size and may require long follow-up . Survival analysis may be confounded because of subsequent therapies administered after a study drug is discontinued. OS should be evaluated in randomized, controlled trials.", 
            "title": "Overall Survival"
        }, 
        {
            "location": "/other-analysis/survival-analysis/#time-to-tumor-progression-and-progression-free-survival", 
            "text": "Time to tumor progression (TTP) , is defined as the time from  randomization to time of progressive disease . The  progression-free survival (PFS)  duration is defined as the time from  randomization to objective tumor progression or death . Compared with TTP, PFS may be a preferred regulatory endpoint because it includes death and may correlate better with OS. In TTP analysis, deaths are censored either at the time of death or at an earlier visit representing informative censoring (nonrandom pattern of loss from\nthe study). PFS assumes patient deaths are randomly related to tumor progression. However, in situations where the majority of deaths are unrelated to cancer, TTP can be an acceptable endpoint.  Assessment of either PFS or TTP needs to be conducted in randomized trials. Because of the subjectivity that may be introduced in endpoint assessment, blinding of trials or the use of an external blinded review committee is recommended. In assessing TTP or PFS, patients must be evaluated on a regular basis in all treatment arms, and an assessment of all disease sites should be performed. To reduce bias, the same assessment technique should be used at each follow-up, and the same evaluation schedule should be consistently used.", 
            "title": "Time to Tumor Progression and Progression-Free Survival"
        }, 
        {
            "location": "/other-analysis/survival-analysis/#time-to-treatment-failure", 
            "text": "Time to treatment failure (TTF)  is defined as the time from  randomization to treatment discontinuation for any reason , including disease progression, treatment toxicity, patient preference, or death. From a regulatory point of view, TTF is generally not accepted as a valid endpoint. TTF is a composite endpoint influenced by factors unrelated to efficacy. Discontinuation may be a result of toxicity, patient preference, or a physician's reluctance to continue therapy. These factors are not a direct assessment of the effectiveness of a drug.", 
            "title": "Time to Treatment Failure"
        }, 
        {
            "location": "/other-analysis/survival-analysis/#duration-of-response", 
            "text": "Duration of response (DoR)  is usually measured as the time from documentation of tumor response to disease progression or death. Patients whose first documented response assessment is disease progression will be exluded from the DoR analysis.  It can be assessed in single-arm trials, requires a smaller population and can be assessed earlier (compared to survival trials) and its effect is attributable directly to the drug, not the natural history of the disease. However, is not a comprehensive measure of drug activity.", 
            "title": "Duration of Response"
        }, 
        {
            "location": "/other-analysis/survival-analysis/#understanding-the-basis-of-survival-analysis", 
            "text": "Survival data are generally described and modelled in terms of two related probabilities, namely  survival  and  hazard .    The  survival  probability (which is also called the survivor function) $S(t)$ is the probability that an individual survives from the time origin to a specified future time $t$. It is fundamental to a survival analysis because survival probabilities for different values of $t$ provide crucial summary information from time to event data. These values describe directly the survival experience of a study cohort.  The  hazard  is usually denoted by $h(t)$ or $l(t)$ and is the probability that an individual who is under observation at a time $t$ has an event at that time. Put another way, it represents the instantaneous event rate for an individual who has already survived to time $t$. Note that, in contrast to the survivor function, which focuses on not having an event, the hazard function focuses on the event occurring. It is of interest because it provides insight into the conditional failure rates and provides a vehicle for specifying a survival model.    In summary, the hazard relates to the incident (current) event rate, while survival reflects the cumulative non-occurrence.  Understanding the mechanics behind survival analysis is aided by facility with the distributions used, which can be derived from the  probability density function  and  cumulative density functions  of survival times.", 
            "title": "Understanding the Basis of Survival Analysis"
        }, 
        {
            "location": "/other-analysis/survival-analysis/#the-probability-density-function", 
            "text": "Imagine we have a random variable, $Time$, which records survival times. The function that describes likelihood of observing $Time$ at time $t$ relative to all other survival times is known as the probability density function ( pdf ), or $f(t)$. Integrating the pdf over a range of survival times gives the probability of observing a survival time within that interval. For example, if the survival times were known to be exponentially distributed, then the probability of observing a survival time within the interval $\\left [ a,b \\right ]$ is   $Pr\\left ( a \\leq Time \\leq b \\right )=\\int_{a}^{b}f(t)dt=\\int_{a}^{b} \\lambda e^{-\\lambda t}dt$,   where $\\lambda$ is the rate parameter of the exponential distribution and is equal to the reciprocal of the mean survival time.  Most of the time we will not know  a priori  the distribution generating our observed survival times, but we can get and idea of what it looks like using nonparametric methods in SAS with  PROC UNIVARIATE .   1\n2\n3\n4 PROC UNIVARIATE DATA=SAS-data-set(WHERE=(censoring-variable=1));\n    VAR survival-time-variable;\n    HISTOGRAM survival-time-variable / KERNEL;\nRUN;   In the graph above we see the correspondence between pdfs and histograms. Density functions are essentially histograms comprised of bins of vanishingly small widths. Technically, because there are no times less than $0$, there should be no graph to the left $Time=0$.", 
            "title": "The Probability Density Function"
        }, 
        {
            "location": "/other-analysis/survival-analysis/#the-cumulative-distribution-function", 
            "text": "The cumulative distribution function ( cdf ), $F(t)$, describes the probability of observing $Time$ less than or equal to some time $t$, or $Pr(Time \\le t)$. Above we described that integrating the pdf over some range yields the probability of observing $Time$ in that range. Thus, we define the cumulative distribution function as:  $F(t)=\\int_{0}^{t}f(t)dt$  The above relationship between the cdf and pdf also implies:  $f(t)=\\frac{dF(t)}{dt}$  In SAS, we can graph an estimate of the cdf using  PROC UNIVARIATE .  1\n2\n3\n4 PROC UNIVARIATE DATA=SAS-data-set(WHERE=(censoring-variable=1));\n    VAR survival-time-variable;\n    CDFPLOT survival-time-variable;\nRUN;   In the graph produced with the code above we can check the probability of surviving a number of days. In intervals where event times are more probable, the cdf will increase faster.", 
            "title": "The Cumulative Distribution Function"
        }, 
        {
            "location": "/other-analysis/survival-analysis/#the-survival-function", 
            "text": "A simple transformation of the cumulative distribution function produces the survival function, $S(t)$:  $S(t) = 1 - F(T)$.  The survivor function, $S(t)$, describes the probability of surviving past time $t$, or $Pr(Time   t)$. If we were to plot the estimate of $S(t)$, we would see that it is a reflection of $F(t)$ (about $y=0$ and shifted up by $1$). We can use  PROC LIFETEST  to graph $S(t)$.  1\n2\n3 PROC LIFETEST DATA=SAS-data-set(WHERE=(censoring-variable=1)) PLOTS=SURVIVAL(ATRISK);\n    TIME survival-time-variable*censoring-variable(0);\nRUN;    The probability of surviving beyond a number of days according to this survival plot can be confirmed by the cdf produced above, where the probability of surviving a number of days or fewer is equivalent.", 
            "title": "The Survival Function"
        }, 
        {
            "location": "/other-analysis/survival-analysis/#the-hazard-function", 
            "text": "The primary focus of survival analysis is typically to  model the hazard rate , which has the following relationship with the $f(t)$ and $S(t)$:  $h(t)=\\frac{f(t)}{S(t)}$  The hazard function, then, describes the relative likelihood of the event occurring at time $t(f(t))$, conditional on the subject\u2019s survival up to that time $t(S(t))$. The hazard rate thus describes the instantaneous rate of failure at time $t$ and ignores the accumulation of hazard up to time $t$ (unlike $F(t)$ and $S(t)$). We can estimate the hazard function is SAS as well using  PROC LIFETEST :  1\n2\n3 PROC LIFETEST DATA=SAS-data-set(WHERE=(censoring-variable=1)) PLOTS=HAZARD(BW=200); /* BW = Bandwidth*/\n    TIME survival-time-variable*censoring-variable(0);\nRUN;    The plot shows the Estimated Hazard Rate which is the expected number of failures per time unit (per day in our example).", 
            "title": "The Hazard Function"
        }, 
        {
            "location": "/other-analysis/survival-analysis/#the-cumulative-hazard-function", 
            "text": "Also useful to understand is the cumulative hazard function, which as the name implies, cumulates hazards over time. It is calculated by integrating the hazard function over an interval of time:  $H(t)=\\int_{0}^{t}h(u)du$  Let us again think of the hazard function, $h(t)$, as the rate at which failures occur at time $t$. Let us further suppose, for illustrative purposes, that the hazard rate stays constant at $\\frac{x}{t}$ ($x$ number of failures per unit time $t$) over the interval $[0,t]$. Summing over the entire interval, then, we would expect to observe $x$ failures, as $\\frac{x}{t} t = x$, (assuming repeated failures are possible, such that failing does not remove one from observation). One interpretation of the cumulative hazard function is thus the expected number of failures over time interval $[0,t]$. It is not at all necessary that the hazard function stay constant for the above interpretation of the cumulative hazard function to hold, but for illustrative purposes it is easier to calculate the expected number of failures since integration is not needed. Expressing the above relationship as $\\frac{d}{dt}H(t)=h(t)$, we see that the hazard function describes the rate at which hazards are accumulated over time.  Using the equations, $h(t)=\\frac{f(t)}{S(t)}$ and $f(t)=\u2212\\frac{dS}{dt}$, we can derive the following relationships between the cumulative hazard function and the other survival functions:  $S(t)=exp(-H(t))$  $F(t)=1-exp(-H(t))$  $f(t)=h(t) \\cdot exp(-H(t))$  From these equations we can see that the cumulative hazard function $H(t)$ and the survival function $S(t)$ have a simple monotonic relationship, such that when the Survival function is at its maximum at the beginning of analysis time, the cumulative hazard function is at its minimum. As time progresses, the Survival function proceeds towards it minimum, while the cumulative hazard function proceeds to its maximum. From these equations we can also see that we would expect the pdf, $f(t)$, to be high when $h(t)$ the hazard rate is high (its location depends on the study) and when the cumulative hazard $H(t)$ is low (the beginning, for all studies). In other words, we would expect to find a lot of failure times in a given time interval if    the hazard rate is high and   there are still a lot of subjects at-risk.   We can estimate the cumulative hazard function using  PROC LIFETEST , the results of which we send to  PROC SGPLOT  for plotting.   1\n2\n3\n4\n5\n6\n7\n8\n9 ODS OUTPUT ProductLimitEstimates=PLE;\n\nPROC LIFETEST DATA=SAS-data-set whas500(WHERE=(censoring-variable=1)) NELSON OUTS=output-name;\n    TIME survival-time-variable*censoring-variable(0);\nRUN;\n\nPROC SGPLOT DATA=PLE;\n    SERIES x = survival-time-variable y = CumHaz;\nRUN;", 
            "title": "The Cumulative Hazard Function"
        }, 
        {
            "location": "/other-analysis/survival-analysis/#data-preparation-and-exploration", 
            "text": "", 
            "title": "Data Preparation and Exploration"
        }, 
        {
            "location": "/other-analysis/survival-analysis/#structure-of-the-data", 
            "text": "To work with  PROC LIFETEST  and  PROC PHREG  for survival analysis, data can be structured in one of these 2 ways:   One row of data per subject , with one outcome variable representing the time to event, one variable that codes for whether the event occurred or not (censored), and explanatory variables of interest, each with fixed values across follow up time. Both  PROC LIFETEST  and  PROC PHREG  will accept data structured this way.  Multiple rows of data per subject  (only accepted by  PROC PHREG ) following the \"counting process\" style of input. For each subject, the whole follow up period is partitioned into intervals, each defined by a \"start\" and \"stop\" time. Covariates are permitted to change value between intervals. Additionally, another variable counts the number of events occurring in each interval (either 0 or 1 in Cox regression, same as the censoring variable). This structuring allows the modeling of time-varying covariates, or explanatory variables whose values change across follow-up time.    Data that are structured in the first, single-row way can be modified to be structured like the second, multi-row way, but the reverse is typically not true.", 
            "title": "Structure of the Data"
        }, 
        {
            "location": "/other-analysis/survival-analysis/#data-exploration-with-proc-univariate-and-proc-corr", 
            "text": "Any serious endeavor into data analysis should begin with data exploration, in which the researcher becomes familiar with the distributions and typical values of each variable individually, as well as relationships between pairs or sets of variables. Within SAS,  PROC UNIVARIATE  provides easy, quick looks into the distributions of each variable, whereas  PROC CORR  can be used to examine bivariate relationships.", 
            "title": "Data Exploration with PROC UNIVARIATE and PROC CORR"
        }, 
        {
            "location": "/other-analysis/survival-analysis/#nonparametric-methods-proc-lifetest", 
            "text": "1\n2\n3\n4\n5\n6\n7\n8 oms = (lastcontact - starttreatment + 1) / 30.45\nif exdate = . then censor = 1;\nelse censor = 0;\n\nPROC LIFETEST DATA=SAS-data-set plots=(s);\n  TIME osm * censor(1);   \n    STRATA alg              /* aleatorization group, (= . they didn t get to randomization) */\nRUN;    In the  TIME  statement, only patients that haven't been censored are analysed   The  STRATA  statement includes only non-missing data points (no  WHERE  filtering is needed)     Tip  If you are performing a survival analysis which only applies to part of your population but you need the probabilities to be referred to the total population just define the excluded subpopulation as having event at time = 0. This way your plot will not start at 1/0. This applies, for example, to  locoregional control time  plots on which patients without complete response are excluded (event at time = 0) and", 
            "title": "Nonparametric Methods (PROC LIFETEST)"
        }, 
        {
            "location": "/other-analysis/survival-analysis/#p-value-calculation", 
            "text": "We select only 2 groups from the test data set (High and Low risk):  1\n2\n3\n4 DATA bmt_small;\n    SET SASHELP.bmt;\n    WHERE group IN (2, 3);\nRUN;   We generate the  OUTSURV  data set:  1\n2\n3\n4 PROC LIFETEST DATA=bmt_small PLOTS=SURVIVAL(CL CB=HW STRATA=PANEL) METHOD=LT INTERVALS=(0 to 800 by 100) OUTSURV=bmt_param STDERR;\n   TIME t * status(0);\n   STRATA group / ORDER=INTERNAL; \nrun;   We calculate the p-values from this data:  1\n2\n3\n4\n5\n6\n7 PROC SQL;\n    SELECT t, range(survival) AS RangeSurvival, sqrt(sum(sdf_stderr**2)) AS Squares, range(survival)/sqrt(sum(sdf_stderr**2)) AS z,\n           probnorm(abs(range(survival)/sqrt(sum(sdf_stderr**2)))) AS pz, 2 * (1-probnorm(abs(range(survival)/sqrt(sum(sdf_stderr**2))))) AS pvalue\n    FROM btm_param \n    WHERE t   0\n    GROUP BY t;\nQUIT;   Other method:   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 DATA bmt600 ;\n  brazo= A ; valor=1; contador=32; OUTPUT;\n  brazo= A ; valor=2; contador=13; OUTPUT;\n  brazo= B ; valor=1; contador=18; OUTPUT;\n  brazo= B ; valor=2; contador=36; OUTPUT;\nRUN;\n\nPROC FREQ DATA=bmt600;\n    TABLES brazo*valor / CHISQ MEASURES RISKDIFF PLOTS=(FREQPLOT(TWOWAY=GROUPVERTICAL SCALE=PERCENT));\n    WEIGHT contador;\nRUN;", 
            "title": "P-value Calculation"
        }, 
        {
            "location": "/other-analysis/survival-analysis/#cox-proportional-hazards-regression-model-proc-phreg", 
            "text": "", 
            "title": "Cox Proportional Hazards Regression Model (PROC PHREG)"
        }, 
        {
            "location": "/other-analysis/survival-analysis/#censoring", 
            "text": "Right-censoring : for some subjects we do not know when they died after the issue, but we do know at least how many days they survived.", 
            "title": "Censoring"
        }, 
        {
            "location": "/other-analysis/survival-analysis/#informative-censoring", 
            "text": "Check these papers   Censoring in survival analysis: Potential for bias  Impact of Informative Censoring on the Kaplan-Meier Estimate of Progression-Free Survival in Phase II Clinical Trials", 
            "title": "Informative Censoring"
        }, 
        {
            "location": "/other-analysis/orr-dcr/", 
            "text": "Objective Response Rate (ORR)\n\n\nIn medicine, a \nresponse rate\n is the percentage of patients whose cancer shrinks or disappears after treatment. When used as a clinical endpoint for clinical trials of cancer treatments, this often called the \nobjective response rate (ORR)\n. The FDA definition of ORR is \n\"the proportion of patients with tumor size reduction of a predefined amount and for a minimum time period\"\n. \n\n\nWhen defined in this manner, ORR is a direct measure of drug antitumor activity, which can be evaluated in a single-arm study. Stable disease should not be a component of ORR. Stable disease can reflect the natural history of disease, whereas tumor reduction is a direct therapeutic effect.\n\n\nDisease Control Rate (DCR)\n\n\nThe \ndisease control rate (DCR)\n or \nclinical benefit rate (CBR)\n is defined as the percentage of patients with advanced or metastatic cancer who have achieved complete response, partial response or stable disease to a therapeutic intervention in clinical trials of anticancer agents. \n\n\nCalculation\n\n\n\n\nRemember\n\n\nORR and DCR are usually calculated both taking into account \nnot evaluable (NE)\n best responses and without them included in the total number of patients.\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\ndata orr_dcr;\n    input pt $ bestresp $ orr dcr c_orr $ c_dcr $;\n    datalines;\n001 PD 0 0 N N\n002 SD 0 1 N Y\n003 SD 0 1 N Y\n004 PD 0 0 N N\n005 PD 0 0 N N \n006 PD 0 0 N N \n007 SD 0 1 N Y\n008 SD 0 1 N Y\n009 PD 0 0 N N \n010 SD 0 1 N Y\n011 PR 1 1 Y Y \n012 PR 1 1 Y Y \n;\nrun;\n\nods exclude all;\nproc freq data=orr_dcr;\n    table orr / nocum binomial(exact level=2) alpha=0.05;\n    ods output BinomialCLs=orr_prop_ci;\nrun;\nproc freq data=orr_dcr;\n    table dcr / nocum binomial(exact level=2) alpha=0.05;\n    ods output BinomialCLs=dcr_prop_ci;\nrun;\nods exclude none;\n\ndata orr_dcr_prop_ci;\n    set orr_prop_ci dcr_prop_ci;\n    propci = trim(left(put(Proportion*100,5.2)))||\n (\n||trim(left(put(LowerCL*100,5.2)))||\n, \n||trim(left(put(UpperCL*100,5.2)))||\n)\n;\n    prop = trim(left(put(Proportion*100,5.2)));\n    ci = trim(left(put(LowerCL*100,5.2)))||\n, \n||trim(left(put(UpperCL*100,5.2)));\n    if table = \nTable orr\n then texto = \nORR\n;\n    if table = \nTable dcr\n then texto = \nDCR\n;\n    drop type;\nrun;\n\nproc report data=orr_dcr_prop_ci nowd headline style(header)={background=very light grey fontsize=8pt} missing style(column)={fontsize=8pt} style(report)={width=100%} split=\n*\n;\n    title \nResponse Rates\n;\n    column (\nResponse Rates\n texto propci prop ci);\n\n    define texto / display \n \n flow;\n    define propci / display \nProportion (CI 95%)\n flow;\n    define prop / display \nProportion\n flow;\n    define ci / display \nCI 95%\n flow;\nrun;", 
            "title": "ORR and DCR"
        }, 
        {
            "location": "/other-analysis/orr-dcr/#objective-response-rate-orr", 
            "text": "In medicine, a  response rate  is the percentage of patients whose cancer shrinks or disappears after treatment. When used as a clinical endpoint for clinical trials of cancer treatments, this often called the  objective response rate (ORR) . The FDA definition of ORR is  \"the proportion of patients with tumor size reduction of a predefined amount and for a minimum time period\" .   When defined in this manner, ORR is a direct measure of drug antitumor activity, which can be evaluated in a single-arm study. Stable disease should not be a component of ORR. Stable disease can reflect the natural history of disease, whereas tumor reduction is a direct therapeutic effect.", 
            "title": "Objective Response Rate (ORR)"
        }, 
        {
            "location": "/other-analysis/orr-dcr/#disease-control-rate-dcr", 
            "text": "The  disease control rate (DCR)  or  clinical benefit rate (CBR)  is defined as the percentage of patients with advanced or metastatic cancer who have achieved complete response, partial response or stable disease to a therapeutic intervention in clinical trials of anticancer agents.", 
            "title": "Disease Control Rate (DCR)"
        }, 
        {
            "location": "/other-analysis/orr-dcr/#calculation", 
            "text": "Remember  ORR and DCR are usually calculated both taking into account  not evaluable (NE)  best responses and without them included in the total number of patients.    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48 data orr_dcr;\n    input pt $ bestresp $ orr dcr c_orr $ c_dcr $;\n    datalines;\n001 PD 0 0 N N\n002 SD 0 1 N Y\n003 SD 0 1 N Y\n004 PD 0 0 N N\n005 PD 0 0 N N \n006 PD 0 0 N N \n007 SD 0 1 N Y\n008 SD 0 1 N Y\n009 PD 0 0 N N \n010 SD 0 1 N Y\n011 PR 1 1 Y Y \n012 PR 1 1 Y Y \n;\nrun;\n\nods exclude all;\nproc freq data=orr_dcr;\n    table orr / nocum binomial(exact level=2) alpha=0.05;\n    ods output BinomialCLs=orr_prop_ci;\nrun;\nproc freq data=orr_dcr;\n    table dcr / nocum binomial(exact level=2) alpha=0.05;\n    ods output BinomialCLs=dcr_prop_ci;\nrun;\nods exclude none;\n\ndata orr_dcr_prop_ci;\n    set orr_prop_ci dcr_prop_ci;\n    propci = trim(left(put(Proportion*100,5.2)))||  ( ||trim(left(put(LowerCL*100,5.2)))|| ,  ||trim(left(put(UpperCL*100,5.2)))|| ) ;\n    prop = trim(left(put(Proportion*100,5.2)));\n    ci = trim(left(put(LowerCL*100,5.2)))|| ,  ||trim(left(put(UpperCL*100,5.2)));\n    if table =  Table orr  then texto =  ORR ;\n    if table =  Table dcr  then texto =  DCR ;\n    drop type;\nrun;\n\nproc report data=orr_dcr_prop_ci nowd headline style(header)={background=very light grey fontsize=8pt} missing style(column)={fontsize=8pt} style(report)={width=100%} split= * ;\n    title  Response Rates ;\n    column ( Response Rates  texto propci prop ci);\n\n    define texto / display     flow;\n    define propci / display  Proportion (CI 95%)  flow;\n    define prop / display  Proportion  flow;\n    define ci / display  CI 95%  flow;\nrun;", 
            "title": "Calculation"
        }, 
        {
            "location": "/other-analysis/clustering/", 
            "text": "Introduction to Cluster Analysis\n\n\nWhen subjects are sampled, randomized or allocated by clusters, several statistical problems arise. If observations within a cluster are correlated, one of the assumptions of estimation and hypothesis testing is violated. Because of this correlation, the analyses must be modified to take into account the cluster design effect. When cluster designs are used, there are two sources of variations in the observations. The first is the \none between subjects within a cluster\n, and the second is the \nvariability among clusters\n. These two sources of variation cause the variance to inflate and must be taken into account in the analysis.\n\n\nGettin' Ready for a Cluster Analysis\n\n\nCheck for Missing Data\n\n\nVariables with missing data should be excluded from the calculation unless they can be imputed.\n\n\n1\n2\n3\n4\nDATA SAS-data-set-without-missing;\n    SET SAS-data-set-with-missing;\n    IF CMISS(OF _ALL_) THEN DELETE;\nRUN;\n\n\n\n\n\n\nDealing with Categorical Variables\n\n\nComposite variables\n\n\nOther questionnaire data like binary (yes/no questions) or a spectrum of responses can be transformed into \ncomposite variables\n to capture multiple questions into a \nranked ordinal scale\n. A \ncomposite variable\n is a variable created by combining two or more individual variables, called indicators, into a single variable. Each indicator alone doesn't provide sufficient information, but altogether they can represent the more complex concept.\n\n\nA lot of work goes into creating composite variables. The indicators of the multidimensional concept must be specified. It's important that each indicator contribute unique information to the final score. The formula for combining the indicators into a single score, called aggregating data, must be established. The computation involved will depend on the type of data that is being aggregated. To aggregate the data, raw scores might be summed, averaged, transformed, and/or weighted.\n\n\nHot encoding\n\n\nCheck \nthis website\n for a macro to generate dummy variables.\n\n\nOrdinal Categorical Variables\n\n\nIf your categorical variables have an ordinal meaning you can create an auxiliary numeric variable with indexes representing the ordinal scale and use this new variable, which can be standardized, for the analysis.\n\n\nMethods for data reduction\n\n\nYou may need to reduce the number of variables to include in the analysis. There are several methods for this:\n\n\n\n\nPrincipal Component Analysis with \nPROC FACTOR\n\n\nVariable Reduction for Modeling using \nPROC VARCLUS\n\n\n\n\nFactor Analysis\n\n\nFactor analysis is a method of data reduction.  It does this by seeking underlying unobservable variables (\nlatent variables\n) that are reflected in the observed variables (\nmanifest variables\n). \n\n\n\n\nThere are many different \nmethods\n that can be used to conduct a factor analysis (such as principal axis factor, maximum likelihood, generalized least squares, unweighted least squares). Each of them generates uncorrelated factors. \n\n\nThere are also many different \ntypes of rotations\n that can be done after the initial extraction of factors, including orthogonal rotations, such as varimax and equimax, which impose the restriction that the factors cannot be correlated, and oblique rotations, such as promax, which allow the factors to be correlated with one another. It is generally considered that using a rotation in factor analysis will produce more interpretable results. If the factor analysis is being performed specifically to gain an explanation of what factors or groups exist in the data or to confirm hypothesized assumptions about the data, rotation can be especially helpful. Factor patterns can be rotated through two different ways:\n\n\nOrthogonal rotations which retain uncorrelated factors\n\n\nOblique rotations which create correlated factors\nWhile arguments exist supporting both types of rotation methods, factor analysis which uses an \northogonal rotation often creates a solution that is easier to grasp and interpret\n than a solution obtained from an oblique rotation. \n\n\n\n\n\n\nYou also need to determine the \nnumber of factors\n that you want to extract. Normally, the objetive during the initial factor analysis is to determine the minimum number of factors that will adequately account for the covariation among the larger number of analysis variables. This objective can be achieved by using any of the initial factoring methods.\n\n\n\n\nGiven the number of factor analytic techniques and options, it is not surprising that different analysts could reach very different results analyzing the same data set. However, all analysts are looking for simple structure. Simple structure is pattern of results such that \neach variable loads highly onto one and only one factor\n.\n\n\nFactor analysis is a technique that requires a \nlarge sample size\n because it is based on the correlation matrix of the variables involved, and correlations usually need a large sample size before they stabilize. \n\n\n\n\nAdvisable sample size\n\n\n\n\n50 cases is very poor, 100 is poor, 200 is fair, 300 is good, 500 is very good, and 1000 or more is excellent \n\n\nAs a rule of thumb, a bare minimum of \n10 observations per variable\n is necessary to avoid computational difficulties\n\n\n\n\n\n\n1\n2\n3\nPROC FACTOR DATA=SAS-data-set NFACTORS=3 CORR SCREE EV REORDER ROTATE=VARIMAX METHOD=PRINIT PRIORS=SMC;\n    VAR var1 var2 var3 ... varn;\nRUN;\n\n\n\n\n\n\n\n\nMETHOD=\n specifies what type of method is to be used to extract the initial factors. The \nPrincipal Components Analysis (PCA)\n method (\nPRINCIPAL\n) simply transforms the set of variables into another set of variables; that is, the data is summarized by means of a linear combination of the observed data. This transformation is performed when the objective is to account for as much variation as possible in the data. With PCA, the first component/factor is defined in such a way that the largest amount of variance in the data is explained by the first component. The second component/factor explains the second most about the variance in the data and is \nperpendicular (thus, uncorrelated)\n to the first component. The remaining components/factors are found in a similar manner.\n\n\nMINEIGEN =\n specifies the smallest eigenvalue for which a factor is retained.\n\n\nNFACTOR =\n specifies the number of factors to be extracted from the data. The default value, if this option is not specified, is the number of variables in the data. The optimum number can be selected by first running \nPROC FACTOR\n without the \nNFACTOR\n option and analyzing the eigenvalues and \nSCREE\n plot. \n\n\nREORDER\n: This option reorders the output, so the variables that explain the largest amount of the variance for each factor are printed in descending order down to those that explain the smallest amount of the variance for each factor.\n\n\nCORR\n generates the \nCorrelations\n table containing the correlations between the original variables (the ones specified on the \nVAR\n statement). Before conducting a principal components analysis, you want to check the correlations between the variables.  If any of the correlations are too high (say above .9), you may need to remove one of the variables from the analysis, as the two variables seem to be measuring the same thing.  Another alternative would be to combine the variables in some way (perhaps by taking the average).  If the correlations are too low, say below .1, then one or more of the variables might load only onto one factor (in other words, make its own factor).\n\n\nEV\n displays the eigenvectors of the reduced correlation matrix, of which the diagonal elements are replaced with the communality estimates.\n\n\nSCREE\n or \nPLOTS=SCREE\n graph the eigenvalue against the factor number. The ploted values are contained in the \nInitial Factor Method: Iterated Principal Factor Analysis\n table:\n\n\nIteration\n: This column lists the number of the iteration. \n\n\nChange\n: When the change becomes smaller than the criterion, the iterating process stops.  The numbers in this column are the largest absolute difference between iterations. The difference given for the first iteration is the difference between the values at the first iteration and the squared multiple correlations (sometimes called iteration 0).\n\n\nCommunalities\n: These are the communality estimates at each iteration.  For each iteration, the communality for each variable is listed.\n\n\n\n\n\n\nPRIORS=SMC\n enables the squared multiple correlation to be used on the diagonal of the correlation matrix. If this option is not used, 1\u2019s are on the diagonal, and you will do a PCA instead of a principal axis factor analysis. The \nPrior Communality Estimates:  SMC\n table gives the communality estimates prior to the rotation.  The communalities (also known as h2) are the estimates of the variance of the factors, as opposed to the variance of the variable which includes measurement error. This table contains different values:\n\n\nEigenvalue\n: This is the initial eigenvalue.  An eigenvalue is the variance of the factor.  Because this is an unrotated solution, the first factor will account for the most variance, the second will account for the second highest amount of variance, and so on.  Some of the eigenvalues are negative because the matrix is not of full rank.  This means that there are probably only four dimensions (corresponding to the four factors whose eigenvalues are greater than zero).  Although it is strange to have a negative variance, this happens because the factor analysis is only analyzing the common variance, which is less than the total variance.  If we were doing a principal components analysis, we would have had 1\u2019s on the diagonal, which means that all of the variance is being analyzed (which is another way of saying that we are assuming that we have no measurement error), and we would not have negative eigenvalues.  In general, it is not uncommon to have negative eigenvalues.\n\n\nDifference\n: This column gives the difference between the eigenvalues and allows you to see how quickly the eigenvalues are decreasing.\n\n\nProportion\n: This is the proportion of the total variance that each factor accounts for.\n\n\nCumulative\n: This is the sum of the proportion column.\n\n\n\n\n\n\nEigenvalues of the Reduced Correlation Matrix\n table contains:\n\n\nEigenvalue\n: This is the eigenvalue obtained after the principal axis factoring but \nbefore the varimax rotation\n.    \n\n\nDifference\n, \nProportion\n and \nCumulative\n show the same information described for the \nPrior Communality Estimates:  SMC\n table\n\n\n\n\n\n\nThe \nEigenvectors\n table shows the linear combinations of the original variables. They tell you about the strength of the relationship between the original (manifest) variables and the (latent) factors.\n\n\nThe \nFactor Pattern\n table contains the unrotated factor loadings, which are the correlations between the variable and the factor.  Because these are correlations, possible \nvalues range from -1 to +1\n.\n\n\nThe \nFinal Communality Estimates\n table shows the proportion of each variable\u2019s variance that can be explained by the retained factors prior to the rotation. Variables with high values are well represented in the common factor space, while variables with low values are not well represented. They are the reproduced variances from the factors that you have extracted.  You can find these values on the diagonal of the reproduced correlation matrix.\n\n\nThe \nOrthogonal Transformation Matrix\n table shows the matrix by which you multiply the unrotated factor matrix to get the rotated factor matrix.\n\n\nThe \nRotated Factor Pattern\n table contains the rotated factor loadings, which are the correlations between the variable and the factor.  Because these are correlations, possible \nvalues range from -1 to +1\n.\n\n\nThe \nFactor\n columns in this table are the rotated factors that have been extracted. These are the factors that analysts are most interested in and try to name looking at the items that load highly on it. The second factor might be called \"relating to students\" because items like \"instructor is sensitive to students\" and \"instructor allows me to ask questions\" load highly on it.  The third factor has to do with comparisons to other instructors and courses.\n\n\n\n\n\n\nROTATE = PROMAX\n | \nVARIMAX\n invokes one of the rotation methods available. If no method is specified, the SAS default is to use no rotation. \n\n\nWith an orthogonal rotation, such as the \nVARIMAX\n, the factors are not permitted to be correlated. Orthogonal transformations simplify the interpretation of the factors by maximizing the variances of the squared loadings for each factor, which are the columns of the factor pattern. Please note that with orthogonal rotations the factor pattern and the factor structure matrices are the equal.\n\n\nWith an oblique rotation, such as a \nPROMAX\n rotation, the factors are permitted to be correlated with one another and both factor pattern and factor structure matrices are produced. The factor pattern matrix gives the linear combination of the variables that make up the factors. The factor structure matrix presents the correlations between the variables and the factors. To completely interpret an oblique rotation one needs to take into account both the factor pattern and the factor structure matrices and the correlations among the factors. \n\n\n\n\n\n\n\n\n\n\nCheck these websites\n\n\n\n\nFactor Analysis: SAS Annotated Output\n\n\nPROC FACTOR\n: How to Interpret the Output of a Real-World Example\n\n\n\n\n\n\nStandardize your Data\n\n\nWhen performing multivariate analysis, having variables that are measured at different scales can influence the numerical stability and precision of the estimators. Standardizing the data prior to performing statistical analysis can often prevent this problem.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\nDATA SAS-data-set;\n    SET SAS-data-set-unformatted;\n    format variable1 variable2 variable3 10.4;\nRUN;\n\nPROC STANDARD DATA=SAS-data-set out=SAS-output-data-set MEAN=0 STD=1;\n    VAR variable1 variable2 variable3;\nRUN;\n\n\n\n\n\n\n\n\nWarning\n\n\nDo not forget to change the format of your numerical data and increase the number of decimal places before performing the standardization. Otherwise you may lose a lot of details on this process that can be crucial for the data analysis.\n\n\n\n\n\n\nCheck these websites\n\n\n\n\nStandardization Procedures\n\n\nStandardization of Variables in Cluster Analysis\n\n\n\n\n\n\nSAS Procedures to Perform Cluster Analysis\n\n\nWard's minimum-variance hierarchical clustering method using agglomerative (bottom-up) approach and Ward's linkage.\n\n\n\n\nCheck these websites\n\n\n\n\nIntroduction to Clustering Procedures\n\n\n\n\n\n\nPROC CLUSTER\n: Hierarchical Cluster Analysis\n\n\nThe \nCLUSTER\n procedure \nhierarchically clusters the observations\n in a SAS data set by using one of 11 methods. The data can be coordinates or distances. \n\n\nAll methods are based on the usual agglomerative hierarchical clustering procedure. Each observation begins in a cluster by itself. The two closest clusters are merged to form a new cluster that replaces the two old clusters. Merging of the two closest clusters is repeated until only one cluster is left. The various clustering methods differ in how the distance between two clusters is computed.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\ndata t;\n    input cid $ 1-2 income educ;\ncards;\nc1 5 5\nc2 6 6\nc3 15 14\nc4 16 15\nc5 25 20\nc6 30 19\nrun;\n\nproc cluster simple noeigen method=centroid rmsstd rsquare nonorm out=tree;\nid cid;\nvar income educ;\nrun;\n\n\n\n\n\n\n\n\nThe \nSIMPLE\n option displays simple, descriptive statistics. \n\n\nThe \nNOEIGEN\n option suppresses computation of eigenvalues. Specifying the \nNOEIGEN\n option saves time if the number of variables is large, but it should be used only if the variables are nearly uncorrelated or if you are not interested in the cubic clustering criterion. \n\n\nThe \nMETHOD=\n specification determines the clustering method used by the procedure. Here, we are using \nCENTROID\n method. The \nCentroid Distance\n that appears in the output is simply the Euclidian distance between the centroid of the two clusters that are to be joined or merged. It is a measure of the homogeneity of merged clusters and the value should be small.\n\n\nThe \nRMSSTD\n option displays the root-mean-square standard deviation of each cluster. \nRMSSTD\n is the pooled standard deviation of all the variables forming the cluster. Since the objective of cluster analysis is to form homogeneous groups, the \nRMSSTD\n of a cluster should be as small as possible.\n\n\nThe \nRSQUARE\n option displays the $R^2$ (\nRSQ\n) and semipartial $R^2$ (\nSPRSQ\n) to evaluate cluster solution. \nRSQ\n measures the extent to which groups or clusters are different from each other (so, when you have just one cluster \nRSQ\n value is, intuitively, zero). Thus, the \nRSQ\n value should be high.\nSPRSQ\n is a measure of the homogeneity of merged clusters, i.e. the loss of homogeneity due to combining two groups or clusters to form a new group or cluster.  Thus, its value should be small to imply that we are merging two homogeneous groups. \n\n\nThe \nNONORM\n option prevents the distances from being normalized to unit mean or unit root mean square with most methods. \n\n\nThe values of the \nID\n variable identify observations in the displayed cluster history and in the \nOUTTREE=\n data set. If the \nID\n statement is omitted, each observation is denoted by \nOBn\n, where n is the observation number.\n\n\nThe \nVAR\n statement lists numeric variables to be used in the cluster analysis. If you omit the \nVAR\n statement, all numeric variables not listed in other statements are used.\n\n\n\n\nPROC FASTCLUS\n: Disjoint Cluster Analysis\n\n\nThe \nFASTCLUS\n procedure performs a \ndisjoint cluster analysis\n on the basis of distances computed from one or more quantitative variables. The observations are \ndivided into clusters such that every observation belongs to one and only one cluster\n; the clusters \ndo not form a tree structure\n as they do in the \nCLUSTER\n procedure. If you want separate analyses for different numbers of clusters, you can run \nPROC FASTCLUS\n once for each analysis. The \nFASTCLUS\n procedure requires time proportional to the number of observations and thus can be used with much larger data sets than \nPROC CLUSTER\n.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\ndata t2;\n    input cid $ 1-2 income educ;\ncards;\nc1 5 5\nc2 6 6\nc3 15 14\nc4 16 15\nc5 25 20\nc6 30 19\nrun;\n\nproc fastclus radius=0 replace=full maxclusters=3 maxiter=20 list distance;\nid cid;\nvar income educ;\nrun;\n\n\n\n\n\n\nYou must specify either the \nMAXCLUSTERS=\n or the \nRADIUS=\n argument in the \nPROC FASTCLUS\n statement.\n\n\n\n\nThe \nRADIUS=\n option establishes the minimum distance criterion for selecting new seeds. No observation is considered as a new seed unless its minimum distance to previous seeds exceeds the value given by the \nRADIUS=\n option. The default value is 0. \n\n\nThe \nMAXCLUSTERS=\n option specifies the maximum number of clusters allowed. If you omit the \nMAXCLUSTERS=\n option, a value of 100 is assumed. \n\n\n\n\nThe \nREPLACE=\n option specifies how seed replacement is performed. \n\n\n\n\nFULL\n requests default seed replacement. \n\n\nPART\n requests seed replacement only when the distance between the observation and the closest seed is greater than the minimum distance between seeds. \n\n\nNONE\n suppresses seed replacement. \n\n\nRANDOM\n selects a simple pseudo-random sample of complete observations as initial cluster seeds. \n\n\n\n\n\n\n\n\nThe \nMAXITER=\n option specifies the maximum number of iterations for recomputing cluster seeds. When the value of the \nMAXITER=\n option is greater than 0, each observation is assigned to the nearest seed, and the seeds are recomputed as the means of the clusters. \n\n\n\n\nThe \nLIST\n option lists all observations, giving the value of the \nID\n variable (if any), the number of the cluster to which the observation is assigned, and the distance between the observation and the final cluster seed. \n\n\nThe \nDISTANCE\n option computes distances between the cluster means. \n\n\nThe \nID\n variable, which can be character or numeric, identifies observations on the output when you specify the \nLIST\n option.\n\n\nThe \nVAR\n statement lists the numeric variables to be used in the cluster analysis. If you omit the \nVAR\n statement, all numeric variables not listed in other statements are used.\n\n\n\n\nThe cluster analysis may converge to a solution at the $n^{th}$ iteration because the change in cluster seeds at this iteration is less than the convergence criterion.  Note that a zero change in the centroid of the cluster seeds for the $n^{th}$ iteration implies that the reallocation did not result in any reassignment of observations.\n\n\nThe statistics used for the evaluation of the cluster solution are the same as in the hierarchical cluster analysis.\n\n\nThe cluster solution can also be \nevaluated with respect to each clustering variable\n. If the measurement scales are not the same, then for each variable one should obtain the \nratio\n of the respective \nWithin STD\n to the \nTotal STD\n, and compare this ratio across the variables.\n\n\n\n\nInteresting Examples\n\n\n\n\nMultivariate Statistical Analysis in SAS: Segmentation and Classification of Behavioral Data\n\n\n\n\n\n\nMixed Clustering\n\n\nOn large data sets a useful methodology consists first in summarizing the observations in a large enough number of clusters (100 may be a standard value) and then applying a hierarchical clustering technique for aggregating these groups (Example \nhere\n).\n\n\nThis procedure has the advantages of the hierarchical method for showing an optimal number of clusters and solves the difficulty of the too high initial number of observations by first clustering them, using a non hierarchical method, in a smaller number of clusters. This number is a parameter of the procedure; it must be high enough in order not to impose a prior partitionning of the data.\n\n\nPROC VARCLUS\n: Variable Clustering\n\n\nThe \nVARCLUS\n procedure divides a set of numeric variables into disjoint or hierarchical clusters. \nPROC VARCLUS\n tries to maximize the variance that is explained by the cluster components, summed over all the clusters.\n\n\nIn an ordinary principal component analysis, all components are computed from the same variables, and the first principal component is orthogonal to the second principal component and to every other principal component. In \nPROC VARCLUS\n, each cluster component is computed from a set of variables that is different from all the other cluster components. The first principal component of one cluster might be correlated with the first principal component of another cluster. Hence, the \nPROC VARCLUS\n algorithm is a type of oblique component analysis.\n\n\nPROC VARCLUS\n can be used as a \nvariable-reduction method\n. A large set of variables can often be replaced by the set of cluster components with little loss of information. A given number of cluster components does not generally explain as much variance as the same number of principal components on the full set of variables, but the cluster components are usually easier to interpret than the principal components, even if the latter are rotated.\n\n\n1\n2\n3\nPROC VARCLUS DATA=SAS-data-set MAXEIGEN=0.7 OUTTREE=fortree short noprint;  \n    VAR variable1 variable2 variable3;\nRUN;\n\n\n\n\n\n\nPROC TREE\n\n\nThe \nTREE\n procedure produces a tree diagram from a \ndata set created by the \nCLUSTER\n or \nVARCLUS\n procedure\n that contains the results of \nhierarchical clustering\n as a tree structure.\n\n\n1\n2\n3\n4\nPROC TREE DATA=tree OUT=clus3 NCLUSTERS=3;\n    ID id-variable;\n    COPY variable1 variable2 variable3;\nRUN;\n\n\n\n\n\n\nThe \nTREE\n procedure produces a tree diagram, also known as a dendrogram or phenogram, using a data set created by the \nCLUSTER\n procedure. The \nCLUSTER\n procedure creates output data sets that contain the results of \nhierarchical clustering as a tree structure\n. The \nTREE\n procedure uses the output data set to produce a diagram of the tree structure.\n\n\n\n\nThe \nNCLUSTERS=\n option specifies the number of clusters desired in the \nOUT=\n data set.\n\n\nThe \nID\n variable is used to identify the objects (leaves) in the tree on the output. The \nID\n variable can be a character or numeric variable of any length. \n\n\nThe \nCOPY\n statement specifies one or more character or numeric variables to be copied to the \nOUT=\n data set.\n\n\n\n\nChoosing the Optimal Number of Clusters for the Analysis\n\n\nIn most cases, before using a clustering technique you have no prior idea of the number of clusters which will give the better differenciation of the data. The main objective is to summarize the data in the best way possible, i.e. getting a compromise between a good degree of differentiation and a not too high number of clusters.\n\n\nFor hierarchical clustering try the Sarle's Cubic Clustering Criterion in PROC CLUSTER:\nplot \nCCC\n versus the number of clusters and look for peaks where \nccc\n \n 3 or look for local peaks of pseudo-F statistic (\nPSF\n) combined with a small value of the pseudo-t^2 statistic (\nPST2\n) and a larger pseudo t^2 for the next cluster fusion.\nFor K-Means clustering use this approach on a sample of your data to determine the max limit for k and assign it to the maxc= option in PROC FASTCLUS on the complete data. \n\n\nFor K-means cluster analysis, one can use \nPROC FASTCLUS\n like\n\nPROC FASTCLUS DATA=SAS-data-set OUT=out MAXC=4 MAXITER=20;\n\nand change the number defined by \nMAXC=\n, and run a number of times, then compare the \nPseduo F\n and \nCCC\n values, to see which number of clusters gives peaks.\n\n\nYou can also use \nPROC CLUSTER\n\n\nPROC CLUSTER data=mydata METHOD=WARD out=out ccc pseudo print=15;\n\nto find the number of clusters with \npseudo F\n, \npseudo-$t^2$\n and \nCCC\n, and also look at junp in \nSemipartial R-Square\n.\n\n\nSometimes these indications do not agree to each other. which indicator is more reliable?\nIf you are doubting between 2 k-values, you can use Beale's F-type statistic to determine the final number of clusters. It will tell you whether the larger solution is significantly better or not (in the latter case the solution with fewer clusters is preferable).\nThis technique is discussed in the \"Applied Clustering Techniques\" course notes.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n*\n \nDefine\n \nthe\n \nvariables\n \nfor\n \nclustering\n;\n\n\n*------------------------------------\n;\n\n\n%let\n \nvarlist\n=\nvar1\n \nvar2\n \nvar3\n \nvar4\n \nvar5\n;\n\n\n\n*\n \nMacro\n \nwith\n \nthe\n \ncluster\n \nprocedure\n,\n \nto\n \ncall\n \nit\n \nwith\n \ndifferent\n \nnumber\n \nof\n \nclusters\n;\n\n\n*-------------------------------------------------------------------------------\n;\n\n\n%\nMACRO\n \nCLUSTERSIZE\n(\n\n         \ndatain\n=\n\n        \n,\ndataout\n=\n\n        \n,\nmaxclusters\n=\n\n        \n,\nmaxiter\n=\n\n        \n);\n\n    \nproc\n \nfastclus\n \ndata\n=\ndatain\n.\n \nout\n=\ndataout\n.\nmaxclusters\n.\n \noutstat\n=\nstatdata\nmaxclusters\n.\n \nmaxclusters\n=\nmaxclusters\n.\n \nmaxiter\n=\nmaxiter\n.\n \nnoprint\n;\n\n        \nid\n \npatient\n;\n\n        \nvar\n \nvarlist\n.;\n\n    \nrun\n;\n\n    \ntitle\n;\n\n\n%\nMEND\n;\n\n\n\n*\n \nCalcultion\n \nof\n \nthe\n \ncluster\n \nanalysis\n \nstatistics\n \nfor\n \n1\n-\n20\n \nclusters\n \nand\n \ndata\n \nset\n \ncreation\n \nfor\n \nelbow\n \nplot\n \nwith\n \nRSQ\n \nvalues\n;\n\n\n*---------------------------------------------------------------------------------------------------------------------\n;\n\n\n%macro\n \nstatCLUSTER\n;\n\n     \n%do\n \nk\n=\n \n1\n \n%to\n \n20\n;\n\n        \n%\nCLUSTERSIZE\n(\ndatain\n=\nSAS\n-\ndata\n-\nset\n,\n \ndataout\n=\nclusterdata\n,\n \nmaxclusters\n=\nk\n.,\n \nmaxiter\n=\n1000\n);\n\n        \ndata\n \nclusrsq\nk\n.;\n\n            \nset\n \nstatdata\nk\n.;\n\n            \nnclust\n=\nk\n.;\n\n            \nif\n \n_type_\n=\nRSQ\n;\n\n            \nkeep\n \nnclust\n \nover_all\n;\n\n        \nrun\n;\n\n     \n%end\n;\n\n\n%mend\n;\n\n\n%stat\nCLUSTER\n;\n\n\n\ndata\n \nclus_rsq\n;\n\n    \nset\n \nclusrsq1\n \nclusrsq2\n \nclusrsq3\n \nclusrsq4\n \nclusrsq5\n \nclusrsq6\n \nclusrsq7\n \nclusrsq8\n \nclusrsq9\n \nclusrsq10\n \nclusrsq11\n \nclusrsq12\n \nclusrsq13\n \nclusrsq14\n \nclusrsq15\n \nclusrsq16\n \nclusrsq17\n \nclusrsq18\n \nclusrsq19\n \nclusrsq20\n;\n\n\nrun\n;\n\n\n\n*\n \nRemove\n \nuseless\n \ndata\n \nsets\n;\n\n\n*-------------------------\n;\n\n\nproc\n \ndatasets\n \nlib\n=\nwork\n \nnowarn\n \nnolist\n \nnodetails\n;\n \n    \ndelete\n \nclusrsq\n:\n \nstatdata\n:\n \nclusterdata\n:\n \n;\n\n\nrun\n;\n \n\nquit\n;\n\n\n\n*\n \nPlot\n \nelbow\n \ncurve\n \nusing\n \nr\n-\nsquare\n \nvalues\n \nhighlighting\n \nthe\n \nbest\n \ncandidates\n \nto\n \noptimum\n \nnumber\n \nof\n \nclusters\n;\n\n\n*------------------------------------------------------------------------------------------------------\n;\n\n\nsymbol1\n \ncolor\n=\nblue\n \ninterpol\n=\njoin\n;\n\n\naxis1\n \nlabel\n=\n(\nNumber\n \nof\n \nclusters\n \nin\n \nthe\n \nanalysis\n)\n \norder\n=\n(\n0\n \nto\n \n15\n \nby\n \n1\n)\n \nreflabel\n=\n(\nj\n=\nc\n \nh\n=\n9\npt\n \nCandidate\n \n1\n \nCandidate\n \n2\n);\n\n\naxis2\n \nlabel\n=\n(\nR\n^\n2\n \nvalues\n \nj\n=\nc\n);\n\n\nproc\n \ngplot\n \ndata\n=\nclus_rsq\n;\n\n    \nplot\n \nover_all\n*\nnclust\n \n/\n \nhaxis\n=\naxis1\n \nvaxis\n=\naxis2\n \nhref\n=\n3\n \n5\n;\n\n\nrun\n;\n\n\n\n\n\n\n\n\n\nCheck these websites\n\n\n\n\nThe Number of Clusters\n\n\n\n\n\n\nFurther Examination of the Cluster Analysis Solution\n\n\nVisualizing the Results\n\n\nTo interpret a cluster analysis solution, the first thing you want to try is to graph the cluster in a scatter plot to see whether or not they overlap with each other in terms of their location in the $p-$dimensional space. If the vectors have a high dimensionality,  we use \nCanonical Discriminant Analysis (CDA)\n. It is a \ndimension-reduction technique related to principal component analysis and canonical correlation\n. It creates a smaller number of variables that are linear combinations of the original $p$ clustering variables. The new variables, called canonical variables, are ordered in terms of the proportion of variance in the clustering variables that is accounted for by each of the canonical variables. Usually, the majority of the variants in the clustering variable will be accounted for by the first couple of canonical varaibles and those are the variables we can plot.\n\n\nIn SAS we can use the \nCANDISC\nprocedure to create the canonical variables for our cluster analysis output data set that has the cluster assignment variable that we created when we ran the cluster analysis. \n\n\n1\n2\n3\n4\nPROC CANDISC DATA=clusterdata OUT=candata;\n    CLASS cluster-assignment-variable;\n    VAR clustering1 clustering2;\nRUN;\n\n\n\n\n\n\n\n\nThe \nOUT=\n is the output data set that includes the canonical variables that are estimated by the canonical discriminate analysis.\n\n\nThe \nCLASS\n variable (mandatory) is the cluster assignment variable which is a categorical variable.\n\n\nIn the \nVAR\n statement we list the clustering variables\n\n\n\n\nYou can then plot the first two canonical variables using the \nSGPLOT\n procedure:\n\n\n1\n2\n3\nPROC SGPLOT DATA=candata;\n    SCATTER Y=can2 X=can1 / GROUP=cluster-assignment-variable;\nRUN;\n\n\n\n\n\n\nLet's analyze the following example result for a 4-cluster analysis.\n\n\n\n\nWhat this shows is that the observations in \nclusters 1 and 4\n are densely packed, meaning they are pretty highly correlated with each other, and \nwithin cluster variance is relatively low\n. In addition, they \ndo not overlap\n very much with the other clusters. The observations from \ncluster 2\n are a little more spread out, indicating less correlation among the observations and higher within cluster variance. But generally, the cluster is relatively distinct with the exception of some observations which are closer to clusters 1 and 4 indicating \nsome overlap\n with these clusters. However, \ncluster 3\n is all over the place. There is come indication of a cluster but the observations are spread out more than the other clusters. This means that the \nwithin cluster variance is high\n as there is less correlation between the observations in this cluster, so we do not really know what is going to happen with that cluster. What this suggests is that the \nbest cluster solution may have fewer than 4 clusters\n, meaning that it would be especially important to further evaluate the cluster solutions with fewer than four clusters.\n\n\n\n\nCheck these websites\n\n\n\n\nDiscriminant Function Analysis in SAS (UCLA)\n\n\nIntroduction to Discriminant Procedures\n\n\n\n\n\n\nCluster Means and Standard Deviations\n\n\nYou can also check these values per cluster to detect possible similarities between groups and detect the most different ones.", 
            "title": "Clustering"
        }, 
        {
            "location": "/other-analysis/clustering/#introduction-to-cluster-analysis", 
            "text": "When subjects are sampled, randomized or allocated by clusters, several statistical problems arise. If observations within a cluster are correlated, one of the assumptions of estimation and hypothesis testing is violated. Because of this correlation, the analyses must be modified to take into account the cluster design effect. When cluster designs are used, there are two sources of variations in the observations. The first is the  one between subjects within a cluster , and the second is the  variability among clusters . These two sources of variation cause the variance to inflate and must be taken into account in the analysis.", 
            "title": "Introduction to Cluster Analysis"
        }, 
        {
            "location": "/other-analysis/clustering/#gettin-ready-for-a-cluster-analysis", 
            "text": "", 
            "title": "Gettin' Ready for a Cluster Analysis"
        }, 
        {
            "location": "/other-analysis/clustering/#check-for-missing-data", 
            "text": "Variables with missing data should be excluded from the calculation unless they can be imputed.  1\n2\n3\n4 DATA SAS-data-set-without-missing;\n    SET SAS-data-set-with-missing;\n    IF CMISS(OF _ALL_) THEN DELETE;\nRUN;", 
            "title": "Check for Missing Data"
        }, 
        {
            "location": "/other-analysis/clustering/#dealing-with-categorical-variables", 
            "text": "", 
            "title": "Dealing with Categorical Variables"
        }, 
        {
            "location": "/other-analysis/clustering/#composite-variables", 
            "text": "Other questionnaire data like binary (yes/no questions) or a spectrum of responses can be transformed into  composite variables  to capture multiple questions into a  ranked ordinal scale . A  composite variable  is a variable created by combining two or more individual variables, called indicators, into a single variable. Each indicator alone doesn't provide sufficient information, but altogether they can represent the more complex concept.  A lot of work goes into creating composite variables. The indicators of the multidimensional concept must be specified. It's important that each indicator contribute unique information to the final score. The formula for combining the indicators into a single score, called aggregating data, must be established. The computation involved will depend on the type of data that is being aggregated. To aggregate the data, raw scores might be summed, averaged, transformed, and/or weighted.", 
            "title": "Composite variables"
        }, 
        {
            "location": "/other-analysis/clustering/#hot-encoding", 
            "text": "Check  this website  for a macro to generate dummy variables.", 
            "title": "Hot encoding"
        }, 
        {
            "location": "/other-analysis/clustering/#ordinal-categorical-variables", 
            "text": "If your categorical variables have an ordinal meaning you can create an auxiliary numeric variable with indexes representing the ordinal scale and use this new variable, which can be standardized, for the analysis.", 
            "title": "Ordinal Categorical Variables"
        }, 
        {
            "location": "/other-analysis/clustering/#methods-for-data-reduction", 
            "text": "You may need to reduce the number of variables to include in the analysis. There are several methods for this:   Principal Component Analysis with  PROC FACTOR  Variable Reduction for Modeling using  PROC VARCLUS", 
            "title": "Methods for data reduction"
        }, 
        {
            "location": "/other-analysis/clustering/#factor-analysis", 
            "text": "Factor analysis is a method of data reduction.  It does this by seeking underlying unobservable variables ( latent variables ) that are reflected in the observed variables ( manifest variables ).    There are many different  methods  that can be used to conduct a factor analysis (such as principal axis factor, maximum likelihood, generalized least squares, unweighted least squares). Each of them generates uncorrelated factors.   There are also many different  types of rotations  that can be done after the initial extraction of factors, including orthogonal rotations, such as varimax and equimax, which impose the restriction that the factors cannot be correlated, and oblique rotations, such as promax, which allow the factors to be correlated with one another. It is generally considered that using a rotation in factor analysis will produce more interpretable results. If the factor analysis is being performed specifically to gain an explanation of what factors or groups exist in the data or to confirm hypothesized assumptions about the data, rotation can be especially helpful. Factor patterns can be rotated through two different ways:  Orthogonal rotations which retain uncorrelated factors  Oblique rotations which create correlated factors\nWhile arguments exist supporting both types of rotation methods, factor analysis which uses an  orthogonal rotation often creates a solution that is easier to grasp and interpret  than a solution obtained from an oblique rotation.     You also need to determine the  number of factors  that you want to extract. Normally, the objetive during the initial factor analysis is to determine the minimum number of factors that will adequately account for the covariation among the larger number of analysis variables. This objective can be achieved by using any of the initial factoring methods.   Given the number of factor analytic techniques and options, it is not surprising that different analysts could reach very different results analyzing the same data set. However, all analysts are looking for simple structure. Simple structure is pattern of results such that  each variable loads highly onto one and only one factor .  Factor analysis is a technique that requires a  large sample size  because it is based on the correlation matrix of the variables involved, and correlations usually need a large sample size before they stabilize.    Advisable sample size   50 cases is very poor, 100 is poor, 200 is fair, 300 is good, 500 is very good, and 1000 or more is excellent   As a rule of thumb, a bare minimum of  10 observations per variable  is necessary to avoid computational difficulties    1\n2\n3 PROC FACTOR DATA=SAS-data-set NFACTORS=3 CORR SCREE EV REORDER ROTATE=VARIMAX METHOD=PRINIT PRIORS=SMC;\n    VAR var1 var2 var3 ... varn;\nRUN;    METHOD=  specifies what type of method is to be used to extract the initial factors. The  Principal Components Analysis (PCA)  method ( PRINCIPAL ) simply transforms the set of variables into another set of variables; that is, the data is summarized by means of a linear combination of the observed data. This transformation is performed when the objective is to account for as much variation as possible in the data. With PCA, the first component/factor is defined in such a way that the largest amount of variance in the data is explained by the first component. The second component/factor explains the second most about the variance in the data and is  perpendicular (thus, uncorrelated)  to the first component. The remaining components/factors are found in a similar manner.  MINEIGEN =  specifies the smallest eigenvalue for which a factor is retained.  NFACTOR =  specifies the number of factors to be extracted from the data. The default value, if this option is not specified, is the number of variables in the data. The optimum number can be selected by first running  PROC FACTOR  without the  NFACTOR  option and analyzing the eigenvalues and  SCREE  plot.   REORDER : This option reorders the output, so the variables that explain the largest amount of the variance for each factor are printed in descending order down to those that explain the smallest amount of the variance for each factor.  CORR  generates the  Correlations  table containing the correlations between the original variables (the ones specified on the  VAR  statement). Before conducting a principal components analysis, you want to check the correlations between the variables.  If any of the correlations are too high (say above .9), you may need to remove one of the variables from the analysis, as the two variables seem to be measuring the same thing.  Another alternative would be to combine the variables in some way (perhaps by taking the average).  If the correlations are too low, say below .1, then one or more of the variables might load only onto one factor (in other words, make its own factor).  EV  displays the eigenvectors of the reduced correlation matrix, of which the diagonal elements are replaced with the communality estimates.  SCREE  or  PLOTS=SCREE  graph the eigenvalue against the factor number. The ploted values are contained in the  Initial Factor Method: Iterated Principal Factor Analysis  table:  Iteration : This column lists the number of the iteration.   Change : When the change becomes smaller than the criterion, the iterating process stops.  The numbers in this column are the largest absolute difference between iterations. The difference given for the first iteration is the difference between the values at the first iteration and the squared multiple correlations (sometimes called iteration 0).  Communalities : These are the communality estimates at each iteration.  For each iteration, the communality for each variable is listed.    PRIORS=SMC  enables the squared multiple correlation to be used on the diagonal of the correlation matrix. If this option is not used, 1\u2019s are on the diagonal, and you will do a PCA instead of a principal axis factor analysis. The  Prior Communality Estimates:  SMC  table gives the communality estimates prior to the rotation.  The communalities (also known as h2) are the estimates of the variance of the factors, as opposed to the variance of the variable which includes measurement error. This table contains different values:  Eigenvalue : This is the initial eigenvalue.  An eigenvalue is the variance of the factor.  Because this is an unrotated solution, the first factor will account for the most variance, the second will account for the second highest amount of variance, and so on.  Some of the eigenvalues are negative because the matrix is not of full rank.  This means that there are probably only four dimensions (corresponding to the four factors whose eigenvalues are greater than zero).  Although it is strange to have a negative variance, this happens because the factor analysis is only analyzing the common variance, which is less than the total variance.  If we were doing a principal components analysis, we would have had 1\u2019s on the diagonal, which means that all of the variance is being analyzed (which is another way of saying that we are assuming that we have no measurement error), and we would not have negative eigenvalues.  In general, it is not uncommon to have negative eigenvalues.  Difference : This column gives the difference between the eigenvalues and allows you to see how quickly the eigenvalues are decreasing.  Proportion : This is the proportion of the total variance that each factor accounts for.  Cumulative : This is the sum of the proportion column.    Eigenvalues of the Reduced Correlation Matrix  table contains:  Eigenvalue : This is the eigenvalue obtained after the principal axis factoring but  before the varimax rotation .      Difference ,  Proportion  and  Cumulative  show the same information described for the  Prior Communality Estimates:  SMC  table    The  Eigenvectors  table shows the linear combinations of the original variables. They tell you about the strength of the relationship between the original (manifest) variables and the (latent) factors.  The  Factor Pattern  table contains the unrotated factor loadings, which are the correlations between the variable and the factor.  Because these are correlations, possible  values range from -1 to +1 .  The  Final Communality Estimates  table shows the proportion of each variable\u2019s variance that can be explained by the retained factors prior to the rotation. Variables with high values are well represented in the common factor space, while variables with low values are not well represented. They are the reproduced variances from the factors that you have extracted.  You can find these values on the diagonal of the reproduced correlation matrix.  The  Orthogonal Transformation Matrix  table shows the matrix by which you multiply the unrotated factor matrix to get the rotated factor matrix.  The  Rotated Factor Pattern  table contains the rotated factor loadings, which are the correlations between the variable and the factor.  Because these are correlations, possible  values range from -1 to +1 .  The  Factor  columns in this table are the rotated factors that have been extracted. These are the factors that analysts are most interested in and try to name looking at the items that load highly on it. The second factor might be called \"relating to students\" because items like \"instructor is sensitive to students\" and \"instructor allows me to ask questions\" load highly on it.  The third factor has to do with comparisons to other instructors and courses.    ROTATE = PROMAX  |  VARIMAX  invokes one of the rotation methods available. If no method is specified, the SAS default is to use no rotation.   With an orthogonal rotation, such as the  VARIMAX , the factors are not permitted to be correlated. Orthogonal transformations simplify the interpretation of the factors by maximizing the variances of the squared loadings for each factor, which are the columns of the factor pattern. Please note that with orthogonal rotations the factor pattern and the factor structure matrices are the equal.  With an oblique rotation, such as a  PROMAX  rotation, the factors are permitted to be correlated with one another and both factor pattern and factor structure matrices are produced. The factor pattern matrix gives the linear combination of the variables that make up the factors. The factor structure matrix presents the correlations between the variables and the factors. To completely interpret an oblique rotation one needs to take into account both the factor pattern and the factor structure matrices and the correlations among the factors.       Check these websites   Factor Analysis: SAS Annotated Output  PROC FACTOR : How to Interpret the Output of a Real-World Example", 
            "title": "Factor Analysis"
        }, 
        {
            "location": "/other-analysis/clustering/#standardize-your-data", 
            "text": "When performing multivariate analysis, having variables that are measured at different scales can influence the numerical stability and precision of the estimators. Standardizing the data prior to performing statistical analysis can often prevent this problem.  1\n2\n3\n4\n5\n6\n7\n8 DATA SAS-data-set;\n    SET SAS-data-set-unformatted;\n    format variable1 variable2 variable3 10.4;\nRUN;\n\nPROC STANDARD DATA=SAS-data-set out=SAS-output-data-set MEAN=0 STD=1;\n    VAR variable1 variable2 variable3;\nRUN;    Warning  Do not forget to change the format of your numerical data and increase the number of decimal places before performing the standardization. Otherwise you may lose a lot of details on this process that can be crucial for the data analysis.    Check these websites   Standardization Procedures  Standardization of Variables in Cluster Analysis", 
            "title": "Standardize your Data"
        }, 
        {
            "location": "/other-analysis/clustering/#sas-procedures-to-perform-cluster-analysis", 
            "text": "Ward's minimum-variance hierarchical clustering method using agglomerative (bottom-up) approach and Ward's linkage.   Check these websites   Introduction to Clustering Procedures", 
            "title": "SAS Procedures to Perform Cluster Analysis"
        }, 
        {
            "location": "/other-analysis/clustering/#proc-cluster-hierarchical-cluster-analysis", 
            "text": "The  CLUSTER  procedure  hierarchically clusters the observations  in a SAS data set by using one of 11 methods. The data can be coordinates or distances.   All methods are based on the usual agglomerative hierarchical clustering procedure. Each observation begins in a cluster by itself. The two closest clusters are merged to form a new cluster that replaces the two old clusters. Merging of the two closest clusters is repeated until only one cluster is left. The various clustering methods differ in how the distance between two clusters is computed.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15 data t;\n    input cid $ 1-2 income educ;\ncards;\nc1 5 5\nc2 6 6\nc3 15 14\nc4 16 15\nc5 25 20\nc6 30 19\nrun;\n\nproc cluster simple noeigen method=centroid rmsstd rsquare nonorm out=tree;\nid cid;\nvar income educ;\nrun;    The  SIMPLE  option displays simple, descriptive statistics.   The  NOEIGEN  option suppresses computation of eigenvalues. Specifying the  NOEIGEN  option saves time if the number of variables is large, but it should be used only if the variables are nearly uncorrelated or if you are not interested in the cubic clustering criterion.   The  METHOD=  specification determines the clustering method used by the procedure. Here, we are using  CENTROID  method. The  Centroid Distance  that appears in the output is simply the Euclidian distance between the centroid of the two clusters that are to be joined or merged. It is a measure of the homogeneity of merged clusters and the value should be small.  The  RMSSTD  option displays the root-mean-square standard deviation of each cluster.  RMSSTD  is the pooled standard deviation of all the variables forming the cluster. Since the objective of cluster analysis is to form homogeneous groups, the  RMSSTD  of a cluster should be as small as possible.  The  RSQUARE  option displays the $R^2$ ( RSQ ) and semipartial $R^2$ ( SPRSQ ) to evaluate cluster solution.  RSQ  measures the extent to which groups or clusters are different from each other (so, when you have just one cluster  RSQ  value is, intuitively, zero). Thus, the  RSQ  value should be high. SPRSQ  is a measure of the homogeneity of merged clusters, i.e. the loss of homogeneity due to combining two groups or clusters to form a new group or cluster.  Thus, its value should be small to imply that we are merging two homogeneous groups.   The  NONORM  option prevents the distances from being normalized to unit mean or unit root mean square with most methods.   The values of the  ID  variable identify observations in the displayed cluster history and in the  OUTTREE=  data set. If the  ID  statement is omitted, each observation is denoted by  OBn , where n is the observation number.  The  VAR  statement lists numeric variables to be used in the cluster analysis. If you omit the  VAR  statement, all numeric variables not listed in other statements are used.", 
            "title": "PROC CLUSTER: Hierarchical Cluster Analysis"
        }, 
        {
            "location": "/other-analysis/clustering/#proc-fastclus-disjoint-cluster-analysis", 
            "text": "The  FASTCLUS  procedure performs a  disjoint cluster analysis  on the basis of distances computed from one or more quantitative variables. The observations are  divided into clusters such that every observation belongs to one and only one cluster ; the clusters  do not form a tree structure  as they do in the  CLUSTER  procedure. If you want separate analyses for different numbers of clusters, you can run  PROC FASTCLUS  once for each analysis. The  FASTCLUS  procedure requires time proportional to the number of observations and thus can be used with much larger data sets than  PROC CLUSTER .   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15 data t2;\n    input cid $ 1-2 income educ;\ncards;\nc1 5 5\nc2 6 6\nc3 15 14\nc4 16 15\nc5 25 20\nc6 30 19\nrun;\n\nproc fastclus radius=0 replace=full maxclusters=3 maxiter=20 list distance;\nid cid;\nvar income educ;\nrun;   You must specify either the  MAXCLUSTERS=  or the  RADIUS=  argument in the  PROC FASTCLUS  statement.   The  RADIUS=  option establishes the minimum distance criterion for selecting new seeds. No observation is considered as a new seed unless its minimum distance to previous seeds exceeds the value given by the  RADIUS=  option. The default value is 0.   The  MAXCLUSTERS=  option specifies the maximum number of clusters allowed. If you omit the  MAXCLUSTERS=  option, a value of 100 is assumed.    The  REPLACE=  option specifies how seed replacement is performed.    FULL  requests default seed replacement.   PART  requests seed replacement only when the distance between the observation and the closest seed is greater than the minimum distance between seeds.   NONE  suppresses seed replacement.   RANDOM  selects a simple pseudo-random sample of complete observations as initial cluster seeds.      The  MAXITER=  option specifies the maximum number of iterations for recomputing cluster seeds. When the value of the  MAXITER=  option is greater than 0, each observation is assigned to the nearest seed, and the seeds are recomputed as the means of the clusters.    The  LIST  option lists all observations, giving the value of the  ID  variable (if any), the number of the cluster to which the observation is assigned, and the distance between the observation and the final cluster seed.   The  DISTANCE  option computes distances between the cluster means.   The  ID  variable, which can be character or numeric, identifies observations on the output when you specify the  LIST  option.  The  VAR  statement lists the numeric variables to be used in the cluster analysis. If you omit the  VAR  statement, all numeric variables not listed in other statements are used.   The cluster analysis may converge to a solution at the $n^{th}$ iteration because the change in cluster seeds at this iteration is less than the convergence criterion.  Note that a zero change in the centroid of the cluster seeds for the $n^{th}$ iteration implies that the reallocation did not result in any reassignment of observations.  The statistics used for the evaluation of the cluster solution are the same as in the hierarchical cluster analysis.  The cluster solution can also be  evaluated with respect to each clustering variable . If the measurement scales are not the same, then for each variable one should obtain the  ratio  of the respective  Within STD  to the  Total STD , and compare this ratio across the variables.   Interesting Examples   Multivariate Statistical Analysis in SAS: Segmentation and Classification of Behavioral Data", 
            "title": "PROC FASTCLUS: Disjoint Cluster Analysis"
        }, 
        {
            "location": "/other-analysis/clustering/#mixed-clustering", 
            "text": "On large data sets a useful methodology consists first in summarizing the observations in a large enough number of clusters (100 may be a standard value) and then applying a hierarchical clustering technique for aggregating these groups (Example  here ).  This procedure has the advantages of the hierarchical method for showing an optimal number of clusters and solves the difficulty of the too high initial number of observations by first clustering them, using a non hierarchical method, in a smaller number of clusters. This number is a parameter of the procedure; it must be high enough in order not to impose a prior partitionning of the data.", 
            "title": "Mixed Clustering"
        }, 
        {
            "location": "/other-analysis/clustering/#proc-varclus-variable-clustering", 
            "text": "The  VARCLUS  procedure divides a set of numeric variables into disjoint or hierarchical clusters.  PROC VARCLUS  tries to maximize the variance that is explained by the cluster components, summed over all the clusters.  In an ordinary principal component analysis, all components are computed from the same variables, and the first principal component is orthogonal to the second principal component and to every other principal component. In  PROC VARCLUS , each cluster component is computed from a set of variables that is different from all the other cluster components. The first principal component of one cluster might be correlated with the first principal component of another cluster. Hence, the  PROC VARCLUS  algorithm is a type of oblique component analysis.  PROC VARCLUS  can be used as a  variable-reduction method . A large set of variables can often be replaced by the set of cluster components with little loss of information. A given number of cluster components does not generally explain as much variance as the same number of principal components on the full set of variables, but the cluster components are usually easier to interpret than the principal components, even if the latter are rotated.  1\n2\n3 PROC VARCLUS DATA=SAS-data-set MAXEIGEN=0.7 OUTTREE=fortree short noprint;  \n    VAR variable1 variable2 variable3;\nRUN;", 
            "title": "PROC VARCLUS: Variable Clustering"
        }, 
        {
            "location": "/other-analysis/clustering/#proc-tree", 
            "text": "The  TREE  procedure produces a tree diagram from a  data set created by the  CLUSTER  or  VARCLUS  procedure  that contains the results of  hierarchical clustering  as a tree structure.  1\n2\n3\n4 PROC TREE DATA=tree OUT=clus3 NCLUSTERS=3;\n    ID id-variable;\n    COPY variable1 variable2 variable3;\nRUN;   The  TREE  procedure produces a tree diagram, also known as a dendrogram or phenogram, using a data set created by the  CLUSTER  procedure. The  CLUSTER  procedure creates output data sets that contain the results of  hierarchical clustering as a tree structure . The  TREE  procedure uses the output data set to produce a diagram of the tree structure.   The  NCLUSTERS=  option specifies the number of clusters desired in the  OUT=  data set.  The  ID  variable is used to identify the objects (leaves) in the tree on the output. The  ID  variable can be a character or numeric variable of any length.   The  COPY  statement specifies one or more character or numeric variables to be copied to the  OUT=  data set.", 
            "title": "PROC TREE"
        }, 
        {
            "location": "/other-analysis/clustering/#choosing-the-optimal-number-of-clusters-for-the-analysis", 
            "text": "In most cases, before using a clustering technique you have no prior idea of the number of clusters which will give the better differenciation of the data. The main objective is to summarize the data in the best way possible, i.e. getting a compromise between a good degree of differentiation and a not too high number of clusters.  For hierarchical clustering try the Sarle's Cubic Clustering Criterion in PROC CLUSTER:\nplot  CCC  versus the number of clusters and look for peaks where  ccc    3 or look for local peaks of pseudo-F statistic ( PSF ) combined with a small value of the pseudo-t^2 statistic ( PST2 ) and a larger pseudo t^2 for the next cluster fusion.\nFor K-Means clustering use this approach on a sample of your data to determine the max limit for k and assign it to the maxc= option in PROC FASTCLUS on the complete data.   For K-means cluster analysis, one can use  PROC FASTCLUS  like PROC FASTCLUS DATA=SAS-data-set OUT=out MAXC=4 MAXITER=20; \nand change the number defined by  MAXC= , and run a number of times, then compare the  Pseduo F  and  CCC  values, to see which number of clusters gives peaks.  You can also use  PROC CLUSTER  PROC CLUSTER data=mydata METHOD=WARD out=out ccc pseudo print=15; \nto find the number of clusters with  pseudo F ,  pseudo-$t^2$  and  CCC , and also look at junp in  Semipartial R-Square .  Sometimes these indications do not agree to each other. which indicator is more reliable?\nIf you are doubting between 2 k-values, you can use Beale's F-type statistic to determine the final number of clusters. It will tell you whether the larger solution is significantly better or not (in the latter case the solution with fewer clusters is preferable).\nThis technique is discussed in the \"Applied Clustering Techniques\" course notes.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53 *   Define   the   variables   for   clustering ;  *------------------------------------ ;  %let   varlist = var1   var2   var3   var4   var5 ;  *   Macro   with   the   cluster   procedure ,   to   call   it   with   different   number   of   clusters ;  *------------------------------------------------------------------------------- ;  % MACRO   CLUSTERSIZE ( \n          datain = \n         , dataout = \n         , maxclusters = \n         , maxiter = \n         ); \n     proc   fastclus   data = datain .   out = dataout . maxclusters .   outstat = statdata maxclusters .   maxclusters = maxclusters .   maxiter = maxiter .   noprint ; \n         id   patient ; \n         var   varlist .; \n     run ; \n     title ;  % MEND ;  *   Calcultion   of   the   cluster   analysis   statistics   for   1 - 20   clusters   and   data   set   creation   for   elbow   plot   with   RSQ   values ;  *--------------------------------------------------------------------------------------------------------------------- ;  %macro   statCLUSTER ; \n      %do   k =   1   %to   20 ; \n         % CLUSTERSIZE ( datain = SAS - data - set ,   dataout = clusterdata ,   maxclusters = k .,   maxiter = 1000 ); \n         data   clusrsq k .; \n             set   statdata k .; \n             nclust = k .; \n             if   _type_ = RSQ ; \n             keep   nclust   over_all ; \n         run ; \n      %end ;  %mend ;  %stat CLUSTER ;  data   clus_rsq ; \n     set   clusrsq1   clusrsq2   clusrsq3   clusrsq4   clusrsq5   clusrsq6   clusrsq7   clusrsq8   clusrsq9   clusrsq10   clusrsq11   clusrsq12   clusrsq13   clusrsq14   clusrsq15   clusrsq16   clusrsq17   clusrsq18   clusrsq19   clusrsq20 ;  run ;  *   Remove   useless   data   sets ;  *------------------------- ;  proc   datasets   lib = work   nowarn   nolist   nodetails ;  \n     delete   clusrsq :   statdata :   clusterdata :   ;  run ;   quit ;  *   Plot   elbow   curve   using   r - square   values   highlighting   the   best   candidates   to   optimum   number   of   clusters ;  *------------------------------------------------------------------------------------------------------ ;  symbol1   color = blue   interpol = join ;  axis1   label = ( Number   of   clusters   in   the   analysis )   order = ( 0   to   15   by   1 )   reflabel = ( j = c   h = 9 pt   Candidate   1   Candidate   2 );  axis2   label = ( R ^ 2   values   j = c );  proc   gplot   data = clus_rsq ; \n     plot   over_all * nclust   /   haxis = axis1   vaxis = axis2   href = 3   5 ;  run ;     Check these websites   The Number of Clusters", 
            "title": "Choosing the Optimal Number of Clusters for the Analysis"
        }, 
        {
            "location": "/other-analysis/clustering/#further-examination-of-the-cluster-analysis-solution", 
            "text": "", 
            "title": "Further Examination of the Cluster Analysis Solution"
        }, 
        {
            "location": "/other-analysis/clustering/#visualizing-the-results", 
            "text": "To interpret a cluster analysis solution, the first thing you want to try is to graph the cluster in a scatter plot to see whether or not they overlap with each other in terms of their location in the $p-$dimensional space. If the vectors have a high dimensionality,  we use  Canonical Discriminant Analysis (CDA) . It is a  dimension-reduction technique related to principal component analysis and canonical correlation . It creates a smaller number of variables that are linear combinations of the original $p$ clustering variables. The new variables, called canonical variables, are ordered in terms of the proportion of variance in the clustering variables that is accounted for by each of the canonical variables. Usually, the majority of the variants in the clustering variable will be accounted for by the first couple of canonical varaibles and those are the variables we can plot.  In SAS we can use the  CANDISC procedure to create the canonical variables for our cluster analysis output data set that has the cluster assignment variable that we created when we ran the cluster analysis.   1\n2\n3\n4 PROC CANDISC DATA=clusterdata OUT=candata;\n    CLASS cluster-assignment-variable;\n    VAR clustering1 clustering2;\nRUN;    The  OUT=  is the output data set that includes the canonical variables that are estimated by the canonical discriminate analysis.  The  CLASS  variable (mandatory) is the cluster assignment variable which is a categorical variable.  In the  VAR  statement we list the clustering variables   You can then plot the first two canonical variables using the  SGPLOT  procedure:  1\n2\n3 PROC SGPLOT DATA=candata;\n    SCATTER Y=can2 X=can1 / GROUP=cluster-assignment-variable;\nRUN;   Let's analyze the following example result for a 4-cluster analysis.   What this shows is that the observations in  clusters 1 and 4  are densely packed, meaning they are pretty highly correlated with each other, and  within cluster variance is relatively low . In addition, they  do not overlap  very much with the other clusters. The observations from  cluster 2  are a little more spread out, indicating less correlation among the observations and higher within cluster variance. But generally, the cluster is relatively distinct with the exception of some observations which are closer to clusters 1 and 4 indicating  some overlap  with these clusters. However,  cluster 3  is all over the place. There is come indication of a cluster but the observations are spread out more than the other clusters. This means that the  within cluster variance is high  as there is less correlation between the observations in this cluster, so we do not really know what is going to happen with that cluster. What this suggests is that the  best cluster solution may have fewer than 4 clusters , meaning that it would be especially important to further evaluate the cluster solutions with fewer than four clusters.   Check these websites   Discriminant Function Analysis in SAS (UCLA)  Introduction to Discriminant Procedures", 
            "title": "Visualizing the Results"
        }, 
        {
            "location": "/other-analysis/clustering/#cluster-means-and-standard-deviations", 
            "text": "You can also check these values per cluster to detect possible similarities between groups and detect the most different ones.", 
            "title": "Cluster Means and Standard Deviations"
        }, 
        {
            "location": "/macros/introducing-macrovar/", 
            "text": "Basic Concepts\n\n\nMacro variables substitute text into your SAS programs. The macro facility enables you to create and resolve macro variables anywhere within a SAS program. There are two types of macro variables: \nautomatic macro variables\n, which SAS provides, and \nuser-defined macro variables\n, which you create and define.\n\n\nAutomatic Macro Variables\n\n\nAutomatic macro variables contain system information such as the date and time that the current SAS session began. Some automatic macro variables have fixed values that SAS defines when the session starts. This table shows several common automatic macro variables:\n\n\n\n\n\n\n\n\nName\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nSYSDATE\n\n\nDate when the current SAS session began (DATE7.)\n\n\n16JAN13\n\n\n\n\n\n\nSYSDATE9\n\n\nDate when the current SAS session began (DATE9.)\n\n\n16JAN2013\n\n\n\n\n\n\nSYSDAY\n\n\nDay of the week when the current SAS session began\n\n\nFriday\n\n\n\n\n\n\nSYSSCP\n\n\nAbbreviation for the operating system being used\n\n\nOS, WIN, HP 64\n\n\n\n\n\n\nSYSTIME\n\n\nTime that the current SAS session began\n\n\n13:39\n\n\n\n\n\n\nSYSUSERID\n\n\nThe user ID or login of the current SAS process\n\n\nMyUserid\n\n\n\n\n\n\nSYSVER\n\n\nRelease of SAS software being used\n\n\n9.4\n\n\n\n\n\n\n\n\nOther automatic macro variables have values that change automatically, based on the SAS statements that you submit.\n\n\n\n\n\n\n\n\nName\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSYSLAST\n\n\nName of the most recently created data set in the form \nlibref.name\n (the value is \n_NULL_\n when none has been created)\n\n\n\n\n\n\nSYSPARM\n\n\nValue specified at SAS invocation\n\n\n\n\n\n\nSYSERR\n\n\nSAS \nDATA\n or \nPROC\n step return code (0=success)\n\n\n\n\n\n\nSYSLIBRC\n\n\nLIBNAME\n statement return code (0=success)\n\n\n\n\n\n\n\n\nCreating User-defined Macro Variables\n\n\nYou use the \n%LET\n statement to create a macro variable and assign a value to it. Macro variable names \nstart with a letter or an underscore\n and can be followed by letters, digits, or underscores. The prefixes \nAF\n, \nDMS\n, \nSQL\n, and \nSYS\n are not recommended\n because they are frequently used in SAS software when creating macro variables. If you assign a macro variable name that isn't valid, SAS writes an error message to the log.\n\n\nWhen assigning values to macro variables in the \n%LET\n statement, SAS does the following:\n\n\n\n\nStores all macro variable values as text strings, even if they contain only numbers\n\n\nDoesn't evaluate mathematical expressions in macro variable values\n\n\nStores the value in the case that is specified in the \n%LET\n statement\n\n\nStores quotation marks as part of the macro variable value\n\n\nRemoves leading and trailing blanks from the macro variable value before storing it\n\n\nSAS doesn't remove blanks within the macro variable value\n\n\n\n\nTo \nreference a user-defined macro variable\n, you precede the name of the macro variable with an ampersand (\nmacrovariable\n). When you submit the program, the macro processor resolves the reference and substitutes the macro variable's value before the program compiles and executes.\n\n\n\n\nTip\n\n\nIf you need to reference a macro variable within quotation marks, such as in a title, you must use double quotation marks.\n\n\n\n\nMacro variables remain in the global symbol table until they are deleted or the session ends. To \ndelete macro variables\n, you use the \n%SYMDEL\n statement followed by the name or names of the macro variables that you want to delete.\n\n\nDisplaying Macro Variables in the SAS Log\n\n\nThere are several methods that you can use to display the values of macro variables in the SAS log.\n\n\nUsing the \n%PUT\n Statement\n\n\nYou can use the \n%PUT\n statement to write your own messages, including macro variable values, to the SAS log: \n%PUT The value of the macro variable is: \nmacrovar;\n or \n%PUT \n=macrovar;\n.\n\n\nYou can add one of the following optional arguments to the \n%PUT\n statement:\n  \n%PUT \ntext | _ALL_ / _AUTOMATIC_ / _USER_\n;\n\n\n\n\n\n\n\n\nArgument\n\n\nResult in the SAS Log\n\n\n\n\n\n\n\n\n\n\n_ALL_\n\n\nLists the values of all macro variables\n\n\n\n\n\n\n_AUTOMATIC_\n\n\nLists the value of all automatic macro variables\n\n\n\n\n\n\n_USER_\n\n\nLists the values of all user-defined macro variables\n\n\n\n\n\n\n\n\nUsing the \nSYMBOLGEN\n system option\n\n\nYou can also use the \nSYMBOLGEN\n system option to display the values of macro variables.\n\n\n1\nOPTIONS SYMBOLGEN | NOSYMBOLGEN;\n\n\n\n\n\n\nThe default option is \nNOSYMBOLGEN\n. When you turn the \nSYMBOLGEN\n system option on, SAS writes macro variable values to the SAS log as they are resolved. The message states the macro variable name and the resolved value.\n\n\nBecause \nSYMBOLGEN\n is a system option, its setting remains in effect until you modify it or until you end your SAS session.\n\n\nProcessing Macro Variables\n\n\nWhen you submit a SAS program, it's copied to an area of memory called the input stack. The word scanner reads the text and breaks the text into fundamental units, called tokens, and passes the tokens, one at time, to the appropriate compiler upon demand. The compiler requests tokens until it receives a semicolon and then performs a syntax check on the statement. The compiler repeats this process for each additional statement.\n\n\n\n\nSAS suspends compilation when a step boundary (\nRUN\n statements or the beginning of a new \nDATA\n/\nPROC\n step) is encountered. If there are no compilation errors, SAS executes the compiled code. This process is repeated for each program step. A token is the fundamental unit that the word scanner passes to the compiler. The word scanner recognizes the four classes of tokens shown in the table below.\n\n\n\n\n\n\n\n\nClass\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nname\n\n\nA character string that begins with a letter or underscore and continues with underscores, letters, or numerals\n\n\ninfile \n_n_\n dollar10.2\n\n\n\n\n\n\nspecial\n\n\nAny character, or combination of characters, other than a letter, numeral, or underscore\n\n\n* / + ** ; $ ( ) . \n %\n\n\n\n\n\n\nliteral\n\n\nA string of characters enclosed in single or double quotation marks\n\n\n'Report for May' \"Sydney Office\"\n\n\n\n\n\n\nnumber\n\n\nInteger numbers, including SAS date constants or floating point numbers, that contain a decimal point and/or an exponent\n\n\n23  109 5e8 42.7 '01jan2012'd\n\n\n\n\n\n\n\n\nCertain token sequences, known as \nmacro triggers\n, alert the word scanner that the subsequent code should be sent to the macro processor. The word scanner recognizes the following token sequences as macro triggers and passes the code to the macro processor for evaluation:\n\n\n\n\nA percent sign followed immediately by a name token (such as \n%LET\n)\n\n\nAn ampersand followed immediately by a name token (such as \nmacrovar\n)\n\n\n\n\nA macro variable reference triggers the macro processor to search the symbol table for the reference. The macro processor resolves the macro variable reference by replacing it with the value in the symbol table.\n\n\nUsing Double Quoting to Reference Macro Variables\n\n\nYou need to use double quotes, not single, to reference a macro variable in the code, otherwise the macro variable won't be resolved by the Macro Processor.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n%LET firstletter = a;\n\n\n\nPROC\n \nPRINT\n \nDATA\n=\nSAS\n-\ndat\n-\nset\n;\n\n    \nWHERE\n \nletter\n=\nfirstletter\n;\n\n\nRUN\n;\n\n\n/*\n \nWrong\n \n*/\n\n\n\nPROC\n \nPRINT\n \nDATA\n=\nSAS\n-\ndat\n-\nset\n;\n\n    \nWHERE\n \nletter\n=\nfirstletter\n;\n\n\nRUN\n;\n\n\n/*\n \nCorrect\n \n*/\n\n\n\n\n\n\n\nReferencing Macro Variables Using \n.\n\n\nA period \n.\n is used as delimiter that defines the end of a macro variable. It is usually not necessary but there are cases on which it can be really useful.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n%LET firstletter = a;\n\n\n%LET thirdletter = c;\n\n\n\n%LET abc = \nfirstletterb\nthirdletter;\n\n\n/*\n \nWrong\n \n*/\n\n\n/*\n \nWARNING\n:\n \nApparent\n \nsymbolic\n \nreference\n \nFIRSTLETTERB\n \nnot\n \nresolved\n.\n \n*/\n\n\n\n%LET abc = \nfirstletter.b\nthirdletter;\n\n\n/*\n \nCorrect\n \n*/\n\n\n\n\n\n\n\nYou can even need to use two \n.\n in certain cases:\n\n\n1\n2\n3\n4\n5\n%LET library = library-name;\n\n\n%LET dataset = dataset-name;\n\n\n\nPROC\n \nPRINT\n \nDATA\n=\nlibrary\n..\ndataset\n;\n\n\nRUN\n;", 
            "title": "Introducing Macro Variables"
        }, 
        {
            "location": "/macros/introducing-macrovar/#basic-concepts", 
            "text": "Macro variables substitute text into your SAS programs. The macro facility enables you to create and resolve macro variables anywhere within a SAS program. There are two types of macro variables:  automatic macro variables , which SAS provides, and  user-defined macro variables , which you create and define.", 
            "title": "Basic Concepts"
        }, 
        {
            "location": "/macros/introducing-macrovar/#automatic-macro-variables", 
            "text": "Automatic macro variables contain system information such as the date and time that the current SAS session began. Some automatic macro variables have fixed values that SAS defines when the session starts. This table shows several common automatic macro variables:     Name  Description  Example      SYSDATE  Date when the current SAS session began (DATE7.)  16JAN13    SYSDATE9  Date when the current SAS session began (DATE9.)  16JAN2013    SYSDAY  Day of the week when the current SAS session began  Friday    SYSSCP  Abbreviation for the operating system being used  OS, WIN, HP 64    SYSTIME  Time that the current SAS session began  13:39    SYSUSERID  The user ID or login of the current SAS process  MyUserid    SYSVER  Release of SAS software being used  9.4     Other automatic macro variables have values that change automatically, based on the SAS statements that you submit.     Name  Description      SYSLAST  Name of the most recently created data set in the form  libref.name  (the value is  _NULL_  when none has been created)    SYSPARM  Value specified at SAS invocation    SYSERR  SAS  DATA  or  PROC  step return code (0=success)    SYSLIBRC  LIBNAME  statement return code (0=success)", 
            "title": "Automatic Macro Variables"
        }, 
        {
            "location": "/macros/introducing-macrovar/#creating-user-defined-macro-variables", 
            "text": "You use the  %LET  statement to create a macro variable and assign a value to it. Macro variable names  start with a letter or an underscore  and can be followed by letters, digits, or underscores. The prefixes  AF ,  DMS ,  SQL , and  SYS  are not recommended  because they are frequently used in SAS software when creating macro variables. If you assign a macro variable name that isn't valid, SAS writes an error message to the log.  When assigning values to macro variables in the  %LET  statement, SAS does the following:   Stores all macro variable values as text strings, even if they contain only numbers  Doesn't evaluate mathematical expressions in macro variable values  Stores the value in the case that is specified in the  %LET  statement  Stores quotation marks as part of the macro variable value  Removes leading and trailing blanks from the macro variable value before storing it  SAS doesn't remove blanks within the macro variable value   To  reference a user-defined macro variable , you precede the name of the macro variable with an ampersand ( macrovariable ). When you submit the program, the macro processor resolves the reference and substitutes the macro variable's value before the program compiles and executes.   Tip  If you need to reference a macro variable within quotation marks, such as in a title, you must use double quotation marks.   Macro variables remain in the global symbol table until they are deleted or the session ends. To  delete macro variables , you use the  %SYMDEL  statement followed by the name or names of the macro variables that you want to delete.", 
            "title": "Creating User-defined Macro Variables"
        }, 
        {
            "location": "/macros/introducing-macrovar/#displaying-macro-variables-in-the-sas-log", 
            "text": "There are several methods that you can use to display the values of macro variables in the SAS log.", 
            "title": "Displaying Macro Variables in the SAS Log"
        }, 
        {
            "location": "/macros/introducing-macrovar/#using-the-put-statement", 
            "text": "You can use the  %PUT  statement to write your own messages, including macro variable values, to the SAS log:  %PUT The value of the macro variable is:  macrovar;  or  %PUT  =macrovar; .  You can add one of the following optional arguments to the  %PUT  statement:\n   %PUT  text | _ALL_ / _AUTOMATIC_ / _USER_ ;     Argument  Result in the SAS Log      _ALL_  Lists the values of all macro variables    _AUTOMATIC_  Lists the value of all automatic macro variables    _USER_  Lists the values of all user-defined macro variables", 
            "title": "Using the %PUT Statement"
        }, 
        {
            "location": "/macros/introducing-macrovar/#using-the-symbolgen-system-option", 
            "text": "You can also use the  SYMBOLGEN  system option to display the values of macro variables.  1 OPTIONS SYMBOLGEN | NOSYMBOLGEN;   The default option is  NOSYMBOLGEN . When you turn the  SYMBOLGEN  system option on, SAS writes macro variable values to the SAS log as they are resolved. The message states the macro variable name and the resolved value.  Because  SYMBOLGEN  is a system option, its setting remains in effect until you modify it or until you end your SAS session.", 
            "title": "Using the SYMBOLGEN system option"
        }, 
        {
            "location": "/macros/introducing-macrovar/#processing-macro-variables", 
            "text": "When you submit a SAS program, it's copied to an area of memory called the input stack. The word scanner reads the text and breaks the text into fundamental units, called tokens, and passes the tokens, one at time, to the appropriate compiler upon demand. The compiler requests tokens until it receives a semicolon and then performs a syntax check on the statement. The compiler repeats this process for each additional statement.   SAS suspends compilation when a step boundary ( RUN  statements or the beginning of a new  DATA / PROC  step) is encountered. If there are no compilation errors, SAS executes the compiled code. This process is repeated for each program step. A token is the fundamental unit that the word scanner passes to the compiler. The word scanner recognizes the four classes of tokens shown in the table below.     Class  Description  Example      name  A character string that begins with a letter or underscore and continues with underscores, letters, or numerals  infile  _n_  dollar10.2    special  Any character, or combination of characters, other than a letter, numeral, or underscore  * / + ** ; $ ( ) .   %    literal  A string of characters enclosed in single or double quotation marks  'Report for May' \"Sydney Office\"    number  Integer numbers, including SAS date constants or floating point numbers, that contain a decimal point and/or an exponent  23  109 5e8 42.7 '01jan2012'd     Certain token sequences, known as  macro triggers , alert the word scanner that the subsequent code should be sent to the macro processor. The word scanner recognizes the following token sequences as macro triggers and passes the code to the macro processor for evaluation:   A percent sign followed immediately by a name token (such as  %LET )  An ampersand followed immediately by a name token (such as  macrovar )   A macro variable reference triggers the macro processor to search the symbol table for the reference. The macro processor resolves the macro variable reference by replacing it with the value in the symbol table.", 
            "title": "Processing Macro Variables"
        }, 
        {
            "location": "/macros/introducing-macrovar/#using-double-quoting-to-reference-macro-variables", 
            "text": "You need to use double quotes, not single, to reference a macro variable in the code, otherwise the macro variable won't be resolved by the Macro Processor.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 %LET firstletter = a;  PROC   PRINT   DATA = SAS - dat - set ; \n     WHERE   letter = firstletter ;  RUN ;  /*   Wrong   */  PROC   PRINT   DATA = SAS - dat - set ; \n     WHERE   letter = firstletter ;  RUN ;  /*   Correct   */", 
            "title": "Using Double Quoting to Reference Macro Variables"
        }, 
        {
            "location": "/macros/introducing-macrovar/#referencing-macro-variables-using", 
            "text": "A period  .  is used as delimiter that defines the end of a macro variable. It is usually not necessary but there are cases on which it can be really useful.  1\n2\n3\n4\n5\n6\n7\n8\n9 %LET firstletter = a;  %LET thirdletter = c;  %LET abc =  firstletterb thirdletter;  /*   Wrong   */  /*   WARNING :   Apparent   symbolic   reference   FIRSTLETTERB   not   resolved .   */  %LET abc =  firstletter.b thirdletter;  /*   Correct   */    You can even need to use two  .  in certain cases:  1\n2\n3\n4\n5 %LET library = library-name;  %LET dataset = dataset-name;  PROC   PRINT   DATA = library .. dataset ;  RUN ;", 
            "title": "Referencing Macro Variables Using ."
        }, 
        {
            "location": "/macros/using-macrofunc/", 
            "text": "Overview of Macro Functions\n\n\nMacro Functions\n\n\nMacro functions enable you to manipulate text strings that SAS inserts in your code. When you submit a program, SAS executes the macro functions before the program compiles. To use a macro function, specify the function name, which starts with a percent sign. Enclose the function arguments in parentheses, separated by commas.\n\n\n1\n%function\n-\nname\n \n(\nargument\n-\n1\n,\n \nargument\n-\nn\n)\n\n\n\n\n\n\n\nThe arguments can include:\n\n\n\n\nConstant text (\nS\nP 500\n)\n\n\nMacro variable references (\nsysdate9\n)\n\n\nMacro functions (\n%lenght(\nvar)\n)\n\n\nMacro calls (\n%time\n)\n\n\n\n\n\n\nTip\n\n\nWhen you use constant text, do not enclose the text in quotation marks. If you do include them, they'll become part of the argument.\n\n\n\n\nYou can use all macro functions in both open code and macro definitions. Macro functions are categorized in four types:\n\n\n\n\nMacro character functions\n\n\nMacro evaluation functions\n\n\nMacro quoting functions\n\n\nOther macro functions\n\n\n\n\nUsing Macro Character Functions\n\n\nMacro character functions enable you to manipulate character strings or obtain information about them.\n\n\n%SUBSTR\n Function\n\n\n%SUBSTR\n extracts a substring of characters from an argument consisting of a character string or text expression. Position specifies where the extraction should begin. n specifies the number of characters to extract. If you don't specify n, the remainder of the string is extracted.\n\n\n1\n%SUBSTR (argument, position \n, n\n)\n\n\n\n\n\n\n\n%SCAN\n Function\n\n\nThe \n%SCAN\n function enables you to extract words from a macro variable or text expression. \n%SCAN\n returns the nth word in an argument, where the words in the argument are separated by delimiters. If n is greater than the number of words in the argument, the function returns a null string. Delimiters refers to the characters that separate words or text expressions.\n\n\n1\n%SCAN (argument, n \n, delimiters\n)\n\n\n\n\n\n\n\nIf you omit the optional delimiter information, \n%SCAN\n uses a default set of delimiters shown below.\n\n\n\n\n\n\n\n\nEncoding\n\n\nType\n\n\nDefault Delimiters\n\n\n\n\n\n\n\n\n\n\nASCII\n\n\nblank\n\n\n. \n ( + \n ! $ * ) ; ^ - / , % \n\n\n\n\n\n\nEBCDIC\n\n\nblank\n\n\n. \n ( + \n \n ! $ * ) ; \u00ac - / , % \u00a6 \u00a2\n\n\n\n\n\n\n\n\n%UPCASE\n Function\n\n\nThe \n%UPCASE\n function enables you to convert characters to uppercase before substituting that value in a SAS program.\n\n\n1\n%UPCASE (argument)\n\n\n\n\n\n\n\n%INDEX\n Function\n\n\nThe \n%INDEX\n function enables you to search for a string within a source. \n%INDEX\n searches source for the first occurrence of string and returns the position of its first character. If an exact match of string is not found, the function returns 0.\n\n\n1\n%INDEX (source, string)\n\n\n\n\n\n\n\nUsing Arithmetic and Logical Expressions\n\n\n%EVAL\n Function\n\n\nThe \n%EVAL\n function evaluates arithmetic and logical expressions.\n\n\n1\n%EVAL (arithmetic or logical expression)\n\n\n\n\n\n\n\n\n\n\n\n\n\nArithmetic Expressions\n\n\nLogical Expressions\n\n\n\n\n\n\n\n\n\n\n1 + 2\n\n\nDAY = FRIDAY\n\n\n\n\n\n\n4 * 3\n\n\nA \n a\n\n\n\n\n\n\n4 / 2\n\n\n1 \n \nINDEX\n\n\n\n\n\n\n00FFx - 003Ax\n\n\nSTART NE \nEND\n\n\n\n\n\n\n\n\n\n\nWhen \n%EVAL\n evaluates an \narithmetic expression\n, it temporarily converts operands to numeric values and performs an integer arithmetic operation. If the result of the expression is noninteger, \n%EVAL\n truncates the value to an integer. The result is expressed as text. The %EVAL function generates an error message in the log when it encounters an expression that contains noninteger values.\n\n\n\n\n1\n2\n3\n4\n/* Example */\n\n\n%let\n \nx\n \n=\n \n%eval\n(\n5\n,\n3\n);\n\n\n%put\n \nx\n=\nx\n\n\n/*result 1*/\n\n\n\n\n\n\n\n\n\nWhen \n%EVAL\n evaluates a \nlogical expression\n, it returns a value of 0 to indicate that the expression is false, or a value of 1 to indicate that the expression is true.\n\n\n\n\n1\n2\n3\n4\n5\n/* Example */\n\n\n%let\n \nx\n \n=\n \n%eval\n(\n10\n \nlt\n \n2\n);\n \n/*10 less than 2*/\n\n\n/*The expression is false*/\n\n\n%put\n \nx\n=\nx\n\n\n/*result 0*/\n\n\n\n\n\n\n\n%SYSEVALF\n Function\n\n\nThe \n%SYSEVALF\n function evaluates arithmetic and logical expressions using floating-point arithmetic and returns a value that is formatted using the \nBEST32.\n format (meaning that decimal contributions are not accounted in the operation). The result of the evaluation is always text.\n\n\n1\n2\n3\n4\n5\n6\n%\nSYSEVALF\n \n(\nexpression\n \n,\n \nconversion\n-\ntype\n)\n\n\n\n/* Example */\n\n\n%let\n \nvalue\n=\n%sysevalf\n(\n10.5\n+\n20.8\n);\n\n\n%put\n \n10.5\n+\n20.8\n \n=\n \nvalue\n;\n\n\n/* result 10.5+20.8 = 30 */\n\n\n\n\n\n\n\nYou can use \n%SYSEVALF\n with an optional conversion type (\nBOOLEAN\n, \nCEIL\n, \nFLOOR\n, or \nINTEGER\n) that tailors the value returned by \n%SYSEVALF\n.\n\n\nUsing SAS Functions with Macro Variables\n\n\n%SYSFUNC\n Function\n\n\nYou can use the \n%SYSFUNC\n macro function to execute SAS functions within the macro facility.\n\n\n1\n2\n3\n4\n%\nSYSFUNC\n \n(\nSAS\n-\nfunction\n(\nargument\n(\ns\n))\n \n,\n \nformat\n)\n\n\n\n/* Example */\n\n\n%let\n \ncurrent\n \n=\n \n%sysfunc\n(\ntime\n(),\n \ntime\n.);\n\n\n\n\n\n\n\nBecause \n%SYSFUNC\n is a macro function, you don't enclose character values in quotation marks, as you do in SAS functions. You can specify an optional format for the value returned by the function. If you do not specify a format, numeric results are converted to a character string using the \nBEST12.\n format. SAS returns character results as they are, without formatting or translation.\n\n\nYou can use almost any SAS function with the \n%SYSFUNC\n macro function. The exceptions are shown in this table.\n\n\n\n\n\n\n\n\nFunction Type\n\n\nFunction Name\n\n\n\n\n\n\n\n\n\n\nArray processing\n\n\nDIM\n, \nHBOUND\n, \nLBOUND\n\n\n\n\n\n\nVariable information\n\n\nVNAME\n, \nVLABEL\n, \nMISSING\n\n\n\n\n\n\nMacro interface\n\n\nRESOLVE\n, \nSYMGET\n\n\n\n\n\n\nData conversion\n\n\nINPUT\n, \nPUT\n\n\n\n\n\n\nOther functions\n\n\nORCMSG\n, \nLAG\n, \nDIF\n\n\n\n\n\n\n\n\n\n\nTip\n\n\nUse \nINPUTC\n and \nINPUTN\n in place of \nINPUT\n, and \nPUTC\n and \nPUTN\n in place of \nPUT\n.\n\n\n\n\nUsing Macro Functions to Mask Special Characters\n\n\nMacro quoting functions enable you to clearly indicate to the macro processor how it is to interpret special characters and mnemonics.\n\n\n%STR\n Macro Quoting Function\n\n\nThe macro quoting function \n%STR\n masks (or quotes) special characters during compilation so that the macro processor does not interpret them as macro-level syntax.\n\n\n1\n%STR (argument)\n\n\n\n\n\n\n\n%STR\n can also be used to quote tokens that typically occur in pairs, such as the apostrophe, quotation marks, and open and closed parentheses. The unmatched token must be preceded by a \n%\n.\nHere is a list of all of the special characters and mnemonics masked by \n%STR\n.\n\n\n1\n2\n3\n+ - * / \n \n = \u00ac ^ ~ ; , # blank\nAND OR NOT EQ NE LE LT GE GT IN\n\n \n ) (\n\n\n\n\n\n\nNote that \n%STR\n does not mask the characters \n or \n%\n.\n\n\n%NRSTR\n Macro Quoting Function\n\n\n%NRSTR\n masks the same characters as \n%STR\n and also masks the special characters \n and \n%\n. Using \n%NRSTR\n instead of \n%STR\n prevents macro and macro variable resolution.\n\n\n1\n%NSTR (argument)\n\n\n\n\n\n\n\n%INCLUDE\n Is Not a Macro Language Statement\n\n\nThe \n%INCLUDE\n statement retrieves SAS source code from an external file and places it on the input stack. It is a global SAS statement, not a macro language statement, and it can be used only on a statement boundary.\n\n\n1\n%INCLUDE file-specification \n/SOURCE2 \n;\n\n\n\n\n\n\n\nTo use the \n%INCLUDE\n statement, you specify the keyword \n%INCLUDE\n, followed by a file specification. The file-specification value is the physical name or fileref of the file to be retrieved. \n\n\nYou can optionally specify \nSOURCE2\n following the file specification. \nSOURCE2\n causes the SAS statements that are inserted into the input stack to be displayed in the SAS log. If you don't specify \nSOURCE2\n in the \n%INCLUDE\n statement, the setting of the SAS system option \nSOURCE2\n controls whether the inserted code is displayed.", 
            "title": "Using Macro Functions"
        }, 
        {
            "location": "/macros/using-macrofunc/#overview-of-macro-functions", 
            "text": "", 
            "title": "Overview of Macro Functions"
        }, 
        {
            "location": "/macros/using-macrofunc/#macro-functions", 
            "text": "Macro functions enable you to manipulate text strings that SAS inserts in your code. When you submit a program, SAS executes the macro functions before the program compiles. To use a macro function, specify the function name, which starts with a percent sign. Enclose the function arguments in parentheses, separated by commas.  1 %function - name   ( argument - 1 ,   argument - n )    The arguments can include:   Constant text ( S P 500 )  Macro variable references ( sysdate9 )  Macro functions ( %lenght( var) )  Macro calls ( %time )    Tip  When you use constant text, do not enclose the text in quotation marks. If you do include them, they'll become part of the argument.   You can use all macro functions in both open code and macro definitions. Macro functions are categorized in four types:   Macro character functions  Macro evaluation functions  Macro quoting functions  Other macro functions", 
            "title": "Macro Functions"
        }, 
        {
            "location": "/macros/using-macrofunc/#using-macro-character-functions", 
            "text": "Macro character functions enable you to manipulate character strings or obtain information about them.", 
            "title": "Using Macro Character Functions"
        }, 
        {
            "location": "/macros/using-macrofunc/#substr-function", 
            "text": "%SUBSTR  extracts a substring of characters from an argument consisting of a character string or text expression. Position specifies where the extraction should begin. n specifies the number of characters to extract. If you don't specify n, the remainder of the string is extracted.  1 %SUBSTR (argument, position  , n )", 
            "title": "%SUBSTR Function"
        }, 
        {
            "location": "/macros/using-macrofunc/#scan-function", 
            "text": "The  %SCAN  function enables you to extract words from a macro variable or text expression.  %SCAN  returns the nth word in an argument, where the words in the argument are separated by delimiters. If n is greater than the number of words in the argument, the function returns a null string. Delimiters refers to the characters that separate words or text expressions.  1 %SCAN (argument, n  , delimiters )    If you omit the optional delimiter information,  %SCAN  uses a default set of delimiters shown below.     Encoding  Type  Default Delimiters      ASCII  blank  .   ( +   ! $ * ) ; ^ - / , %     EBCDIC  blank  .   ( +     ! $ * ) ; \u00ac - / , % \u00a6 \u00a2", 
            "title": "%SCAN Function"
        }, 
        {
            "location": "/macros/using-macrofunc/#upcase-function", 
            "text": "The  %UPCASE  function enables you to convert characters to uppercase before substituting that value in a SAS program.  1 %UPCASE (argument)", 
            "title": "%UPCASE Function"
        }, 
        {
            "location": "/macros/using-macrofunc/#index-function", 
            "text": "The  %INDEX  function enables you to search for a string within a source.  %INDEX  searches source for the first occurrence of string and returns the position of its first character. If an exact match of string is not found, the function returns 0.  1 %INDEX (source, string)", 
            "title": "%INDEX Function"
        }, 
        {
            "location": "/macros/using-macrofunc/#using-arithmetic-and-logical-expressions", 
            "text": "", 
            "title": "Using Arithmetic and Logical Expressions"
        }, 
        {
            "location": "/macros/using-macrofunc/#eval-function", 
            "text": "The  %EVAL  function evaluates arithmetic and logical expressions.  1 %EVAL (arithmetic or logical expression)       Arithmetic Expressions  Logical Expressions      1 + 2  DAY = FRIDAY    4 * 3  A   a    4 / 2  1    INDEX    00FFx - 003Ax  START NE  END      When  %EVAL  evaluates an  arithmetic expression , it temporarily converts operands to numeric values and performs an integer arithmetic operation. If the result of the expression is noninteger,  %EVAL  truncates the value to an integer. The result is expressed as text. The %EVAL function generates an error message in the log when it encounters an expression that contains noninteger values.   1\n2\n3\n4 /* Example */  %let   x   =   %eval ( 5 , 3 );  %put   x = x  /*result 1*/     When  %EVAL  evaluates a  logical expression , it returns a value of 0 to indicate that the expression is false, or a value of 1 to indicate that the expression is true.   1\n2\n3\n4\n5 /* Example */  %let   x   =   %eval ( 10   lt   2 );   /*10 less than 2*/  /*The expression is false*/  %put   x = x  /*result 0*/", 
            "title": "%EVAL Function"
        }, 
        {
            "location": "/macros/using-macrofunc/#sysevalf-function", 
            "text": "The  %SYSEVALF  function evaluates arithmetic and logical expressions using floating-point arithmetic and returns a value that is formatted using the  BEST32.  format (meaning that decimal contributions are not accounted in the operation). The result of the evaluation is always text.  1\n2\n3\n4\n5\n6 % SYSEVALF   ( expression   ,   conversion - type )  /* Example */  %let   value = %sysevalf ( 10.5 + 20.8 );  %put   10.5 + 20.8   =   value ;  /* result 10.5+20.8 = 30 */    You can use  %SYSEVALF  with an optional conversion type ( BOOLEAN ,  CEIL ,  FLOOR , or  INTEGER ) that tailors the value returned by  %SYSEVALF .", 
            "title": "%SYSEVALF Function"
        }, 
        {
            "location": "/macros/using-macrofunc/#using-sas-functions-with-macro-variables", 
            "text": "", 
            "title": "Using SAS Functions with Macro Variables"
        }, 
        {
            "location": "/macros/using-macrofunc/#sysfunc-function", 
            "text": "You can use the  %SYSFUNC  macro function to execute SAS functions within the macro facility.  1\n2\n3\n4 % SYSFUNC   ( SAS - function ( argument ( s ))   ,   format )  /* Example */  %let   current   =   %sysfunc ( time (),   time .);    Because  %SYSFUNC  is a macro function, you don't enclose character values in quotation marks, as you do in SAS functions. You can specify an optional format for the value returned by the function. If you do not specify a format, numeric results are converted to a character string using the  BEST12.  format. SAS returns character results as they are, without formatting or translation.  You can use almost any SAS function with the  %SYSFUNC  macro function. The exceptions are shown in this table.     Function Type  Function Name      Array processing  DIM ,  HBOUND ,  LBOUND    Variable information  VNAME ,  VLABEL ,  MISSING    Macro interface  RESOLVE ,  SYMGET    Data conversion  INPUT ,  PUT    Other functions  ORCMSG ,  LAG ,  DIF      Tip  Use  INPUTC  and  INPUTN  in place of  INPUT , and  PUTC  and  PUTN  in place of  PUT .", 
            "title": "%SYSFUNC Function"
        }, 
        {
            "location": "/macros/using-macrofunc/#using-macro-functions-to-mask-special-characters", 
            "text": "Macro quoting functions enable you to clearly indicate to the macro processor how it is to interpret special characters and mnemonics.", 
            "title": "Using Macro Functions to Mask Special Characters"
        }, 
        {
            "location": "/macros/using-macrofunc/#str-macro-quoting-function", 
            "text": "The macro quoting function  %STR  masks (or quotes) special characters during compilation so that the macro processor does not interpret them as macro-level syntax.  1 %STR (argument)    %STR  can also be used to quote tokens that typically occur in pairs, such as the apostrophe, quotation marks, and open and closed parentheses. The unmatched token must be preceded by a  % .\nHere is a list of all of the special characters and mnemonics masked by  %STR .  1\n2\n3 + - * /     = \u00ac ^ ~ ; , # blank\nAND OR NOT EQ NE LE LT GE GT IN    ) (   Note that  %STR  does not mask the characters   or  % .", 
            "title": "%STR Macro Quoting Function"
        }, 
        {
            "location": "/macros/using-macrofunc/#nrstr-macro-quoting-function", 
            "text": "%NRSTR  masks the same characters as  %STR  and also masks the special characters   and  % . Using  %NRSTR  instead of  %STR  prevents macro and macro variable resolution.  1 %NSTR (argument)", 
            "title": "%NRSTR Macro Quoting Function"
        }, 
        {
            "location": "/macros/using-macrofunc/#include-is-not-a-macro-language-statement", 
            "text": "The  %INCLUDE  statement retrieves SAS source code from an external file and places it on the input stack. It is a global SAS statement, not a macro language statement, and it can be used only on a statement boundary.  1 %INCLUDE file-specification  /SOURCE2  ;    To use the  %INCLUDE  statement, you specify the keyword  %INCLUDE , followed by a file specification. The file-specification value is the physical name or fileref of the file to be retrieved.   You can optionally specify  SOURCE2  following the file specification.  SOURCE2  causes the SAS statements that are inserted into the input stack to be displayed in the SAS log. If you don't specify  SOURCE2  in the  %INCLUDE  statement, the setting of the SAS system option  SOURCE2  controls whether the inserted code is displayed.", 
            "title": "%INCLUDE Is Not a Macro Language Statement"
        }, 
        {
            "location": "/macros/macrovar-at-execution-time/", 
            "text": "Creating a Macro Variable during \nDATA\n Step Execution\n\n\nUsing the \nCALL SYMPUTX\n Routine\n\n\nYou can use the SAS \nCALL\n routine \nSYMPUTX\n to create and assign a value to a macro variable during execution.\n\n\n1\nCALL SYMPUTX(macro-variable, value);\n\n\n\n\n\n\nCALL\n routines are similar to functions in that you can pass arguments to them and they perform a calculation or manipulation of the arguments. \nCALL\n routines differ from functions in that you can't use \nCALL\n routines in assignment statements or expressions. \nCALL\n routines don't return a value. Instead, they might alter the value of one or more arguments.\n\n\nTo invoke a SAS \nCALL\n routine, you specify the name of the routine after the keyword \nCALL\n in the \nCALL\n statement. The first argument to \nSYMPUTX\n is the name of the macro variable to be created or modified. It can be a character literal, a variable, or an expression that resolves to a valid macro variable name. The second argument is the value to be assigned to the macro variable. It can be a character or numeric constant, a variable, or an expression. If character literals are used as arguments, they must be quoted.\n\n\nExample:\n What is the value of \nfoot\n after execution of this \nDATA\n step?\n\n\n1\n2\n3\n4\ndata\n \n_null_\n;\n\n    \ncall\n \nsymputx\n(\nFoot\n,\nNo\n \nInternet\n \norders\n);\n\n    \n%let\n \nfoot\n=\nSome\n \nInternet\n \norders\n;\n\n\nrun\n;\n\n\n\n\n\n\n\nThe value of \nfoot\n is 'No Internet Orders'. Word scanning begins. The \n%LET\n is executed by the macro processor. The step boundary is reached and the \nDATA\n step executes. \nSYMPUTX\n changes the value of foot.\n\n\nUsing \nSYMPUTX\n with a \nDATA\n Step Variable\n\n\nYou can use the \nSYMPUTX\n routine to assign the value of a \nDATA\n step variable to a macro variable. This time, the second argument will be a \nDATA\n step variable.\n\n\n1\nCALL SYMPUTX(macro-variable, DATA-step-variable);\n\n\n\n\n\n\n\n\nThe value of the DATA step variable is assigned to the macro variable during execution.\n\n\nNumeric values are automatically converted to character using the BEST32. format.\n\n\nLeading and trailing blanks are removed from both arguments.\n\n\nYou can assign a maximum of 32,767 characters to the receiving macro variable.\n\n\n\n\nUsing \nSYMPUTX\n with a \nDATA\n Step Expression\n\n\nThe second argument to the \nSYMPUTX\n routine can also be an expression, and it can include arithmetic operations or function calls to manipulate data.\n\n\n1\nCALL SYMPUTX(macro-variable, expression);\n\n\n\n\n\n\nYou can use the \nPUT\n function as part of the call to the SYMPUTX routine in a \nDATA\n step to explicitly convert a numeric value to a character value.\n\n\n1\n2\n3\n4\nPUT (source, format.);\n\n/* Example: */\nCALL SYMPUTX (\ndate\n, PUT(Begin_Date, mmddyy10.));\n\n\n\n\n\n\nThis \nCALL\n statement assigns the value of the \nDATA\n step variable \nBegin_Date\n to the macro variable date. The \nPUT\n function explicitly converts the value of \nBegin_Dat\ne to a character value using the \nMMDDYY10.\n format. The conversion occurs before the value is assigned to the macro variable.\n\n\nPassing Data between Steps\n\n\n\n\nTip\n\n\nYou can use a \nDATA _NULL_\n step with the \nSYMPUTX\n routine to create macro variables and pass data between program steps.\n\n\n\n\nCreating Indirect References to Macro Variables\n\n\nYou can use the \nSYMPUTX\n routine with \nDATA\n step expressions for both arguments to create a series of macro variables, each with a unique name. \n\n\nTo create an \nindirect reference\n, you precede a name token with \nmultiple ampersands\n. When the macro processor encounters two ampersands, it resolves them to one ampersand and continues to rescan from left to right, from the point where the multiple ampersands begin. This action is known as the \nForward Rescan Rule\n.\n\n\nExample:\n Given the macro variables and values shown in the following global symbol table, the \nPROC PRINT\n step will print all classes that are taught in a particular city. The statement is written in such a way that you would need to change only the value of \ncrslo\nc in order for the \nPROC PRINT\n step to print classes that are taught in a different city.\n\n\nGlobal Symbol Table\n\n\n\n\n\n\n\n\nName\n\n\nValue\n\n\n\n\n\n\n\n\n\n\ncity1\n\n\nDallas\n\n\n\n\n\n\ncity2\n\n\nBoston\n\n\n\n\n\n\ncity3\n\n\nSeattle\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n%let\n \ncrsloc\n=\n2\n;\n\n\nproc\n \nprint\n \ndata\n=\nschedule\n;\n\n    \nwhere\n \nlocation\n=\ncity\ncrsloc\n;\n\n\nrun\n;\n\n\n\n\n\n\n\nYou precede the macro variable name \ncity\n with \ntwo ampersands\n. Then you add a reference to the macro variable \ncrsloc\n immediately after the first reference in order to build a new token.\n\n\nYou need to use \nthree ampersands\n in front of a macro variable name when its value exactly \nmatches the name of a second macro variable\n.\n\n\nExample:\n Given the macro variables and values shown in this global symbol table, the correspondance between each macro variable reference and its resolved value.\n\n\nGlobal Symbol Table\n\n\n\n\n\n\n\n\nName\n\n\nValue\n\n\n\n\n\n\n\n\n\n\ncat100\n\n\nOuterwear\n\n\n\n\n\n\ncat120\n\n\nAccessories\n\n\n\n\n\n\nsale\n\n\ncat100\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMacro variable\n\n\nResolved value\n\n\n\n\n\n\n\n\n\n\nsale\n\n\ncat100\n\n\n\n\n\n\nsale\n\n\ncat100\n\n\n\n\n\n\nsale\n\n\nOuterwear\n\n\n\n\n\n\n\n\nCreating Macro Variables Using \nPROC SQL\n\n\nCreating Multiple Macro Variables at a Time\n\n\nYou can also create or update macro variables during the execution of a \nPROC SQL\n step.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\nPROC SQL;\n      SELECT column-1\n,column-2,\u2026\n\n            INTO :macro-variable-1\n, :macro-variable-2,\u2026\n\n            FROM table-1 | view-1\n            \nWHERE expression\n\n            \nORDER BY order-by-item \n,...order-by-item\n\n            \nother clauses\n;\nQUIT;\n\n\n\n\n\n\n\n\nThe \nSELECT\n statement generates a report by selecting one or more columns from a table\n\n\nThe \nINTO\n clause in a \nSELECT\n statement enables you to create or update macro variables. The values of the selected columns are assigned to the new macro variables. You specify the keyword \nINTO\n, followed by a colon and then the name of the macro variable(s) to be created. Separate multiple macro variables with commas; each must start with a colon. The \ncolon doesn't become part of the name\n.\n\n\n\n\nUnlike the \n%LET\n statement and the \nSYMPUTX\n routine, the \nPROC SQL INTO\n clause doesn't remove leading and trailing blanks from the values. You can use a \n%LET\n statement to remove any leading or trailing blanks that are stored in the value.\n\n\n\n\nTip\n\n\nYou can use the \nPROC SQL NOPRINT\n option to suppress the report if you don't want output to be displayed.\n\n\n\n\nExample:\n The following \nSELECT\n statement creates a series of macro variables named \nplace1\n, \nplace2\n, \nplace3\n, and so on (as many new macro variables as are needed so that each new macro variable will be assigned a value of a distinct city and state where a student lives as provided in the data set variable \nCity_State\n). The first \nSELECT\n statement uses the \nN\n function to count the number of distinct \ncity_state\n values and assigns this number to the macro variable \nnumlocs\n.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nproc\n \nsql\n;\n\n    \nselect\n \nN\n(\ndistinct\n \ncity_state\n)\n\n      \ninto\n \n:\nnumlocs\n\n    \nfrom\n \nstudents\n;\n\n\n    \n%let\n \nnumlocs\n=\nnumlocs\n;\n\n    \nselect\n \ndistinct\n \ncity_state\n\n      \ninto\n \n:\nplace1\n-:\nplace\nnumlocs\n\n    \nfrom\n \nstudents\n;\n\n\nquit\n;\n\n\n\n\n\n\n\nStoring a List of Values in a Macro Variable\n\n\nYou can use the \nINTO\n clause in a \nPROC SQL\n step to create one new macro variable for each row in the result of the \nSELECT\n statement. You can use an alternate form of the \nINTO\n clause in order to take all values of a column (variable) and concatenate them into the value of one macro variable.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nPROC SQL NOPRINT;\n      SELECT \nDISTINCT\ncolumn-1\n            INTO :macro-variable-1\n            SEPARATED BY \ndelimiter-1\n\n            FROM table-1 | view-1\n            \nWHERE expression\n\n            \nother clauses\n;\nQUIT;\n\n%PUT \n=macro-variable-1;\n\n\n\n\n\n\n\n\nThe \nINTO\n clause names the macro variable to be created\n\n\nThe \nSEPARATED BY\n clause specifies the character that will be used as a delimiter in the value of the macro variable (notice that the character is enclosed in quotation marks)\n\n\nThe \nDISTINCT\n keyword eliminates duplicates by selecting unique values of the selected column\n\n\nAfter you execute the \nPROC SQL\n step, you can use the \n%PUT\n statement to write the values to the log", 
            "title": "Creating Macro Variables at Execution Time"
        }, 
        {
            "location": "/macros/macrovar-at-execution-time/#creating-a-macro-variable-during-data-step-execution", 
            "text": "", 
            "title": "Creating a Macro Variable during DATA Step Execution"
        }, 
        {
            "location": "/macros/macrovar-at-execution-time/#using-the-call-symputx-routine", 
            "text": "You can use the SAS  CALL  routine  SYMPUTX  to create and assign a value to a macro variable during execution.  1 CALL SYMPUTX(macro-variable, value);   CALL  routines are similar to functions in that you can pass arguments to them and they perform a calculation or manipulation of the arguments.  CALL  routines differ from functions in that you can't use  CALL  routines in assignment statements or expressions.  CALL  routines don't return a value. Instead, they might alter the value of one or more arguments.  To invoke a SAS  CALL  routine, you specify the name of the routine after the keyword  CALL  in the  CALL  statement. The first argument to  SYMPUTX  is the name of the macro variable to be created or modified. It can be a character literal, a variable, or an expression that resolves to a valid macro variable name. The second argument is the value to be assigned to the macro variable. It can be a character or numeric constant, a variable, or an expression. If character literals are used as arguments, they must be quoted.  Example:  What is the value of  foot  after execution of this  DATA  step?  1\n2\n3\n4 data   _null_ ; \n     call   symputx ( Foot , No   Internet   orders ); \n     %let   foot = Some   Internet   orders ;  run ;    The value of  foot  is 'No Internet Orders'. Word scanning begins. The  %LET  is executed by the macro processor. The step boundary is reached and the  DATA  step executes.  SYMPUTX  changes the value of foot.", 
            "title": "Using the CALL SYMPUTX Routine"
        }, 
        {
            "location": "/macros/macrovar-at-execution-time/#using-symputx-with-a-data-step-variable", 
            "text": "You can use the  SYMPUTX  routine to assign the value of a  DATA  step variable to a macro variable. This time, the second argument will be a  DATA  step variable.  1 CALL SYMPUTX(macro-variable, DATA-step-variable);    The value of the DATA step variable is assigned to the macro variable during execution.  Numeric values are automatically converted to character using the BEST32. format.  Leading and trailing blanks are removed from both arguments.  You can assign a maximum of 32,767 characters to the receiving macro variable.", 
            "title": "Using SYMPUTX with a DATA Step Variable"
        }, 
        {
            "location": "/macros/macrovar-at-execution-time/#using-symputx-with-a-data-step-expression", 
            "text": "The second argument to the  SYMPUTX  routine can also be an expression, and it can include arithmetic operations or function calls to manipulate data.  1 CALL SYMPUTX(macro-variable, expression);   You can use the  PUT  function as part of the call to the SYMPUTX routine in a  DATA  step to explicitly convert a numeric value to a character value.  1\n2\n3\n4 PUT (source, format.);\n\n/* Example: */\nCALL SYMPUTX ( date , PUT(Begin_Date, mmddyy10.));   This  CALL  statement assigns the value of the  DATA  step variable  Begin_Date  to the macro variable date. The  PUT  function explicitly converts the value of  Begin_Dat e to a character value using the  MMDDYY10.  format. The conversion occurs before the value is assigned to the macro variable.", 
            "title": "Using SYMPUTX with a DATA Step Expression"
        }, 
        {
            "location": "/macros/macrovar-at-execution-time/#passing-data-between-steps", 
            "text": "Tip  You can use a  DATA _NULL_  step with the  SYMPUTX  routine to create macro variables and pass data between program steps.", 
            "title": "Passing Data between Steps"
        }, 
        {
            "location": "/macros/macrovar-at-execution-time/#creating-indirect-references-to-macro-variables", 
            "text": "You can use the  SYMPUTX  routine with  DATA  step expressions for both arguments to create a series of macro variables, each with a unique name.   To create an  indirect reference , you precede a name token with  multiple ampersands . When the macro processor encounters two ampersands, it resolves them to one ampersand and continues to rescan from left to right, from the point where the multiple ampersands begin. This action is known as the  Forward Rescan Rule .  Example:  Given the macro variables and values shown in the following global symbol table, the  PROC PRINT  step will print all classes that are taught in a particular city. The statement is written in such a way that you would need to change only the value of  crslo c in order for the  PROC PRINT  step to print classes that are taught in a different city.  Global Symbol Table     Name  Value      city1  Dallas    city2  Boston    city3  Seattle     1\n2\n3\n4 %let   crsloc = 2 ;  proc   print   data = schedule ; \n     where   location = city crsloc ;  run ;    You precede the macro variable name  city  with  two ampersands . Then you add a reference to the macro variable  crsloc  immediately after the first reference in order to build a new token.  You need to use  three ampersands  in front of a macro variable name when its value exactly  matches the name of a second macro variable .  Example:  Given the macro variables and values shown in this global symbol table, the correspondance between each macro variable reference and its resolved value.  Global Symbol Table     Name  Value      cat100  Outerwear    cat120  Accessories    sale  cat100        Macro variable  Resolved value      sale  cat100    sale  cat100    sale  Outerwear", 
            "title": "Creating Indirect References to Macro Variables"
        }, 
        {
            "location": "/macros/macrovar-at-execution-time/#creating-macro-variables-using-proc-sql", 
            "text": "", 
            "title": "Creating Macro Variables Using PROC SQL"
        }, 
        {
            "location": "/macros/macrovar-at-execution-time/#creating-multiple-macro-variables-at-a-time", 
            "text": "You can also create or update macro variables during the execution of a  PROC SQL  step.  1\n2\n3\n4\n5\n6\n7\n8 PROC SQL;\n      SELECT column-1 ,column-2,\u2026 \n            INTO :macro-variable-1 , :macro-variable-2,\u2026 \n            FROM table-1 | view-1\n             WHERE expression \n             ORDER BY order-by-item  ,...order-by-item \n             other clauses ;\nQUIT;    The  SELECT  statement generates a report by selecting one or more columns from a table  The  INTO  clause in a  SELECT  statement enables you to create or update macro variables. The values of the selected columns are assigned to the new macro variables. You specify the keyword  INTO , followed by a colon and then the name of the macro variable(s) to be created. Separate multiple macro variables with commas; each must start with a colon. The  colon doesn't become part of the name .   Unlike the  %LET  statement and the  SYMPUTX  routine, the  PROC SQL INTO  clause doesn't remove leading and trailing blanks from the values. You can use a  %LET  statement to remove any leading or trailing blanks that are stored in the value.   Tip  You can use the  PROC SQL NOPRINT  option to suppress the report if you don't want output to be displayed.   Example:  The following  SELECT  statement creates a series of macro variables named  place1 ,  place2 ,  place3 , and so on (as many new macro variables as are needed so that each new macro variable will be assigned a value of a distinct city and state where a student lives as provided in the data set variable  City_State ). The first  SELECT  statement uses the  N  function to count the number of distinct  city_state  values and assigns this number to the macro variable  numlocs .   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 proc   sql ; \n     select   N ( distinct   city_state ) \n       into   : numlocs \n     from   students ; \n\n     %let   numlocs = numlocs ; \n     select   distinct   city_state \n       into   : place1 -: place numlocs \n     from   students ;  quit ;", 
            "title": "Creating Multiple Macro Variables at a Time"
        }, 
        {
            "location": "/macros/macrovar-at-execution-time/#storing-a-list-of-values-in-a-macro-variable", 
            "text": "You can use the  INTO  clause in a  PROC SQL  step to create one new macro variable for each row in the result of the  SELECT  statement. You can use an alternate form of the  INTO  clause in order to take all values of a column (variable) and concatenate them into the value of one macro variable.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 PROC SQL NOPRINT;\n      SELECT  DISTINCT column-1\n            INTO :macro-variable-1\n            SEPARATED BY  delimiter-1 \n            FROM table-1 | view-1\n             WHERE expression \n             other clauses ;\nQUIT;\n\n%PUT  =macro-variable-1;    The  INTO  clause names the macro variable to be created  The  SEPARATED BY  clause specifies the character that will be used as a delimiter in the value of the macro variable (notice that the character is enclosed in quotation marks)  The  DISTINCT  keyword eliminates duplicates by selecting unique values of the selected column  After you execute the  PROC SQL  step, you can use the  %PUT  statement to write the values to the log", 
            "title": "Storing a List of Values in a Macro Variable"
        }, 
        {
            "location": "/macros/macroprog/", 
            "text": "Basic Concepts\n\n\nMacros are compiled programs that you can execute independently or call as part of a SAS program. Like macro variables, you generally use macros to generate text. But, macros provide additional capabilities. Macros can contain programming statements that enable you to control how and when the text is generated. Also, macros can accept parameters that specify variables. Using parameters, you can write generic macros that can serve a number of uses.\n\n\nA SAS program can reference any number of macros, and you can invoke a macro any number of times within a single SAS program. There are three steps to create and use a macro: \ndefine\n the macro, \ncompile\n the macro, and \ncall\n the macro.\n\n\nDefine a Macro\n\n\nA macro definition is the container that holds a macro program. You begin a macro definition with a \n%MACRO\n statement and end it with a \n%MEND\n statement.\n\n\n1\n2\n3\n%MACRO macro-name;\n\n  \ntext\n\n\n%MEND \nmacro-name\n;\n\n\n\n\n\n\n\nEach macro that you define has a distinct name. Choose a name for your macro that indicates what the macro does and follow standard SAS naming conventions. Avoid SAS language keywords or call routine names, as well as words that are reserved by the SAS macro facility. You are not required to include the macro name in the \n%MEND\n statement. But if you do, be sure that the two macro names match.\n\n\nThe text can include constant text, SAS data set names, SAS variable names, and SAS statements. It can also include macro variables (\ncountry\n), macro functions (\n%sysfunc()\n), macro program statements (\n%put sysdate=\nsysdate;\n) and any combination of the above.\n\n\nCompile a Macro\n\n\nTo compile a macro, submit the code. If the macro processor finds syntax errors in the macro language statements, it writes any error messages to the SAS log and creates a dummy, non-executable macro.\n\n\nIf the macro processor does not find any macro-level syntax errors, it compiles the macro and stores the compiled definition in a temporary SAS catalog named \nWork.Sasmacr\n. Macros in this catalog are known as \nsession-compiled\n macros and are available for execution during the SAS session in which they're compiled. SAS deletes the temporary catalog that stores the macros at the end of the session.\n\n\nCall a Macro\n\n\nTo call a macro, precede the name of the macro with a percent sign anywhere in a program, except within the data lines of a \nDATALINES\n statement. The macro call requires no semicolon because it is not a SAS statement.\n\n\n1\n%macro\n-\nname\n\n\n\n\n\n\n\nWhen you call a macro, the word scanner passes the macro call to the macro processor. The macro processor searches \nWork.Sasmacr\n for that macro. Then it executes compiled macro language statements within the macro and sends any remaining text to the input stack. When the SAS compiler receives a global SAS statement or encounters a SAS step boundary, the macro processor suspends macro execution. After the SAS code executes, the macro processor resumes execution of macro language statements. The macro processor ends execution when it reaches the \n%MEND\n statement.\n\n\nUsing Macro Parameters\n\n\nUsing Parameters in the Macro Definition\n\n\nYou can use parameter lists in order to update the macro variables within your macros. Each time that you call the macro, the parameter list passes values to the macro variables as part of the call, rather than by using separate \n%LET\n statements.\n\n\nA parameter list is an optional part of the \n%MACRO\n statement. The list names one or more macro variables whose values you specify when you call the macro. When you call a macro, SAS automatically creates a macro variable for each parameter.\n\n\nThere are two types of parameters that you can use to update macro variables in your macro programs: \npositional parameters\n and \nkeyword parameters\n.\n\n\nUsing Positional Parameters\n\n\nTo define macros that include positional parameters, you list the names of the macro variables in parentheses and separate the names using commas in the \n%MACRO\n statement.\n\n\n1\n2\n3\n%MACRO macro-name(parameter-1 \n, ...parameter-n\n);\n\n  \ntext\n\n\n%MEND \nmacro-name\n;\n\n\n\n\n\n\n\nTo call a macro that includes positional parameters, you precede the name of the macro with a percent sign and enclose the parameter values in parentheses. You must specify the values in the same order as in the macro definition and separate them with commas.\n\n\n1\n%macro\n-\nname\n(\nvalue\n-\n1\n \n,\n \n...\nvalue\n-\nn\n)\n\n\n\n\n\n\n\nThe values listed in a macro call can be text, macro variable references, macro calls, or null values. SAS assigns these values to the parameters using a one-to-one correspondence.\n\n\nUsing Keyword Parameters\n\n\nWhen you use keyword parameters, you list both the name and the value of each macro variable in the macro definition. First specify the keyword, or macro variable name, then the equal sign, and then the value.\n\n\n1\n2\n3\n%MACRO macro-name(keyword=value, ..., keyword=value);\n\n  \ntext\n\n\n%MEND \nmacro-name\n;\n\n\n\n\n\n\n\nYou can list keyword parameters in any order. Whatever value you assign to each parameter, or macro variable, in the \n%MACRO\n statement becomes its default value. Null values are allowed.\n\n\nTo call a macro that includes keyword parameters, you specify both the keyword and the value for the desired parameters, in any order. If you omit a keyword parameter from the macro call, SAS assigns the keyword parameter its default value.\n\n\n1\n%macro\n-\nname\n(\nkeyword\n=\nvalue\n,\n \n...,\n \nkeyword\n=\nvalue\n)\n\n\n\n\n\n\n\nMixed Parameter Lists\n\n\nYou can use a parameter list that includes both positional and keyword parameters. You must list all \npositional parameters in the \n%MACRO\n statement before any keyword parameters\n.\n\n\n1\n2\n3\n4\n%MACRO macro-name(parameter-1\n, ...parameter-n\n\n                  \nkeyword\n=\nvalue\n,\n \n..., keyword=value);\n\n    \ntext\n\n\n%MEND \nmacro-name\n;\n\n\n\n\n\n\n\nSimilarly, when you call a macro that includes a mixed parameter list, you must list the positional values before any keyword values.\n\n\n1\n2\n%macro\n-\nname\n(\nvalue\n-\n1\n,\n \n...\nvalue\n-\nn\n,\n\n                        \nkeyword\n=\nvalue\n,\n \n...,\n \nkeyword\n=\nvalue\n);\n\n\n\n\n\n\n\nGlobal and Local Symbol Tables\n\n\nGlobal Symbol Table\n\n\nThe \nglobal symbol table\n is created during SAS initialization, initialized with automatic macro variables and their values, and deleted at the end of the session. The values of user-defined macro variables are often stored in the global symbol table as well.\n\n\nMacro variables stored in the global symbol table are called global macro variables. To create a global macro variable, you can use a \n%LET\n statement anywhere in a program, except within a macro definition. You can also call the \nSYMPUTX\n routine in a \nDATA\n step or use a \nSELECT\n statement that contains an \nINTO\n clause in a \nPROC SQL\n step.\n\n\nThe \n%GLOBAL\n statement enables you to create one or more global macro variables. You specify the keyword \n%GLOBAL\n, followed by a list of the macro variables that you want to create, separated by spaces.\n\n\n1\n%GLOBAL macro-variable-1 ...macro-variable-n;\n\n\n\n\n\n\n\nYou can use the \n%GLOBAL\n statement anywhere in a SAS program, either inside or outside a macro definition. If the named variables do not exist in the global symbol table, SAS creates them and assigns a null value. If the macro variables already exist, then this statement does not change their values.\n\n\nTo delete a macro variable from the global symbol table, you use the %SYMDEL statement.\n\n\n1\n%SYMDEL macro-variables;\n\n\n\n\n\n\n\nWorking with Local Macro Variables\n\n\nSAS creates a \nlocal symbol table\n when you call a macro that includes a parameter list or when a local macro variable is created during macro execution. You should use local macro variables instead of global macro variables whenever possible. Local macro variables exist only while the macro is executing. When the macro ends, SAS deletes the local symbol table and the memory used by that table can be reused.\n\n\nTo create local macro variables, you can use parameters in a macro definition or you can use one of the following methods inside a macro definition: a \n%LET\n statement, the \nSYMPUTX\n routine in a \nDATA\n step, or a \nSELECT\n statement that contains an \nINTO\n clause in a \nPROC SQL\n step.\n\n\nThe \nSYMPUTX\n routine only creates a local macro variable if a local symbol table already exists. If no local symbol table exists, the \nSYMPUTX\n routine creates a global variable. Another way to create a local macro variable is to use the \n%LOCAL\n statement inside a macro definition.\n\n\n1\n%LOCAL macro-variable-1 ...macro-variable-n;\n\n\n\n\n\n\n\nYou specify the keyword, \n%LOCAL\n, followed by a list of the macro variables that you want to create, separated by spaces. The \n%LOCAL\n statement creates one or more macro variables in the local symbol table and assigns null values to them. It has no effect on variables that are already in the local table.\n\n\nBecause local symbol tables exist separately from the global symbol table, it is possible to have a local macro variable and a global macro variable that have the same name and different values.\n\n\nYou should use a local variable as the index variable for macro loops. This prevents accidental contamination of a like-named macro variable in the global symbol table or in another local table. If you define a macro program that calls another macro program, and if both macros create local symbol tables, then two local symbol tables will exist while the second macro executes.\n\n\nYou can use the \nSYMPUTX\n routine with an optional third argument, scope, in order to specify where a macro variable should be stored.\n\n\n1\nCALL SYMPUTX(macro-variable, value \n,scope\n);\n\n\n\n\n\n\nYou specify either a scope of \nG\n to indicate that the macro variable is to be created in the global symbol table, or a scope of \nL\n to indicate that the macro variable is to be created in the local symbol table. If no local symbol table exists for the current macro, then one will be created.\n\n\nWriting Utility Macros\n\n\nSometimes there are routine tasks that you need to do repeatedly. It can be useful to define a macro so that the program code for these tasks can be easily reused. To save these utility macros so that you can reuse them in the future, \nyou can store the compiled macro definitions in a permanent SAS catalog\n.\n\n\nSetting the two system options, \nMSTORED\n and \nSASMSTORE=\n, enables you to store macros in a permanent library and specifies that the macro facility will search for compiled macros in the SAS data library that is referenced by the \nSASMSTORE=\n option. This libref cannot be work.\n\n\n1\n2\nOPTIONS MSTORED | NOMSTORED;\nOPTIONS SAMSTORE=libref;\n\n\n\n\n\n\nTo create a permanently stored compiled macro, you use the \nSTORE\n option in the \n%MACRO\n statement when you submit the macro definition. You can assign a descriptive title for the macro entry in the SAS catalog, by specifying the \nDES=\n option.\n\n\n1\n2\n3\n%MACRO macro-name \n(parameter-list)\n / STORE \nDES=\ndescription\n;\n\n        \ntext\n\n\n%MEND \nmacro-name\n;\n\n\n\n\n\n\n\nTo access a stored compiled macro, you must set the \nMSTORED\n and \nSASMSTORE=\n system options, if they are not already set, and then simply call the macro.\n\n\n1\n2\n3\nOPTIONS\n \nMSTORED\n;\n\n\nOPTIONS\n \nSAMSTORE\n=\nlibref\n;\n\n\n%macro\n-\nname", 
            "title": "Creating and Using Macro Programs"
        }, 
        {
            "location": "/macros/macroprog/#basic-concepts", 
            "text": "Macros are compiled programs that you can execute independently or call as part of a SAS program. Like macro variables, you generally use macros to generate text. But, macros provide additional capabilities. Macros can contain programming statements that enable you to control how and when the text is generated. Also, macros can accept parameters that specify variables. Using parameters, you can write generic macros that can serve a number of uses.  A SAS program can reference any number of macros, and you can invoke a macro any number of times within a single SAS program. There are three steps to create and use a macro:  define  the macro,  compile  the macro, and  call  the macro.", 
            "title": "Basic Concepts"
        }, 
        {
            "location": "/macros/macroprog/#define-a-macro", 
            "text": "A macro definition is the container that holds a macro program. You begin a macro definition with a  %MACRO  statement and end it with a  %MEND  statement.  1\n2\n3 %MACRO macro-name; \n   text  %MEND  macro-name ;    Each macro that you define has a distinct name. Choose a name for your macro that indicates what the macro does and follow standard SAS naming conventions. Avoid SAS language keywords or call routine names, as well as words that are reserved by the SAS macro facility. You are not required to include the macro name in the  %MEND  statement. But if you do, be sure that the two macro names match.  The text can include constant text, SAS data set names, SAS variable names, and SAS statements. It can also include macro variables ( country ), macro functions ( %sysfunc() ), macro program statements ( %put sysdate= sysdate; ) and any combination of the above.", 
            "title": "Define a Macro"
        }, 
        {
            "location": "/macros/macroprog/#compile-a-macro", 
            "text": "To compile a macro, submit the code. If the macro processor finds syntax errors in the macro language statements, it writes any error messages to the SAS log and creates a dummy, non-executable macro.  If the macro processor does not find any macro-level syntax errors, it compiles the macro and stores the compiled definition in a temporary SAS catalog named  Work.Sasmacr . Macros in this catalog are known as  session-compiled  macros and are available for execution during the SAS session in which they're compiled. SAS deletes the temporary catalog that stores the macros at the end of the session.", 
            "title": "Compile a Macro"
        }, 
        {
            "location": "/macros/macroprog/#call-a-macro", 
            "text": "To call a macro, precede the name of the macro with a percent sign anywhere in a program, except within the data lines of a  DATALINES  statement. The macro call requires no semicolon because it is not a SAS statement.  1 %macro - name    When you call a macro, the word scanner passes the macro call to the macro processor. The macro processor searches  Work.Sasmacr  for that macro. Then it executes compiled macro language statements within the macro and sends any remaining text to the input stack. When the SAS compiler receives a global SAS statement or encounters a SAS step boundary, the macro processor suspends macro execution. After the SAS code executes, the macro processor resumes execution of macro language statements. The macro processor ends execution when it reaches the  %MEND  statement.", 
            "title": "Call a Macro"
        }, 
        {
            "location": "/macros/macroprog/#using-macro-parameters", 
            "text": "", 
            "title": "Using Macro Parameters"
        }, 
        {
            "location": "/macros/macroprog/#using-parameters-in-the-macro-definition", 
            "text": "You can use parameter lists in order to update the macro variables within your macros. Each time that you call the macro, the parameter list passes values to the macro variables as part of the call, rather than by using separate  %LET  statements.  A parameter list is an optional part of the  %MACRO  statement. The list names one or more macro variables whose values you specify when you call the macro. When you call a macro, SAS automatically creates a macro variable for each parameter.  There are two types of parameters that you can use to update macro variables in your macro programs:  positional parameters  and  keyword parameters .", 
            "title": "Using Parameters in the Macro Definition"
        }, 
        {
            "location": "/macros/macroprog/#using-positional-parameters", 
            "text": "To define macros that include positional parameters, you list the names of the macro variables in parentheses and separate the names using commas in the  %MACRO  statement.  1\n2\n3 %MACRO macro-name(parameter-1  , ...parameter-n ); \n   text  %MEND  macro-name ;    To call a macro that includes positional parameters, you precede the name of the macro with a percent sign and enclose the parameter values in parentheses. You must specify the values in the same order as in the macro definition and separate them with commas.  1 %macro - name ( value - 1   ,   ... value - n )    The values listed in a macro call can be text, macro variable references, macro calls, or null values. SAS assigns these values to the parameters using a one-to-one correspondence.", 
            "title": "Using Positional Parameters"
        }, 
        {
            "location": "/macros/macroprog/#using-keyword-parameters", 
            "text": "When you use keyword parameters, you list both the name and the value of each macro variable in the macro definition. First specify the keyword, or macro variable name, then the equal sign, and then the value.  1\n2\n3 %MACRO macro-name(keyword=value, ..., keyword=value); \n   text  %MEND  macro-name ;    You can list keyword parameters in any order. Whatever value you assign to each parameter, or macro variable, in the  %MACRO  statement becomes its default value. Null values are allowed.  To call a macro that includes keyword parameters, you specify both the keyword and the value for the desired parameters, in any order. If you omit a keyword parameter from the macro call, SAS assigns the keyword parameter its default value.  1 %macro - name ( keyword = value ,   ...,   keyword = value )", 
            "title": "Using Keyword Parameters"
        }, 
        {
            "location": "/macros/macroprog/#mixed-parameter-lists", 
            "text": "You can use a parameter list that includes both positional and keyword parameters. You must list all  positional parameters in the  %MACRO  statement before any keyword parameters .  1\n2\n3\n4 %MACRO macro-name(parameter-1 , ...parameter-n \n                   keyword = value ,   ..., keyword=value); \n     text  %MEND  macro-name ;    Similarly, when you call a macro that includes a mixed parameter list, you must list the positional values before any keyword values.  1\n2 %macro - name ( value - 1 ,   ... value - n , \n                         keyword = value ,   ...,   keyword = value );", 
            "title": "Mixed Parameter Lists"
        }, 
        {
            "location": "/macros/macroprog/#global-and-local-symbol-tables", 
            "text": "", 
            "title": "Global and Local Symbol Tables"
        }, 
        {
            "location": "/macros/macroprog/#global-symbol-table", 
            "text": "The  global symbol table  is created during SAS initialization, initialized with automatic macro variables and their values, and deleted at the end of the session. The values of user-defined macro variables are often stored in the global symbol table as well.  Macro variables stored in the global symbol table are called global macro variables. To create a global macro variable, you can use a  %LET  statement anywhere in a program, except within a macro definition. You can also call the  SYMPUTX  routine in a  DATA  step or use a  SELECT  statement that contains an  INTO  clause in a  PROC SQL  step.  The  %GLOBAL  statement enables you to create one or more global macro variables. You specify the keyword  %GLOBAL , followed by a list of the macro variables that you want to create, separated by spaces.  1 %GLOBAL macro-variable-1 ...macro-variable-n;    You can use the  %GLOBAL  statement anywhere in a SAS program, either inside or outside a macro definition. If the named variables do not exist in the global symbol table, SAS creates them and assigns a null value. If the macro variables already exist, then this statement does not change their values.  To delete a macro variable from the global symbol table, you use the %SYMDEL statement.  1 %SYMDEL macro-variables;", 
            "title": "Global Symbol Table"
        }, 
        {
            "location": "/macros/macroprog/#working-with-local-macro-variables", 
            "text": "SAS creates a  local symbol table  when you call a macro that includes a parameter list or when a local macro variable is created during macro execution. You should use local macro variables instead of global macro variables whenever possible. Local macro variables exist only while the macro is executing. When the macro ends, SAS deletes the local symbol table and the memory used by that table can be reused.  To create local macro variables, you can use parameters in a macro definition or you can use one of the following methods inside a macro definition: a  %LET  statement, the  SYMPUTX  routine in a  DATA  step, or a  SELECT  statement that contains an  INTO  clause in a  PROC SQL  step.  The  SYMPUTX  routine only creates a local macro variable if a local symbol table already exists. If no local symbol table exists, the  SYMPUTX  routine creates a global variable. Another way to create a local macro variable is to use the  %LOCAL  statement inside a macro definition.  1 %LOCAL macro-variable-1 ...macro-variable-n;    You specify the keyword,  %LOCAL , followed by a list of the macro variables that you want to create, separated by spaces. The  %LOCAL  statement creates one or more macro variables in the local symbol table and assigns null values to them. It has no effect on variables that are already in the local table.  Because local symbol tables exist separately from the global symbol table, it is possible to have a local macro variable and a global macro variable that have the same name and different values.  You should use a local variable as the index variable for macro loops. This prevents accidental contamination of a like-named macro variable in the global symbol table or in another local table. If you define a macro program that calls another macro program, and if both macros create local symbol tables, then two local symbol tables will exist while the second macro executes.  You can use the  SYMPUTX  routine with an optional third argument, scope, in order to specify where a macro variable should be stored.  1 CALL SYMPUTX(macro-variable, value  ,scope );   You specify either a scope of  G  to indicate that the macro variable is to be created in the global symbol table, or a scope of  L  to indicate that the macro variable is to be created in the local symbol table. If no local symbol table exists for the current macro, then one will be created.", 
            "title": "Working with Local Macro Variables"
        }, 
        {
            "location": "/macros/macroprog/#writing-utility-macros", 
            "text": "Sometimes there are routine tasks that you need to do repeatedly. It can be useful to define a macro so that the program code for these tasks can be easily reused. To save these utility macros so that you can reuse them in the future,  you can store the compiled macro definitions in a permanent SAS catalog .  Setting the two system options,  MSTORED  and  SASMSTORE= , enables you to store macros in a permanent library and specifies that the macro facility will search for compiled macros in the SAS data library that is referenced by the  SASMSTORE=  option. This libref cannot be work.  1\n2 OPTIONS MSTORED | NOMSTORED;\nOPTIONS SAMSTORE=libref;   To create a permanently stored compiled macro, you use the  STORE  option in the  %MACRO  statement when you submit the macro definition. You can assign a descriptive title for the macro entry in the SAS catalog, by specifying the  DES=  option.  1\n2\n3 %MACRO macro-name  (parameter-list)  / STORE  DES= description ; \n         text  %MEND  macro-name ;    To access a stored compiled macro, you must set the  MSTORED  and  SASMSTORE=  system options, if they are not already set, and then simply call the macro.  1\n2\n3 OPTIONS   MSTORED ;  OPTIONS   SAMSTORE = libref ;  %macro - name", 
            "title": "Writing Utility Macros"
        }, 
        {
            "location": "/macros/macroprog-processing/", 
            "text": "Processing Statements Conditionally\n\n\nYou can use \n%IF-%THEN\n and \n%ELSE\n statements to perform conditional processing in a macro program.\n\n\n1\n2\n%IF expression %THEN text;\n\n\n%ELSE text;\n\n\n\n\n\n\n\nYou specify the keyword \n%IF\n followed by an expression. The expression can be any valid macro expression that resolves to an integer. Then, you specify the keyword \n%THEN\n followed by some text. The text can be a SAS constant, a text expression, a macro variable reference, a macro call, or a macro program statement. The \n%ELSE\n statement is optional. The text in a \n%ELSE\n statement can also be a SAS constant, a text expression, a macro variable reference, a macro call, or a macro program statement.\n\n\nIf the expression following \n%IF\n \nresolves to zero\n, the expression is false and the \n%THEN\n text isn't processed. If you include an optional \n%ELSE\n statement, that text is processed instead of the \n%THEN\n text. If the expression \nresolves to any integer other than zero\n, then the expression is true and the \n%THEN\n text is processed. If the expression \nresolves to null or to any noninteger value\n, SAS issues an error message.\n\n\nThere are several important differences between the macro \n%IF-%THEN\n statement and the \nDATA\n step \nIF-THEN\n statement.\n\n\n\n\n\n\n\n\n%IF-%THEN\n\n\nIF-THEN\n\n\n\n\n\n\n\n\n\n\nIs used only in a macro program\n\n\nIs used only in a \nDATA\n step program\n\n\n\n\n\n\nExecutes during macro execution\n\n\nExecutes during \nDATA\n step execution\n\n\n\n\n\n\nUses macro variables in logical expressions and cannot refer to \nDATA\n step variables in logical expressions\n\n\nUses \nDATA\n step variables in logical expressions\n\n\n\n\n\n\nDetermines what text should be copied to the input stack\n\n\nDetermines what \nDATA\n step statement(s) should be executed\n\n\n\n\n\n\n\n\nMacro expressions are similar to SAS expressions in the following ways:\n\n\n\n\nThey use the same arithmetic, logical, and comparison operators as SAS expressions\n\n\nThey are case sensitive\n\n\nSpecial \nWHERE\n operators are not valid\n\n\n\n\nMacro expressions are dissimilar to SAS expressions in the following ways:\n\n\n\n\nCharacter operands are not quoted\n\n\nExpressions in which comparison operators surround a macro expression might resolve with unexpected results\n\n\n\n\nYou can use \n%DO\n and \n%END\n statements along with a \n%IF-%THEN\n statement to generate code that contains semicolons.\n\n\n1\n2\n3\n4\n5\n6\n%IF expression %THEN %DO;\n\n           \ntext\n \nand\n/\nor\n \nmacro\n \nlanguage\n \nelements\n;\n\n\n%END;\n\n\n%ELSE %DO;\n\n           \ntext\n \nand\n/\nor\n \nmacro\n \nlanguage\n \nelements\n;\n\n\n%END;\n\n\n\n\n\n\n\nThe syntax for using \n%DO\n and \n%END\n statements with a \n%IF-%THEN\n statement is shown here. The keyword \n%DO\n follows the keyword \n%THEN\n. You must pair each \n%DO\n statement with a \n%END\n statement. Between the \n%DO\n and the \n%END\n keywords, you insert one or more statements that contain constant text, text expressions, or macro statements.\n\n\nUsing Conditional Processing to Validate Parameters: the \nIN\n operator\n\n\nYou can use the comparison operator \nIN\n to search for character and numeric values that are equal to one from a list of values. You can also use the \nIN\n operator when you evaluate arithmetic or logical expressions during macro execution.\n\n\n1\n%MACRO macro-name \n(parameter-list) \n/MINOPERATOR | NOMINOPERATOR;\n\n\n\n\n\n\n\nTo use the macro \nIN\n operator, you use the \nMINOPERATOR\n option with the \n%MACRO\n statement, preceded by a slash. Then you can use the macro \nIN\n operator to modify the \n%IF\n statement. The macro \nIN\n operator is similar to the SAS \nIN\n operator, but it doesn't require parentheses.\n\n\nIf you use \nNOT\n with the \nIN\n operator, \nNOT\n must precede the \nIN\n expression and parentheses are required around the expression.\nYou can use the \nPROC SQL INTO\n clause with the \nSEPARATED BY\n argument to create a macro variable that contains a list of unique values. You can combine \nPROC SQL\n with conditional processing to validate the parameters in a macro.\n\n\nProcessing Statements Iteratively: the \n%DO\n statement\n\n\nWith the iterative \n%DO\n statement, you can repeatedly execute macro programming code and generate SAS code.\n\n\n1\n2\n3\n%DO index-variable=start %TO stop \n%BY increment\n;\n\n           \ntext\n \nand\n/\nor\n \nmacro\n \nlanguage\n \nelements\n;\n\n\n%END;\n\n\n\n\n\n\n\n\n\nThe values for \nstart\n, \nstop\n, and \nincrement\n must be integers or macro expressions that resolve to integer values.\n\n\nThe \n%BY\n clause is optional. Increment specifies either an integer (other than 0) or a macro expression that generates an integer to be added to the value of the index variable in each iteration of the loop. By default, the increment is 1.\n\n\nA \n%END\n statement ends the iterative loop. \n\n\n\n\n\n\nWarning\n\n\n%DO\n and \n%END\n statements are valid \nonly inside a macro definition\n.\n\n\n\n\nOther Flavors of \n%DO\n Loops\n\n\nYou can execute a \n%DO\n loop conditionally with \n%DO %WHILE\n and \n%DO %UNTIL\n statements.\n\n\n\n\n%DO %WHILE\n: executes a section of a macro repetitively while a condition is true\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n%\nDO\n \n%\nWHILE\n \n(\nexpression\n);\n \n  \ntext\n \nand\n \nmacro\n \nprogram\n \nstatements\n\n\n%\nEND\n;\n\n\n\n/* Example: Removing Markup Tags from a Title */\n\n\n%macro\n \nuntag\n(\ntitle\n);\n\n      \n%let\n \nstbk\n=\n%str\n(\n);\n\n      \n%let\n \netbk\n=\n%str\n(\n);\n\n      \n/* Do loop while tags exist  */\n\n   \n%do\n \n%while\n \n(\n%index\n(\ntitle\n,\nstbk\n)\n0\n)\n \n;\n\n      \n%let\n \npretag\n=\n;\n\n      \n%let\n \nposttag\n=\n;\n\n      \n%let\n \npos_et\n=\n%index\n(\ntitle\n,\netbk\n);\n\n      \n%let\n \nlen_ti\n=\n%length\n(\ntitle\n);\n\n          \n/* Is \n first character? */\n\n      \n%if\n \n(\n%qsubstr\n(\ntitle\n,\n1\n,\n1\n)\n=\nstbk\n)\n \n%then\n \n%do\n;\n\n         \n%if\n \n(\npos_et\n \nne\n \nlen_ti\n)\n \n%then\n\n            \n%let\n \nposttag\n=\n%qsubstr\n(\ntitle\n,\npos_et\n+\n1\n);\n\n      \n%end\n;\n\n      \n%else\n \n%do\n;\n\n         \n%let\n \npretag\n=\n%qsubstr\n(\ntitle\n,\n1\n,(\n%index\n(\ntitle\n,\nstbk\n)\n-\n1\n));\n\n            \n/* More characters beyond end of tag (\n) ? */\n\n         \n%if\n \n(\npos_et\n \nne\n \nlen_ti\n)\n \n%then\n\n            \n%let\n \nposttag\n=\n%qsubstr\n(\ntitle\n,\npos_et\n+\n1\n);\n\n      \n%end\n;\n\n         \n/* Build title with text before and after tag */\n\n      \n%let\n \ntitle\n=\npretag\nposttag\n;\n\n   \n%end\n;\n\n\ntitle\n \ntitle\n;\n\n\n%mend\n \nuntag\n;\n\n\n\n%untag\n(\ntitle\nTotal\n \nemph\nOverdue\n \n/\nemph\nAccounts\n/\ntitle\n)\n\n\n/* TITLE \nTotal Overdue Accounts\n; */\n\n\n\n\n\n\n\n\n\n%DO %UNTIL\n: executes a section of a macro repetitively until a condition is true\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n%\nDO\n \n%\nUNTIL\n \n(\nexpression\n);\n \n   \ntext\n \nand\n \nmacro\n \nlanguage\n \nstatements\n\n\n%\nEND\n;\n\n\n\n/* Example: Validating a Parameter */\n\n\n%macro\n \ngrph\n(\ntype\n);\n\n   \n%let\n \ntype\n=\n%upcase\n(\ntype\n);\n\n   \n%let\n \noptions\n=\nBLOCK\n \nHBAR\n \nVBAR\n;\n\n   \n%let\n \ni\n=\n0\n;\n\n   \n%do\n \n%until\n \n(\ntype\n=\n%scan\n(\noptions\n,\ni\n)\n \nor\n \n(\ni\n3\n))\n \n;\n\n      \n%let\n \ni\n \n=\n \n%eval\n(\ni\n+\n1\n);\n\n   \n%end\n;\n\n   \n%if\n \ni\n3\n \n%then\n \n%do\n;\n\n      \n%put\n \nERROR\n:\n \ntype\n \ntype\n \nnot\n \nsupported\n;\n\n   \n%end\n;\n\n   \n%else\n \n%do\n;\n\n      \nproc\n \nchart\n;\ntype\n \nsex\n \n/\n \ngroup\n=\ndept\n;\n\n      \nrun\n;\n\n   \n%end\n;\n\n\n%mend\n \ngrph\n;\n\n\n\n%macro\n \ngrph\n(\nHBAR\n);\n\n\n/* PROC CHART;\n\n\nHBAR SEX / GROUP=DEPT;\n\n\nRUN;*/\n\n\n\n%macro\n \ngrph\n(\nPIE\n);\n\n\n/* ERROR: PIE type not supported */\n\n\n\n\n\n\n\nGenerating Repetitive Pieces of Text Using \n%DO\n Loops\n\n\nTo generate repetitive pieces of text, use an iterative \n%DO\n loop. For example, the following macro, \nNAMES\n, uses an iterative \n%DO\n loop to create a series of names to be used in a \nDATA\n statement:\n\n\n1\n2\n3\n4\n5\n%macro\n \nnames\n(\nname\n=\n \n,\nnumber\n=\n \n);\n\n   \n%do\n \nn\n=\n1\n \n%to\n \nnumber\n;\n\n      \nname\nn\n\n   \n%end\n;\n\n\n%mend\n \nnames\n;\n\n\n\n\n\n\n\nThe macro \nNAMES\n creates a series of names by concatenating the value of the parameter \nNAME\n and the value of the macro variable \nN\n. You supply the stopping value for \nN\n as the value of the parameter \nNUMBER\n, as in the following \nDATA\n statement:\n\n\n1\ndata %names(name=dsn,number=5);\n\n\n\n\n\n\nSubmitting this statement produces the following complete \nDATA\n statement:\n\n\n1\ndata dsn1 dsn2 dsn3 dsn4 dsn5;", 
            "title": "Using Conditional and Iterative Processing in Macro Programs"
        }, 
        {
            "location": "/macros/macroprog-processing/#processing-statements-conditionally", 
            "text": "You can use  %IF-%THEN  and  %ELSE  statements to perform conditional processing in a macro program.  1\n2 %IF expression %THEN text;  %ELSE text;    You specify the keyword  %IF  followed by an expression. The expression can be any valid macro expression that resolves to an integer. Then, you specify the keyword  %THEN  followed by some text. The text can be a SAS constant, a text expression, a macro variable reference, a macro call, or a macro program statement. The  %ELSE  statement is optional. The text in a  %ELSE  statement can also be a SAS constant, a text expression, a macro variable reference, a macro call, or a macro program statement.  If the expression following  %IF   resolves to zero , the expression is false and the  %THEN  text isn't processed. If you include an optional  %ELSE  statement, that text is processed instead of the  %THEN  text. If the expression  resolves to any integer other than zero , then the expression is true and the  %THEN  text is processed. If the expression  resolves to null or to any noninteger value , SAS issues an error message.  There are several important differences between the macro  %IF-%THEN  statement and the  DATA  step  IF-THEN  statement.     %IF-%THEN  IF-THEN      Is used only in a macro program  Is used only in a  DATA  step program    Executes during macro execution  Executes during  DATA  step execution    Uses macro variables in logical expressions and cannot refer to  DATA  step variables in logical expressions  Uses  DATA  step variables in logical expressions    Determines what text should be copied to the input stack  Determines what  DATA  step statement(s) should be executed     Macro expressions are similar to SAS expressions in the following ways:   They use the same arithmetic, logical, and comparison operators as SAS expressions  They are case sensitive  Special  WHERE  operators are not valid   Macro expressions are dissimilar to SAS expressions in the following ways:   Character operands are not quoted  Expressions in which comparison operators surround a macro expression might resolve with unexpected results   You can use  %DO  and  %END  statements along with a  %IF-%THEN  statement to generate code that contains semicolons.  1\n2\n3\n4\n5\n6 %IF expression %THEN %DO; \n            text   and / or   macro   language   elements ;  %END;  %ELSE %DO; \n            text   and / or   macro   language   elements ;  %END;    The syntax for using  %DO  and  %END  statements with a  %IF-%THEN  statement is shown here. The keyword  %DO  follows the keyword  %THEN . You must pair each  %DO  statement with a  %END  statement. Between the  %DO  and the  %END  keywords, you insert one or more statements that contain constant text, text expressions, or macro statements.", 
            "title": "Processing Statements Conditionally"
        }, 
        {
            "location": "/macros/macroprog-processing/#using-conditional-processing-to-validate-parameters-the-in-operator", 
            "text": "You can use the comparison operator  IN  to search for character and numeric values that are equal to one from a list of values. You can also use the  IN  operator when you evaluate arithmetic or logical expressions during macro execution.  1 %MACRO macro-name  (parameter-list)  /MINOPERATOR | NOMINOPERATOR;    To use the macro  IN  operator, you use the  MINOPERATOR  option with the  %MACRO  statement, preceded by a slash. Then you can use the macro  IN  operator to modify the  %IF  statement. The macro  IN  operator is similar to the SAS  IN  operator, but it doesn't require parentheses.  If you use  NOT  with the  IN  operator,  NOT  must precede the  IN  expression and parentheses are required around the expression.\nYou can use the  PROC SQL INTO  clause with the  SEPARATED BY  argument to create a macro variable that contains a list of unique values. You can combine  PROC SQL  with conditional processing to validate the parameters in a macro.", 
            "title": "Using Conditional Processing to Validate Parameters: the IN operator"
        }, 
        {
            "location": "/macros/macroprog-processing/#processing-statements-iteratively-the-do-statement", 
            "text": "With the iterative  %DO  statement, you can repeatedly execute macro programming code and generate SAS code.  1\n2\n3 %DO index-variable=start %TO stop  %BY increment ; \n            text   and / or   macro   language   elements ;  %END;     The values for  start ,  stop , and  increment  must be integers or macro expressions that resolve to integer values.  The  %BY  clause is optional. Increment specifies either an integer (other than 0) or a macro expression that generates an integer to be added to the value of the index variable in each iteration of the loop. By default, the increment is 1.  A  %END  statement ends the iterative loop.     Warning  %DO  and  %END  statements are valid  only inside a macro definition .", 
            "title": "Processing Statements Iteratively: the %DO statement"
        }, 
        {
            "location": "/macros/macroprog-processing/#other-flavors-of-do-loops", 
            "text": "You can execute a  %DO  loop conditionally with  %DO %WHILE  and  %DO %UNTIL  statements.   %DO %WHILE : executes a section of a macro repetitively while a condition is true    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33 % DO   % WHILE   ( expression );  \n   text   and   macro   program   statements  % END ;  /* Example: Removing Markup Tags from a Title */  %macro   untag ( title ); \n       %let   stbk = %str ( ); \n       %let   etbk = %str ( ); \n       /* Do loop while tags exist  */ \n    %do   %while   ( %index ( title , stbk ) 0 )   ; \n       %let   pretag = ; \n       %let   posttag = ; \n       %let   pos_et = %index ( title , etbk ); \n       %let   len_ti = %length ( title ); \n           /* Is   first character? */ \n       %if   ( %qsubstr ( title , 1 , 1 ) = stbk )   %then   %do ; \n          %if   ( pos_et   ne   len_ti )   %then \n             %let   posttag = %qsubstr ( title , pos_et + 1 ); \n       %end ; \n       %else   %do ; \n          %let   pretag = %qsubstr ( title , 1 ,( %index ( title , stbk ) - 1 )); \n             /* More characters beyond end of tag ( ) ? */ \n          %if   ( pos_et   ne   len_ti )   %then \n             %let   posttag = %qsubstr ( title , pos_et + 1 ); \n       %end ; \n          /* Build title with text before and after tag */ \n       %let   title = pretag posttag ; \n    %end ;  title   title ;  %mend   untag ;  %untag ( title Total   emph Overdue   / emph Accounts / title )  /* TITLE  Total Overdue Accounts ; */     %DO %UNTIL : executes a section of a macro repetitively until a condition is true    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28 % DO   % UNTIL   ( expression );  \n    text   and   macro   language   statements  % END ;  /* Example: Validating a Parameter */  %macro   grph ( type ); \n    %let   type = %upcase ( type ); \n    %let   options = BLOCK   HBAR   VBAR ; \n    %let   i = 0 ; \n    %do   %until   ( type = %scan ( options , i )   or   ( i 3 ))   ; \n       %let   i   =   %eval ( i + 1 ); \n    %end ; \n    %if   i 3   %then   %do ; \n       %put   ERROR :   type   type   not   supported ; \n    %end ; \n    %else   %do ; \n       proc   chart ; type   sex   /   group = dept ; \n       run ; \n    %end ;  %mend   grph ;  %macro   grph ( HBAR );  /* PROC CHART;  HBAR SEX / GROUP=DEPT;  RUN;*/  %macro   grph ( PIE );  /* ERROR: PIE type not supported */", 
            "title": "Other Flavors of %DO Loops"
        }, 
        {
            "location": "/macros/macroprog-processing/#generating-repetitive-pieces-of-text-using-do-loops", 
            "text": "To generate repetitive pieces of text, use an iterative  %DO  loop. For example, the following macro,  NAMES , uses an iterative  %DO  loop to create a series of names to be used in a  DATA  statement:  1\n2\n3\n4\n5 %macro   names ( name =   , number =   ); \n    %do   n = 1   %to   number ; \n       name n \n    %end ;  %mend   names ;    The macro  NAMES  creates a series of names by concatenating the value of the parameter  NAME  and the value of the macro variable  N . You supply the stopping value for  N  as the value of the parameter  NUMBER , as in the following  DATA  statement:  1 data %names(name=dsn,number=5);   Submitting this statement produces the following complete  DATA  statement:  1 data dsn1 dsn2 dsn3 dsn4 dsn5;", 
            "title": "Generating Repetitive Pieces of Text Using %DO Loops"
        }, 
        {
            "location": "/macros/debugging-options/", 
            "text": "Minimizing Errors in Your Macros\n\n\nYou should use a \nfive-step approach\n to developing macro programs that generate SAS code. This approach will streamline your development and debugging process:\n\n\n\n\nWrite and debug the SAS program without macro coding\n\n\nGeneralize by replacing hardcoded values with macro variable references\n\n\nCreate a macro definition with macro parameters\n\n\nAdd macro-level programming for conditional and iterative processing\n\n\nAdd data-driven customization\n\n\n\n\nThere are several system options that are useful for macro debugging:\n\n\n\n\n\n\n\n\nOption\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nMCOMPILENOTE\n\n\nIssues a note to the SAS log after a macro completes compilation\n\n\n\n\n\n\nMLOGIC\n\n\nWrites messages that trace macro execution to the SAS log\n\n\n\n\n\n\nMPRINT\n\n\nSpecifies that the text that is sent to the compiler when a macro executes is printed in the SAS log\n\n\n\n\n\n\nSYMBOLGEN\n\n\nDisplays the values of macro variables as they resolve\n\n\n\n\n\n\n\n\nDebugging During Macro Compilation\n\n\nMCOMPILENOTE=\n Option\n\n\nBy default, SAS does not display any indication that a macro has completed compilation. You can use the \nMCOMPILENOTE=\n option with the \nALL\n argument to issue a note to the SAS log after a macro compiles.\n\n\n1\n  OPTIONS MCOMPILENOTE=NONE | ALL;\n\n\n\n\n\n\nTracking Errors During Macro Execution\n\n\nMPRINT\n Option\n\n\nThe \nMPRINT\n option displays the SAS statements generated by macro execution.\n\n\n1\nOPTIONS MPRINT | NOMPRINT;\n\n\n\n\n\n\nMLOGIC\n Option\n\n\nThe \nMLOGIC\n option prints messages that indicate macro actions that were taken during macro execution.\n\n\n1\nOPTIONS MLOGIC | NOMLOGIC;\n\n\n\n\n\n\nWhen the \nMLOGIC\n system option is in effect, the messages that SAS displays in the log include information about the following:\n\n The beginning of macro execution\n\n The values of any parameters\n\n The results of arithmetic and logical macro operations\n\n The end of macro execution\n\n\nWhen you're working with a program that uses SAS macro language, you should typically turn the \nMLOGIC\n option, along with the \nMPRINT\n option and the \nSYMBOLGEN\n option\n\n \non\n for development and debugging purposes\n\n \noff\n when the program is in production mode\n\n\nComment your Macros\n\n\nYour macros might benefit from comments. Comments can be especially helpful if you plan to save your macros permanently or share them with other users.\n\n\n1\n%* comment;\n\n\n\n\n\n\n\nTo use the macro comment statement, specify the percent sign, followed by an asterisk and then your comment. The comment can be any text. Like other SAS statements, each macro comment statement ends with a semicolon.\n\n\nYou can also use the comment symbols \n/ *\n and \n* /\n inside a macro. When these symbols appear, the macro processor ignores the text within the comment.", 
            "title": "Debugging Options"
        }, 
        {
            "location": "/macros/debugging-options/#minimizing-errors-in-your-macros", 
            "text": "You should use a  five-step approach  to developing macro programs that generate SAS code. This approach will streamline your development and debugging process:   Write and debug the SAS program without macro coding  Generalize by replacing hardcoded values with macro variable references  Create a macro definition with macro parameters  Add macro-level programming for conditional and iterative processing  Add data-driven customization   There are several system options that are useful for macro debugging:     Option  Description      MCOMPILENOTE  Issues a note to the SAS log after a macro completes compilation    MLOGIC  Writes messages that trace macro execution to the SAS log    MPRINT  Specifies that the text that is sent to the compiler when a macro executes is printed in the SAS log    SYMBOLGEN  Displays the values of macro variables as they resolve", 
            "title": "Minimizing Errors in Your Macros"
        }, 
        {
            "location": "/macros/debugging-options/#debugging-during-macro-compilation", 
            "text": "", 
            "title": "Debugging During Macro Compilation"
        }, 
        {
            "location": "/macros/debugging-options/#mcompilenote-option", 
            "text": "By default, SAS does not display any indication that a macro has completed compilation. You can use the  MCOMPILENOTE=  option with the  ALL  argument to issue a note to the SAS log after a macro compiles.  1   OPTIONS MCOMPILENOTE=NONE | ALL;", 
            "title": "MCOMPILENOTE= Option"
        }, 
        {
            "location": "/macros/debugging-options/#tracking-errors-during-macro-execution", 
            "text": "", 
            "title": "Tracking Errors During Macro Execution"
        }, 
        {
            "location": "/macros/debugging-options/#mprint-option", 
            "text": "The  MPRINT  option displays the SAS statements generated by macro execution.  1 OPTIONS MPRINT | NOMPRINT;", 
            "title": "MPRINT Option"
        }, 
        {
            "location": "/macros/debugging-options/#mlogic-option", 
            "text": "The  MLOGIC  option prints messages that indicate macro actions that were taken during macro execution.  1 OPTIONS MLOGIC | NOMLOGIC;   When the  MLOGIC  system option is in effect, the messages that SAS displays in the log include information about the following:  The beginning of macro execution  The values of any parameters  The results of arithmetic and logical macro operations  The end of macro execution  When you're working with a program that uses SAS macro language, you should typically turn the  MLOGIC  option, along with the  MPRINT  option and the  SYMBOLGEN  option   on  for development and debugging purposes   off  when the program is in production mode", 
            "title": "MLOGIC Option"
        }, 
        {
            "location": "/macros/debugging-options/#comment-your-macros", 
            "text": "Your macros might benefit from comments. Comments can be especially helpful if you plan to save your macros permanently or share them with other users.  1 %* comment;    To use the macro comment statement, specify the percent sign, followed by an asterisk and then your comment. The comment can be any text. Like other SAS statements, each macro comment statement ends with a semicolon.  You can also use the comment symbols  / *  and  * /  inside a macro. When these symbols appear, the macro processor ignores the text within the comment.", 
            "title": "Comment your Macros"
        }, 
        {
            "location": "/macros/more-macro/", 
            "text": "Check these Websites\n\n\n\n\nDeveloping Large SAS Macro Applications\n\n\nThe Fundamentals of Macro Quoting Functions\n\n\nHow to define \noptional macro arguments\n.\n\n\n\n\n\n\nRemove element/string from macro variable\n\n\n1\n2\n3\n4\n5\n6\n%put\n \n=\nlist\n;\n     \n/* Check list contents before */\n\n\n\n%let\n \nremovefromlist\n \n=\n \nstring_to_remove\n;\n\n\n%let\n \nlist\n \n=\n \n%sysfunc\n(\ntranwrd\n(\nlist\n.,\n \nremovefromlist\n.,\n \n%str\n()));;\n\n\n\n%put\n \n=\nlist\n;\n     \n/* Check list contents after */\n\n\n\n\n\n\n\nCall a Macro for a List of Variable Names\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n%macro\n \nrunall\n(\nparamlist\n);\n\n \n%let\n \nnum\n \n=\n \n%sysfunc\n(\ncountw\n(\nparamlist\n));\n\n    \n%local\n \ni\n;\n\n    \n%do\n \ni\n \n=\n1\n \n%to\n \nnum\n;\n\n        \n%let\n \nparameter\ni\n \n=\n \n%scan\n(\nparamlist\n,\n \ni\n);\n\n        \n%macro_analysis\n(\nvariablename\n=\nparameter\ni\n);\n\n    \n%end\n;\n\n\n%mend\n;\n\n\n\n%runall\n(\nitem1\n \nitem2\n \nitem3\n \nitem4\n \nitem5\n);\n\n\n\n\n\n\n\nCreate Macrovariable from Data Set Values\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\nDATA\n \n_NULL_\n;\n\n    \nSET\n \nOddsRatios\n;\n\n    \nCALL\n \nSYMPUT\n \n(\nvar1\n,\nOddsRatioEst\n);\n\n    \nCALL\n \nSYMPUT\n \n(\nvar2\n,\nLowerCl\n);\n\n    \nCALL\n \nSYMPUT\n \n(\nvar3\n,\nUpperCL\n);\n\n    \n*\n \nThe\n \nvariables\n \nhave\n \na\n \nlot\n \nof\n \nextra\n \nspaces\n;\n\n    \n%\nLET\n \nOR2report\n=\nOR\n:\n \nvar1\n.\n \n(\nvar2\n.,\nvar3\n.);\n\n    \n%\nPUT\n \nOR2report\n;\n\n\nRUN\n;\n\n\n\nDATA\n \n_NULL_\n;\n\n    \nSET\n \nOddsRatios\n;\n\n    \nlength\n \nOddsRatioEst\n \nLowerCL\n \nUpperCL\n \n7\n;\n\n    \nEstaux\n \n=\n \nint\n(\n1000\n*\nOddsRatioEst\n)/\n1000\n;\n\n    \nLoweraux\n \n=\n \nint\n(\n1000\n*\nLowerCl\n)/\n1000\n;\n\n    \nUpperaux\n \n=\n \nint\n(\n1000\n*\nUpperCL\n)/\n1000\n;\n\n    \n*\n \nThe\n \nextra\n \nblancks\n \nhas\n \nbeen\n \nreduced\n \nwith\n \nthe\n \nCATX\n \nfunction\n;\n\n    \nfullOR\n=\nCATX\n(\n \n,\nOR:\n,\nEstaux\n,\n(\n,\nLoweraux\n,\n,\n,\nUpperaux\n,\n)\n);\n\n    \nCALL\n \nSYMPUT\n \n(\nOR2report\n,\nfullOR\n);\n\n    \n%\nPUT\n \nOR2report\n;\n\n\nRUN\n;\n\n\n\n\n\n\n\nUseful Functions for Macro Programming\n\n\nVVALUEX\n Function\n\n\nReturns the formatted value that is associated with the argument that you specify. The argument specifies a character constant, variable, or expression that evaluates to a variable name. \n\n\n\n\nWarning\n\n\nThe value of the specified expression cannot denote an array reference.\n\n\n\n\n1\n2\n3\n4\n5\ndate1=\n31mar02\nd;\ndate2=\ndate1\n;\nformat date1 date7.;\ndatevalue=vvaluex(date2);\nput datevalue;               /* 31MAR02 */\n\n\n\n\n\n\nMacros Available in SAS\n\n\nCheck this \npowerpoint\n presentation for more tips.\n\n\nColor Utility Macros\n\n\nTo initiate these macros in your current session you call the \n%COLORMAC\n macro.\n\n\nIf you submit the following line:\n\n\n1\n%HELPCLR(HELP);\n\n\n\n\n\n\n\nYou will get a guide of the color utility macros available:\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nColor Utility Macros Help              \n\nHELP is currently available for the following macros \n\n        CMY        CMYK       CNS        HLS          \n        HVS        RGB        HLS2RGB    RGB2HLS      \n\nEnter %HELPCLR(macroname) for details on each macro, \nor %HELPCLR(ALL) for details on all macros.   \n\n\n\n\n\n\nSG\n Annotation Macros\n\n\nThey can be used within a \nDATA\n to simplify the process of creating annotation observations.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n  \n%SGARROW\n\n  \n%SGPOLYGON\n\n  \n%SGIMAGE\n\n  \n%SGPOLYLINE\n\n  \n%SGLINE\n\n  \n%SGRECTANGLE\n\n  \n%SGOVAL\n\n  \n%SGTEXT\n\n  \n%SGPOLYCONT\n\n  \n%SGTEXTCONT\n\n\n\n\n\n\n\nTemplate Modification Macros\n\n\n\n\n%MODSTYLE\n macro allows you to easily make changes to style templates without accessing the code\n\n\n%MODTMPLT\n macro allows you to easily make limited changes to graph templates without accessing the code\n\n\n\n\nGraphical Macros\n\n\n\n\n%CompactMatrixMacro\n (Author: Sanjay Matange): it help you modify graphs based on panels\n\n\n%NEWSURV\n macro (Author: Jeff Meyers): it helps you tune the properties of survival plots\n\n\n%FORESTPLOT\n macro (Author: Jeff Meyers): it allows another way of presenting results\n\n\n%EULER_MACRO\n: useful to present proportion Euler diagrams\n\n\n%VENN\n macro: useful to plot intersection between different events\n\n\n%GTLPieChartMacro\n: useful for pie charts\n\n\n\n\nExport Macros\n\n\n\n\n%DS2CSV\n: exports a dataset to *.csv format.\n\n\n\n\nWhere to Find these Macros?\n\n\n\n\nColor utility macros, SGAnnotation macros, \n%MODSTYLE\n and \n%MODTMPLT\n are SAS autocall macros\n\n\n%AXISBREAK\n\n\n%COMPACTMATRIXMACRO\n\n\n%ORTHO3D_MACRO\n\n\n%NEWSURV\n\n\n%FORESTPLOT\n\n\n%EULER_MACRO\n\n\n%VENN\n\n\n%GTLPIECHARTMACRO\n\n\n\n\nMacro examples\n\n\nMacro Program for Creating Box Plots for All of Predictor Variables\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n%let\n \ncategorical\n=\nHouse_Style2\n \nOverall_Qual2\n \nOverall_Cond2\n \nFireplaces\n \n         \nSeason_Sold\n \nGarage_Type_2\n \nFoundation_2\n \nHeating_QC\n \n         \nMasonry_Veneer\n \nLot_Shape_2\n \nCentral_Air\n;\n\n\n/* Macro Usage: %box(DSN = , Response = , CharVar = ) */\n\n\n%macro\n \nbox\n(\ndsn\n      \n=\n \n,\n\n           \nresponse\n \n=\n \n,\n\n           \nCharvar\n  \n=\n \n);\n\n\n%let\n \ni\n \n=\n \n1\n \n;\n\n\n%do\n \n%while\n(\n%scan\n(\ncharvar\n,\ni\n,\n%str\n(\n \n))\n \n^=\n \n%str\n())\n \n;\n\n    \n%let\n \nvar\n \n=\n \n%scan\n(\ncharvar\n,\ni\n,\n%str\n(\n \n));\n\n    \nproc\n \nsgplot\n \ndata\n=\ndsn\n;\n\n        \nvbox\n \nresponse\n \n/\n \ncategory\n=\nvar\n \n                         \ngrouporder\n=\nascending\n \n                         \nconnect\n=\nmean\n;\n\n        \ntitle\n \nresponse across Levels of \nvar\n;\n\n    \nrun\n;\n\n    \n%let\n \ni\n \n=\n \n%eval\n(\ni\n \n+\n \n1\n \n)\n \n;\n\n\n%end\n \n;\n\n\n%mend\n \nbox\n;\n\n\n%box\n(\ndsn\n      \n=\n \nstatdata\n.\nameshousing3\n,\n\n     \nresponse\n \n=\n \nSalePrice\n,\n\n     \ncharvar\n  \n=\n \ncategorical\n);\n\n\ntitle\n;\n\n\noptions\n \nlabel\n;\n\n\n\n\n\n\n\nChecking if a data set is exists\n\n\n1\n2\n3\n4\n5\n6\nDATA\n \ntest\n-\nSAS\n-\ndata\n-\nset\n;\n\n    \nSET\n     \n%if\n \n%sysfunc\n(\nexist\n(\nSAS\n-\ndata\n-\nset\n-\npart1\n))\n \n%then\n \n%do\n;\n \n            \nSAS\n-\ndata\n-\nset\n-\npart1\n\n        \n%end\n;\n\n                \nSAS\n-\ndata\n-\nset\n-\npart2\n;\n\n\nRUN\n;\n\n\n\n\n\n\n\nMacro's Sources\n\n\n\n\nLes macros SAS de Dominique Ladiray\n\n\nMayo Clinic: Locally written SAS macros\n\n\nHere\n you have some macro repositories.\n\n\nKaplan-Meier Survival Plotting Macro %NEWSURV", 
            "title": "More on Macro Programming"
        }, 
        {
            "location": "/macros/more-macro/#remove-elementstring-from-macro-variable", 
            "text": "1\n2\n3\n4\n5\n6 %put   = list ;       /* Check list contents before */  %let   removefromlist   =   string_to_remove ;  %let   list   =   %sysfunc ( tranwrd ( list .,   removefromlist .,   %str ()));;  %put   = list ;       /* Check list contents after */", 
            "title": "Remove element/string from macro variable"
        }, 
        {
            "location": "/macros/more-macro/#call-a-macro-for-a-list-of-variable-names", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 %macro   runall ( paramlist ); \n  %let   num   =   %sysfunc ( countw ( paramlist )); \n     %local   i ; \n     %do   i   = 1   %to   num ; \n         %let   parameter i   =   %scan ( paramlist ,   i ); \n         %macro_analysis ( variablename = parameter i ); \n     %end ;  %mend ;  %runall ( item1   item2   item3   item4   item5 );", 
            "title": "Call a Macro for a List of Variable Names"
        }, 
        {
            "location": "/macros/more-macro/#create-macrovariable-from-data-set-values", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21 DATA   _NULL_ ; \n     SET   OddsRatios ; \n     CALL   SYMPUT   ( var1 , OddsRatioEst ); \n     CALL   SYMPUT   ( var2 , LowerCl ); \n     CALL   SYMPUT   ( var3 , UpperCL ); \n     *   The   variables   have   a   lot   of   extra   spaces ; \n     % LET   OR2report = OR :   var1 .   ( var2 ., var3 .); \n     % PUT   OR2report ;  RUN ;  DATA   _NULL_ ; \n     SET   OddsRatios ; \n     length   OddsRatioEst   LowerCL   UpperCL   7 ; \n     Estaux   =   int ( 1000 * OddsRatioEst )/ 1000 ; \n     Loweraux   =   int ( 1000 * LowerCl )/ 1000 ; \n     Upperaux   =   int ( 1000 * UpperCL )/ 1000 ; \n     *   The   extra   blancks   has   been   reduced   with   the   CATX   function ; \n     fullOR = CATX (   , OR: , Estaux , ( , Loweraux , , , Upperaux , ) ); \n     CALL   SYMPUT   ( OR2report , fullOR ); \n     % PUT   OR2report ;  RUN ;", 
            "title": "Create Macrovariable from Data Set Values"
        }, 
        {
            "location": "/macros/more-macro/#useful-functions-for-macro-programming", 
            "text": "", 
            "title": "Useful Functions for Macro Programming"
        }, 
        {
            "location": "/macros/more-macro/#vvaluex-function", 
            "text": "Returns the formatted value that is associated with the argument that you specify. The argument specifies a character constant, variable, or expression that evaluates to a variable name.    Warning  The value of the specified expression cannot denote an array reference.   1\n2\n3\n4\n5 date1= 31mar02 d;\ndate2= date1 ;\nformat date1 date7.;\ndatevalue=vvaluex(date2);\nput datevalue;               /* 31MAR02 */", 
            "title": "VVALUEX Function"
        }, 
        {
            "location": "/macros/more-macro/#macros-available-in-sas", 
            "text": "Check this  powerpoint  presentation for more tips.", 
            "title": "Macros Available in SAS"
        }, 
        {
            "location": "/macros/more-macro/#color-utility-macros", 
            "text": "To initiate these macros in your current session you call the  %COLORMAC  macro.  If you submit the following line:  1 %HELPCLR(HELP);    You will get a guide of the color utility macros available:  1\n2\n3\n4\n5\n6\n7\n8\n9 Color Utility Macros Help              \n\nHELP is currently available for the following macros \n\n        CMY        CMYK       CNS        HLS          \n        HVS        RGB        HLS2RGB    RGB2HLS      \n\nEnter %HELPCLR(macroname) for details on each macro, \nor %HELPCLR(ALL) for details on all macros.", 
            "title": "Color Utility Macros"
        }, 
        {
            "location": "/macros/more-macro/#sg-annotation-macros", 
            "text": "They can be used within a  DATA  to simplify the process of creating annotation observations.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10    %SGARROW \n   %SGPOLYGON \n   %SGIMAGE \n   %SGPOLYLINE \n   %SGLINE \n   %SGRECTANGLE \n   %SGOVAL \n   %SGTEXT \n   %SGPOLYCONT \n   %SGTEXTCONT", 
            "title": "SG Annotation Macros"
        }, 
        {
            "location": "/macros/more-macro/#template-modification-macros", 
            "text": "%MODSTYLE  macro allows you to easily make changes to style templates without accessing the code  %MODTMPLT  macro allows you to easily make limited changes to graph templates without accessing the code", 
            "title": "Template Modification Macros"
        }, 
        {
            "location": "/macros/more-macro/#graphical-macros", 
            "text": "%CompactMatrixMacro  (Author: Sanjay Matange): it help you modify graphs based on panels  %NEWSURV  macro (Author: Jeff Meyers): it helps you tune the properties of survival plots  %FORESTPLOT  macro (Author: Jeff Meyers): it allows another way of presenting results  %EULER_MACRO : useful to present proportion Euler diagrams  %VENN  macro: useful to plot intersection between different events  %GTLPieChartMacro : useful for pie charts", 
            "title": "Graphical Macros"
        }, 
        {
            "location": "/macros/more-macro/#export-macros", 
            "text": "%DS2CSV : exports a dataset to *.csv format.", 
            "title": "Export Macros"
        }, 
        {
            "location": "/macros/more-macro/#where-to-find-these-macros", 
            "text": "Color utility macros, SGAnnotation macros,  %MODSTYLE  and  %MODTMPLT  are SAS autocall macros  %AXISBREAK  %COMPACTMATRIXMACRO  %ORTHO3D_MACRO  %NEWSURV  %FORESTPLOT  %EULER_MACRO  %VENN  %GTLPIECHARTMACRO", 
            "title": "Where to Find these Macros?"
        }, 
        {
            "location": "/macros/more-macro/#macro-examples", 
            "text": "", 
            "title": "Macro examples"
        }, 
        {
            "location": "/macros/more-macro/#macro-program-for-creating-box-plots-for-all-of-predictor-variables", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24 %let   categorical = House_Style2   Overall_Qual2   Overall_Cond2   Fireplaces  \n          Season_Sold   Garage_Type_2   Foundation_2   Heating_QC  \n          Masonry_Veneer   Lot_Shape_2   Central_Air ;  /* Macro Usage: %box(DSN = , Response = , CharVar = ) */  %macro   box ( dsn        =   , \n            response   =   , \n            Charvar    =   );  %let   i   =   1   ;  %do   %while ( %scan ( charvar , i , %str (   ))   ^=   %str ())   ; \n     %let   var   =   %scan ( charvar , i , %str (   )); \n     proc   sgplot   data = dsn ; \n         vbox   response   /   category = var  \n                          grouporder = ascending  \n                          connect = mean ; \n         title   response across Levels of  var ; \n     run ; \n     %let   i   =   %eval ( i   +   1   )   ;  %end   ;  %mend   box ;  %box ( dsn        =   statdata . ameshousing3 , \n      response   =   SalePrice , \n      charvar    =   categorical );  title ;  options   label ;", 
            "title": "Macro Program for Creating Box Plots for All of Predictor Variables"
        }, 
        {
            "location": "/macros/more-macro/#checking-if-a-data-set-is-exists", 
            "text": "1\n2\n3\n4\n5\n6 DATA   test - SAS - data - set ; \n     SET       %if   %sysfunc ( exist ( SAS - data - set - part1 ))   %then   %do ;  \n             SAS - data - set - part1 \n         %end ; \n                 SAS - data - set - part2 ;  RUN ;", 
            "title": "Checking if a data set is exists"
        }, 
        {
            "location": "/macros/more-macro/#macros-sources", 
            "text": "Les macros SAS de Dominique Ladiray  Mayo Clinic: Locally written SAS macros  Here  you have some macro repositories.  Kaplan-Meier Survival Plotting Macro %NEWSURV", 
            "title": "Macro's Sources"
        }, 
        {
            "location": "/procedures/regression-models/", 
            "text": "Interpreting the results of the \nSOLUTION\n option in the \nMODEL\n for categorical variables\n\n\nIn procedures that use the \nGLM\n parameterization for \nCLASS\n variables such as \nPROC GLM\n, \nPROC MIXED\n and \nPROC GLIMMIX\n, a predictor variable specified in the \nCLASS\n statement is represented in the model by a set of design variables created using \nGLM\n parameterization. \n\n\nThis is a less than full-rank parameterization in which a \nCLASS\n variable with k levels is represented in the design matrix by a set of k 0,1-coded indicator (or \ndummy\n) variables. If the \nSOLUTION\n option in the \nMODEL\n statement is also specified, the following note is included in the displayed results below the parameter estimates table:\n\n\n1\nNOTE\n:\n \nThe\n \nX\nX matrix has been found to be singular, and a generalized inverse was used to solve the normal equations. Terms whose estimates are followed by the letter \nB\n \nare\n \nnot\n \nuniquely\n \nestimable\n.\n\n\n\n\n\n\n\nNote that there are many possible parameterizations, each of which imposes a \ndifferent interpretation on the model parameters\n.\n\n\n\n\nSeealso\n\n\n\n\nCheck \nthis\n for a full explanation of the interpretation of the results.\n\n\nRead more\n about the \nGLM\n parametrization.", 
            "title": "General Notes on Regression Procedures"
        }, 
        {
            "location": "/procedures/regression-models/#interpreting-the-results-of-the-solution-option-in-the-model-for-categorical-variables", 
            "text": "In procedures that use the  GLM  parameterization for  CLASS  variables such as  PROC GLM ,  PROC MIXED  and  PROC GLIMMIX , a predictor variable specified in the  CLASS  statement is represented in the model by a set of design variables created using  GLM  parameterization.   This is a less than full-rank parameterization in which a  CLASS  variable with k levels is represented in the design matrix by a set of k 0,1-coded indicator (or  dummy ) variables. If the  SOLUTION  option in the  MODEL  statement is also specified, the following note is included in the displayed results below the parameter estimates table:  1 NOTE :   The   X X matrix has been found to be singular, and a generalized inverse was used to solve the normal equations. Terms whose estimates are followed by the letter  B   are   not   uniquely   estimable .    Note that there are many possible parameterizations, each of which imposes a  different interpretation on the model parameters .   Seealso   Check  this  for a full explanation of the interpretation of the results.  Read more  about the  GLM  parametrization.", 
            "title": "Interpreting the results of the SOLUTION option in the MODEL for categorical variables"
        }, 
        {
            "location": "/procedures/glimmix/", 
            "text": "Modelling a binary reponse variable:\n\n\n1\n2\n3\n4\n5\n6\nPROC GLIMMIX DATA=SAS-data-set;\n  CLASS categorical1 categorical2;\n  MODEL response = continuous2 continuous2 categorical1 / DIST=BINARY LINK=LOGIT ODDSRATIO SOLUTION CL;\n  RANDOM intercept / SUBJECT=categorical2 SOLUTION CL;\n  COVTEST / WALD;\nRUN;\n\n\n\n\n\n\nModelling a multinomial reponse variable:\n\n\n1\n2\n3\n4\n5\n6\nPROC GLIMMIX DATA=SAS-data-set;\n  CLASS categorical1 categorical2;\n  MODEL response = continuous2 continuous2 categorical1 / DIST=MULTINOMIAL LINK=CLOGIT ODDSRATIO SOLUTION CL;\n  RANDOM intercept / SUBJECT=categorical2 SOLUTION CL;\n  COVTEST / WALD;\nRUN;\n\n\n\n\n\n\n\n\nLINK\n specifies the link function in the generalized linear mixed model (\nlink functions available\n)\n\n\nLINK=LOGIT\n to use the \nlogit\n link\n\n\nLINK=GENLOGIT | GLOGIT\n to use the \ngeneralized logit\n link\n\n\nLINK=CUMLOGIT | CLOGIT\n to use the \ncumulative logit\n link\n\n\n\n\n\n\nDIST\n specifies the built-in (conditional) probability distribution of the data (\ndistrubutions available and their corresponfing default link functions\n)\n\n\n\n\nEstimating an Odds Ratio for a Variable Involved in an Interaction\n\n\nIn models with \nLINK=LOGIT | GLOGIT | CLOGIT\n, you can obtain estimates of odds ratios through the \nODDSRATIO\n options in the \nPROC GLIMMIX\n, \nLSMEANS\n, and \nMODEL\n statements. Note that for these link functions the \nEXP\n option in the \nESTIMATE\n and \nLSMESTIMATE\n statements also produces odds or odds ratios. The \nODDSRATIO\n option on the \nPROC GLIMMIX\n statement, requests odds ratio calculations for main effects. \n\n\nEXP\n requests exponentiation of the estimate (\nESTIMATE\n statement) or least squares means estimate (\nLSMESTIMATE\n statement). If you specify the \nCL\n or \nALPHA=\n option, the (adjusted) confidence bounds are also exponentiated.\n\n\nBy default \nLSMEANS\n produces estimates on the \nlogit scale\n. The \nILINK\n option on the \nLSMEANS\n statement requests that the estimates be transformed back to the scale of the original data. The \nLSMEANS\n output will include estimates of the probability of each combination of the predictors interactions included in your model. The \nCL\n option requests confidence intervals for the estimates. For non-normal data, the \nEXP\n and \nILINK\n options give you a way to obtain the quantity of interest on the scale of the mean (inverse link). Results presented in this fashion can be much easier to interpret than data on the link scale. \n\n\n\n\nIs it correct to assume, if you're using a different link function than \nLINK=LOGIT | GLOGIT | CLOGIT\n, that the exponentiated estimate can still be interpreted as the Odds Ratio?\n\n\nNo, that is not correct. The odds ratio only make sense when you are comparing the predicted PROBABILITIES for two or more level of classification variables. If you use \nDIST=GAUSSIAN\n and \nLINK=IDENTITY\n, you are merely fitting a linear model to a response that has values 0 and 1. \nCheck \nthis\n dicussion for more information.\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\ntreatarm = {1, 2}\nvisit = {1, 2, 3, 4, 5}\n\nPROC GLIMMIX DATA=SAS-data-set;\n  CLASS categorical-variable(s);\n  MODEL response = predictor(s) / DIST=BINARY LINK=LOGIT ODDSRATIO SOLUTION CL;\n  RANDOM intercept / SUBJECT=repeated-variable SOLUTION CL;\n  COVTEST / WALD;\n  LSMEANS treatarm*visit / SLICEDIFF=visit ODDSRATIO ILINK CL;\n  LSMESTIMATES treatarm*visit \nOR Visit 1\n 1 0 0 0 0 -1 0 0 0 0,\n               treatarm*visit \nOR Visit 2\n 0 1 0 0 0 0 -1 0 0 0 / EXP ILINK CL;\nRUN;\n\n\n\n\n\n\n\n\nCheck these websites\n\n\n\n\nFor more details check the \nSAS documentation\n\n\nAn example different procedures (\nPROC LOGISTIC\n and \nPROC GLIMMIX\n) can be found \nhere\n\n\nSome other options are also discussed \nhere\n and \nhere", 
            "title": "PROC GLIMMIX"
        }, 
        {
            "location": "/procedures/glimmix/#estimating-an-odds-ratio-for-a-variable-involved-in-an-interaction", 
            "text": "In models with  LINK=LOGIT | GLOGIT | CLOGIT , you can obtain estimates of odds ratios through the  ODDSRATIO  options in the  PROC GLIMMIX ,  LSMEANS , and  MODEL  statements. Note that for these link functions the  EXP  option in the  ESTIMATE  and  LSMESTIMATE  statements also produces odds or odds ratios. The  ODDSRATIO  option on the  PROC GLIMMIX  statement, requests odds ratio calculations for main effects.   EXP  requests exponentiation of the estimate ( ESTIMATE  statement) or least squares means estimate ( LSMESTIMATE  statement). If you specify the  CL  or  ALPHA=  option, the (adjusted) confidence bounds are also exponentiated.  By default  LSMEANS  produces estimates on the  logit scale . The  ILINK  option on the  LSMEANS  statement requests that the estimates be transformed back to the scale of the original data. The  LSMEANS  output will include estimates of the probability of each combination of the predictors interactions included in your model. The  CL  option requests confidence intervals for the estimates. For non-normal data, the  EXP  and  ILINK  options give you a way to obtain the quantity of interest on the scale of the mean (inverse link). Results presented in this fashion can be much easier to interpret than data on the link scale.    Is it correct to assume, if you're using a different link function than  LINK=LOGIT | GLOGIT | CLOGIT , that the exponentiated estimate can still be interpreted as the Odds Ratio?  No, that is not correct. The odds ratio only make sense when you are comparing the predicted PROBABILITIES for two or more level of classification variables. If you use  DIST=GAUSSIAN  and  LINK=IDENTITY , you are merely fitting a linear model to a response that has values 0 and 1. \nCheck  this  dicussion for more information.    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 treatarm = {1, 2}\nvisit = {1, 2, 3, 4, 5}\n\nPROC GLIMMIX DATA=SAS-data-set;\n  CLASS categorical-variable(s);\n  MODEL response = predictor(s) / DIST=BINARY LINK=LOGIT ODDSRATIO SOLUTION CL;\n  RANDOM intercept / SUBJECT=repeated-variable SOLUTION CL;\n  COVTEST / WALD;\n  LSMEANS treatarm*visit / SLICEDIFF=visit ODDSRATIO ILINK CL;\n  LSMESTIMATES treatarm*visit  OR Visit 1  1 0 0 0 0 -1 0 0 0 0,\n               treatarm*visit  OR Visit 2  0 1 0 0 0 0 -1 0 0 0 / EXP ILINK CL;\nRUN;    Check these websites   For more details check the  SAS documentation  An example different procedures ( PROC LOGISTIC  and  PROC GLIMMIX ) can be found  here  Some other options are also discussed  here  and  here", 
            "title": "Estimating an Odds Ratio for a Variable Involved in an Interaction"
        }, 
        {
            "location": "/procedures/mixed/", 
            "text": "Linear Mixed Model General Concepts\n\n\nThe linear mixed model is an extension of the general linear model, in which factors and covariates are assumed to have a linear relationship to the dependent variable.\n\n\nFactors.\n\u00a0Categorical\u00a0predictors should be selected as\u00a0factors\u00a0in the model. Each\u00a0level\u00a0of a factor can have a different linear effect on the value of the dependent variable.\n\n\n\n\nFixed-effects factors\n\u00a0are generally thought of as variables whose values of interest are all represented in the data file.\n\n\nRandom-effects factors\n\u00a0are variables whose values in the data file can be considered a random sample from a larger population of values. They are useful for explaining excess variability in the dependent variable.\n\n\n\n\n\n\nPractical Example\n\n\nA grocery store chain is interested in the effects of five different types of coupons on customer spending. At several store locations, these coupons are handed out to customers who frequent that location; one coupon \nselected at random\n is distributed to each customer.\n  The type of coupon is a \nfixed effect\n because the company is interested in those particular coupons. The store location is a \nrandom effect\n because the locations used are a sample from the larger population of interest, and while there is likely to be store-to-store variation in customer spending, the company is not directly interested in that variation in the context of this problem.\n\n\n\n\nCovariates.\n\u00a0Scale predictors\u00a0should be selected as\u00a0covariates\u00a0in the model. Within combinations of factor levels (or\u00a0cells), values of covariates are assumed to be linearly correlated with values of the dependent variables.\n\n\nInteractions.\n\u00a0The Linear Mixed Models procedure allows you to specify factorial interactions, which means that each combination of factor levels can have a different linear effect on the dependent variable. Additionally, you may specify factor-covariate interactions, if you believe that the linear relationship between a covariate and the dependent variable changes for different levels of a factor.\n\n\nRandom effects covariance structure.\n The Linear Mixed Models procedure allows you to specify the relationship between the levels of random effects. By default, levels of random effects are uncorrelated and have the same variance. \n\n\nRepeated effects.\n\u00a0Factors and covariates are features of the general linear model. In the Linear Mixed Models procedure, repeated effects variables are added, allowing you to relax the assumption of independence of the error terms. In order to model the covariance structure of the error terms, you need to specify the following:\n\n\n\n\nRepeated effects variables\n\u00a0are variables whose values in the data file can be considered as markers of multiple observations of a single subject.\n\n\nSubject variables\n\u00a0define the individual subjects of the repeated measurements. The error terms for each individual are independent of those of other individuals.\n\n\nThe\u00a0\ncovariance structure\n\u00a0specifies the relationship between the levels of the repeated effects. The types of covariance structures available allow for residual terms with a wide variety of variances and covariances.\n\n\n\n\n\n\nPractical Example\n\n\nIf the grocery store recorded the purchasing habits of their customers for four consecutive weeks, then the variable\u00a0Week\u00a0would be a \nrepeated effects variable\n. Specifying a subject variable denoting the\u00a0Customer ID\u00a0differentiates the repeated observations of separate customers. Specifying a first-order autoregressive covariance structure reflects your belief that a higher-than-average volume of purchases in one week will correspond to a higher (or lower)-than-average volume in the following week.\n\n\n\n\nSAS Formulation\n\n\n1\n2\n3\n4\n5\n6\n7\nPROC MIXED DATA=SAS-data-set;\n  CLASS categorical1 categorical2;\n  MODEL response = continuous1 categorical1 continuous1*categorical1 / solution;\n  RANDOM categorical2;\n  LSMEANS continuous1*categorical1 / CL PDIFF DIFFS E;\nRUN;\nQUIT;", 
            "title": "PROC MIXED"
        }, 
        {
            "location": "/procedures/mixed/#linear-mixed-model-general-concepts", 
            "text": "The linear mixed model is an extension of the general linear model, in which factors and covariates are assumed to have a linear relationship to the dependent variable.  Factors. \u00a0Categorical\u00a0predictors should be selected as\u00a0factors\u00a0in the model. Each\u00a0level\u00a0of a factor can have a different linear effect on the value of the dependent variable.   Fixed-effects factors \u00a0are generally thought of as variables whose values of interest are all represented in the data file.  Random-effects factors \u00a0are variables whose values in the data file can be considered a random sample from a larger population of values. They are useful for explaining excess variability in the dependent variable.    Practical Example  A grocery store chain is interested in the effects of five different types of coupons on customer spending. At several store locations, these coupons are handed out to customers who frequent that location; one coupon  selected at random  is distributed to each customer.\n  The type of coupon is a  fixed effect  because the company is interested in those particular coupons. The store location is a  random effect  because the locations used are a sample from the larger population of interest, and while there is likely to be store-to-store variation in customer spending, the company is not directly interested in that variation in the context of this problem.   Covariates. \u00a0Scale predictors\u00a0should be selected as\u00a0covariates\u00a0in the model. Within combinations of factor levels (or\u00a0cells), values of covariates are assumed to be linearly correlated with values of the dependent variables.  Interactions. \u00a0The Linear Mixed Models procedure allows you to specify factorial interactions, which means that each combination of factor levels can have a different linear effect on the dependent variable. Additionally, you may specify factor-covariate interactions, if you believe that the linear relationship between a covariate and the dependent variable changes for different levels of a factor.  Random effects covariance structure.  The Linear Mixed Models procedure allows you to specify the relationship between the levels of random effects. By default, levels of random effects are uncorrelated and have the same variance.   Repeated effects. \u00a0Factors and covariates are features of the general linear model. In the Linear Mixed Models procedure, repeated effects variables are added, allowing you to relax the assumption of independence of the error terms. In order to model the covariance structure of the error terms, you need to specify the following:   Repeated effects variables \u00a0are variables whose values in the data file can be considered as markers of multiple observations of a single subject.  Subject variables \u00a0define the individual subjects of the repeated measurements. The error terms for each individual are independent of those of other individuals.  The\u00a0 covariance structure \u00a0specifies the relationship between the levels of the repeated effects. The types of covariance structures available allow for residual terms with a wide variety of variances and covariances.    Practical Example  If the grocery store recorded the purchasing habits of their customers for four consecutive weeks, then the variable\u00a0Week\u00a0would be a  repeated effects variable . Specifying a subject variable denoting the\u00a0Customer ID\u00a0differentiates the repeated observations of separate customers. Specifying a first-order autoregressive covariance structure reflects your belief that a higher-than-average volume of purchases in one week will correspond to a higher (or lower)-than-average volume in the following week.", 
            "title": "Linear Mixed Model General Concepts"
        }, 
        {
            "location": "/procedures/mixed/#sas-formulation", 
            "text": "1\n2\n3\n4\n5\n6\n7 PROC MIXED DATA=SAS-data-set;\n  CLASS categorical1 categorical2;\n  MODEL response = continuous1 categorical1 continuous1*categorical1 / solution;\n  RANDOM categorical2;\n  LSMEANS continuous1*categorical1 / CL PDIFF DIFFS E;\nRUN;\nQUIT;", 
            "title": "SAS Formulation"
        }, 
        {
            "location": "/procedures/sql/", 
            "text": "Introduction to \nPROC SQL\n\n\n\n\nCheck these websites\n\n\n\n\nSQL syntax available in SAS\n\n\n\n\n\n\nCode Examples\n\n\nCreating Tables Filtering Specific Data\n\n\n1\n2\n3\n4\n5\n6\nPROC SQL;\n   CREATE TABLE table-name AS\n   SELECT DISTINCT variable1\n   FROM origin-data-set\n   WHERE variable2 NE \n AND variable3 IN (\nvalue1\n \nvalue2\n) ; \nQUIT;\n\n\n\n\n\n\nMacrovariable Creation\n\n\nCheck more information in \nthis section\n.\n\n\nIn this code a macrovariable is created containing a list of a variable distinct values (\nlist\n). Another macrovariable is also created whose value is the number of elements in the list (\nnelements\n).\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n* Count distinct elements;\nPROC SQL NOPRINT;\n    SELECT COUNT(DISTINCT variable2check) \n    INTO : nelements\n    FROM original-data-set;\nQUIT;\n\n* Load these distinct elements in a macrovariable list;\nPROC SQL NOPRINT;\n    SELECT DISTINCT variable2check. \n    INTO : list SEPARATED BY \n$\n \n    FROM original-data-set;\nQUIT;\n\n\n\n\n\n\nIs there any way to do these two operations in just one \nPROC SQL\n?\n Two different \nINTO:\n are not compatible.\n\n\n\n\nGeneral \nPROC SQL\n options:\n\n\nThe \nNOPRINT\n option avoids any output printing\n\n\n\n\n\n\nSELECT\n statement elements:\n\n\nThe \nCOUNT(*)\n option counts the number of elements of the table designated in the \nFROM\n statement\n\n\nThe \nDISTINCT\n option selects only different variable values\n\n\nINTO: name\n creates a \nname\n macrovariable containing the result of that specific query\n\n\nSEPARATED BY ' '\n defines a the separator between elements\n\n\n\n\n\n\n\n\nSelecting Distinct Values of a Variable to Create a Data Set with them\n\n\n1\n2\n3\n4\nPROC SQL NOPRINT;\n    CREATE TABLE new-SAS-data-set AS SELECT DISTINCT analyzed-variable\n    FROM original-SAS-data-set;\nQUIT;\n\n\n\n\n\n\nCartesian Product of Data Sets (All Possible Combinations)\n\n\n1\n2\n3\nPROC SQL;\n    CREATE TABLE new-SAS-data-set AS SELECT variable1, variable2, variablE3 FROM original-SAS-data-set1 AS f1 CROSS JOIN original-SAS-data-set2 AS f2 CROSS JOIN original-SAS-data-set3 AS f3;\nQUIT;\n\n\n\n\n\n\nSelecting the Maximum Value\n\n\nIf you need only the maximum value of a certain variable for equal registers except this value, you will use the \nMAX()\n function. If you want to keep in the selection any other variable that you do not need to group you will need to apply the \nMAX()\n function to it as well.\n\n\n1\n2\n3\n4\n5\n6\nPROC SQL;\n    CREATE TABLE new-SAS-data-set AS\n    SELECT grouped-variable1, grouped-variable2, MAX(ungrouped-variable) AS alias-ungrouped, MAX(maximized-variable) AS alias-maximized\n    FROM original-SAS-data-set\n    GROUP BY grouped-variable1, grouped-variable2;\nQUIT;\n\n\n\n\n\n\nCounting Grouped Elements\n\n\nA simple example first.\n\n\n1\n2\n3\n4\n5\nPROC SQL NOPRINT;\n    SELECT COUNTt(*)\n    INTO :nlabs\n    FROM SAS-data-set;\nQUIT;\n\n\n\n\n\n\nIn this first \nPROC SQL\n a number of count variables are created:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\nPROC SQL;\n    CREATE TABLE AESWPP AS\n        SELECT COHORT, AEBODSY, AEDECOD, PT, MAX(AETOXGRN) AS GRADE, COUNT(*) AS NEVENTS, \n            NTOTPATSCOHORT.NPATS AS NTOTPATSCOHORT, \n         100/NTOTPATSCOHORT.NPATS AS PCTPATC,\n            NTOTEVENTSCOHORT.NEVENTS AS NTOTEVENTSCOHORT, \n         100/NTOTEVENTSCOHORT.NEVENTS AS PCTEVENTSC,\n            NTOTPATS.NPATS AS NTOTPATS, \n         100/NTOTPATS.NPATS AS PCTPATTOT,\n            NTOTEVENTS.NEVENTS AS NTOTEVENTS, \n         100/NTOTEVENTS.NEVENTS AS PCTOTEVENTS\n        FROM ADS.TEAE AS TEAE, \n            (SELECT COUNT(DISTINCT PT) AS NPATS, COHORT AS COHORTP FROM ADS.TEAE GROUP BY COHORT) NTOTPATSCOHORT,\n            (SELECT COUNT(*) AS NEVENTS, COHORT AS COHORTE FROM ADS.TEAE GROUP BY COHORT) NTOTEVENTSCOHORT,\n            (SELECT COUNT(DISTINCT PT) AS NPATS FROM ADS.TEAE) NTOTPATS,\n            (SELECT COUNT(*) AS NEVENTS FROM ADS.TEAE) NTOTEVENTS\n                WHERE NTOTPATSCOHORT.COHORTP EQ TEAE.COHORT\n                    AND NTOTEVENTSCOHORT.COHORTE EQ TEAE.COHORT\n                GROUP BY COHORT, AEBODSY, AEDECOD, PT;\nQUIT;\n\n\n\n\n\n\nIn this second \nPROC SQL\n the number of certain ocurrences are counted.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\nPROC SQL;\n    CREATE TABLE AESWPP_1\n        AS SELECT * ,\n            CASE \n                WHEN GRADE EQ 1 THEN COUNT(1) \n                ELSE 0 \n            END \n        AS GRADE1,\n            CASE \n                WHEN GRADE EQ 2 THEN COUNT(1) \n                ELSE 0 \n            END \n        AS GRADE2,\n            CASE \n                WHEN GRADE EQ 3 THEN COUNT(1) \n                ELSE 0 \n            END \n        AS GRADE3,\n            CASE \n                WHEN GRADE EQ 4 THEN COUNT(1) \n                ELSE 0 \n            END \n        AS GRADE4,\n            CASE \n                WHEN GRADE EQ 5 THEN COUNT(1) \n                ELSE 0 \n            END \n        AS GRADE5\n            FROM AESWPP\n                GROUP BY COHORT, AEBODSY, AEDECOD, GRADE\n                    ORDER BY COHORT, AEBODSY, AEDECOD;\nQUIT;\n\n\n\n\n\n\nSelecting First and Last Dates Related to a Patient\n\n\n1\n2\n3\n4\n5\n6\nproc sql;\n    create table firstlastdates as \n        select min(STARTDATE) as date1, max(ENDDATE) as date2, pt, visit\n            from origin-SAS-data-set \n            group by pt, visit;\nquit;", 
            "title": "PROC SQL"
        }, 
        {
            "location": "/procedures/sql/#introduction-to-proc-sql", 
            "text": "Check these websites   SQL syntax available in SAS", 
            "title": "Introduction to PROC SQL"
        }, 
        {
            "location": "/procedures/sql/#code-examples", 
            "text": "", 
            "title": "Code Examples"
        }, 
        {
            "location": "/procedures/sql/#creating-tables-filtering-specific-data", 
            "text": "1\n2\n3\n4\n5\n6 PROC SQL;\n   CREATE TABLE table-name AS\n   SELECT DISTINCT variable1\n   FROM origin-data-set\n   WHERE variable2 NE   AND variable3 IN ( value1   value2 ) ; \nQUIT;", 
            "title": "Creating Tables Filtering Specific Data"
        }, 
        {
            "location": "/procedures/sql/#macrovariable-creation", 
            "text": "Check more information in  this section .  In this code a macrovariable is created containing a list of a variable distinct values ( list ). Another macrovariable is also created whose value is the number of elements in the list ( nelements ).   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 * Count distinct elements;\nPROC SQL NOPRINT;\n    SELECT COUNT(DISTINCT variable2check) \n    INTO : nelements\n    FROM original-data-set;\nQUIT;\n\n* Load these distinct elements in a macrovariable list;\nPROC SQL NOPRINT;\n    SELECT DISTINCT variable2check. \n    INTO : list SEPARATED BY  $  \n    FROM original-data-set;\nQUIT;   Is there any way to do these two operations in just one  PROC SQL ?  Two different  INTO:  are not compatible.   General  PROC SQL  options:  The  NOPRINT  option avoids any output printing    SELECT  statement elements:  The  COUNT(*)  option counts the number of elements of the table designated in the  FROM  statement  The  DISTINCT  option selects only different variable values  INTO: name  creates a  name  macrovariable containing the result of that specific query  SEPARATED BY ' '  defines a the separator between elements", 
            "title": "Macrovariable Creation"
        }, 
        {
            "location": "/procedures/sql/#selecting-distinct-values-of-a-variable-to-create-a-data-set-with-them", 
            "text": "1\n2\n3\n4 PROC SQL NOPRINT;\n    CREATE TABLE new-SAS-data-set AS SELECT DISTINCT analyzed-variable\n    FROM original-SAS-data-set;\nQUIT;", 
            "title": "Selecting Distinct Values of a Variable to Create a Data Set with them"
        }, 
        {
            "location": "/procedures/sql/#cartesian-product-of-data-sets-all-possible-combinations", 
            "text": "1\n2\n3 PROC SQL;\n    CREATE TABLE new-SAS-data-set AS SELECT variable1, variable2, variablE3 FROM original-SAS-data-set1 AS f1 CROSS JOIN original-SAS-data-set2 AS f2 CROSS JOIN original-SAS-data-set3 AS f3;\nQUIT;", 
            "title": "Cartesian Product of Data Sets (All Possible Combinations)"
        }, 
        {
            "location": "/procedures/sql/#selecting-the-maximum-value", 
            "text": "If you need only the maximum value of a certain variable for equal registers except this value, you will use the  MAX()  function. If you want to keep in the selection any other variable that you do not need to group you will need to apply the  MAX()  function to it as well.  1\n2\n3\n4\n5\n6 PROC SQL;\n    CREATE TABLE new-SAS-data-set AS\n    SELECT grouped-variable1, grouped-variable2, MAX(ungrouped-variable) AS alias-ungrouped, MAX(maximized-variable) AS alias-maximized\n    FROM original-SAS-data-set\n    GROUP BY grouped-variable1, grouped-variable2;\nQUIT;", 
            "title": "Selecting the Maximum Value"
        }, 
        {
            "location": "/procedures/sql/#counting-grouped-elements", 
            "text": "A simple example first.  1\n2\n3\n4\n5 PROC SQL NOPRINT;\n    SELECT COUNTt(*)\n    INTO :nlabs\n    FROM SAS-data-set;\nQUIT;   In this first  PROC SQL  a number of count variables are created:   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20 PROC SQL;\n    CREATE TABLE AESWPP AS\n        SELECT COHORT, AEBODSY, AEDECOD, PT, MAX(AETOXGRN) AS GRADE, COUNT(*) AS NEVENTS, \n            NTOTPATSCOHORT.NPATS AS NTOTPATSCOHORT, \n         100/NTOTPATSCOHORT.NPATS AS PCTPATC,\n            NTOTEVENTSCOHORT.NEVENTS AS NTOTEVENTSCOHORT, \n         100/NTOTEVENTSCOHORT.NEVENTS AS PCTEVENTSC,\n            NTOTPATS.NPATS AS NTOTPATS, \n         100/NTOTPATS.NPATS AS PCTPATTOT,\n            NTOTEVENTS.NEVENTS AS NTOTEVENTS, \n         100/NTOTEVENTS.NEVENTS AS PCTOTEVENTS\n        FROM ADS.TEAE AS TEAE, \n            (SELECT COUNT(DISTINCT PT) AS NPATS, COHORT AS COHORTP FROM ADS.TEAE GROUP BY COHORT) NTOTPATSCOHORT,\n            (SELECT COUNT(*) AS NEVENTS, COHORT AS COHORTE FROM ADS.TEAE GROUP BY COHORT) NTOTEVENTSCOHORT,\n            (SELECT COUNT(DISTINCT PT) AS NPATS FROM ADS.TEAE) NTOTPATS,\n            (SELECT COUNT(*) AS NEVENTS FROM ADS.TEAE) NTOTEVENTS\n                WHERE NTOTPATSCOHORT.COHORTP EQ TEAE.COHORT\n                    AND NTOTEVENTSCOHORT.COHORTE EQ TEAE.COHORT\n                GROUP BY COHORT, AEBODSY, AEDECOD, PT;\nQUIT;   In this second  PROC SQL  the number of certain ocurrences are counted.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32 PROC SQL;\n    CREATE TABLE AESWPP_1\n        AS SELECT * ,\n            CASE \n                WHEN GRADE EQ 1 THEN COUNT(1) \n                ELSE 0 \n            END \n        AS GRADE1,\n            CASE \n                WHEN GRADE EQ 2 THEN COUNT(1) \n                ELSE 0 \n            END \n        AS GRADE2,\n            CASE \n                WHEN GRADE EQ 3 THEN COUNT(1) \n                ELSE 0 \n            END \n        AS GRADE3,\n            CASE \n                WHEN GRADE EQ 4 THEN COUNT(1) \n                ELSE 0 \n            END \n        AS GRADE4,\n            CASE \n                WHEN GRADE EQ 5 THEN COUNT(1) \n                ELSE 0 \n            END \n        AS GRADE5\n            FROM AESWPP\n                GROUP BY COHORT, AEBODSY, AEDECOD, GRADE\n                    ORDER BY COHORT, AEBODSY, AEDECOD;\nQUIT;", 
            "title": "Counting Grouped Elements"
        }, 
        {
            "location": "/procedures/sql/#selecting-first-and-last-dates-related-to-a-patient", 
            "text": "1\n2\n3\n4\n5\n6 proc sql;\n    create table firstlastdates as \n        select min(STARTDATE) as date1, max(ENDDATE) as date2, pt, visit\n            from origin-SAS-data-set \n            group by pt, visit;\nquit;", 
            "title": "Selecting First and Last Dates Related to a Patient"
        }, 
        {
            "location": "/sas-outputs/ods/", 
            "text": "Setting the SAS System Options\n\n\n1\nOPTIONS NODATE PAGENO=1 LINESIZE=80 PAGESIZE=40;\n\n\n\n\n\n\n\n\nThe \nNODATE\n option suppresses the display of the date and time in the output\n\n\nPAGENO=\n specifies the starting page number\n\n\nLINESIZE=\n specifies the output line length\n\n\nPAGESIZE=\n specifies the number of lines on an output page\n\n\nNONUMBER\n removes pagination from the automatic output header\n\n\n\n\nODS\n\n\nIn order to produce outputs from SAS, the three more common \nODS\n techniques, that produces different output files, are HTML, RTF, and PDF. Each \nODS\n statement uses options that are specific to that destination. The \nODS\n options (other than the \nFILE=\n option) used in the program are shown in the table below. \n\n\n\n\n\n\n\n\nRTF\n\n\nPDF\n\n\nHTML\n\n\n\n\n\n\n\n\n\n\nBODYTITLE\n \n \nSTARTPAGE=NO\n \n \nKEEPN\n \n \nNOTOC_DATA\n / \nTOC_DATA\n \n \nCONTENTS\n \n \nCOLUMNS=\n \n \nTEXT=\n\n\nBOOKMARKGEN=NO\n \n \nSTARTPAGE=NO\n \n \nCOMPRESS=9\n \n \nTEXT=\n\n\nSTYLE=SASWEB\n \n \nRS=NONE\n\n\n\n\n\n\n\n\nFor an explanation of the options, refer to \nthis page\n or to the \nODS\n User's Guide\n.\n\n\n\n\nRemove graph's external borders:\n\n\n\n\n1\nODS GRAPHICS / NOBORDER;\n\n\n\n\n\n\nBasic \nODS\n Options\n\n\nYou need to add this command to get the plots displayed in the output:\n\n\n1\n2\n3\nODS GRAPHICS ON;\n[your code here]\nODS GRAPHICS OFF;\n\n\n\n\n\n\nWhen you add the \nODS TRACE\n statement, SAS writes a trace record to the log that includes information about each output object (name, label, template, path):\n\n\n1\n2\n3\nODS TRACE ON;\n[your code here]\nODS TRACE OFF;\n\n\n\n\n\n\nYou produce a list of the possible output elements in the log that you may specify in the \nODS SELECT/EXCLUDE\n statement:\n\n\n1\n2\n3\nODS SELECT output-name1 output-name2 output-name3;\n[your code here]\nODS SELECT ALL;  /* Reset this option to the default */\n\n\n\n\n\n\nYo can keeps some of the outputs in SAS-data-sets:\n\n\n1\nODS OUTPUT output-name1=generated-data-set1 output-name1=generated-data-set2 output-name1=generated-data-set3;\n\n\n\n\n\n\nControl the output via \nODS EXCLUDE\n\n\nSome interesting articles on this topic by \nRick Wicklin\n:\n\n\n\n\nTurn off \nODS\n when running simulations in SAS\n\n\nWhat is the best way to suppress \nODS\n output in SAS?\n\n\nFive reasons to use \nODS EXCLUDE\n to suppress SAS output\n\n\n\n\nExample on combining \nODS EXCLUDE\n with \nODS OUTPUT\n to control the obtained output:\n\n\n1\n2\n3\n4\n5\n6\nODS EXCLUDE ALL;\nPROC FREQ DATA=SAS-data-set;\n    TABLE variable1*variable2;\n    ODS OUTPUT CROSSTABFREQS=custom-SAS-data-set;\nRUN;\nODS EXCLUDE NONE;", 
            "title": "Output Delivery System"
        }, 
        {
            "location": "/sas-outputs/ods/#setting-the-sas-system-options", 
            "text": "1 OPTIONS NODATE PAGENO=1 LINESIZE=80 PAGESIZE=40;    The  NODATE  option suppresses the display of the date and time in the output  PAGENO=  specifies the starting page number  LINESIZE=  specifies the output line length  PAGESIZE=  specifies the number of lines on an output page  NONUMBER  removes pagination from the automatic output header", 
            "title": "Setting the SAS System Options"
        }, 
        {
            "location": "/sas-outputs/ods/#ods", 
            "text": "In order to produce outputs from SAS, the three more common  ODS  techniques, that produces different output files, are HTML, RTF, and PDF. Each  ODS  statement uses options that are specific to that destination. The  ODS  options (other than the  FILE=  option) used in the program are shown in the table below.      RTF  PDF  HTML      BODYTITLE     STARTPAGE=NO     KEEPN     NOTOC_DATA  /  TOC_DATA     CONTENTS     COLUMNS=     TEXT=  BOOKMARKGEN=NO     STARTPAGE=NO     COMPRESS=9     TEXT=  STYLE=SASWEB     RS=NONE     For an explanation of the options, refer to  this page  or to the  ODS  User's Guide .   Remove graph's external borders:   1 ODS GRAPHICS / NOBORDER;", 
            "title": "ODS"
        }, 
        {
            "location": "/sas-outputs/ods/#basic-ods-options", 
            "text": "You need to add this command to get the plots displayed in the output:  1\n2\n3 ODS GRAPHICS ON;\n[your code here]\nODS GRAPHICS OFF;   When you add the  ODS TRACE  statement, SAS writes a trace record to the log that includes information about each output object (name, label, template, path):  1\n2\n3 ODS TRACE ON;\n[your code here]\nODS TRACE OFF;   You produce a list of the possible output elements in the log that you may specify in the  ODS SELECT/EXCLUDE  statement:  1\n2\n3 ODS SELECT output-name1 output-name2 output-name3;\n[your code here]\nODS SELECT ALL;  /* Reset this option to the default */   Yo can keeps some of the outputs in SAS-data-sets:  1 ODS OUTPUT output-name1=generated-data-set1 output-name1=generated-data-set2 output-name1=generated-data-set3;", 
            "title": "Basic ODS Options"
        }, 
        {
            "location": "/sas-outputs/ods/#control-the-output-via-ods-exclude", 
            "text": "Some interesting articles on this topic by  Rick Wicklin :   Turn off  ODS  when running simulations in SAS  What is the best way to suppress  ODS  output in SAS?  Five reasons to use  ODS EXCLUDE  to suppress SAS output   Example on combining  ODS EXCLUDE  with  ODS OUTPUT  to control the obtained output:  1\n2\n3\n4\n5\n6 ODS EXCLUDE ALL;\nPROC FREQ DATA=SAS-data-set;\n    TABLE variable1*variable2;\n    ODS OUTPUT CROSSTABFREQS=custom-SAS-data-set;\nRUN;\nODS EXCLUDE NONE;", 
            "title": "Control the output via ODS EXCLUDE"
        }, 
        {
            "location": "/sas-outputs/graphs/", 
            "text": "Check these websites\n\n\n\n\nHere\n are some examples of complex graphs.\n\n\nHere\n there are instructions to play with the axis' attributes.\n\n\nGraphically speaking\n blog with useful tips for graphics.\n\n\nWelcome to the Three Ring %CIRCOS: An Example of Creating a Circular Graph without a Polar Axis\n\n\n\n\n\n\nBasic \nODS\n Options\n\n\nYou need to add this command to get the plots displayed in the output:\n\n\n1\n2\n3\nODS GRAPHICS ON;\n[your code here]\nODS GRAPHICS OFF;\n\n\n\n\n\n\nWhen you add the \nODS TRACE\n statement, SAS writes a trace record to the log that includes information about each output object (name, label, template, path):\n\n\n1\n2\n3\nODS TRACE ON;\n[your code here]\nODS TRACE OFF;\n\n\n\n\n\n\nYou produce a list of the possible output elements in the log that you may specify in the \nODS SELECT/EXCLUDE\n statement:\n\n\n1\n2\n3\nODS SELECT output-name1 output-name2 output-name3;\n[your code here]\nODS SELECT ALL;  /* Reset this option to the default */\n\n\n\n\n\n\nYo can keeps some of the outputs in SAS-data-sets:\n\n\n1\nODS OUTPUT output-name1=generated-data-set1 output-name1=generated-data-set2 output-name1=generated-data-set3;\n\n\n\n\n\n\n\n\n\n\nRemove date and pagination from the automatic output header:\n\n\n\n\n1\nOPTIONS NODATE NONUMBER;\n\n\n\n\n\n\n\n\nRemove graph's external borders:\n\n\n\n\n1\nODS GRAPHICS / NOBORDER;\n\n\n\n\n\n\nPlot Procedures\n\n\nGPLOT\n\n\n\n\nReference lines:\n\n\n\n\n1\n2\n3\n4\n5\n6\nSYMBOL1 COLOR=blue INTERPOL=join;\nAXIS1 LABEL=(\nX axis label\n) order=(0 to 15 by 1) reflabel=(j=c h=9pt \nReference line label 1\n \nReference line label 2\n \nReference line label 3\n);\nAXIS2 LABEL=(\nY axis label\n j=c);\nPROC GPLOT DATA=SAS-data-set;\n    PLOT variabley*variablex / HAXIS=AXIS1 VAXIS=AXIS2 HREF=6 9 13 /*location of ref lines*/;\nRUN;\n\n\n\n\n\n\nSGPLOT\n\n\n\n\nHighlight a certain boxplot and get the plot narrower: \n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\nPROC SGPLOT DATA=sashelp.heart;\n    /* The order matters: first thing defined goes to the back */\n    REFLINE \nCoronary Heart Disease\n / AXIS=x \n        LINEATTRS=(THICKNESS=70 COLOR=yellow) TRANSPARENCY=0.5 ;\n    VBOX cholesterol / CATEGORY=deathcause;\n    XAXIS OFFSETMIN=0.25 OFFSETMAX=0.25 DISCRETEORDER=data;\n    YAXIS GRID;\nRUN;\n\n\n\n\n\n\n\n\nSpecify the colors/point styles of groups in SAS statistical graphics\n\n\n\n\nType of Plots\n\n\n\n\nCheck these websites\n\n\n\n\nClinical Graphs using SAS\n\n\nGraphical Results in Clinical Oncology Studies\n\n\n\n\n\n\nSwimmer plot\n\n\nA swimmer plot is a graphical tool involving horizontal bars that can be used to show multiple pieces of information about a given data set in one glance. In this example a swimmer plot is used to tell \u201ca story\u201d about the effects of a study treatment on tumor response for individual subjects in an oncology study. Through the use of a swimmer plot we are able to look at our data on an individual subject level rather than an aggregate level that is often done for \u201ctime to response\u201d analysis using Kaplan-Meier methods.\n\n\n\n\n\n\nCheck these websites\n\n\n\n\nSwimmer Plot\n\n\n[Swimmer Plot: Tell a Graphical Story of Your Time to Response Data Using PROC\n\n\n\n\n\n\nSGPLOT](http://www.pharmasug.org/proceedings/2014/DG/PharmaSUG-2014-DG07.pdf)\n\n\nWaterfall plot\n\n\nA waterfall chart is commonly used in the Oncology domain to track the change in tumor size for subjects in a study by treatment. The graph displays the change in tumor size for each subject in the study by descending percent change from baseline. A bar is displayed for each subject in decreasing order. Each bar is classified by the treatment. The response category is displayed at the end of the bar. Reference lines are drawn at RECIST threshold of -30% and at 20%.\n\n\n\n\n\n\nCheck these websites\n\n\n\n\nClinical graphs: Waterfall plot ++\n\n\nCreate a waterfall plot in SAS\n\n\nWaterfall plot: two different approaches, one beautiful graph\n\n\n[Waterfall Charts in Oncology Trials - Ride the Wave}(https://www.pharmasug.org/proceedings/2012/DG/PharmaSUG-2012-DG13.pdf)\n\n\nA 3D waterfall chart\n\n\n\n\n\n\nMiscellanea\n\n\nAvailable Colors at the SAS Registry\n\n\nYou can check the \nlist of SAS predefined colors\n and even list it using the SAS registry:\n\n\n1\n2\nPROC REGISTRY LIST STARTAT=\n\\COLORNAMES\\HTML\n; \nRUN; \n\n\n\n\n\n\n\n\nCheck this website\n\n\n\n\nUsing the SAS Registry to Control Color", 
            "title": "Graphs and Plots"
        }, 
        {
            "location": "/sas-outputs/graphs/#basic-ods-options", 
            "text": "You need to add this command to get the plots displayed in the output:  1\n2\n3 ODS GRAPHICS ON;\n[your code here]\nODS GRAPHICS OFF;   When you add the  ODS TRACE  statement, SAS writes a trace record to the log that includes information about each output object (name, label, template, path):  1\n2\n3 ODS TRACE ON;\n[your code here]\nODS TRACE OFF;   You produce a list of the possible output elements in the log that you may specify in the  ODS SELECT/EXCLUDE  statement:  1\n2\n3 ODS SELECT output-name1 output-name2 output-name3;\n[your code here]\nODS SELECT ALL;  /* Reset this option to the default */   Yo can keeps some of the outputs in SAS-data-sets:  1 ODS OUTPUT output-name1=generated-data-set1 output-name1=generated-data-set2 output-name1=generated-data-set3;     Remove date and pagination from the automatic output header:   1 OPTIONS NODATE NONUMBER;    Remove graph's external borders:   1 ODS GRAPHICS / NOBORDER;", 
            "title": "Basic ODS Options"
        }, 
        {
            "location": "/sas-outputs/graphs/#plot-procedures", 
            "text": "", 
            "title": "Plot Procedures"
        }, 
        {
            "location": "/sas-outputs/graphs/#gplot", 
            "text": "Reference lines:   1\n2\n3\n4\n5\n6 SYMBOL1 COLOR=blue INTERPOL=join;\nAXIS1 LABEL=( X axis label ) order=(0 to 15 by 1) reflabel=(j=c h=9pt  Reference line label 1   Reference line label 2   Reference line label 3 );\nAXIS2 LABEL=( Y axis label  j=c);\nPROC GPLOT DATA=SAS-data-set;\n    PLOT variabley*variablex / HAXIS=AXIS1 VAXIS=AXIS2 HREF=6 9 13 /*location of ref lines*/;\nRUN;", 
            "title": "GPLOT"
        }, 
        {
            "location": "/sas-outputs/graphs/#sgplot", 
            "text": "Highlight a certain boxplot and get the plot narrower:    1\n2\n3\n4\n5\n6\n7\n8 PROC SGPLOT DATA=sashelp.heart;\n    /* The order matters: first thing defined goes to the back */\n    REFLINE  Coronary Heart Disease  / AXIS=x \n        LINEATTRS=(THICKNESS=70 COLOR=yellow) TRANSPARENCY=0.5 ;\n    VBOX cholesterol / CATEGORY=deathcause;\n    XAXIS OFFSETMIN=0.25 OFFSETMAX=0.25 DISCRETEORDER=data;\n    YAXIS GRID;\nRUN;    Specify the colors/point styles of groups in SAS statistical graphics", 
            "title": "SGPLOT"
        }, 
        {
            "location": "/sas-outputs/graphs/#type-of-plots", 
            "text": "Check these websites   Clinical Graphs using SAS  Graphical Results in Clinical Oncology Studies", 
            "title": "Type of Plots"
        }, 
        {
            "location": "/sas-outputs/graphs/#swimmer-plot", 
            "text": "A swimmer plot is a graphical tool involving horizontal bars that can be used to show multiple pieces of information about a given data set in one glance. In this example a swimmer plot is used to tell \u201ca story\u201d about the effects of a study treatment on tumor response for individual subjects in an oncology study. Through the use of a swimmer plot we are able to look at our data on an individual subject level rather than an aggregate level that is often done for \u201ctime to response\u201d analysis using Kaplan-Meier methods.    Check these websites   Swimmer Plot  [Swimmer Plot: Tell a Graphical Story of Your Time to Response Data Using PROC    SGPLOT](http://www.pharmasug.org/proceedings/2014/DG/PharmaSUG-2014-DG07.pdf)", 
            "title": "Swimmer plot"
        }, 
        {
            "location": "/sas-outputs/graphs/#waterfall-plot", 
            "text": "A waterfall chart is commonly used in the Oncology domain to track the change in tumor size for subjects in a study by treatment. The graph displays the change in tumor size for each subject in the study by descending percent change from baseline. A bar is displayed for each subject in decreasing order. Each bar is classified by the treatment. The response category is displayed at the end of the bar. Reference lines are drawn at RECIST threshold of -30% and at 20%.    Check these websites   Clinical graphs: Waterfall plot ++  Create a waterfall plot in SAS  Waterfall plot: two different approaches, one beautiful graph  [Waterfall Charts in Oncology Trials - Ride the Wave}(https://www.pharmasug.org/proceedings/2012/DG/PharmaSUG-2012-DG13.pdf)  A 3D waterfall chart", 
            "title": "Waterfall plot"
        }, 
        {
            "location": "/sas-outputs/graphs/#miscellanea", 
            "text": "", 
            "title": "Miscellanea"
        }, 
        {
            "location": "/sas-outputs/graphs/#available-colors-at-the-sas-registry", 
            "text": "You can check the  list of SAS predefined colors  and even list it using the SAS registry:  1\n2 PROC REGISTRY LIST STARTAT= \\COLORNAMES\\HTML ; \nRUN;     Check this website   Using the SAS Registry to Control Color", 
            "title": "Available Colors at the SAS Registry"
        }, 
        {
            "location": "/sas-outputs/print/", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\nPROC FORMAT;\n    VALUE $nfmt \n        \nAlfred\n = \nverylightgreen\n\n        \nHenry\n = \nverylightblue\n\n        \nBarbara\n = \nverylightyellow\n\n        \nCarol\n = \nverylightpurple\n;\n    VALUE wfmt \n        LOW-85.99 = \nred\n\n        86-99.99 = \norange\n\n        100-HIGH = \ngreen\n;\nRUN;\n\nPROC SORT DATA=sashelp.class OUT=classtest;\n    BY sex;\nRUN;\n\nPROC PRINT DATA=classtest(FIRSTOBS=2 OBS=16) N\n    STYLE(TABLE)      = {BACKGROUND=yellow RULES=rows FRAME=void CELLSPACING=0 BORDERCOLOR=red BORDERWIDTH=5}\n    STYLE(HEADER)     = {BACKGROUND=purple FONT_SIZE=12pt}\n    STYLE(GRANDTOTAL) = {BACKGROUND=pink FONT_SIZE=14pt}\n    STYLE(OBS)        = {BACKGROUND=pink COLOR=BLACK FONT_SIZE=12pt}\n    STYLE(OBSHEADER)  = {BACKGROUND=green FONT_SIZE=12pt};\n    TITLE \nPROC PRINT example\n;\n    VAR name   / STYLE(HEADER)=HEADER{FONT_SIZE=12pt} STYLE(DATA)=HEADER{FONT_SIZE=12pt BACKGROUND=$nfmt. FOREGROUND=black};\n    VAR age    / STYLE(HEADER)=HEADER{FONT_SIZE=12pt} STYLE(DATA)=HEADER{FONT_SIZE=12pt};\n    VAR height / STYLE(HEADER)=HEADER{FONT_SIZE=12pt} STYLE(DATA)={FONT_SIZE=12pt};\n    VAR weight / STYLE(HEADER)=HEADER{FONT_SIZE=12pt} STYLE(DATA)={FONT_SIZE=12pt FOREGROUND=wfmt. FONT_WEIGHT=bold};\n    BY sex;\n    SUM height weight;\nRUN; \n\n\n\n\n\n\n\n\n\n\nFIRSTOBS\n is the first observation printed\n\n\nOBS\n is the last observation printed\n\n\nN\n prints the number of observations in the data set\n\n\nBY\n is to generate a table for each \nBY\n variable level (the data is expected to be ordered)\n\n\nPAGEBY\n is for creating page breaks using a variable\n\n\nSUM\n produces totaling variables (you will get a subtotal for each \nBY\n group in which there is more than one observation)\n\n\nYou can print only certain variables or changing their order using \nVAR\n\n\nFRAME=\n defines the borders of the cells: \nVOID\n means no borders while \nBOX\n means all borders", 
            "title": "PROC PRINT"
        }, 
        {
            "location": "/sas-outputs/report/", 
            "text": "Examples\n\n\nHow to Write a Header/Footer in your Tables\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\nODS ESCAPECHAR=\n^\n;\n\nPROC REPORT DATA=sashelp.cars;\n    WHERE Make = \nJaguar\n;\n    COLUMN (\n1) Label 1\n model Invoice)\n            (\n2) Label 2\n Horsepower Weight Length);\n    COMPUTE BEFORE _PAGE_ / STYLE=HEADER{JUST=L FONTWEIGHT=BOLD COLOR=PURPLE};\n        LINE \nTest of custom header\n;\n    ENDCOMP;\n    COMPUTE AFTER / STYLE={TEXTDECORATION=UNDERLINE JUST=C COLOR=RED};\n        LINE \nTest of a custom footer\n;\n        LINE \n^S={color=green} Test of a custom footer with a different style\n;\n    ENDCOMP;\nRUN;\n\n\n\n\n\n\nSpecify the \nSTYLE\n of Your Global Header\n\n\n1\n2\n3\n4\n5\nPROC REPORT DATA=SAS-data-set HEADSKIP HEADLINE NOWINDOWS STYLE(header)={ASIS=on BACKGROUND=very light grey FONTWEIGHT=BOLD};\n    COLUMN (\nStyle of this global header\n var1 var2);\n    DEFINE var1 / DISPLAY \nParameters\n LEFT STYLE=[FONTWEIGHT=BOLD];\n    DEFINE var2 / DISPLAY \nValues\n CENTER;\nRUN;\n\n\n\n\n\n\n\n\nCheck these websites\n\n\n\n\nBeyond the Basics: Advanced \nPROC REPORT\n Tips and Tricks\n\n\nCreating a Plan for Your Reports and Avoiding Common Pitfalls in \nREPORT\n Procedure Coding\n\n\nTurn Your Plain Report into a Painted Report Using ODS Styles\n\n\n\n\n\n\nSpecify the \nSTYLE\n of a Cell Based on Other Cell's Value\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nPROC REPORT DATA=SAS-data-set NOWD;\n    COLUMN timeinterval date1 date2;\n    DEFINE timeinterval / DISPLAY NOPRINT; \n    DEFINE date1 / DISPLAY;\n    DEFINE date2 / DISPLAY;\n    COMPUTE date2;\n        IF timeinterval lt 0 and timeinterval ne . then call define(_col_,\nstyle\n,\nstyle={foreground=red font_weight=bold}\n);\n        ELSE call define(_col_,\nstyle\n,\nstyle={foreground=green font_weight=bold}\n);\n    ENDCOMP;\nRUN;\n\n\n\n\n\n\n\n\nDEFINE\n the variables involved in your conditional structure before the variable to which you want to apply the new format \n\n\nDEFINE\n your variables as \nDISPLAY NOPRINT\n if you want to use them for the conditional structure but you don't want them to appear in your table\n\n\n\n\n\n\nTip\n\n\nRemember, \nPROC REPORT\n builds each row from left to right, so the value used as a condition to define the style must be to the left of the values whose style/format you want to change. \n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\ndata test;\n    input flag pt $ firstdate $ lastdate $ ;\n    datalines;\n0 X1 XX-XX-XXXX XX-XX-XXXX\n1 X2 XX-XX-XXXX XX-XX-XXXX\n0 X3 XX-XX-XXXX XX-XX-XXXX\n0 X4 XX-XX-XXXX XX-XX-XXXX\n0 X5 XX-XX-XXXX XX-XX-XXXX\n2 X6 XX-XX-XXXX XX-XX-XXXX\n;\nrun;\n\nproc report data=test nowindows headline style(header)={background=very light grey} missing split=\n*\n;\n    column (\nFirst and Last Dates in the Study\n (\nPatient\n pt) (\nFirst Study Date\n firstdate) (\nLast Study Date\n lastdate) flag color1 color2);\n    define pt / \n display order=internal;\n    define firstdate / \n display;\n    define lastdate / \n display;\n    define flag / display noprint;\n    define color1 / computed noprint;\n    define color2 / computed noprint;\n\n    compute color1;\n        if flag eq 1 then call define(\nfirstdate\n,\nstyle\n,\nstyle={background=yellow}\n);\n    endcomp;\n\n    compute color2;\n        if flag eq 2 then call define(\nlastdate\n,\nstyle\n,\nstyle={background=yellow}\n);\n    endcomp;\nrun;\n\n\n\n\n\n\n\n\nControl Output Table Width\n\n\nIf the margins of the page are defined\n\n\n1\noptions topmargin=.98in bottommargin=.98in leftmargin=1.18in rightmargin=.98in;\n\n\n\n\n\n\nand you \n\n\n1\n2\n3\n4\n5\n6\n7\n8\nproc\n \nreport\n \ndata\n=\nLEDDall2report_ago\n \nsplit\n=\n*\n \nnowd\n \ncolwidth\n=\n10\n \nheadline\n \nheadskip\n \nmissing\n \nspanrows\n \nstyle\n(\nreport\n)=\n{\nwidth=100%\n}\n;\n\n            \ncolumn\n \n(\nDosis equivalente de levodopa diaria y n\u00famero de f\u00e1rmacos de agonistas dopamin\u00e9rgicos: descripci\u00f3n del cambio respecto a basal\n \nind\n \ntexto\n \nparam\n \ngrupo1\n \ngrupo2\n \ngrupo3\n \ngrupo4\n \npvalor\n \nflag\n \nflag2\n);\n\n            \ndefine\n \nind\n \n/\n \norder\n \nnoprint\n;\n\n            \ndefine\n \ntexto\n \n/\n \n \nstyle\n(\ncolumn\n)=\n[\nwidth\n=\n7\n%\n]\n;\n\n            \ndefine\n \nparam\n \n/\n \n \nstyle\n(\ncolumn\n)=\n[\nwidth\n=\n5\n%\n]\n;\n\n            \ndefine\n \ngrupo1\n \n/\n \nVisita 1 * (N=59)\n \nstyle\n(\ncolumn\n)=\n[\nwidth\n=\n5\n%\n]\n;\n\n            \n(...)\n\n\nrun\n;\n\n\n\n\n\n\n\nIntroducing Line Breaks\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\nPROC REPORT DATA=SAS-data-set NOWINDOWS HEADLINE STYLE(HEADER)={BACKGROUND=VERY LIGHT GREY} MISSING SPLIT=\n*\n;\n    COLUMN (\nSample report\n var1 var2 var3);\n    DEFINE var1 / \nLabel 1\n GROUP ORDER=INTERNAL;\n    DEFINE var2 / \nLabel 2\n DISPLAY;\n    DEFINE var3 / \nLabel 3\n DISPLAY;\n\n    * Introduce some line separations between var1 values;\n    BREAK BEFORE var1 / SUMMARIZE STYLE=[BACKGROUND=VERY LIGHT GREY];\n    * Avoid repeated labels;\n    COMPUTE var2;\n         IF MISSING(_BREAK_) THEN var1=\n \n;\n    ENDCOMP;\nRUN;\n\n\n\n\n\n\n\n\nTip\n\n\nIf the variable is numeric a \n.\n will apear in the break row. You need to create a format to assign \n'            '\n to \n.\n.\nIf you describe the new value of \n.\n as \n' '\n your numbers will be truncated.\n\n\n\n\nWorking with \nACROSS\n\n\n\n\nSimple example:\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nPROC REPORT DATA=_AUX3 NOWINDOWS  \n    HEADLINE STYLE(HEADER)={BACKGROUND=VERY LIGHT GREY} MISSING SPLIT=\n*\n;\n    COLUMN(\nVAR\nI (\nUNIT\nI)\n \nTIMEVAR. (\nSTATISTICS\n _LABEL_) \nSTRATAVAR., COL1);\n    DEFINE _LABEL_ / \n GROUP ORDER=DATA;\n    DEFINE \nSTRATAVAR./ \n ACROSS NOZERO ORDER=INTERNAL;\n    /* NOZERO = SINCE ALL PRODUCT CATEGORIES WILL NOT BE REPRESENTED FOR EACH PRODUCT LINE IN THE TABLE */\n    DEFINE \nTIMEVAR./ \n F=\nTIMEFMT. GROUP ORDER=INTERNAL; \n    DEFINE COL1/ \n GROUP;\nRUN;\n\n\n\n\n\n\n\n\nComplex example:\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\nproc report data=_data2report nowindows headline style(header)={background=very light grey} missing split=\n*\n;\n    column(\nLab tests (Hematology): normal, high abnormal and low abnormal results n(%)\n lbtest trtgrpnum (\nValue at * screening\n clinsigSCR) (\nDay 6\n clinsig60, npctn60 _dummy) (\nEarly Termination Day\n clinsigET, npctnET _dummy));\n    define lbtest / \n group order=internal;\n    define trtgrpnum/ \n group order=internal;\n    define clinsigSCR / \n group order=internal;\n    define clinsig60 / \n across nozero order=internal;\n    define clinsigET / \n across nozero order=internal;\n    * nozdero = since all product categories will not be represented for each product line in the table;\n    define npctn60/ group \n;\n    define npctnET/ group \n;\n    define _dummy / computed noprint; /* This variable is created to avoid an error message */\n\n    compute after/style=[just=L foreground=black FONT_SIZE=9pt];\n        line \nTable footer line 1\n;\n        line \nTable footer line 2\n;\n    endcomp;\n\n    * Introduce some line separations between arms of treatment;\n    break after trtgrpnum / skip;\n    * Introduce some line separations between tests;\n    break before lbtest / summarize style=[background=very light grey FONT_WEIGHT=BOLD];\n    * Avoid repeated labels;\n    compute npctn60;\n        if missing(_break_) then lbtest=\n \n;\n    endcomp;\nrun;\n\n\n\n\n\n\n\n\nCheck these websites\n\n\n\n\nSailing Over the \nACROSS\n Hurdle in \nPROC REPORT\n\n\n\n\n\n\nDefining your own variables\n\n\n1\n2\n3\n4\n5\n6\nDEFINE obs / COMPUTED; \n\nCOMPUTE obs;\n    dsobs + 1;\n    obs = dsobs;\nENDCOMPUTE;", 
            "title": "PROC REPORT"
        }, 
        {
            "location": "/sas-outputs/report/#examples", 
            "text": "", 
            "title": "Examples"
        }, 
        {
            "location": "/sas-outputs/report/#how-to-write-a-headerfooter-in-your-tables", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 ODS ESCAPECHAR= ^ ;\n\nPROC REPORT DATA=sashelp.cars;\n    WHERE Make =  Jaguar ;\n    COLUMN ( 1) Label 1  model Invoice)\n            ( 2) Label 2  Horsepower Weight Length);\n    COMPUTE BEFORE _PAGE_ / STYLE=HEADER{JUST=L FONTWEIGHT=BOLD COLOR=PURPLE};\n        LINE  Test of custom header ;\n    ENDCOMP;\n    COMPUTE AFTER / STYLE={TEXTDECORATION=UNDERLINE JUST=C COLOR=RED};\n        LINE  Test of a custom footer ;\n        LINE  ^S={color=green} Test of a custom footer with a different style ;\n    ENDCOMP;\nRUN;", 
            "title": "How to Write a Header/Footer in your Tables"
        }, 
        {
            "location": "/sas-outputs/report/#specify-the-style-of-your-global-header", 
            "text": "1\n2\n3\n4\n5 PROC REPORT DATA=SAS-data-set HEADSKIP HEADLINE NOWINDOWS STYLE(header)={ASIS=on BACKGROUND=very light grey FONTWEIGHT=BOLD};\n    COLUMN ( Style of this global header  var1 var2);\n    DEFINE var1 / DISPLAY  Parameters  LEFT STYLE=[FONTWEIGHT=BOLD];\n    DEFINE var2 / DISPLAY  Values  CENTER;\nRUN;    Check these websites   Beyond the Basics: Advanced  PROC REPORT  Tips and Tricks  Creating a Plan for Your Reports and Avoiding Common Pitfalls in  REPORT  Procedure Coding  Turn Your Plain Report into a Painted Report Using ODS Styles", 
            "title": "Specify the STYLE of Your Global Header"
        }, 
        {
            "location": "/sas-outputs/report/#specify-the-style-of-a-cell-based-on-other-cells-value", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 PROC REPORT DATA=SAS-data-set NOWD;\n    COLUMN timeinterval date1 date2;\n    DEFINE timeinterval / DISPLAY NOPRINT; \n    DEFINE date1 / DISPLAY;\n    DEFINE date2 / DISPLAY;\n    COMPUTE date2;\n        IF timeinterval lt 0 and timeinterval ne . then call define(_col_, style , style={foreground=red font_weight=bold} );\n        ELSE call define(_col_, style , style={foreground=green font_weight=bold} );\n    ENDCOMP;\nRUN;    DEFINE  the variables involved in your conditional structure before the variable to which you want to apply the new format   DEFINE  your variables as  DISPLAY NOPRINT  if you want to use them for the conditional structure but you don't want them to appear in your table    Tip  Remember,  PROC REPORT  builds each row from left to right, so the value used as a condition to define the style must be to the left of the values whose style/format you want to change.     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29 data test;\n    input flag pt $ firstdate $ lastdate $ ;\n    datalines;\n0 X1 XX-XX-XXXX XX-XX-XXXX\n1 X2 XX-XX-XXXX XX-XX-XXXX\n0 X3 XX-XX-XXXX XX-XX-XXXX\n0 X4 XX-XX-XXXX XX-XX-XXXX\n0 X5 XX-XX-XXXX XX-XX-XXXX\n2 X6 XX-XX-XXXX XX-XX-XXXX\n;\nrun;\n\nproc report data=test nowindows headline style(header)={background=very light grey} missing split= * ;\n    column ( First and Last Dates in the Study  ( Patient  pt) ( First Study Date  firstdate) ( Last Study Date  lastdate) flag color1 color2);\n    define pt /   display order=internal;\n    define firstdate /   display;\n    define lastdate /   display;\n    define flag / display noprint;\n    define color1 / computed noprint;\n    define color2 / computed noprint;\n\n    compute color1;\n        if flag eq 1 then call define( firstdate , style , style={background=yellow} );\n    endcomp;\n\n    compute color2;\n        if flag eq 2 then call define( lastdate , style , style={background=yellow} );\n    endcomp;\nrun;", 
            "title": "Specify the STYLE of a Cell Based on Other Cell's Value"
        }, 
        {
            "location": "/sas-outputs/report/#control-output-table-width", 
            "text": "If the margins of the page are defined  1 options topmargin=.98in bottommargin=.98in leftmargin=1.18in rightmargin=.98in;   and you   1\n2\n3\n4\n5\n6\n7\n8 proc   report   data = LEDDall2report_ago   split = *   nowd   colwidth = 10   headline   headskip   missing   spanrows   style ( report )= { width=100% } ; \n             column   ( Dosis equivalente de levodopa diaria y n\u00famero de f\u00e1rmacos de agonistas dopamin\u00e9rgicos: descripci\u00f3n del cambio respecto a basal   ind   texto   param   grupo1   grupo2   grupo3   grupo4   pvalor   flag   flag2 ); \n             define   ind   /   order   noprint ; \n             define   texto   /     style ( column )= [ width = 7 % ] ; \n             define   param   /     style ( column )= [ width = 5 % ] ; \n             define   grupo1   /   Visita 1 * (N=59)   style ( column )= [ width = 5 % ] ; \n             (...)  run ;", 
            "title": "Control Output Table Width"
        }, 
        {
            "location": "/sas-outputs/report/#introducing-line-breaks", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 PROC REPORT DATA=SAS-data-set NOWINDOWS HEADLINE STYLE(HEADER)={BACKGROUND=VERY LIGHT GREY} MISSING SPLIT= * ;\n    COLUMN ( Sample report  var1 var2 var3);\n    DEFINE var1 /  Label 1  GROUP ORDER=INTERNAL;\n    DEFINE var2 /  Label 2  DISPLAY;\n    DEFINE var3 /  Label 3  DISPLAY;\n\n    * Introduce some line separations between var1 values;\n    BREAK BEFORE var1 / SUMMARIZE STYLE=[BACKGROUND=VERY LIGHT GREY];\n    * Avoid repeated labels;\n    COMPUTE var2;\n         IF MISSING(_BREAK_) THEN var1=   ;\n    ENDCOMP;\nRUN;    Tip  If the variable is numeric a  .  will apear in the break row. You need to create a format to assign  '            '  to  . .\nIf you describe the new value of  .  as  ' '  your numbers will be truncated.", 
            "title": "Introducing Line Breaks"
        }, 
        {
            "location": "/sas-outputs/report/#working-with-across", 
            "text": "Simple example:   1\n2\n3\n4\n5\n6\n7\n8\n9 PROC REPORT DATA=_AUX3 NOWINDOWS  \n    HEADLINE STYLE(HEADER)={BACKGROUND=VERY LIGHT GREY} MISSING SPLIT= * ;\n    COLUMN( VAR I ( UNIT I)   TIMEVAR. ( STATISTICS  _LABEL_)  STRATAVAR., COL1);\n    DEFINE _LABEL_ /   GROUP ORDER=DATA;\n    DEFINE  STRATAVAR./   ACROSS NOZERO ORDER=INTERNAL;\n    /* NOZERO = SINCE ALL PRODUCT CATEGORIES WILL NOT BE REPRESENTED FOR EACH PRODUCT LINE IN THE TABLE */\n    DEFINE  TIMEVAR./   F= TIMEFMT. GROUP ORDER=INTERNAL; \n    DEFINE COL1/   GROUP;\nRUN;    Complex example:    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26 proc report data=_data2report nowindows headline style(header)={background=very light grey} missing split= * ;\n    column( Lab tests (Hematology): normal, high abnormal and low abnormal results n(%)  lbtest trtgrpnum ( Value at * screening  clinsigSCR) ( Day 6  clinsig60, npctn60 _dummy) ( Early Termination Day  clinsigET, npctnET _dummy));\n    define lbtest /   group order=internal;\n    define trtgrpnum/   group order=internal;\n    define clinsigSCR /   group order=internal;\n    define clinsig60 /   across nozero order=internal;\n    define clinsigET /   across nozero order=internal;\n    * nozdero = since all product categories will not be represented for each product line in the table;\n    define npctn60/ group  ;\n    define npctnET/ group  ;\n    define _dummy / computed noprint; /* This variable is created to avoid an error message */\n\n    compute after/style=[just=L foreground=black FONT_SIZE=9pt];\n        line  Table footer line 1 ;\n        line  Table footer line 2 ;\n    endcomp;\n\n    * Introduce some line separations between arms of treatment;\n    break after trtgrpnum / skip;\n    * Introduce some line separations between tests;\n    break before lbtest / summarize style=[background=very light grey FONT_WEIGHT=BOLD];\n    * Avoid repeated labels;\n    compute npctn60;\n        if missing(_break_) then lbtest=   ;\n    endcomp;\nrun;    Check these websites   Sailing Over the  ACROSS  Hurdle in  PROC REPORT", 
            "title": "Working with ACROSS"
        }, 
        {
            "location": "/sas-outputs/report/#defining-your-own-variables", 
            "text": "1\n2\n3\n4\n5\n6 DEFINE obs / COMPUTED; \n\nCOMPUTE obs;\n    dsobs + 1;\n    obs = dsobs;\nENDCOMPUTE;", 
            "title": "Defining your own variables"
        }, 
        {
            "location": "/sas-outputs/tabulate/", 
            "text": "PROC TABULATE\n is a procedure that displays descriptive statistics in tabular format. It computes many statistics that other procedures compute, such as \nMEANS\n, \nFREQ\n, and \nREPORT\n and displays these statistics in a table format. \nTABULATE\n will produce tables in up to three dimensions and allows, within each dimension, multiple variables to be reported one after another hierarchically. There are also some very nice mechanisms that can be used to label and format the results. \n\n\n1\n2\n3\n4\n5\n6\nPROC TABULATE \noptions\n;\n    CLASS variables \n/ options\n;\n    VAR variables \n/ options\n;\n    TABLE \npage\n, \nrow\n, column \n/ options\n;\n    ... other statements...;\nRUN;\n\n\n\n\n\n\n\n\nVAR\n is used to list the variables you intend to use to create summary statistics on. They \nmust be numeric\n.\n\n\nCLASS\n variables allow you to get statistics by category. You will get one column/row for each value of the classification variable. You can also specify the universal CLASS variable \nALL\n which allows you to \nget totals\n. They can be \neither numeric or character\n and you can only request counts and percents as statistics. This is almost like using a \nBY\nstatement within the \nTABLE\n.\n\n\nTABLE\n consists of up to three dimension expressions and the table options. You can have multiple table statements in one \nPROC TABULATE\n. This will generate one table for each statement.\n\n\nA \ncomma\n specifies to add a new \ndimension\n. The order of the dimensions is page, row and column. If you only specify one dimension, then it is assumed to be column. If two are specified, row, then column.\n\n\nThe \nasterisk\n is used to produce a \ncross tabulation\n of one variable with another (within the same dimension however, different from \nPROC FREQ\n).\n\n\nA \nblank\n is used to represent \nconcatenation\n (i.e. place this output element after the preceding variable listed).\n\n\nParenthesis\n will \ngroup elements\n and associate an operator with each element in the group.\n\n\nAngle brackets\n specify a \ndenominator definition\n for use in percentage calculations (e.g. \npctn\nvariable\n).\n\n\n\n\n\n\n\n\n\n\nCheck these websites\n\n\n\n\nPROC TABULATE\n and the Neat Things You Can Do With It\n\n\n\n\n\n\n\n\nVARDEF=divisor\n specifies the divisor to be used in the calculation of the variances. If divisor is \nDF\n (default), the degrees of freedom (N-1) are used as the divisor.\n\n\nORDER=\n specifies the order of appearance in the table of the \nCLASS\n variable levels\n\n\nFORMATTED\n: ordered by the formatted values\n\n\nDATA\n: the order that the observations are read from the data set\n\n\nFREQ\n: order the values so the one that occurs most frequently in the data set appears first\n\n\nINTERNAL\n: ordered by the \nSORT\n procedure (defaults)\n\n\n\n\n\n\n/CONDENSE\n prints multiple logical pages on a single physical page\n\n\n/PRINTMISS\n species that row and column headings are the same for all logical pages of the table\n\n\n/ROW = spacing\n specifies whether all title elements in a row crossing are allotted space even when they are blank. When \nROW=CONSTANT\n (or \nCONST\n), the default, all row title elements have space allotted to them; when \nROW=FLOAT\n, the row title space is divided equally among the nonblank title elements in the crossing\n\n\n\n\nStatistics that Are Available in \nPROC TABULATE\n\n\nIf you do not provide a statistic name, the default statistic produced will be \nN\n for the \nCLASS\n variables and \nSUM\n for the \nVAR\n variables. Use the following keywords to request statistics in the \nTABLE\n statement or to specify statistic keywords in the \nKEYWORD\n or \nKEYLABEL\n statement.\n\n\n\n\nTip\n\n\nIf a variable name (class or analysis) and a statistic name are the same, then enclose the statistic name in single quotation marks (for example, \n'MAX'\n ).\n\n\n\n\nDescriptive statistic keywords\n\n Percentages: \nPCTN\n, \nCOLPCTN\n, \nROWPCTN\n, \nREPPCTN\n, \nPAGEPCTN\n\n\n Additions: \nSUM\n, \nSUMWGT\n, \nPCTSUM\n, \nCOLPCTSUM\n, \nROWPCTSUM\n, \nREPPCTSUM\n, \nPAGEPCTSUM\n\n\n Elements: \nN\n, \nNMISS\n\n\n Basic statistics: \nMEAN\n, \nSTDDEV\n | \nSTD\n, \nSTDERR\n, \nMIN\n, \nMAX\n, \nRANGE\n, \nMODE\n, \nLCLM\n, \nUCLM\n, \nKURTOSIS\n | \nKURT\n, \nSKEWNESS\n | \nSKEW\n\n\n Quantile statistics: \nP1\n, \nP5\n, \nP10\n, \nQ1\n | \nP25\n, \nMEDIAN\n | \nP50\n, \nQ3\n | \nP75\n, \nP90\n, \nP95\n, \nP99\n, \nQRANGE\n\n\n Hypothesis testing: \nPROBT\n | \nPRT\n, \nT\n\n* Others: \nCSS\n, \nCV\n, \nUSS\n, \nVAR\n\n\nTo compute standard error of the mean (\nSTDERR\n) or Student's t-test, you must use the default value of the \nVARDEF=\n option, which is \nDF\n. The \nVARDEF=\n option is specified in the \nPROC TABULATE\n statement.\n\n\nTo compute weighted quantiles, you must use \nQMETHOD=OS\n in the \nPROC TABULATE\n statement.\n\n\nUse both \nLCLM\n and \nUCLM\n to compute a two-sided confidence limit for the mean. Use only \nLCLM\n or \nUCLM\n to compute a one-sided confidence limit. Use the \nALPHA=\n option in the \nPROC TABULATE\n statement to specify a confidence level.\n\n\nSingle Dimensional Table\n\n\n1\n2\n3\n4\n5\nPROC TABULATE DATA=sashelp.class;\n    CLASS Sex;\n    VAR Height Weight;\n    TABLE Height * (N MEAN) Height * MEAN * Sex Weight * MEAN * Sex;\nRUN; \n\n\n\n\n\n\n\n\nTwo Dimensional Table\n\n\nYou can get very different table structures by changing where the statistic definitions are placed. They can be attached to either the \nVAR\n or the \nCLASS\n variable, but the numbers in the cells will \nalways\n be calculated using the \nVAR\n variable(s). \n\n\nThe statistic specification can be \nattached to the columns\n,\n\n\n1\n2\n3\n4\n5\nPROC TABULATE data=sashelp.class;\n    CLASS Sex;\n    VAR Height Weight;\n    TABLE Sex, Height * (N MEAN MAX) Weight * (N MEAN MAX) ;\nRUN; \n\n\n\n\n\n\n\n\nor they can be \nattached to the rows\n.\n\n\n1\n2\n3\n4\n5\nPROC TABULATE data=sashelp.class;\n    CLASS Sex;\n    VAR Height Weight;\n    TABLE Sex * (N MEAN MAX), Height  Weight;\nRUN; \n\n\n\n\n\n\n\n\nYou can specify \nmultiple classification variables\n. They can be used in any of the dimensions and can be nested. When you have multiple \nCLASS\n variables, it is recommended to use the option \nMISSING\n to keep the observations that have any missing values and consider them as valid levels for the \nCLASS\n variable(s) instead of dropping those observations from all the tables.\n\n\n1\n2\n3\n4\n5\nPROC TABULATE DATA=sashelp.cars;\n    CLASS DriveTrain Origin Type / MISSING;\n    VAR Weight Length;\n    TABLE Origin * Type, DriveTrain * Weight * MEAN DriveTrain * Length * MEAN;\nRUN; \n\n\n\n\n\n\n\n\nIn order to get \nmarginal statistics\n in your table, you use the \nALL\n keyword. You can use the keyword in multiple places and, depending on where you put the keyword, there will be different subtotals produced. \n\n\n1\n2\n3\n4\nPROC TABULATE DATA=sashelp.cars;\n    CLASS DriveTrain Origin Type;\n    TABLE (Origin All=\nTotal\n) * (DriveTrain ALL=\nSubtotal DriveTrain\n), (Type ALL=\nSubtotal Type\n* N);\nRUN; \n\n\n\n\n\n\n\n\nThree Dimensional Table\n\n\nThree dimensional tables have a nice way to fill in the upper left area. Instead of the label of the page dimension appearing above the table, you can use the \nBOX=_page_\n option to place that label inside the big white box.\n\n\n1\n2\n3\n4\n5\nPROC TABULATE data=sashelp.cars;\n    CLASS DriveTrain Origin Type;\n    VAR Weight Length;\n    TABLE Origin = \nMade in\n, Type = \nCategory\n, DriveTrain * Weight * MEAN / BOX=_PAGE_;\nRUN; \n\n\n\n\n\n\n\n\nFormatting tables\n\n\nThere are two ways to \nadd labels for your variables\n:\n\n\n\n\nAdd the text after the variable name: \nvariable =\u2018label\u2019\n. This will work for both variables and\nstatistics.\n\n\nAdd a \nLABEL\n statement for variables and/or a \nKEYLABEL\n statement for statistics to your code:\n\n\n\n\n1\n2\nLABEL var=\u2018label\u2019;\nKEYLABEL stat=\u2018label\u2019;\n\n\n\n\n\n\nIn order to hide variable or statistic labels, you leave the label specification blank (\nvariable =\u2018 \u2019\n). \n\n\nTo \nchange the font color\n of each variable you can define their style as in the following example:\n\n\n1\n2\n3\n4\n5\n6\n7\n8\nPROC TABULATE DATA=sashelp.class;\n   CLASS Age Sex;\n   TABLES Age,\n          Sex=\nGender\n *(N*{STYLE=[COLOR=Black]}\n                  PCTN=\nPercent\n   *{STYLE=[COLOR=Green]}\n                  ROWPCTN=\nRow Percent\n*{STYLE=[COLOR=Purple]}\n                  COLPCTN=\nColumn Percent\n *{STYLE=[COLOR=Red]});\nRUN;\n\n\n\n\n\n\n\n\n\n\nYou can also \nspecify formats\n for numbers in the cells of the table using the \nvariable-or-statistic*F=fmt.\n expression.\n\n\nThe \nCLASSLEV\n statement is used to assign some style attributes to the variable values only (not to the column header)\n\n\nThe \nNOSEPS\n option \nremoves the horizontal dividers\n between the row values from your table\n\n\n\n\n1\nPROC TABULATE DATA=SAS-data-set NOSEPS;\n\n\n\n\n\n\n\n\nINDENT=\n is used for subsetting row subheaders\n\n\nRTS=\n specifies how wide you want the row header field to be\n\n\nUse the \nBOX=\n option to \nfill in the big white box in the upper left\n\n\n\n\n1\nTABLE (...) / BOX={LABEL=\nCustom label for upper left box\n} INDENT=3 RTS=12;\n\n\n\n\n\n\nDepending on where you place the style options, many different results can be achieved. If you place the style options on the \nPROC TABULATE\n statement, for example, you will affect all the table cells. Note that for the \nCLASS\n, \nCLASSLEV\n, \nVAR\n, and \nKEYWORD\n statements, the style options can also be specified in the dimension expression in the Table statement. See below for a list of some of the different places where you can put the style options and what portion of the table they will affect.\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\nPROC TABULATE DATA=SAS-data-set F=10.2 S=[custom style attributes]; \n    CLASS variable1 / S=[custom style attributes];\n    CLASSLEV variable1 / S=[custom style attributes];\n    VAR variable2;\n    TABLE variable1=\n all={label=\nTotal\n S=[custom style attributes], \n          MEAN={S=[custom style attributes]} * variable2 \n          / BOX={LABEL=\ncustom label\n S=[custom style attributes]};\nRUN;\n\n\n\n\n\n\n\n\nPossible style attributes\n\n\n\n\nBackground color: \nBACKGROUND=yellow\n \n\n\nForeground color: \nFOREGROUND=black\n\n\nFont color: \nCOLOR=red\n\n\nChange font characteristics: \nFONTt_WEIGHT\n|\nFONT_FACE\n|\nFONT_SIZE\n\n\nVertical justification: \nVJUST=B|C|T\n\n\nHorizontal justification: \nJUST=R|C|L\n \n\n\nSpecify thickness of borders: \nBORDERWIDTH=\n\n\nChange size of table cells: \nCELLWIDTH=200\n|\nCELLHEIGHT=50\n\n\nSpecify vertical and horizontal rule dividers: \nRULES=none\n (removes all ines from the table)\n\n\nSpecify white space around cell: \nCELLSPACING=0\n \n\n\nSpecify thickness of spacing around cell: \nCELLPADDING=10\n\n\nSpecify width of table: \nOUTPUTWIDTH=\n \n\n\n\n\n\n\nExamples\n\n\n1\n2\n3\n4\n5\n6\n7\nPROC TABULATE DATA=SAS-data-set ORDER=FREQ;\n    VAR var1 var2;\n    CLASS AEencoding;\n    CLASS grade / ORDER=FORMATTED;\n    CLASS treatment / ORDER=FORMATTED;\n    TABLE AEencoding=\n, treatment=\nTreatment/Grade\n*grade=\n*(N=\nN\n var1=\n%\n*SUM=\n) ALL=\nTotal (N=# cases)\n*(N=\nN\n var2=\n%\n*SUM=\n) / BOX=\nPreferred MeDDRA Term\n;\nRUN;\n\n\n\n\n\n\nHow to Force Missing Values to Appear\n\n\nProvided that the specific category is present (non-missing) at least in one of the variables in the \nTABLE\n statement, you can force the missing values to appear.\n\n\n1\n2\n3\n4\nPROC TABULATE DATA=SAS-data-set ORDER=FREQ OUT=Output-SAS-data-set MISSING;\n    CLASS var1 var2 crossvar;\n    TABLE var1 var2, crossvar*(N COLPCTN) / PRINTMISS MISSTEXT=\n0\n;\nRUN;", 
            "title": "PROC TABULATE"
        }, 
        {
            "location": "/sas-outputs/tabulate/#statistics-that-are-available-in-proc-tabulate", 
            "text": "If you do not provide a statistic name, the default statistic produced will be  N  for the  CLASS  variables and  SUM  for the  VAR  variables. Use the following keywords to request statistics in the  TABLE  statement or to specify statistic keywords in the  KEYWORD  or  KEYLABEL  statement.   Tip  If a variable name (class or analysis) and a statistic name are the same, then enclose the statistic name in single quotation marks (for example,  'MAX'  ).   Descriptive statistic keywords  Percentages:  PCTN ,  COLPCTN ,  ROWPCTN ,  REPPCTN ,  PAGEPCTN   Additions:  SUM ,  SUMWGT ,  PCTSUM ,  COLPCTSUM ,  ROWPCTSUM ,  REPPCTSUM ,  PAGEPCTSUM   Elements:  N ,  NMISS   Basic statistics:  MEAN ,  STDDEV  |  STD ,  STDERR ,  MIN ,  MAX ,  RANGE ,  MODE ,  LCLM ,  UCLM ,  KURTOSIS  |  KURT ,  SKEWNESS  |  SKEW   Quantile statistics:  P1 ,  P5 ,  P10 ,  Q1  |  P25 ,  MEDIAN  |  P50 ,  Q3  |  P75 ,  P90 ,  P95 ,  P99 ,  QRANGE   Hypothesis testing:  PROBT  |  PRT ,  T \n* Others:  CSS ,  CV ,  USS ,  VAR  To compute standard error of the mean ( STDERR ) or Student's t-test, you must use the default value of the  VARDEF=  option, which is  DF . The  VARDEF=  option is specified in the  PROC TABULATE  statement.  To compute weighted quantiles, you must use  QMETHOD=OS  in the  PROC TABULATE  statement.  Use both  LCLM  and  UCLM  to compute a two-sided confidence limit for the mean. Use only  LCLM  or  UCLM  to compute a one-sided confidence limit. Use the  ALPHA=  option in the  PROC TABULATE  statement to specify a confidence level.", 
            "title": "Statistics that Are Available in PROC TABULATE"
        }, 
        {
            "location": "/sas-outputs/tabulate/#single-dimensional-table", 
            "text": "1\n2\n3\n4\n5 PROC TABULATE DATA=sashelp.class;\n    CLASS Sex;\n    VAR Height Weight;\n    TABLE Height * (N MEAN) Height * MEAN * Sex Weight * MEAN * Sex;\nRUN;", 
            "title": "Single Dimensional Table"
        }, 
        {
            "location": "/sas-outputs/tabulate/#two-dimensional-table", 
            "text": "You can get very different table structures by changing where the statistic definitions are placed. They can be attached to either the  VAR  or the  CLASS  variable, but the numbers in the cells will  always  be calculated using the  VAR  variable(s).   The statistic specification can be  attached to the columns ,  1\n2\n3\n4\n5 PROC TABULATE data=sashelp.class;\n    CLASS Sex;\n    VAR Height Weight;\n    TABLE Sex, Height * (N MEAN MAX) Weight * (N MEAN MAX) ;\nRUN;     or they can be  attached to the rows .  1\n2\n3\n4\n5 PROC TABULATE data=sashelp.class;\n    CLASS Sex;\n    VAR Height Weight;\n    TABLE Sex * (N MEAN MAX), Height  Weight;\nRUN;     You can specify  multiple classification variables . They can be used in any of the dimensions and can be nested. When you have multiple  CLASS  variables, it is recommended to use the option  MISSING  to keep the observations that have any missing values and consider them as valid levels for the  CLASS  variable(s) instead of dropping those observations from all the tables.  1\n2\n3\n4\n5 PROC TABULATE DATA=sashelp.cars;\n    CLASS DriveTrain Origin Type / MISSING;\n    VAR Weight Length;\n    TABLE Origin * Type, DriveTrain * Weight * MEAN DriveTrain * Length * MEAN;\nRUN;     In order to get  marginal statistics  in your table, you use the  ALL  keyword. You can use the keyword in multiple places and, depending on where you put the keyword, there will be different subtotals produced.   1\n2\n3\n4 PROC TABULATE DATA=sashelp.cars;\n    CLASS DriveTrain Origin Type;\n    TABLE (Origin All= Total ) * (DriveTrain ALL= Subtotal DriveTrain ), (Type ALL= Subtotal Type * N);\nRUN;", 
            "title": "Two Dimensional Table"
        }, 
        {
            "location": "/sas-outputs/tabulate/#three-dimensional-table", 
            "text": "Three dimensional tables have a nice way to fill in the upper left area. Instead of the label of the page dimension appearing above the table, you can use the  BOX=_page_  option to place that label inside the big white box.  1\n2\n3\n4\n5 PROC TABULATE data=sashelp.cars;\n    CLASS DriveTrain Origin Type;\n    VAR Weight Length;\n    TABLE Origin =  Made in , Type =  Category , DriveTrain * Weight * MEAN / BOX=_PAGE_;\nRUN;", 
            "title": "Three Dimensional Table"
        }, 
        {
            "location": "/sas-outputs/tabulate/#formatting-tables", 
            "text": "There are two ways to  add labels for your variables :   Add the text after the variable name:  variable =\u2018label\u2019 . This will work for both variables and\nstatistics.  Add a  LABEL  statement for variables and/or a  KEYLABEL  statement for statistics to your code:   1\n2 LABEL var=\u2018label\u2019;\nKEYLABEL stat=\u2018label\u2019;   In order to hide variable or statistic labels, you leave the label specification blank ( variable =\u2018 \u2019 ).   To  change the font color  of each variable you can define their style as in the following example:  1\n2\n3\n4\n5\n6\n7\n8 PROC TABULATE DATA=sashelp.class;\n   CLASS Age Sex;\n   TABLES Age,\n          Sex= Gender  *(N*{STYLE=[COLOR=Black]}\n                  PCTN= Percent    *{STYLE=[COLOR=Green]}\n                  ROWPCTN= Row Percent *{STYLE=[COLOR=Purple]}\n                  COLPCTN= Column Percent  *{STYLE=[COLOR=Red]});\nRUN;     You can also  specify formats  for numbers in the cells of the table using the  variable-or-statistic*F=fmt.  expression.  The  CLASSLEV  statement is used to assign some style attributes to the variable values only (not to the column header)  The  NOSEPS  option  removes the horizontal dividers  between the row values from your table   1 PROC TABULATE DATA=SAS-data-set NOSEPS;    INDENT=  is used for subsetting row subheaders  RTS=  specifies how wide you want the row header field to be  Use the  BOX=  option to  fill in the big white box in the upper left   1 TABLE (...) / BOX={LABEL= Custom label for upper left box } INDENT=3 RTS=12;   Depending on where you place the style options, many different results can be achieved. If you place the style options on the  PROC TABULATE  statement, for example, you will affect all the table cells. Note that for the  CLASS ,  CLASSLEV ,  VAR , and  KEYWORD  statements, the style options can also be specified in the dimension expression in the Table statement. See below for a list of some of the different places where you can put the style options and what portion of the table they will affect.   1\n2\n3\n4\n5\n6\n7\n8 PROC TABULATE DATA=SAS-data-set F=10.2 S=[custom style attributes]; \n    CLASS variable1 / S=[custom style attributes];\n    CLASSLEV variable1 / S=[custom style attributes];\n    VAR variable2;\n    TABLE variable1=  all={label= Total  S=[custom style attributes], \n          MEAN={S=[custom style attributes]} * variable2 \n          / BOX={LABEL= custom label  S=[custom style attributes]};\nRUN;    Possible style attributes   Background color:  BACKGROUND=yellow    Foreground color:  FOREGROUND=black  Font color:  COLOR=red  Change font characteristics:  FONTt_WEIGHT | FONT_FACE | FONT_SIZE  Vertical justification:  VJUST=B|C|T  Horizontal justification:  JUST=R|C|L    Specify thickness of borders:  BORDERWIDTH=  Change size of table cells:  CELLWIDTH=200 | CELLHEIGHT=50  Specify vertical and horizontal rule dividers:  RULES=none  (removes all ines from the table)  Specify white space around cell:  CELLSPACING=0    Specify thickness of spacing around cell:  CELLPADDING=10  Specify width of table:  OUTPUTWIDTH=", 
            "title": "Formatting tables"
        }, 
        {
            "location": "/sas-outputs/tabulate/#examples", 
            "text": "1\n2\n3\n4\n5\n6\n7 PROC TABULATE DATA=SAS-data-set ORDER=FREQ;\n    VAR var1 var2;\n    CLASS AEencoding;\n    CLASS grade / ORDER=FORMATTED;\n    CLASS treatment / ORDER=FORMATTED;\n    TABLE AEencoding= , treatment= Treatment/Grade *grade= *(N= N  var1= % *SUM= ) ALL= Total (N=# cases) *(N= N  var2= % *SUM= ) / BOX= Preferred MeDDRA Term ;\nRUN;", 
            "title": "Examples"
        }, 
        {
            "location": "/sas-outputs/tabulate/#how-to-force-missing-values-to-appear", 
            "text": "Provided that the specific category is present (non-missing) at least in one of the variables in the  TABLE  statement, you can force the missing values to appear.  1\n2\n3\n4 PROC TABULATE DATA=SAS-data-set ORDER=FREQ OUT=Output-SAS-data-set MISSING;\n    CLASS var1 var2 crossvar;\n    TABLE var1 var2, crossvar*(N COLPCTN) / PRINTMISS MISSTEXT= 0 ;\nRUN;", 
            "title": "How to Force Missing Values to Appear"
        }, 
        {
            "location": "/sas-outputs/template/", 
            "text": "Check these websites\n\n\n\n\nHere\n you can find some notes on \nGraph Template Language\n (categories of statements)\n\n\nBob Rodr\u00edguez\n is has written a lot about templates, check his papers for more information\n\n\nHere\n you can find the official documentation on ODS Graphics Template Modification\n\n\nPROC TEMPLATE style tips\n\n\n\n\n\n\nStyle Templates vs Graph Templates\n\n\nModifying Style Templates\n\n\n\n\nObtain the source code\n\n\n\n\n1\n2\n3\nPROC TEMPLATE;\nSOURCE styles.default;\nRUN;\n\n\n\n\n\n\n\n\nModify the code\n\n\n\n\n1\n2\n3\n4\n5\n6\nPROC TEMPLATE;\n    DEFINE STYLE MyListingStyle;\n    PARENT=styles.listing;\n        [make desired changes in code]\n    END;\nRUN;\n\n\n\n\n\n\n\n\nGenerate the plot\n\n\n\n\n1\n2\nODS\n \nLISTING\n \nSTYLE\n=\nmylistingstyle\n;\n\n\n[\nSGPLOT\n \nStatements\n]\n\n\n\n\n\n\n\nModifying Graph Templates\n\n\n\n\n\n\nObtain the source code\n    \nPROC TEMPLATE;\n            SOURCE Stat.Lifetest.Graphics.ProductLimitSurvival;\n    RUN;\n\n\n\n\n\n\nModify the code\n    \nPROC TEMPLATE;\n            DEFINE Stat.Lifetest.Graphics.ProductLimitSurvival;\n            SOURCE Stat.Lifetest.Graphics.ProductLimitSurvival;\n            [make desired changes in code]\n        END;\n    RUN;\n\n\n\n\n\n\nGenerate the plot\n    \nPROC LIFETEST DATA=db PLOTS=S;\n            [statements]\n    RUN;\n\n\n\n\n\n\nRevert to default template\n    \nPROC TEMPLATE;\n        DELETE Stat.Lifetest.Graphics.ProductLimitSurvival;\n    RUN;\n\n\n\n\n\n\nBasic Graph Template Functionalities\n\n\nObtaining the Default Templates\n\n\nFirst you need to know the name of the template. For this you can either look for its name listing all the available default templates that are kept in \nsashelp.tmplmst\n...\n\n\n1\n2\n3\n4\nPROC TEMPLATE;\n  PATH sashelp.tmplmst;\n  LIST Base.Freq / SORT=path DESCENDING;\nRUN; \n\n\n\n\n\n\n... or use the \nODS TRACE ON\n to obtain the name of an specific template.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\nODS TRACE ON;\nODS GRAPHICS ON;\nPROC FREQ DATA=sashelp.baseball;\n    TABLE League*Division / AGREE NOCOL NOROW; \n    TEST KAPPA;\nRUN;\nODS GRAPHICS OFF;\nODS TRACE OFF;\n\n\n\n\n\n\nYou will obtain the following log output for the \nAgreement Plot\n where you can obtain the name of the template you are interested in:\n\n\n1\n2\n3\n4\n5\n6\n7\nOutput Added:\n\n\n-------------\n\nName:       AgreePlot\nLabel:      Agreement Plot\nTemplate:   Base.Freq.Graphics.AgreePlot\n\nPath:       Freq.Table1.AgreePlot\n\n\n-------------\n\n\n\n\n\n\n\nThen you use the \nSOURCE\n option from the \nTEMPLATE\n procedure to show in the log the full object template.\n\n\n1\n2\n3\n4\n%let\n \npath\n=\nC\n:\n\\\nyour\n-\npath\n-\nhere\n;\n\n\nPROC\n \nTEMPLATE\n;\n\n    \nSOURCE\n \nBase\n.\nFreq\n.\nGraphics\n.\nAgreePlot\n \n/\n \nfile\n=\npath.\n\\a\ngreeplot.sas\n;\n\n\nRUN\n;\n\n\n\n\n\n\n\n\n\nNote\n\n\nRemember that you must add a \nPROC TEMPLATE;\n statement before the generated source statements and optionally a \nRUN;\n statement after the \nEND;\n statement before you submit your modified definition.\n\n\n\n\nEditing Templates\n\n\nGraph definitions are self-contained and do not support inheritance (via the \nPARENT=\n option) as do table definitions. Consequently, the \nEDIT\n statement in \nPROC TEMPLATE\n is not supported for graph definitions.\n\n\nHere are some important points about what you can and cannot change in a template:\n\n\n\n\nDo not change the template name\n. A statistical procedure can access only a predefined list of templates. If you change the name, the procedure cannot find your template. You must make sure that it is in a template store that is read before \nSashelp.Tmplmst\n through the \nODS PATH\n statement.\n\n\nDo not change the names of columns\n. The underlying data object contains predefined column names that you must use. Be very careful if you change how a column is used in a template. Usually, columns are not interchangeable.\n\n\nDo not change the names of \nDYNAMIC\n variables\n. Changing dynamic variable names can lead to runtime errors. Do not add dynamic variables, because the procedure cannot set their values.\n\n\nDo not change the names of statements\n (for example, from a \nSCATTERPLOT\n to a \nNEEDLEPLOT\n or other type of plot).\n\n\n\n\nYou can change any of the following:\n\n\n\n\nYou can add macro variables that behave like dynamic variables\n. They are resolved at the time that the statistical procedure is run, and not at the time that the template is compiled. They are defined with an \nMVAR\n or \nNMVAR\n statement at the beginning the template. You can also move a variable from a \nDYNAMIC\n statement to an \nMVAR\n or \nNMVAR\n statement if you want to set it yourself rather than letting the procedure set it.\n\n\nYou can change the graph size\n.\n\n\nYou can change graph titles, footnotes, axis labels, and any other text that appears in the graph\n.\n\n\nYou can change which plot features are displayed\n.\n\n\nYou can change axis features, such as grid lines, offsets, view ports, tick value formatting, and so on\n.\n\n\nYou can change the content and arrangement of insets\n (small tables of statistics embedded in some graphs).\n\n\nYou can change the legend location, contents, border, background, title, and so on\n.\n\n\n\n\nUsing Customized Templates\n\n\nThe \nODS PATH\n statement specifies the template stores to search, as well as the order in which to search them. You can change the default template search path by using the \nODS PATH\n statement.\n\n\n1\nODS PATH work.mystore(update) sashelp.tmplmst(read);\n\n\n\n\n\n\nYou can display the current template search path with the following statement:\n\n\n1\nODS PATH SHOW;\n\n\n\n\n\n\nThe log messages for the default template search path are as follows:\n\n\n1\n2\n3\n4\nCurrent ODS PATH list is:\n\n1. WORK.MYSTORE(UPDATE)\n2. SASHELP.TMPLMST(READ)\n\n\n\n\n\n\nWhen you are done, you can reset the default template search path as follows:\n\n\n1\nODS PATH RESET;\n\n\n\n\n\n\n1\n2\n3\n4\nCurrent ODS PATH list is:\n\n1. SASUSER.TEMPLAT(UPDATE)\n2. SASHELP.TMPLMST(READ)\n\n\n\n\n\n\nReverting to the Default Templates\n\n\nThe following statements delete the modified template from \nSASUSER.TEMPLAT\n and revert to the default template in\n\nSASHELP.TMPLMST\n, which is where the SAS templates are stored.\n\n\n1\n2\n3\nPROC TEMPLATE;\n    DELETE Base.Freq.Graphics.AgreePlot;\nRUN;\n\n\n\n\n\n\nThe following note is printed in the SAS log:\n\n\n1\nNOTE: \nBase.Freq.Graphics.AgreePlot\n has been deleted from: SASUSER.TEMPLAT\n\n\n\n\n\n\nYou can run the following step to delete the entire \nSASUSER.TEMPLAT\n store of customized templates:\n\n\n1\n2\n3\n4\n5\nODS PATH sashelp.tmplmst(read);\nPROC DATASETS LIBRARY=sasuser NOLIST;\n   DELETE TEMPLAT(MEMTYPE=ITEMSTOR);\nRUN;\nODS PATH sasuser.templat(update) sashelp.tmplmst(read);\n\n\n\n\n\n\nPROC TEMPLATE\n Features\n\n\nInclude an Image as the Header\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\nPROC TEMPLATE;\n    DEFINE STYLE template_header_image;\n        PARENT = styles.default;\n        (...)\n        STYLE SYSTEMTITLE /\n            TEXTALIGN=l \n            VERTICALALIGN=t\n            PREIMAGE=\nc:\\path-to-your-file\\header-image.png\n \n            FOREGROUND = #ffffff;\n    END;\nRUN;\n\nODS PDF DPI=700 STYLE=template_header_image FILE=\nyour-path\\your-file (\nsysdate).pdf\n;\n\nTITLE \n;\n(...)\n\nODS PDF CLOSE;\n\n\n\n\n\n\n\n\nDPI\n needs to be increased to show the \nPREIMAGE\n logo with good definition\n\n\nYou need to specify \nTITLE \"\";\n for the \nPREIMAGE\n to appear\n\n\n\n\nCell Values Alignment\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\nPROC TEMPLATE;\n    DEFINE STYLE template_header_image;\n        PARENT = styles.default;\n        (...)\n        CLASS CELL /\n                PADDINGRIGHT = 15   /* Controls the horizontal spacing between one cell and the next one */\n            TEXTALIGN = left;\n        CLASS HEADER /        /* Centers all headers (including rowheaders) */\n            TEXTALIGN=center;\n        CLASS ROWHEADER /     /* Takes rowheaders back to left alignment */\n                TEXTALIGN=left;\n        CONTEXT \n.header\n /   /* Activates the bottom border of the header fields */\n            BORDERBOTTOMSTYLE = solid\n            BORDERBOTTOMCOLOR = black\n            BORDERBOTTOMWIDTH = 1px;\n    END;\nRUN;\n\n\n\n\n\n\nOther Related Topics\n\n\n\n\nSolve the error \n\"\nunable to write to the template store\n\"\n:\n\n\n\n\n1\n2\n3\n4\nERROR\n:\n \nTemplate\n \nxxxxx\n \nwas\n \nunable\n \nto\n \nwrite\n \nto\n \nthe\n \ntemplate\n \nstore\n!\n\n\n\nODS\n \nPATH\n \nSHOW\n;\n\n\nODS\n \nPATH\n(\nPREPEND\n)\n \nwork\n.\ntemplat\n(\nUPDATE\n);", 
            "title": "PROC TEMPLATE"
        }, 
        {
            "location": "/sas-outputs/template/#style-templates-vs-graph-templates", 
            "text": "", 
            "title": "Style Templates vs Graph Templates"
        }, 
        {
            "location": "/sas-outputs/template/#modifying-style-templates", 
            "text": "Obtain the source code   1\n2\n3 PROC TEMPLATE;\nSOURCE styles.default;\nRUN;    Modify the code   1\n2\n3\n4\n5\n6 PROC TEMPLATE;\n    DEFINE STYLE MyListingStyle;\n    PARENT=styles.listing;\n        [make desired changes in code]\n    END;\nRUN;    Generate the plot   1\n2 ODS   LISTING   STYLE = mylistingstyle ;  [ SGPLOT   Statements ]", 
            "title": "Modifying Style Templates"
        }, 
        {
            "location": "/sas-outputs/template/#modifying-graph-templates", 
            "text": "Obtain the source code\n     PROC TEMPLATE;\n            SOURCE Stat.Lifetest.Graphics.ProductLimitSurvival;\n    RUN;    Modify the code\n     PROC TEMPLATE;\n            DEFINE Stat.Lifetest.Graphics.ProductLimitSurvival;\n            SOURCE Stat.Lifetest.Graphics.ProductLimitSurvival;\n            [make desired changes in code]\n        END;\n    RUN;    Generate the plot\n     PROC LIFETEST DATA=db PLOTS=S;\n            [statements]\n    RUN;    Revert to default template\n     PROC TEMPLATE;\n        DELETE Stat.Lifetest.Graphics.ProductLimitSurvival;\n    RUN;", 
            "title": "Modifying Graph Templates"
        }, 
        {
            "location": "/sas-outputs/template/#basic-graph-template-functionalities", 
            "text": "", 
            "title": "Basic Graph Template Functionalities"
        }, 
        {
            "location": "/sas-outputs/template/#obtaining-the-default-templates", 
            "text": "First you need to know the name of the template. For this you can either look for its name listing all the available default templates that are kept in  sashelp.tmplmst ...  1\n2\n3\n4 PROC TEMPLATE;\n  PATH sashelp.tmplmst;\n  LIST Base.Freq / SORT=path DESCENDING;\nRUN;    ... or use the  ODS TRACE ON  to obtain the name of an specific template.  1\n2\n3\n4\n5\n6\n7\n8 ODS TRACE ON;\nODS GRAPHICS ON;\nPROC FREQ DATA=sashelp.baseball;\n    TABLE League*Division / AGREE NOCOL NOROW; \n    TEST KAPPA;\nRUN;\nODS GRAPHICS OFF;\nODS TRACE OFF;   You will obtain the following log output for the  Agreement Plot  where you can obtain the name of the template you are interested in:  1\n2\n3\n4\n5\n6\n7 Output Added:  ------------- \nName:       AgreePlot\nLabel:      Agreement Plot\nTemplate:   Base.Freq.Graphics.AgreePlot Path:       Freq.Table1.AgreePlot  -------------    Then you use the  SOURCE  option from the  TEMPLATE  procedure to show in the log the full object template.  1\n2\n3\n4 %let   path = C : \\ your - path - here ;  PROC   TEMPLATE ; \n     SOURCE   Base . Freq . Graphics . AgreePlot   /   file = path. \\a greeplot.sas ;  RUN ;     Note  Remember that you must add a  PROC TEMPLATE;  statement before the generated source statements and optionally a  RUN;  statement after the  END;  statement before you submit your modified definition.", 
            "title": "Obtaining the Default Templates"
        }, 
        {
            "location": "/sas-outputs/template/#editing-templates", 
            "text": "Graph definitions are self-contained and do not support inheritance (via the  PARENT=  option) as do table definitions. Consequently, the  EDIT  statement in  PROC TEMPLATE  is not supported for graph definitions.  Here are some important points about what you can and cannot change in a template:   Do not change the template name . A statistical procedure can access only a predefined list of templates. If you change the name, the procedure cannot find your template. You must make sure that it is in a template store that is read before  Sashelp.Tmplmst  through the  ODS PATH  statement.  Do not change the names of columns . The underlying data object contains predefined column names that you must use. Be very careful if you change how a column is used in a template. Usually, columns are not interchangeable.  Do not change the names of  DYNAMIC  variables . Changing dynamic variable names can lead to runtime errors. Do not add dynamic variables, because the procedure cannot set their values.  Do not change the names of statements  (for example, from a  SCATTERPLOT  to a  NEEDLEPLOT  or other type of plot).   You can change any of the following:   You can add macro variables that behave like dynamic variables . They are resolved at the time that the statistical procedure is run, and not at the time that the template is compiled. They are defined with an  MVAR  or  NMVAR  statement at the beginning the template. You can also move a variable from a  DYNAMIC  statement to an  MVAR  or  NMVAR  statement if you want to set it yourself rather than letting the procedure set it.  You can change the graph size .  You can change graph titles, footnotes, axis labels, and any other text that appears in the graph .  You can change which plot features are displayed .  You can change axis features, such as grid lines, offsets, view ports, tick value formatting, and so on .  You can change the content and arrangement of insets  (small tables of statistics embedded in some graphs).  You can change the legend location, contents, border, background, title, and so on .", 
            "title": "Editing Templates"
        }, 
        {
            "location": "/sas-outputs/template/#using-customized-templates", 
            "text": "The  ODS PATH  statement specifies the template stores to search, as well as the order in which to search them. You can change the default template search path by using the  ODS PATH  statement.  1 ODS PATH work.mystore(update) sashelp.tmplmst(read);   You can display the current template search path with the following statement:  1 ODS PATH SHOW;   The log messages for the default template search path are as follows:  1\n2\n3\n4 Current ODS PATH list is:\n\n1. WORK.MYSTORE(UPDATE)\n2. SASHELP.TMPLMST(READ)   When you are done, you can reset the default template search path as follows:  1 ODS PATH RESET;   1\n2\n3\n4 Current ODS PATH list is:\n\n1. SASUSER.TEMPLAT(UPDATE)\n2. SASHELP.TMPLMST(READ)", 
            "title": "Using Customized Templates"
        }, 
        {
            "location": "/sas-outputs/template/#reverting-to-the-default-templates", 
            "text": "The following statements delete the modified template from  SASUSER.TEMPLAT  and revert to the default template in SASHELP.TMPLMST , which is where the SAS templates are stored.  1\n2\n3 PROC TEMPLATE;\n    DELETE Base.Freq.Graphics.AgreePlot;\nRUN;   The following note is printed in the SAS log:  1 NOTE:  Base.Freq.Graphics.AgreePlot  has been deleted from: SASUSER.TEMPLAT   You can run the following step to delete the entire  SASUSER.TEMPLAT  store of customized templates:  1\n2\n3\n4\n5 ODS PATH sashelp.tmplmst(read);\nPROC DATASETS LIBRARY=sasuser NOLIST;\n   DELETE TEMPLAT(MEMTYPE=ITEMSTOR);\nRUN;\nODS PATH sasuser.templat(update) sashelp.tmplmst(read);", 
            "title": "Reverting to the Default Templates"
        }, 
        {
            "location": "/sas-outputs/template/#proc-template-features", 
            "text": "", 
            "title": "PROC TEMPLATE Features"
        }, 
        {
            "location": "/sas-outputs/template/#include-an-image-as-the-header", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18 PROC TEMPLATE;\n    DEFINE STYLE template_header_image;\n        PARENT = styles.default;\n        (...)\n        STYLE SYSTEMTITLE /\n            TEXTALIGN=l \n            VERTICALALIGN=t\n            PREIMAGE= c:\\path-to-your-file\\header-image.png  \n            FOREGROUND = #ffffff;\n    END;\nRUN;\n\nODS PDF DPI=700 STYLE=template_header_image FILE= your-path\\your-file ( sysdate).pdf ;\n\nTITLE  ;\n(...)\n\nODS PDF CLOSE;    DPI  needs to be increased to show the  PREIMAGE  logo with good definition  You need to specify  TITLE \"\";  for the  PREIMAGE  to appear", 
            "title": "Include an Image as the Header"
        }, 
        {
            "location": "/sas-outputs/template/#cell-values-alignment", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17 PROC TEMPLATE;\n    DEFINE STYLE template_header_image;\n        PARENT = styles.default;\n        (...)\n        CLASS CELL /\n                PADDINGRIGHT = 15   /* Controls the horizontal spacing between one cell and the next one */\n            TEXTALIGN = left;\n        CLASS HEADER /        /* Centers all headers (including rowheaders) */\n            TEXTALIGN=center;\n        CLASS ROWHEADER /     /* Takes rowheaders back to left alignment */\n                TEXTALIGN=left;\n        CONTEXT  .header  /   /* Activates the bottom border of the header fields */\n            BORDERBOTTOMSTYLE = solid\n            BORDERBOTTOMCOLOR = black\n            BORDERBOTTOMWIDTH = 1px;\n    END;\nRUN;", 
            "title": "Cell Values Alignment"
        }, 
        {
            "location": "/sas-outputs/template/#other-related-topics", 
            "text": "Solve the error  \" unable to write to the template store \" :   1\n2\n3\n4 ERROR :   Template   xxxxx   was   unable   to   write   to   the   template   store !  ODS   PATH   SHOW ;  ODS   PATH ( PREPEND )   work . templat ( UPDATE );", 
            "title": "Other Related Topics"
        }, 
        {
            "location": "/sas-outputs/pdf/", 
            "text": "ODS\n definition for *.pdf format\n\n\n1\n2\n3\nODS PDF DPI=700 STYLE=customstyle FILE=\npath\\file (\nsysdate).pdf\n PDFTOC=1;\n(...)\nODS PDF CLOSE;\n\n\n\n\n\n\n\n\nDPI\n needs to be increased to show the preimage logo with good definition\n\n\nThe \nFILE\n definition must contain the output path too, \nODS PDF\n is not compatible with the \nPATH=\n option\n\n\nPDFTOC\n fixed the number of TOC levels displayed by default when openning the file (even if there are more defined the tree will appear contracted)\n\n\n\n\nTOC customization\n\n\n\n\nFirst level: \nods proclabel=\"Name\";\n\n\nSecond level (remove): \ncontents = \"\"\n inside \nPROC REPORT\n statement\n\n\nThird level (remove): define an auxiliary variable \ncount = 1\n in your data set and include it in the \nPROC REPORT\n\n\n\n\n1\n2\n3\ncolumn (\nTitle\n count ...);\ndefine count / group noprint;\nbreak before count / contents=\n page;", 
            "title": "Write to pdf file"
        }, 
        {
            "location": "/sas-outputs/pdf/#ods-definition-for-42pdf-format", 
            "text": "1\n2\n3 ODS PDF DPI=700 STYLE=customstyle FILE= path\\file ( sysdate).pdf  PDFTOC=1;\n(...)\nODS PDF CLOSE;    DPI  needs to be increased to show the preimage logo with good definition  The  FILE  definition must contain the output path too,  ODS PDF  is not compatible with the  PATH=  option  PDFTOC  fixed the number of TOC levels displayed by default when openning the file (even if there are more defined the tree will appear contracted)", 
            "title": "ODS definition for *.pdf format"
        }, 
        {
            "location": "/sas-outputs/pdf/#toc-customization", 
            "text": "First level:  ods proclabel=\"Name\";  Second level (remove):  contents = \"\"  inside  PROC REPORT  statement  Third level (remove): define an auxiliary variable  count = 1  in your data set and include it in the  PROC REPORT   1\n2\n3 column ( Title  count ...);\ndefine count / group noprint;\nbreak before count / contents=  page;", 
            "title": "TOC customization"
        }, 
        {
            "location": "/sas-outputs/excel/", 
            "text": "Export Data to Excel With Format\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n%macro\n \nExportExcelWithFormat\n(\nlibname\n=\n,\ndataname\n=\n,\nselect\n=\n,\n \noutputname\n=\n,\nsheetname\n=\n \n);\n\n\n    \nproc\n \nsql\n \nnoprint\n;\n\n        \ncreate\n \ntable\n \ntmp_vars\n \nas\n\n            \nselect\n \nname\n,\nformat\n,\n \nlabel\n \nfrom\n \ndictionary\n.\ncolumns\n\n                \nwhere\n \nlibname\n=\nupcase\n(\nlibname.\n)\n \nand\n \nmemname\n=\nupcase\n(\ndataname.\n);\n\n    \nquit\n;\n\n\n    \ndata\n \ntmp_vars\n;\n\n        \nset\n \ntmp_vars\n \nend\n=\nlast\n;\n\n        \nlength\n \nformatcode\n \n$400\n.;\n\n        \nif\n \nformat\n \n^=\n \nthen\n\n            \nformatcode\n=\ncatx\n(\n \n,\ncats\n(\nput\n,\n(\n,\nname\n,\n,\n,\nformat\n,\n)\n),\n \nas\n,\nname\n,\n label\n,\n \n,\nlabel\n,\n,\n,\n);\n\n        \nelse\n \nformatcode\n=\ncats\n(\nname\n,\n,\n);\n\n        \nif\n \nlast\n \nthen\n\n            \nformatcode\n=\nsubstr\n(\nformatcode\n,\n1\n,\nlength\n(\nformatcode\n)\n-\n1\n);\n\n    \nrun\n;\n\n\n    \n%let\n \nformatcodes\n=\n;\n\n\n    \ndata\n \n_null_\n;\n\n        \nset\n \ntmp_vars\n;\n\n        \ncall\n \nsymput\n(\nformatcodes\n,\n \ntrim\n(\nresolve\n(\nformatcodes\n.\n)\n||\n \n||\ntrim\n\n            \n(\nformatcode\n)));\n\n    \nrun\n;\n\n\n    \n%put\n \nformatcodes\n.;\n\n\n    \nproc\n \nsql\n;\n\n        \ncreate\n \nview\n \ntmp_view\n \nas\n\n            \nselect\n \nformatcodes\n.\n\n                \nfrom\n \nlibname\n..\ndataname\n.;\n\n    \nquit\n;\n\n\n    \ndata\n \n_datain\n;\n\n        \nretain\n \nselect\n;\n\n        \nset\n \ntmp_view\n \n(\nkeep\n=\nselect\n);\n\n    \nrun\n;\n\n\n    \n%let\n \nformatcodes\n=\n%str\n();\n\n\n    \nPROC\n \nEXPORT\n \nDATA\n=\n \n_datain\n \nDBMS\n=\nxlsx\n \nREPLACE\n \nlabel\n \n        \nOUTFILE\n=\n \noutputname.\n;\n\n        \nSHEET\n=\nsheetname.\n;\n\n    \nRUN\n;\n\n\n    \nproc\n \nsql\n;\n\n        \ndrop\n \ntable\n \ntmp_vars\n,\n \n_datain\n;\n\n        \ndrop\n \nview\n \ntmp_view\n;\n\n    \nquit\n;\n\n\n\n%mend\n;\n\n\n\n%export\nExcelWithFormat\n(\nlibname\n=\nSAS\n-\nlibname\n,\n\n                      \ndataname\n=\nSAS\n-\ndata\n-\nset\n,\n\n                      \nselect\n=\nvariable1\n \nvariable2\n \nvariable3\n,\n\n                      \noutputname\n=\n%str\n(\npath\n\\\nexceldata\n\\\nFile\n \nname\n \n(\nsysdate\n).\nxlsx\n),\n\n                      \nsheetname\n=\nSheet\n \nname\n);", 
            "title": "Export to Excel file"
        }, 
        {
            "location": "/sas-outputs/excel/#export-data-to-excel-with-format", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58 %macro   ExportExcelWithFormat ( libname = , dataname = , select = ,   outputname = , sheetname =   ); \n\n     proc   sql   noprint ; \n         create   table   tmp_vars   as \n             select   name , format ,   label   from   dictionary . columns \n                 where   libname = upcase ( libname. )   and   memname = upcase ( dataname. ); \n     quit ; \n\n     data   tmp_vars ; \n         set   tmp_vars   end = last ; \n         length   formatcode   $400 .; \n         if   format   ^=   then \n             formatcode = catx (   , cats ( put , ( , name , , , format , ) ),   as , name ,  label ,   , label , , , ); \n         else   formatcode = cats ( name , , ); \n         if   last   then \n             formatcode = substr ( formatcode , 1 , length ( formatcode ) - 1 ); \n     run ; \n\n     %let   formatcodes = ; \n\n     data   _null_ ; \n         set   tmp_vars ; \n         call   symput ( formatcodes ,   trim ( resolve ( formatcodes . ) ||   || trim \n             ( formatcode ))); \n     run ; \n\n     %put   formatcodes .; \n\n     proc   sql ; \n         create   view   tmp_view   as \n             select   formatcodes . \n                 from   libname .. dataname .; \n     quit ; \n\n     data   _datain ; \n         retain   select ; \n         set   tmp_view   ( keep = select ); \n     run ; \n\n     %let   formatcodes = %str (); \n\n     PROC   EXPORT   DATA =   _datain   DBMS = xlsx   REPLACE   label  \n         OUTFILE =   outputname. ; \n         SHEET = sheetname. ; \n     RUN ; \n\n     proc   sql ; \n         drop   table   tmp_vars ,   _datain ; \n         drop   view   tmp_view ; \n     quit ;  %mend ;  %export ExcelWithFormat ( libname = SAS - libname , \n                       dataname = SAS - data - set , \n                       select = variable1   variable2   variable3 , \n                       outputname = %str ( path \\ exceldata \\ File   name   ( sysdate ). xlsx ), \n                       sheetname = Sheet   name );", 
            "title": "Export Data to Excel With Format"
        }, 
        {
            "location": "/sasvsr/introduction/", 
            "text": "Check these websites\n\n\n\n\nChoosing the correct statistical test in SAS, STATA, SPSS and R\n\n\nImport, Export, and Convert Data Files\n\n\nReally useful R package: sas7bdat\n\n\nSAS to R Migration\n\n\nA Saga Of Migrating From SAS To R\n\n\n5 Ways to Convert SAS Data to R", 
            "title": "Introduction"
        }, 
        {
            "location": "/sasvsr/importing-reporting/", 
            "text": "Under construction", 
            "title": "Importing and Reporting Data"
        }, 
        {
            "location": "/sasvsr/variables-functions/", 
            "text": "Under construction", 
            "title": "Creating New Variables, Functions and Data Tables"
        }, 
        {
            "location": "/sasvsr/random-plotting/", 
            "text": "Under construction", 
            "title": "Random Number Generation and Plotting"
        }, 
        {
            "location": "/sasvsr/proc-ods-macros/", 
            "text": "Under construction", 
            "title": "Descriptive Procedures, Output Delivery System and Macros"
        }, 
        {
            "location": "/sasvsr/inferential/", 
            "text": "Under construction", 
            "title": "Analyzing the Data via Inferential Procedures"
        }, 
        {
            "location": "/sasvsr/iml/", 
            "text": "Under construction", 
            "title": "Interactive Matrix Language (IML)"
        }, 
        {
            "location": "/sasvsr/bridge/", 
            "text": "Under construction", 
            "title": "A Bridge Between SAS and R"
        }, 
        {
            "location": "/wizard/", 
            "text": "Despu\u00e9s de crear un c\u00f3digo de recodificaci\u00f3n con el wizard\nclick derecho en el workflow, create as code, template, save code\n\n\nDATA Step Debugger\n\n\n\n\nCheck these websites\n\n\n\n\nDemo: DATA Step Debugging in Enterprise Guide\n\n\nUsing the DATA step debugger in SAS Enterprise Guide\n\n\nStep through Your DATA Step: Introducing the DATA Step Debugger in SAS Enterprise Guide\n\n\nAn Animated Guide: The SAS Data Step Debugger\n\n\n\n\n\n\nThis tool is for debugging \nDATA\n step code. It can't be used to debug \nPROC SQL\n or \nPROC IML\n or SAS macro programs. \n\n\nIt can't be used to debug \nDATA\n steps that read data from \nCARDS\n or \nDATALINES\n. That's an unfortunate limitation, but it's a side effect of the way the \nDATA\n step \ndebug\n mode works with client applications like SAS Enterprise Guide. To workaround this limitation you can load your data in a separate step and then debug your more complex \nDATA\n step logic in a subsequent step.\n\n\n\n\nWhen a variable changes its value, it's colored red. If the value hasn't changed it will remain black.\n\n\nIf you want the \nDATA\n step to break processing when a certain variable changes value, check the \nWatch\n box for that variable.\n\n\nYou can set and clear line-specific breakpoints by clicking in the left space next to the line number.\n\n\nIn the \nDebug Console\n window you can introduce more complex breakpoints through commands:\n\n\nBREAK\n suspends program execution at an executable statement\n\n\nCALCULATE\n evaluates a debugger expression and displays the result\n\n\nDELETE\n deletes breakpoints or the watch status of variables in the \nDATA\n step\n\n\nDESCRIBE\n displays the attributes of one or more variables\n\n\nENTER\n assigns one or more debugger commands to the \nENTER\n key\n\n\nEXAMINE\n displays the value of one or more variables\n\n\nGO\n starts or resumes execution of the \nDATA\n step\n\n\nHELP\n displays information about debugger commands\n\n\nJUMP\n restarts execution of a suspended program\n\n\nLIST\n displays all occurrences of the item that is listed in the argument\n\n\nQUIT\n terminates a debugger session\n\n\nSET\n assigns a new value to a specified variable\n\n\nSTEP\n executes statements one at a time in the active program\n\n\nSWAP\n switches control between the \nSOURCE\n window and the \nLOG\n window\n\n\nTRACE\n controls whether the debugger displays a continuous record of the \nDATA\n step execution\n\n\nWATCH\n suspends execution when the value of a specified variable changes\n\n\n\n\n\n\nExamples\n\n\nbreak 8 when (running_price \n 100)\n will break on line 8 when the value of running_price exceeds 100\n\nbreak 8 after 5\n will break on line 8 after 5 passes through the \nDATA\n step", 
            "title": "Using Wizard Menus"
        }, 
        {
            "location": "/wizard/#data-step-debugger", 
            "text": "Check these websites   Demo: DATA Step Debugging in Enterprise Guide  Using the DATA step debugger in SAS Enterprise Guide  Step through Your DATA Step: Introducing the DATA Step Debugger in SAS Enterprise Guide  An Animated Guide: The SAS Data Step Debugger    This tool is for debugging  DATA  step code. It can't be used to debug  PROC SQL  or  PROC IML  or SAS macro programs.   It can't be used to debug  DATA  steps that read data from  CARDS  or  DATALINES . That's an unfortunate limitation, but it's a side effect of the way the  DATA  step  debug  mode works with client applications like SAS Enterprise Guide. To workaround this limitation you can load your data in a separate step and then debug your more complex  DATA  step logic in a subsequent step.   When a variable changes its value, it's colored red. If the value hasn't changed it will remain black.  If you want the  DATA  step to break processing when a certain variable changes value, check the  Watch  box for that variable.  You can set and clear line-specific breakpoints by clicking in the left space next to the line number.  In the  Debug Console  window you can introduce more complex breakpoints through commands:  BREAK  suspends program execution at an executable statement  CALCULATE  evaluates a debugger expression and displays the result  DELETE  deletes breakpoints or the watch status of variables in the  DATA  step  DESCRIBE  displays the attributes of one or more variables  ENTER  assigns one or more debugger commands to the  ENTER  key  EXAMINE  displays the value of one or more variables  GO  starts or resumes execution of the  DATA  step  HELP  displays information about debugger commands  JUMP  restarts execution of a suspended program  LIST  displays all occurrences of the item that is listed in the argument  QUIT  terminates a debugger session  SET  assigns a new value to a specified variable  STEP  executes statements one at a time in the active program  SWAP  switches control between the  SOURCE  window and the  LOG  window  TRACE  controls whether the debugger displays a continuous record of the  DATA  step execution  WATCH  suspends execution when the value of a specified variable changes    Examples  break 8 when (running_price   100)  will break on line 8 when the value of running_price exceeds 100 break 8 after 5  will break on line 8 after 5 passes through the  DATA  step", 
            "title": "DATA Step Debugger"
        }, 
        {
            "location": "/sasvi/", 
            "text": "Check these websites\n\n\n\n\nVI Tutorials\n\n\n\n\n\n\nHome Page\n\n\nAt the Home Page you can:\n\n\n\n\nClick the icon beside \"Home\" in the banner to access your applications using a side panel\n\n\nAdd application \nshortcuts\n to your Home page (you can customize the color and the name)\n\n\nCreate customized \ncollections\n of documents/projects that you can then share\n\n\nCheck recent projects in the \nRecent\n tile\n\n\nCheck your favorite projects at the \nFavorites\n list (you can Edit $\\rightarrow$ Add a new favorite to the list or include it when you inspect the project by activating the star symbol)\n\n\nCheck your customize list of links in the \nLinks\n tile (you can Edit $\\rightarrow$ Add to include a new one)\n\n\nChange your application settings at the top right corner\n\n\n\n\nData Explorer\n\n\nReport Designer\n\n\nThese are the tipical step that a report author might walk through while creating a report in the designer:\n\n\n\n\nChoose your data source\n\n\nModify data item properties\n\n\nDrag one or more report objects onto the canvas\n\n\nAssign data items to report object roles\n\n\nApply any needed or desired filters\n\n\nUpdate properties for the report\n\n\nUpdate properties and styles for report objects\n\n\nIf desired, add new sections to the report\n\n\nIf required, add interactions between the report objects\n\n\nSave your report\n\n\n\n\nThe minimun steps to create a basic report are 1, 3, 4 and 10.\n\n\nData Preparation\n\n\nReport Viewer", 
            "title": "SAS Visual Analytics"
        }, 
        {
            "location": "/sasvi/#home-page", 
            "text": "At the Home Page you can:   Click the icon beside \"Home\" in the banner to access your applications using a side panel  Add application  shortcuts  to your Home page (you can customize the color and the name)  Create customized  collections  of documents/projects that you can then share  Check recent projects in the  Recent  tile  Check your favorite projects at the  Favorites  list (you can Edit $\\rightarrow$ Add a new favorite to the list or include it when you inspect the project by activating the star symbol)  Check your customize list of links in the  Links  tile (you can Edit $\\rightarrow$ Add to include a new one)  Change your application settings at the top right corner", 
            "title": "Home Page"
        }, 
        {
            "location": "/sasvi/#data-explorer", 
            "text": "", 
            "title": "Data Explorer"
        }, 
        {
            "location": "/sasvi/#report-designer", 
            "text": "These are the tipical step that a report author might walk through while creating a report in the designer:   Choose your data source  Modify data item properties  Drag one or more report objects onto the canvas  Assign data items to report object roles  Apply any needed or desired filters  Update properties for the report  Update properties and styles for report objects  If desired, add new sections to the report  If required, add interactions between the report objects  Save your report   The minimun steps to create a basic report are 1, 3, 4 and 10.", 
            "title": "Report Designer"
        }, 
        {
            "location": "/sasvi/#data-preparation", 
            "text": "", 
            "title": "Data Preparation"
        }, 
        {
            "location": "/sasvi/#report-viewer", 
            "text": "", 
            "title": "Report Viewer"
        }, 
        {
            "location": "/CDISC/introduction/", 
            "text": "Check these websites\n\n\n\n\nCDISC Official Site\n\n\nCDISC Standards\n\n\nMacro-Supported Metadata-Driven Process for Mapping SDTM VISIT and VISITNUM\n\n\n\n\n\n\n\u00bfQu\u00e9 es CDISC?\n\n\nDiferencias entre CDASH, SDTM/SEND y ADAM\n\n\nCDASH\n\n\nClinical Data Acquisition Standards Harmonization\n\n\nLograr que los datos sean recogidos de manera m\u00e1s homog\u00e9nea. No est\u00e1 pensado para presentar los datos sino para recogerlos. Est\u00e1ndar para la recogida de datos. No hay que inspirarse en SDTM (estructura vertical) para dise\u00f1ar la base de datos sino en CDASH, y luego ya se har\u00e1 una transformaci\u00f3n a SDTM.\n\n\nSDTM\n\n\nStudy Data Tabulation Model\n\n\nDepu\u00e9s de la transformaci\u00f3n se tiene que reportar sobre el CRF anotado una nueva anotaci\u00f3n con las variables SDTM. Esta nueva anotaci\u00f3n puede requerir estructuras condicionales para poder reconocer las nuevas variables.\n\n\nADaM\n\n\nAnalysis Data Model\n\n\nDefine los est\u00e1ndares de data sets y metadata de cara a un an\u00e1lisis y presentaci\u00f3n de resultados. Permite una generaci\u00f3n eficiente, replicaci\u00f3n y revisi\u00f3n de los datos usados para realizar an\u00e1lisis estad\u00edsticos en ensayos cl\u00ednicos.\n\n\nSEND\n\n\nStandard for Exchange of Nonclinical Data\n\n\nUna implementaci\u00f3n menos restrictiva de SDTM para estudios no cl\u00ednicos que tienen m\u00e1s variabilidad.\n\n\n\u00bfQu\u00e9 es el CDI?\n\n\nSAS Clinical Data Integration. Te permite hacer un mapeo de tu base de datos original y la documentaci\u00f3n de la transformaci\u00f3n se genera autom\u00e1ticamente.\n\n\nLibrer\u00edas\n\n\nJobs\n\n\nDominios\n\n\nConjuntos de variables. Los est\u00e1ndares definen qu\u00e9 variables tiene cada uno, cuales son obligatorias, opcionales o recomendables y cuales se deben enviar. En algunas te tienes que limitar a una estructura restringida y tienes que meter la informaci\u00f3n extra en un dominio suplementario que va ligado al principal, pero otros dominios son un poco m\u00e1s flexibles. Hay informaci\u00f3n que te puede cuadrar en varios dominios, ah\u00ed entra ya la interpretaci\u00f3n del sponsor o del que mapea la base de datos. El est\u00e1ndar no est\u00e1 totalmente cerrado y hay cosas que hay que decidir y ser\u00e1 necesario justificar las decisiones.\n\n\nPinacle21/OpenCDISC\n\n\nValidar que tus data sets cumplen todas las reglas y est\u00e1ndares que impone CDISC. Te presenta un informe en excel de los errores.\n\n\nDiferentes tipos de dominios\n\n\nDominios de prop\u00f3sito especial\n\n\n\n\nCO (Comments)\n\n\nDM (Demographics): una de las pocas tablas que es plana, se recogen variables que siempre est\u00e1n en todos los estudios que tienen nombre propio.\n\n\nSE (Subject Elements): especifica las visitas que va a tener el paciente a nivel de elemento, el esquema de visitas que deber\u00eda tener el paciente.\n\n\nSV (Subject Visits): fechas de visitas\n\n\n\n\nGeneral Observation Classes\n\n\n\n\nIntervenciones: dominios que recogen acciones o intervenciones sobre los pacientes: CM (Concomitant and Prior Medications), EX (Exposure), EC (Exposure as Collected), PR (Procedures), SU (Substance Use)\n\n\nEventos: dominios donde se recogen los acontecimientos que suceden sobre el paciente: AE (Adverse Events), CE (Clinical Events), DS (Disposition), DV (Protocol Deviations), HO (Helthcare Encounters), MH (Medical History)\n\n\nFindings: todo el resto de la informaci\u00f3n del estudio.\n\n\nExperimental Design\n\n\nScheduling of Assessments\n\n\nTrial Summary Eligilility\n\n\n\n\nCaso pr\u00e1ctico\n\n\n\n\nExtract from OC\n\n\nTransform in SAS CDI\n\n\nLoad SDTM Domains\n\n\nValidate SDTM Domains\n\n\n\n\nImplementaci\u00f3n\n\n\nRequerimientos: se define el estudio, la librer\u00eda, los data sets originales y se carga la versi\u00f3n de los est\u00e1ndares que se van a usar (SDTM, ADaM) que se quedar\u00e1 registrado en la metadata del estudio.\nPasos: \n\n\n\n\nregistrar los data sets originales, \n\n\ncrear un nuevo job, \n\n\nidentificar las variables y data sets que van a componer cada dominio, \n\n\nhacer la transformaci\u00f3n pertinente, \n\n\nvalidar los dominios.\n\n\n\n\nTipos de variables\n\n\n\n\nRequired: obligatorias y no permiten missing ni duplicados\n\n\nExpected: hay que incluirlas pero pueden no estar informadas\n\n\nPermissible: si tiene datos en alg\u00fan registro se supone que hay que reportarla, aunque no saben que la tienes recogida porque no es required, con lo cual a efectos pr\u00e1cticos las puedes quitar si quieres\n\n\n\n\nValidaci\u00f3n de los dominios\n\n\nDefine.xml compliance\n\n\nSAS CDI genera este fichero autom\u00e1ticamente agrupando toda la informaci\u00f3n necesaria para enviar a las agencias regulatorias con hiperv\u00ednculos entre secci\u00f3n.\nEl archivo Define.xml contiene toda la informaci\u00f3n anterior.", 
            "title": "Introduction"
        }, 
        {
            "location": "/CDISC/introduction/#que-es-cdisc", 
            "text": "", 
            "title": "\u00bfQu\u00e9 es CDISC?"
        }, 
        {
            "location": "/CDISC/introduction/#diferencias-entre-cdash-sdtmsend-y-adam", 
            "text": "", 
            "title": "Diferencias entre CDASH, SDTM/SEND y ADAM"
        }, 
        {
            "location": "/CDISC/introduction/#cdash", 
            "text": "Clinical Data Acquisition Standards Harmonization  Lograr que los datos sean recogidos de manera m\u00e1s homog\u00e9nea. No est\u00e1 pensado para presentar los datos sino para recogerlos. Est\u00e1ndar para la recogida de datos. No hay que inspirarse en SDTM (estructura vertical) para dise\u00f1ar la base de datos sino en CDASH, y luego ya se har\u00e1 una transformaci\u00f3n a SDTM.", 
            "title": "CDASH"
        }, 
        {
            "location": "/CDISC/introduction/#sdtm", 
            "text": "Study Data Tabulation Model  Depu\u00e9s de la transformaci\u00f3n se tiene que reportar sobre el CRF anotado una nueva anotaci\u00f3n con las variables SDTM. Esta nueva anotaci\u00f3n puede requerir estructuras condicionales para poder reconocer las nuevas variables.", 
            "title": "SDTM"
        }, 
        {
            "location": "/CDISC/introduction/#adam", 
            "text": "Analysis Data Model  Define los est\u00e1ndares de data sets y metadata de cara a un an\u00e1lisis y presentaci\u00f3n de resultados. Permite una generaci\u00f3n eficiente, replicaci\u00f3n y revisi\u00f3n de los datos usados para realizar an\u00e1lisis estad\u00edsticos en ensayos cl\u00ednicos.", 
            "title": "ADaM"
        }, 
        {
            "location": "/CDISC/introduction/#send", 
            "text": "Standard for Exchange of Nonclinical Data  Una implementaci\u00f3n menos restrictiva de SDTM para estudios no cl\u00ednicos que tienen m\u00e1s variabilidad.", 
            "title": "SEND"
        }, 
        {
            "location": "/CDISC/introduction/#que-es-el-cdi", 
            "text": "SAS Clinical Data Integration. Te permite hacer un mapeo de tu base de datos original y la documentaci\u00f3n de la transformaci\u00f3n se genera autom\u00e1ticamente.", 
            "title": "\u00bfQu\u00e9 es el CDI?"
        }, 
        {
            "location": "/CDISC/introduction/#librerias", 
            "text": "", 
            "title": "Librer\u00edas"
        }, 
        {
            "location": "/CDISC/introduction/#jobs", 
            "text": "", 
            "title": "Jobs"
        }, 
        {
            "location": "/CDISC/introduction/#dominios", 
            "text": "Conjuntos de variables. Los est\u00e1ndares definen qu\u00e9 variables tiene cada uno, cuales son obligatorias, opcionales o recomendables y cuales se deben enviar. En algunas te tienes que limitar a una estructura restringida y tienes que meter la informaci\u00f3n extra en un dominio suplementario que va ligado al principal, pero otros dominios son un poco m\u00e1s flexibles. Hay informaci\u00f3n que te puede cuadrar en varios dominios, ah\u00ed entra ya la interpretaci\u00f3n del sponsor o del que mapea la base de datos. El est\u00e1ndar no est\u00e1 totalmente cerrado y hay cosas que hay que decidir y ser\u00e1 necesario justificar las decisiones.", 
            "title": "Dominios"
        }, 
        {
            "location": "/CDISC/introduction/#pinacle21opencdisc", 
            "text": "Validar que tus data sets cumplen todas las reglas y est\u00e1ndares que impone CDISC. Te presenta un informe en excel de los errores.", 
            "title": "Pinacle21/OpenCDISC"
        }, 
        {
            "location": "/CDISC/introduction/#diferentes-tipos-de-dominios", 
            "text": "", 
            "title": "Diferentes tipos de dominios"
        }, 
        {
            "location": "/CDISC/introduction/#dominios-de-proposito-especial", 
            "text": "CO (Comments)  DM (Demographics): una de las pocas tablas que es plana, se recogen variables que siempre est\u00e1n en todos los estudios que tienen nombre propio.  SE (Subject Elements): especifica las visitas que va a tener el paciente a nivel de elemento, el esquema de visitas que deber\u00eda tener el paciente.  SV (Subject Visits): fechas de visitas", 
            "title": "Dominios de prop\u00f3sito especial"
        }, 
        {
            "location": "/CDISC/introduction/#general-observation-classes", 
            "text": "Intervenciones: dominios que recogen acciones o intervenciones sobre los pacientes: CM (Concomitant and Prior Medications), EX (Exposure), EC (Exposure as Collected), PR (Procedures), SU (Substance Use)  Eventos: dominios donde se recogen los acontecimientos que suceden sobre el paciente: AE (Adverse Events), CE (Clinical Events), DS (Disposition), DV (Protocol Deviations), HO (Helthcare Encounters), MH (Medical History)  Findings: todo el resto de la informaci\u00f3n del estudio.  Experimental Design  Scheduling of Assessments  Trial Summary Eligilility", 
            "title": "General Observation Classes"
        }, 
        {
            "location": "/CDISC/introduction/#caso-practico", 
            "text": "Extract from OC  Transform in SAS CDI  Load SDTM Domains  Validate SDTM Domains", 
            "title": "Caso pr\u00e1ctico"
        }, 
        {
            "location": "/CDISC/introduction/#implementacion", 
            "text": "Requerimientos: se define el estudio, la librer\u00eda, los data sets originales y se carga la versi\u00f3n de los est\u00e1ndares que se van a usar (SDTM, ADaM) que se quedar\u00e1 registrado en la metadata del estudio.\nPasos:    registrar los data sets originales,   crear un nuevo job,   identificar las variables y data sets que van a componer cada dominio,   hacer la transformaci\u00f3n pertinente,   validar los dominios.", 
            "title": "Implementaci\u00f3n"
        }, 
        {
            "location": "/CDISC/introduction/#tipos-de-variables", 
            "text": "Required: obligatorias y no permiten missing ni duplicados  Expected: hay que incluirlas pero pueden no estar informadas  Permissible: si tiene datos en alg\u00fan registro se supone que hay que reportarla, aunque no saben que la tienes recogida porque no es required, con lo cual a efectos pr\u00e1cticos las puedes quitar si quieres", 
            "title": "Tipos de variables"
        }, 
        {
            "location": "/CDISC/introduction/#validacion-de-los-dominios", 
            "text": "", 
            "title": "Validaci\u00f3n de los dominios"
        }, 
        {
            "location": "/CDISC/introduction/#definexml-compliance", 
            "text": "SAS CDI genera este fichero autom\u00e1ticamente agrupando toda la informaci\u00f3n necesaria para enviar a las agencias regulatorias con hiperv\u00ednculos entre secci\u00f3n.\nEl archivo Define.xml contiene toda la informaci\u00f3n anterior.", 
            "title": "Define.xml compliance"
        }, 
        {
            "location": "/miscellanea/arrays/", 
            "text": "Declaring arrays\n\n\nThe dimension has to be known in advance (???)\nThere's no way to write an implicit loop through all the elements of the array (???)\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\ndata _null_;\n    ARRAY arrayname[2,3] \n$\n value11-value13 (0 0 0)\n                         \n$\n value21-value23 (0 0 0);\n\n    DO i=1 TO DIM(arrayname);\n        arrayname[i] = arrayname[i] + 1;\n    END;\n\n    result=CATX(\n,\n,OF value11-value13);\n    PUT result=;\nRUN;\n\n\n\n\n\n\nAssigning Initial Values to Array Variables or Elements\n\n\nThe following \nARRAY\n statements illustrate the initialization of numeric and character values:\n\n\n1\n2\nARRAY sizes[*] petite small medium large extra_large (2, 4, 6, 8, 10);\nARRAY cities[4] $10 (\nNew York\n \nLos Angeles\n \nDallas\n \nChicago\n);\n\n\n\n\n\n\nYou can also initialize the elements of an array with the same value using an iteration factor, as shown in the following\nexample that initializes 10 elements with a value of 0:\n\n\n1\nARRAY values[10] 10*0;\n\n\n\n\n\n\nWhen elements are initialized within an \nARRAY\n statement, the values are automatically retained from one iteration of the\n\nDATA\n step to another; a \nRETAIN\n statement is not necessary. \n\n\nDate Arrays\n\n\nYou can only assign lengths, not formats in an array definition. Use a separate format statement to specify the date format.\n\n\n1\n2\n3\n4\n5\nDATA new-SAS-data-set;\n    SET existing-SAS-data-set;\n    ARRAY arrayname[8] element1-element8;\n    FORMAT element1-element8 ddmmyy10.;\nRUN;", 
            "title": "Working with Arrays"
        }, 
        {
            "location": "/miscellanea/arrays/#declaring-arrays", 
            "text": "The dimension has to be known in advance (???)\nThere's no way to write an implicit loop through all the elements of the array (???)   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 data _null_;\n    ARRAY arrayname[2,3]  $  value11-value13 (0 0 0)\n                          $  value21-value23 (0 0 0);\n\n    DO i=1 TO DIM(arrayname);\n        arrayname[i] = arrayname[i] + 1;\n    END;\n\n    result=CATX( , ,OF value11-value13);\n    PUT result=;\nRUN;", 
            "title": "Declaring arrays"
        }, 
        {
            "location": "/miscellanea/arrays/#assigning-initial-values-to-array-variables-or-elements", 
            "text": "The following  ARRAY  statements illustrate the initialization of numeric and character values:  1\n2 ARRAY sizes[*] petite small medium large extra_large (2, 4, 6, 8, 10);\nARRAY cities[4] $10 ( New York   Los Angeles   Dallas   Chicago );   You can also initialize the elements of an array with the same value using an iteration factor, as shown in the following\nexample that initializes 10 elements with a value of 0:  1 ARRAY values[10] 10*0;   When elements are initialized within an  ARRAY  statement, the values are automatically retained from one iteration of the DATA  step to another; a  RETAIN  statement is not necessary.", 
            "title": "Assigning Initial Values to Array Variables or Elements"
        }, 
        {
            "location": "/miscellanea/arrays/#date-arrays", 
            "text": "You can only assign lengths, not formats in an array definition. Use a separate format statement to specify the date format.  1\n2\n3\n4\n5 DATA new-SAS-data-set;\n    SET existing-SAS-data-set;\n    ARRAY arrayname[8] element1-element8;\n    FORMAT element1-element8 ddmmyy10.;\nRUN;", 
            "title": "Date Arrays"
        }, 
        {
            "location": "/miscellanea/execution-time/", 
            "text": "Measure your code execution time\n\n\n1\n2\n3\n4\n5\n6\n7\n%let\n \ndatetime_start\n \n=\n \n%sysfunc\n(\nTIME\n())\n \n;\n\n\n%put\n \nSTART\n \nTIME\n:\n \n%sysfunc\n(\ndatetime\n(),\ndatetime14\n.);\n\n\n\n[\nYOUR\n \nCODE\n \nHERE\n]\n\n\n\n%put\n \nEND\n \nTIME\n:\n \n%sysfunc\n(\ndatetime\n(),\ndatetime14\n.);\n\n\n%put\n \nTOTAL\n \nTIME\n:\n  \n%sysfunc\n(\nputn\n(\n%sysevalf\n(\n%sysfunc\n(\nTIME\n())\n-\ndatetime_start\n.),\nmmss\n.))\n \n(\nmm\n:\nss\n)\n \n;", 
            "title": "Execution time"
        }, 
        {
            "location": "/miscellanea/execution-time/#measure-your-code-execution-time", 
            "text": "1\n2\n3\n4\n5\n6\n7 %let   datetime_start   =   %sysfunc ( TIME ())   ;  %put   START   TIME :   %sysfunc ( datetime (), datetime14 .);  [ YOUR   CODE   HERE ]  %put   END   TIME :   %sysfunc ( datetime (), datetime14 .);  %put   TOTAL   TIME :    %sysfunc ( putn ( %sysevalf ( %sysfunc ( TIME ()) - datetime_start .), mmss .))   ( mm : ss )   ;", 
            "title": "Measure your code execution time"
        }, 
        {
            "location": "/miscellanea/time-intervals/", 
            "text": "Time periods in months or years\n\n\n1\n2\n3\n4\n5\n* In years;\ninterval = YRDIF(inidat,enddat,\nage\n);\n\n* In months;\ninterval = YRDIF(inidat,enddat,\nage\n)*12;\n\n\n\n\n\n\nTime periods in days\n\n\nIn general, the difference between two SAS dates in days can most easily be calculated as\n\n\n1\nduration = end_date - start_date;\n\n\n\n\n\n\nThe \nINTCK\n function in SAS can calculate the difference between any two dates or datetime values, and return whatever interval you're looking for (days, minutes, hours, weeks, months).\n\n\n1\n2\n3\n4\n5\n6\ndata test;\n  set test;\n  by subj;\n  days=intck(\ndtday\n, date1, date2);\n  put days=;\nrun;\n\n\n\n\n\n\nFor hospital stays, you might have special rules about calculating day intervals. For example, a patient who is admitted and charged on the same day might count as a 1-day stay, and a patient who is discharged on the second day might still count as a 1-day stay -- so you might need to modify the formula to add 1 in the case of a same-day discharge--.  If your data are such that any record with a hospital admission represents at least a 1-day stay (for reporting purposes), then your formula might be:\n\n\n1\n2\n/* return the higher of two values: calculated interval or 1 */\n\ndur\n \n=\n \nmax\n(\nintck\n(\nday\n,\ndate_1\n,\ndate_2\n),\n \n1\n)\n;\n\n\n\n\n\n\n\nThe \nINTICK\n Function\n\n\nEveryone knows that the \nINTCK\n function returns the integer count of the number of interval boundaries between two dates, two times, or two datetime values.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n*\n \nDate\n \nexamples\n \n;\n\n\n*\n---------------\n;\n\n\nyears\n=\nintck\n(\nyear\n,\n01jan2009\nd\n,\n01jan2010\nd\n);\n\n\nSEMIYEAR\n=\nintck\n(\nSEMIYEAR\n,\n01jan2009\nd\n,\n01jan2010\nd\n);\n\n\nquarters\n=\nintck\n(\nqtr\n,\n01jan2009\nd\n,\n01jan2010\nd\n);\n\n\nmonths\n=\nintck\n(\nmonth\n,\n01jan2009\nd\n,\n01jan2010\nd\n);\n\n\nweeks\n=\nintck\n(\nweek\n,\n01jan2009\nd\n,\n01jan2010\nd\n);\n\n\ndays\n=\nintck\n(\nday\n,\n01jan2009\nd\n,\n01jan2010\nd\n);\n\n\n\n*\n \nDate\n \n+\n \ntime\n \nexamples\n \n;\n\n\n*\n----------------------\n;\n\n\nhours\n=\nintck\n(\nhour\n,\n01jan2009:00:00:00\ndt\n,\n01jan2010:00:00:00\ndt\n);\n\n\nminutes\n=\nintck\n(\nminute\n,\n01jan2009:00:00:00\ndt\n,\n01jan2010:00:00:00\ndt\n);\n\n\nseconds\n=\nintck\n(\nsecond\n,\n01jan2009:00:00:00\ndt\n,\n01jan2010:00:00:00\ndt\n);\n\n\n\n*\n \nTime\n \nexamples\n \n;\n\n\n*\n---------------\n;\n\n\nhours\n=\nintck\n(\nhour\n,\n00:00:00\nt\n,\n12:00:00\nt\n);\n\n\nminutes\n=\nintck\n(\nminute\n,\n00:00:00\nt\n,\n12:00:00\nt\n);\n\n\nseconds\n=\nintck\n(\nsecond\n,\n00:00:00\nt\n,\n12:00:00\nt\n);\n\n\n\n*\n \nUse\n \ndays365\n \nto\n \ncalculate\n \nnumber\n \nof\n \nyears\n \ninstead\n \nof\n \nnumber\n \nof\n \nnumber\n \nof\n \ninterval\n \nboundaries\n \n(\nwould\n \nbe\n \n1\n \nfor\n \nthis\n \ncase\n);\n\n\ndays365\n=\nintck\n(\nday365\n,\n31dec2009\nd\n,\n01jan2010\nd\n);\n\n\n\n*\n \nUsing\n \nTimepart()\n \nand\n \nDatepart()\n \n;\n\n\n*\n-------------------------------------\n;\n\n\nformat\n \na1\n \nb1\n \ndate9\n.;\n\n\na0\n=\n01jan2009:00:00:00\ndt\n;\n\n\nb0\n=\n01jan2010:00:00:00\ndt\n;\n\n\na1\n=\ndatepart\n(\na0\n);\n\n\nb1\n=\ndatepart\n(\nb0\n);\n\n\ndays\n=\nintck\n(\nday\n,\na1\n,\nb1\n);\n\n\n\nformat\n \na1\n \nb1\n \ndate9\n.;\n\n\na0\n=\n01jan2009:00:00:00\ndt\n;\n\n\nb0\n=\n01jan2010:12:00:00\ndt\n;\n\n\na1\n=\ntimepart\n(\na0\n);\n\n\nb1\n=\ntimepart\n(\nb0\n);\n\n\nhour\n=\nintck\n(\nhour\n,\na1\n,\nb1\n);\n\n\n\n\n\n\n\nThe \nINTNX\n Function\n\n\nThis function increments a date, time, or datetime value by a given interval or intervals, and returns a date, time, or datetime value.\n\n\n1\n2\n3\n4\n5\n6\nformat day week month_ year date9.;\n\nday=intnx(\nday\n, \n01FEB2010\nd, 7); /* +7 days */\nweek=intnx(\nweek\n, \n01FEB2010\nd, 1); /* 01 of Feb 2010 is Monday*/\nmonth_=intnx(\nmonth\n, \n01FEB2010\nd, 2); /* +2 month */\nyear=intnx(\nyear\n, \n01FEB2010\nd, 1); /* +1 year */", 
            "title": "Time intervals"
        }, 
        {
            "location": "/miscellanea/time-intervals/#time-periods-in-months-or-years", 
            "text": "1\n2\n3\n4\n5 * In years;\ninterval = YRDIF(inidat,enddat, age );\n\n* In months;\ninterval = YRDIF(inidat,enddat, age )*12;", 
            "title": "Time periods in months or years"
        }, 
        {
            "location": "/miscellanea/time-intervals/#time-periods-in-days", 
            "text": "In general, the difference between two SAS dates in days can most easily be calculated as  1 duration = end_date - start_date;   The  INTCK  function in SAS can calculate the difference between any two dates or datetime values, and return whatever interval you're looking for (days, minutes, hours, weeks, months).  1\n2\n3\n4\n5\n6 data test;\n  set test;\n  by subj;\n  days=intck( dtday , date1, date2);\n  put days=;\nrun;   For hospital stays, you might have special rules about calculating day intervals. For example, a patient who is admitted and charged on the same day might count as a 1-day stay, and a patient who is discharged on the second day might still count as a 1-day stay -- so you might need to modify the formula to add 1 in the case of a same-day discharge--.  If your data are such that any record with a hospital admission represents at least a 1-day stay (for reporting purposes), then your formula might be:  1\n2 /* return the higher of two values: calculated interval or 1 */ \ndur   =   max ( intck ( day , date_1 , date_2 ),   1 ) ;", 
            "title": "Time periods in days"
        }, 
        {
            "location": "/miscellanea/time-intervals/#the-intick-function", 
            "text": "Everyone knows that the  INTCK  function returns the integer count of the number of interval boundaries between two dates, two times, or two datetime values.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39 *   Date   examples   ;  * --------------- ;  years = intck ( year , 01jan2009 d , 01jan2010 d );  SEMIYEAR = intck ( SEMIYEAR , 01jan2009 d , 01jan2010 d );  quarters = intck ( qtr , 01jan2009 d , 01jan2010 d );  months = intck ( month , 01jan2009 d , 01jan2010 d );  weeks = intck ( week , 01jan2009 d , 01jan2010 d );  days = intck ( day , 01jan2009 d , 01jan2010 d );  *   Date   +   time   examples   ;  * ---------------------- ;  hours = intck ( hour , 01jan2009:00:00:00 dt , 01jan2010:00:00:00 dt );  minutes = intck ( minute , 01jan2009:00:00:00 dt , 01jan2010:00:00:00 dt );  seconds = intck ( second , 01jan2009:00:00:00 dt , 01jan2010:00:00:00 dt );  *   Time   examples   ;  * --------------- ;  hours = intck ( hour , 00:00:00 t , 12:00:00 t );  minutes = intck ( minute , 00:00:00 t , 12:00:00 t );  seconds = intck ( second , 00:00:00 t , 12:00:00 t );  *   Use   days365   to   calculate   number   of   years   instead   of   number   of   number   of   interval   boundaries   ( would   be   1   for   this   case );  days365 = intck ( day365 , 31dec2009 d , 01jan2010 d );  *   Using   Timepart()   and   Datepart()   ;  * ------------------------------------- ;  format   a1   b1   date9 .;  a0 = 01jan2009:00:00:00 dt ;  b0 = 01jan2010:00:00:00 dt ;  a1 = datepart ( a0 );  b1 = datepart ( b0 );  days = intck ( day , a1 , b1 );  format   a1   b1   date9 .;  a0 = 01jan2009:00:00:00 dt ;  b0 = 01jan2010:12:00:00 dt ;  a1 = timepart ( a0 );  b1 = timepart ( b0 );  hour = intck ( hour , a1 , b1 );", 
            "title": "The INTICK Function"
        }, 
        {
            "location": "/miscellanea/time-intervals/#the-intnx-function", 
            "text": "This function increments a date, time, or datetime value by a given interval or intervals, and returns a date, time, or datetime value.  1\n2\n3\n4\n5\n6 format day week month_ year date9.;\n\nday=intnx( day ,  01FEB2010 d, 7); /* +7 days */\nweek=intnx( week ,  01FEB2010 d, 1); /* 01 of Feb 2010 is Monday*/\nmonth_=intnx( month ,  01FEB2010 d, 2); /* +2 month */\nyear=intnx( year ,  01FEB2010 d, 1); /* +1 year */", 
            "title": "The INTNX Function"
        }, 
        {
            "location": "/miscellanea/patient-identification/", 
            "text": "Dealing with Study Identification Numbers\n\n\nSite calculation from the two first numbers of the patient number:\n\n\n1\nsite = SUBSTR(PUT(patient,z4.),1,2);\n\n\n\n\n\n\n\n\nPUT\n: turns the numeric variable \npatient\n into a string (\nz4.\n adds leading zeroes if needed)\n\n\nSUBSTR\n: takes the first \n2\n characters starting from position \n1\n\n\n\n\n\n\nWarning\n\n\nThe \npatient\n variable has to be \nnumeric\n, otherwise an format note will be generated. Build a numeric version of your \npatient\n variable if it originally it is a character value.\n\n\n\n\nSubtract the patient number (e.g. last 4 characters) from a string:\n\n\n1\npatient = substr(patient_code,max(1,length(patient_code)-3));\n\n\n\n\n\n\nJoin the site number and the patient number to get a more general ID number for each patient:\n\n\n1\npatient = PUT(nsite,z2.) || PUT(npatient,z2.);\n\n\n\n\n\n\nNumber of Patients into Macrovariable\n\n\n1\n2\n3\n4\n5\n6\n7\n8\nproc\n \nsql\n \nnoprint\n;\n\n    \nselect\n \ncount\n(\ndistinct\n \npt\n)\n \n    \ninto\n \n:\n \nnpat\n\n    \nfrom\n \nlibrary\n-\nname\n.\nindex\n(\nwhere\n=\n(\nITT\n \neq\n \n1\n));\n\n\nquit\n;\n\n\n\n%let\n \nnpatients\n \n=\n \n%left\n(\n%trim\n(\nnpat\n.));\n\n\n%put\n \nnpatients\n=\nnpatients\n;", 
            "title": "Patient identification"
        }, 
        {
            "location": "/miscellanea/patient-identification/#dealing-with-study-identification-numbers", 
            "text": "", 
            "title": "Dealing with Study Identification Numbers"
        }, 
        {
            "location": "/miscellanea/patient-identification/#site-calculation-from-the-two-first-numbers-of-the-patient-number", 
            "text": "1 site = SUBSTR(PUT(patient,z4.),1,2);    PUT : turns the numeric variable  patient  into a string ( z4.  adds leading zeroes if needed)  SUBSTR : takes the first  2  characters starting from position  1    Warning  The  patient  variable has to be  numeric , otherwise an format note will be generated. Build a numeric version of your  patient  variable if it originally it is a character value.", 
            "title": "Site calculation from the two first numbers of the patient number:"
        }, 
        {
            "location": "/miscellanea/patient-identification/#subtract-the-patient-number-eg-last-4-characters-from-a-string", 
            "text": "1 patient = substr(patient_code,max(1,length(patient_code)-3));", 
            "title": "Subtract the patient number (e.g. last 4 characters) from a string:"
        }, 
        {
            "location": "/miscellanea/patient-identification/#join-the-site-number-and-the-patient-number-to-get-a-more-general-id-number-for-each-patient", 
            "text": "1 patient = PUT(nsite,z2.) || PUT(npatient,z2.);", 
            "title": "Join the site number and the patient number to get a more general ID number for each patient:"
        }, 
        {
            "location": "/miscellanea/patient-identification/#number-of-patients-into-macrovariable", 
            "text": "1\n2\n3\n4\n5\n6\n7\n8 proc   sql   noprint ; \n     select   count ( distinct   pt )  \n     into   :   npat \n     from   library - name . index ( where = ( ITT   eq   1 ));  quit ;  %let   npatients   =   %left ( %trim ( npat .));  %put   npatients = npatients ;", 
            "title": "Number of Patients into Macrovariable"
        }, 
        {
            "location": "/medical/general/", 
            "text": "Symptons vs Signs\n\n\nA \nsymptom is any subjective evidence\n of disease, while a \nsign is any objective evidence\n of disease. Therefore, a symptom is a phenomenon that is experienced by the individual affected by the disease, while a sign is a phenomenon that can be detected by someone other than the individual affected by the disease.", 
            "title": "General"
        }, 
        {
            "location": "/medical/general/#symptons-vs-signs", 
            "text": "A  symptom is any subjective evidence  of disease, while a  sign is any objective evidence  of disease. Therefore, a symptom is a phenomenon that is experienced by the individual affected by the disease, while a sign is a phenomenon that can be detected by someone other than the individual affected by the disease.", 
            "title": "Symptons vs Signs"
        }, 
        {
            "location": "/medical/labs/", 
            "text": "Hematology\n\n\nHematology\n is the study of blood and blood disorders to help in the diagnosis, treatment, and prevention of diseases of the blood and bone marrow as well as of the immunologic, hemostatic (blood clotting) and vascular systems. Because of the nature of blood, the science of hematology profoundly affects the understanding of many diseases.\n\n\nAll our \nblood cells develop from stem cells in the bone marrow\n. Stem cells are blood cells at the earliest stage of development and they stay inside the bone marrow until they are fully developed which is when they go into the bloodstream. Blood cells do not live long. The bone marrow normally makes millions of new blood cells every day to replace blood cells as they are needed.\n\n\nThe three main types of blood cells are:\n\n\n\n\n\n\nRed blood cells (RBC) or erythrocytes\n are the most common type of blood cells. They lack a cell nucleus and most organelles, in order to accommodate maximum space for hemoglobin; they can be viewed as sacks of hemoglobin, with a plasma membrane as the sack.  \n\n\n\n\nHemoglobin (HGB)\n is an iron-containing biomolecule that can bind oxygen and is responsible for the red color of the cells and the blood.\n\n\nThe \nhematocrit (HCT)\n, is the volume percentage of red blood cells in blood.\n\n\n\n\n\n\n\n\nPlatelets (PLAT) or thrombocytes\n help the blood to clot and prevent bleeding and bruising.\n\n\n\n\nWhite blood cells (WBC) or leucocytes\n fight and prevent infection.\n\n\nBasophils\n\n\nEosinophils\n\n\nNeutrophils\n\n\nMonocytes\n\n\nLymphocytes\n\n\n\n\n\n\n\n\n\n\nNormal Ranges' Rule of Thumb\n\n\n\n\n\n\n\n\nType of blood cell\n\n\nUnit\n\n\nLevels\n\n\n\n\n\n\n\n\n\n\nRed blood cells (RBC)\n\n\ng/l\n\n\nMen: 130-180, Women: 115-165\n\n\n\n\n\n\nHematocrit (HCT)\n\n\n%\n\n\nMen: 41-50, Women: 36-44\n\n\n\n\n\n\nHemoglobin      (Hgb)\n\n\ng/l\n\n\nMen: 13.5-16.5, Women: 12.0-15.0\n\n\n\n\n\n\nPlatelets\n\n\n10^9/L\n\n\n150-400\n\n\n\n\n\n\nWhite blood cells\n\n\n10^9/L\n\n\n4.0-11.0\n\n\n\n\n\n\nNeutrophils\n\n\n10^9/L\n\n\n2.0-7.5\n\n\n\n\n\n\nLymphocytes\n\n\n10^9/L\n\n\n1.5-4.5\n\n\n\n\n\n\n\n\n\n\nCheck these websites\n\n\n\n\n\n\nhttp://globalrph.com/laboratory-values/\n\n\n\n\nBlood-Related Anomalies\n\n\n\n\nLeucocytosis\n, often defined as an elevated white blood cell (WBC) count greater than $11000/mm^3$ ($11.0 \\cdot 10^9/L$) in nonpregnant adults, is a relatively common finding with a wide differential. It is important for clinicians to be able to distinguish malignant from nonmalignant etiologies, and to differentiate between the most common nonmalignant causes of leukocytosis. Leukocytosis in the range of approximately $50-100 \\cdot 10^9/L$ is sometimes referred to as a \nleukemoid reaction\n. This level of elevation can occur in some severe infections, such as Clostridium difficile infection, sepsis, organ rejection, or in patients with solid tumors. Leukocytosis greater than $100 \\cdot 10^9/L$ is almost always caused by leukemias or myeloproliferative disorders.\n\n\nAn abnormally low hematocrit (as well as a decrease in the total amount of red blood cells or hemoglobin in the blood) may suggest \nanemia\n, a decrease in the total amount of red blood cells, while an abnormally high hematocrit is called \npolycythemia\n. Both are potentially life-threatening disorders.\n\n\nThrombocythemia or thrombocytosis\n is the presence of high platelet counts in the blood. Although often symptomless, it can predispose to thrombosis in some patients. Thrombocytosis can be contrasted with \nthrombocytopenia\n, a loss of platelets in the blood.\n\n\n\n\nChemistry\n\n\nUrinalysis\n\n\nCoagulation\n\n\nBiomarkers\n\n\n\n\nCEA (carcinoembryonic antigen)\n is a protein found in many types of cells but associated with tumors and the developing fetus. A common cutoff is $5 \\mu g/L$ (values bigger than this could be a sign of disease).\n\n\nMutations in \nKRAS exon 2\n, \nBRAF\n and \nPIK3CA\n are commonly present in colorectal cancer (CRC).", 
            "title": "Laboratory test"
        }, 
        {
            "location": "/medical/labs/#hematology", 
            "text": "Hematology  is the study of blood and blood disorders to help in the diagnosis, treatment, and prevention of diseases of the blood and bone marrow as well as of the immunologic, hemostatic (blood clotting) and vascular systems. Because of the nature of blood, the science of hematology profoundly affects the understanding of many diseases.  All our  blood cells develop from stem cells in the bone marrow . Stem cells are blood cells at the earliest stage of development and they stay inside the bone marrow until they are fully developed which is when they go into the bloodstream. Blood cells do not live long. The bone marrow normally makes millions of new blood cells every day to replace blood cells as they are needed.  The three main types of blood cells are:    Red blood cells (RBC) or erythrocytes  are the most common type of blood cells. They lack a cell nucleus and most organelles, in order to accommodate maximum space for hemoglobin; they can be viewed as sacks of hemoglobin, with a plasma membrane as the sack.     Hemoglobin (HGB)  is an iron-containing biomolecule that can bind oxygen and is responsible for the red color of the cells and the blood.  The  hematocrit (HCT) , is the volume percentage of red blood cells in blood.     Platelets (PLAT) or thrombocytes  help the blood to clot and prevent bleeding and bruising.   White blood cells (WBC) or leucocytes  fight and prevent infection.  Basophils  Eosinophils  Neutrophils  Monocytes  Lymphocytes", 
            "title": "Hematology"
        }, 
        {
            "location": "/medical/labs/#normal-ranges-rule-of-thumb", 
            "text": "Type of blood cell  Unit  Levels      Red blood cells (RBC)  g/l  Men: 130-180, Women: 115-165    Hematocrit (HCT)  %  Men: 41-50, Women: 36-44    Hemoglobin      (Hgb)  g/l  Men: 13.5-16.5, Women: 12.0-15.0    Platelets  10^9/L  150-400    White blood cells  10^9/L  4.0-11.0    Neutrophils  10^9/L  2.0-7.5    Lymphocytes  10^9/L  1.5-4.5      Check these websites    http://globalrph.com/laboratory-values/", 
            "title": "Normal Ranges' Rule of Thumb"
        }, 
        {
            "location": "/medical/labs/#blood-related-anomalies", 
            "text": "Leucocytosis , often defined as an elevated white blood cell (WBC) count greater than $11000/mm^3$ ($11.0 \\cdot 10^9/L$) in nonpregnant adults, is a relatively common finding with a wide differential. It is important for clinicians to be able to distinguish malignant from nonmalignant etiologies, and to differentiate between the most common nonmalignant causes of leukocytosis. Leukocytosis in the range of approximately $50-100 \\cdot 10^9/L$ is sometimes referred to as a  leukemoid reaction . This level of elevation can occur in some severe infections, such as Clostridium difficile infection, sepsis, organ rejection, or in patients with solid tumors. Leukocytosis greater than $100 \\cdot 10^9/L$ is almost always caused by leukemias or myeloproliferative disorders.  An abnormally low hematocrit (as well as a decrease in the total amount of red blood cells or hemoglobin in the blood) may suggest  anemia , a decrease in the total amount of red blood cells, while an abnormally high hematocrit is called  polycythemia . Both are potentially life-threatening disorders.  Thrombocythemia or thrombocytosis  is the presence of high platelet counts in the blood. Although often symptomless, it can predispose to thrombosis in some patients. Thrombocytosis can be contrasted with  thrombocytopenia , a loss of platelets in the blood.", 
            "title": "Blood-Related Anomalies"
        }, 
        {
            "location": "/medical/labs/#chemistry", 
            "text": "", 
            "title": "Chemistry"
        }, 
        {
            "location": "/medical/labs/#urinalysis", 
            "text": "", 
            "title": "Urinalysis"
        }, 
        {
            "location": "/medical/labs/#coagulation", 
            "text": "", 
            "title": "Coagulation"
        }, 
        {
            "location": "/medical/labs/#biomarkers", 
            "text": "CEA (carcinoembryonic antigen)  is a protein found in many types of cells but associated with tumors and the developing fetus. A common cutoff is $5 \\mu g/L$ (values bigger than this could be a sign of disease).  Mutations in  KRAS exon 2 ,  BRAF  and  PIK3CA  are commonly present in colorectal cancer (CRC).", 
            "title": "Biomarkers"
        }, 
        {
            "location": "/R-with-Shiny/introduction/", 
            "text": "What is Shiny?\n\n\nShiny\n is an R package that makes it easy to build interactive web apps straight from R. You can host standalone apps on a webpage or embed them in R Markdown documents or build dashboards. You can also extend your Shiny apps with CSS themes, htmlwidgets, and JavaScript actions.\n\n\nEvery Shiny app has a webpage that the user visits, and behind this webpage there is a computer that serves this webpage by running R.\n\n\n\n\nWhen running your app locally, the computer serving your app is your own computer.\n\n\nWhen your app is deployed, the computer serving your app is a web server.\n\n\n\n\nGeneral Tips\n\n\n\n\nAlways run the entire script, not just up to the point where you're developing code\n\n\nSometimes the best way to see what's wrong it is to run the app and review the error\n\n\nWatch out for commas!\n\n\n\n\nAnatomy of a Shiny App\n\n\n\n\nWe start by \nloading any necessary packages\n one of which is necessarily Shiny (we also \nload the data\n before the ui and server definitions so that it can be used in both)\n\n\nThen we lay out the \nuser interface\n with the UI object that controls the appearance of our app \n\n\nWe define the \nserver function\n that contains instructions needed to build the app\n\n\nWe end each Shiny app script with a call to the \nshinyApp()\n function that puts these two components together to create the Shiny app object\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\nlibrary\n(\nshiny\n)\n\n\nload\n(\nurl\n(\nhttp://your-data\n))\n\n\n\n# Define UI for application\n\nui \n-\n fluidPage\n()\n\n\n\n# Define server function\n\nserver \n-\n \nfunction\n(\ninput\n,\n output\n)\n \n{}\n\n\n\n# Create a Shiny app object\n\nshinyApp\n(\nui \n=\n ui\n,\n server \n=\n server\n)\n\n\n\n\n\n\n\nLoading Data\n\n\nThe first step in the following example is to load the libraries and data to be used.\n\n\n1\n2\n3\nlibrary\n(\nshiny\n)\n\n\nlibrary\n(\nggplot2\n)\n\n\nload\n(\nurl\n(\nhttp://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata\n))\n\n\n\n\n\n\n\nUser Interface\n\n\nThe user interface, that we'll refer to as the UI going forward, defines and lays out the inputs of your app where users can make their selections. It also lays out the outputs.\n\n\n\n\nAt the outermost layer of out UI definition we begin with the \nfluidPage\n function. This function creates a fluid page layout consisting of \nrows and columns\n. Rows make sure that elements in them appear on the same line and columns within these rows define how much horizontal space each element should occupy. Fluid pages scale their components in realtime to fill all available browser width, which means the app developer don't need to worry about defininf relative widths for individual app components. \n\n\nWe \ndefine the layout\n of our app. Shiny includes a number of options for laying out the components of an application. The \ndefault layout is a layout with a sidebar\n that you can define with the \nsidebarLayout\n function. Under the hood, Shiny implements layout features available in Bootstrap 2, which is a popular HTML/CSS framework, so no prior experience with Bootstrap is necessary. \n\n\nWe define out \nsidebar panel\n that will contain the input controls in the following example. There are two dropdown menus created with the \nselectInput\n function.\n\n\nThe final component of our UI is the \nmainPanel\n. In the example, the main panel contains only one component, a plot output.\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n# Define UI for application that plots features of movies\n\nui \n-\n fluidPage\n(\n\n\n  \n# Sidebar layout with a input and output definitions\n\n  sidebarLayout\n(\n\n\n    \n# Inputs\n\n    sidebarPanel\n(\n\n\n      \n# Select variable for y-axis\n\n      selectInput\n(\ninputId \n=\n \ny\n,\n \n                  label \n=\n \nY-axis:\n,\n\n                  choices \n=\n \nc\n(\nIMDB rating\n \n=\n \nimdb_rating\n,\n \n                              \nIMDB number of votes\n \n=\n \nimdb_num_votes\n,\n \n                              \nCritics score\n \n=\n \ncritics_score\n,\n \n                              \nAudience score\n \n=\n \naudience_score\n,\n \n                              \nRuntime\n \n=\n \nruntime\n),\n \n                  selected \n=\n \naudience_score\n),\n\n\n      \n# Select variable for x-axis\n\n      selectInput\n(\ninputId \n=\n \nx\n,\n \n                  label \n=\n \nX-axis:\n,\n\n                  choices \n=\n \nc\n(\nIMDB rating\n \n=\n \nimdb_rating\n,\n \n                              \nIMDB number of votes\n \n=\n \nimdb_num_votes\n,\n \n                              \nCritics score\n \n=\n \ncritics_score\n,\n \n                              \nAudience score\n \n=\n \naudience_score\n,\n \n                              \nRuntime\n \n=\n \nruntime\n),\n \n                  selected \n=\n \ncritics_score\n),\n\n\n      \n# Select variable for color\n\n      selectInput\n(\ninputId \n=\n \nz\n,\n \n                  label \n=\n \nColor by:\n,\n\n                  choices \n=\n \nc\n(\nTitle type\n \n=\n \ntitle_type\n,\n \n                              \nGenre\n \n=\n \ngenre\n,\n \n                              \nMPAA rating\n \n=\n \nmpaa_rating\n,\n \n                              \nCritics rating\n \n=\n \ncritics_rating\n,\n \n                              \nAudience rating\n \n=\n \naudience_rating\n),\n \n                  selected \n=\n \nmpaa_rating\n)\n\n    \n),\n\n\n    \n# Output\n\n    mainPanel\n(\n\n      plotOutput\n(\noutputId \n=\n \nscatterplot\n)\n\n    \n)\n\n  \n)\n\n\n)\n\n\n\n\n\n\n\n\n\nCheck these websites\n\n\nTo learn more about various layouts check the \nApplication Layout Guide\n.\n\n\n\n\nServer Function\n\n\nThe server function calculates outputs and performs any other calculations needed for the outputs.\n\n\nAt the outermost layer we define our \nserver function\n which takes two arguments: \nan input and an output\n. Both of these are named lists. The server function accesses inputs selected by the user to perform computations and specifies how outputs laid out in the UI should be updated. The server function can take on one more argument, \nsession\n, which is an environment that can be used to access information and functionality relating to the session.\n\n\nIn the following example of server function has only one output, a plot, so it contains the logic necessary to build this plot. The \nrenderPlot\n function specifies how the plot output should be updated through some \nggplot2\n code. The definition of the variables comes from the input list that is built in the UI.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n# Define server function required to create the scatterplot\n\nserver \n-\n \nfunction\n(\ninput\n,\n output\n)\n \n{\n\n\n  \n# Create the scatterplot object the plotOutput function is expecting\n\n  output\n$\nscatterplot \n-\n renderPlot\n({\n\n    ggplot\n(\ndata \n=\n movies\n,\n aes_string\n(\nx \n=\n input\n$\nx\n,\n y \n=\n input\n$\ny\n,\n\n                                     color \n=\n input\n$\nz\n))\n \n+\n\n      geom_point\n()\n\n  \n})\n\n\n}\n\n\n\n\n\n\n\nThere are three rules of building server functions:\n\n\n\n\nAlways save objects to display to the named output list, in other words, something of the form \noutput$plot-to-display\n\n\nBuild objects to display with one of the render functions (\nrender*()\n), like we built our plot with \nrenderPlor\n\n\nUse input values from the named input list, with \noutput$plot-to-display\n\n\n\n\nJust like various inputs, Shiny also provides a wide selection of output types, each of which works with a render function. \n\n\n\n\n\n\n\n\nrender*()\n function\n\n\n*Output()\n function\n\n\n\n\n\n\n\n\n\n\nDT::renderDataTable()\n\n\ndataTableOutput()\n\n\n\n\n\n\nrenderImage()\n\n\nimageOutput()\n\n\n\n\n\n\nrenderPlot()\n\n\nplotOutput()\n\n\n\n\n\n\nrenderPrint()\n\n\nverbatimTextOutput()\n\n\n\n\n\n\nrenderTable()\n\n\ntableOutput()\n\n\n\n\n\n\nrenderText()\n\n\ntextOutput()\n\n\n\n\n\n\nrenderUI()\n\n\nuiOutput()\n or \nhtmlOutput()\n\n\n\n\n\n\n\n\n\n\nTip\n\n\nIt's easy to build interactive applications with Shiny, but to get the most out of it, you'll need to understand the \nreactive programming\n scheme used by Shiny: it automatically updates outputs, such as plots, when inputs that go into them change.\n\n\n\n\nBuilding the Shiny app object\n\n\nThe last component of each Shiny app is a call to the application named \nshinyApp\n function, which puts the UI and the server pieces together to create a Shiny app object.\n\n\n1\n2\n# Create a Shiny app object\n\nshinyApp\n(\nui \n=\n ui\n,\n server \n=\n server\n)", 
            "title": "Introduction and Shiny basics"
        }, 
        {
            "location": "/R-with-Shiny/introduction/#what-is-shiny", 
            "text": "Shiny  is an R package that makes it easy to build interactive web apps straight from R. You can host standalone apps on a webpage or embed them in R Markdown documents or build dashboards. You can also extend your Shiny apps with CSS themes, htmlwidgets, and JavaScript actions.  Every Shiny app has a webpage that the user visits, and behind this webpage there is a computer that serves this webpage by running R.   When running your app locally, the computer serving your app is your own computer.  When your app is deployed, the computer serving your app is a web server.", 
            "title": "What is Shiny?"
        }, 
        {
            "location": "/R-with-Shiny/introduction/#general-tips", 
            "text": "Always run the entire script, not just up to the point where you're developing code  Sometimes the best way to see what's wrong it is to run the app and review the error  Watch out for commas!", 
            "title": "General Tips"
        }, 
        {
            "location": "/R-with-Shiny/introduction/#anatomy-of-a-shiny-app", 
            "text": "We start by  loading any necessary packages  one of which is necessarily Shiny (we also  load the data  before the ui and server definitions so that it can be used in both)  Then we lay out the  user interface  with the UI object that controls the appearance of our app   We define the  server function  that contains instructions needed to build the app  We end each Shiny app script with a call to the  shinyApp()  function that puts these two components together to create the Shiny app object    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 library ( shiny )  load ( url ( http://your-data ))  # Define UI for application \nui  -  fluidPage ()  # Define server function \nserver  -   function ( input ,  output )   {}  # Create a Shiny app object \nshinyApp ( ui  =  ui ,  server  =  server )", 
            "title": "Anatomy of a Shiny App"
        }, 
        {
            "location": "/R-with-Shiny/introduction/#loading-data", 
            "text": "The first step in the following example is to load the libraries and data to be used.  1\n2\n3 library ( shiny )  library ( ggplot2 )  load ( url ( http://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata ))", 
            "title": "Loading Data"
        }, 
        {
            "location": "/R-with-Shiny/introduction/#user-interface", 
            "text": "The user interface, that we'll refer to as the UI going forward, defines and lays out the inputs of your app where users can make their selections. It also lays out the outputs.   At the outermost layer of out UI definition we begin with the  fluidPage  function. This function creates a fluid page layout consisting of  rows and columns . Rows make sure that elements in them appear on the same line and columns within these rows define how much horizontal space each element should occupy. Fluid pages scale their components in realtime to fill all available browser width, which means the app developer don't need to worry about defininf relative widths for individual app components.   We  define the layout  of our app. Shiny includes a number of options for laying out the components of an application. The  default layout is a layout with a sidebar  that you can define with the  sidebarLayout  function. Under the hood, Shiny implements layout features available in Bootstrap 2, which is a popular HTML/CSS framework, so no prior experience with Bootstrap is necessary.   We define out  sidebar panel  that will contain the input controls in the following example. There are two dropdown menus created with the  selectInput  function.  The final component of our UI is the  mainPanel . In the example, the main panel contains only one component, a plot output.    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46 # Define UI for application that plots features of movies \nui  -  fluidPage ( \n\n   # Sidebar layout with a input and output definitions \n  sidebarLayout ( \n\n     # Inputs \n    sidebarPanel ( \n\n       # Select variable for y-axis \n      selectInput ( inputId  =   y ,  \n                  label  =   Y-axis: , \n                  choices  =   c ( IMDB rating   =   imdb_rating ,  \n                               IMDB number of votes   =   imdb_num_votes ,  \n                               Critics score   =   critics_score ,  \n                               Audience score   =   audience_score ,  \n                               Runtime   =   runtime ),  \n                  selected  =   audience_score ), \n\n       # Select variable for x-axis \n      selectInput ( inputId  =   x ,  \n                  label  =   X-axis: , \n                  choices  =   c ( IMDB rating   =   imdb_rating ,  \n                               IMDB number of votes   =   imdb_num_votes ,  \n                               Critics score   =   critics_score ,  \n                               Audience score   =   audience_score ,  \n                               Runtime   =   runtime ),  \n                  selected  =   critics_score ), \n\n       # Select variable for color \n      selectInput ( inputId  =   z ,  \n                  label  =   Color by: , \n                  choices  =   c ( Title type   =   title_type ,  \n                               Genre   =   genre ,  \n                               MPAA rating   =   mpaa_rating ,  \n                               Critics rating   =   critics_rating ,  \n                               Audience rating   =   audience_rating ),  \n                  selected  =   mpaa_rating ) \n     ), \n\n     # Output \n    mainPanel ( \n      plotOutput ( outputId  =   scatterplot ) \n     ) \n   )  )     Check these websites  To learn more about various layouts check the  Application Layout Guide .", 
            "title": "User Interface"
        }, 
        {
            "location": "/R-with-Shiny/introduction/#server-function", 
            "text": "The server function calculates outputs and performs any other calculations needed for the outputs.  At the outermost layer we define our  server function  which takes two arguments:  an input and an output . Both of these are named lists. The server function accesses inputs selected by the user to perform computations and specifies how outputs laid out in the UI should be updated. The server function can take on one more argument,  session , which is an environment that can be used to access information and functionality relating to the session.  In the following example of server function has only one output, a plot, so it contains the logic necessary to build this plot. The  renderPlot  function specifies how the plot output should be updated through some  ggplot2  code. The definition of the variables comes from the input list that is built in the UI.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 # Define server function required to create the scatterplot \nserver  -   function ( input ,  output )   { \n\n   # Create the scatterplot object the plotOutput function is expecting \n  output $ scatterplot  -  renderPlot ({ \n    ggplot ( data  =  movies ,  aes_string ( x  =  input $ x ,  y  =  input $ y , \n                                     color  =  input $ z ))   + \n      geom_point () \n   })  }    There are three rules of building server functions:   Always save objects to display to the named output list, in other words, something of the form  output$plot-to-display  Build objects to display with one of the render functions ( render*() ), like we built our plot with  renderPlor  Use input values from the named input list, with  output$plot-to-display   Just like various inputs, Shiny also provides a wide selection of output types, each of which works with a render function.      render*()  function  *Output()  function      DT::renderDataTable()  dataTableOutput()    renderImage()  imageOutput()    renderPlot()  plotOutput()    renderPrint()  verbatimTextOutput()    renderTable()  tableOutput()    renderText()  textOutput()    renderUI()  uiOutput()  or  htmlOutput()      Tip  It's easy to build interactive applications with Shiny, but to get the most out of it, you'll need to understand the  reactive programming  scheme used by Shiny: it automatically updates outputs, such as plots, when inputs that go into them change.", 
            "title": "Server Function"
        }, 
        {
            "location": "/R-with-Shiny/introduction/#building-the-shiny-app-object", 
            "text": "The last component of each Shiny app is a call to the application named  shinyApp  function, which puts the UI and the server pieces together to create a Shiny app object.  1\n2 # Create a Shiny app object \nshinyApp ( ui  =  ui ,  server  =  server )", 
            "title": "Building the Shiny app object"
        }, 
        {
            "location": "/R-with-Shiny/input-output/", 
            "text": "Reactive Flow\n\n\nSuppose you have a \nslinderInput\nin your app with the \ninputId=\"alpha\"\n. The value of this input is stored in \ninput$alpha\n so when the user moves around the slider the value of the \nalpha\ninput is updated in th einput list. Reactivity automatically occurs when an input value is used to render an output object.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\nlibrary\n(\nshiny\n)\n\n\nlibrary\n(\nggplot2\n)\n\n\nload\n(\nurl\n(\nhttp://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata\n))\n\n\n\n# Define UI for application that plots features of movies\n\nui \n-\n fluidPage\n(\n\n\n  \n# Sidebar layout with a input and output definitions\n\n  sidebarLayout\n(\n\n\n    \n# Inputs\n\n    sidebarPanel\n(\n\n\n      \n# Select variable for y-axis\n\n      selectInput\n(\ninputId \n=\n \ny\n,\n \n                  label \n=\n \nY-axis:\n,\n\n                  choices \n=\n \nc\n(\nimdb_rating\n,\n \nimdb_num_votes\n,\n \ncritics_score\n,\n \naudience_score\n,\n \nruntime\n),\n \n                  selected \n=\n \naudience_score\n),\n\n\n      \n# Select variable for x-axis\n\n      selectInput\n(\ninputId \n=\n \nx\n,\n \n                  label \n=\n \nX-axis:\n,\n\n                  choices \n=\n \nc\n(\nimdb_rating\n,\n \nimdb_num_votes\n,\n \ncritics_score\n,\n \naudience_score\n,\n \nruntime\n),\n \n                  selected \n=\n \ncritics_score\n),\n\n\n      \n# Set alpha level\n\n      sliderInput\n(\ninputId \n=\n \nalpha\n,\n \n                  label \n=\n \nAlpha:\n,\n \n                  min \n=\n \n0\n,\n max \n=\n \n1\n,\n \n                  value \n=\n \n0.5\n)\n\n    \n),\n\n\n    \n# Outputs\n\n    mainPanel\n(\n\n      plotOutput\n(\noutputId \n=\n \nscatterplot\n)\n\n    \n)\n\n  \n)\n\n\n)\n\n\n\n# Define server function required to create the scatterplot\n\nserver \n-\n \nfunction\n(\ninput\n,\n output\n)\n \n{\n\n\n  \n# Create scatterplot object the plotOutput function is expecting\n\n  output\n$\nscatterplot \n-\n renderPlot\n({\n\n    ggplot\n(\ndata \n=\n movies\n,\n aes_string\n(\nx \n=\n input\n$\nx\n,\n y \n=\n input\n$\ny\n))\n \n+\n\n      geom_point\n(\nalpha \n=\n input\n$\nalpha\n)\n\n  \n})\n\n\n}\n\n\n\n# Create the Shiny app object\n\nshinyApp\n(\nui \n=\n ui\n,\n server \n=\n server\n)\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\nlibrary\n(\nshiny\n)\n\n\nlibrary\n(\nggplot2\n)\n\n\nload\n(\nurl\n(\nhttp://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata\n))\n\n\n\n# Define UI for application that plots features of movies\n\nui \n-\n fluidPage\n(\n\n\n  \n# Sidebar layout with a input and output definitions\n\n  sidebarLayout\n(\n\n\n    \n# Inputs\n\n    sidebarPanel\n(\n\n\n      \n# Select variable for y-axis\n\n      selectInput\n(\ninputId \n=\n \ny\n,\n\n                  label \n=\n \nY-axis:\n,\n\n                  choices \n=\n \nc\n(\nimdb_rating\n,\n \nimdb_num_votes\n,\n \ncritics_score\n,\n \naudience_score\n,\n \nruntime\n),\n\n                  selected \n=\n \naudience_score\n),\n\n\n      \n# Select variable for x-axis\n\n      selectInput\n(\ninputId \n=\n \nx\n,\n\n                  label \n=\n \nX-axis:\n,\n\n                  choices \n=\n \nc\n(\nimdb_rating\n,\n \nimdb_num_votes\n,\n \ncritics_score\n,\n \naudience_score\n,\n \nruntime\n),\n\n                  selected \n=\n \ncritics_score\n)\n\n    \n),\n\n\n    \n# Outputs\n\n    mainPanel\n(\n\n      plotOutput\n(\noutputId \n=\n \nscatterplot\n),\n\n      plotOutput\n(\noutputId \n=\n \ndensityplot\n,\n height \n=\n \n200\n)\n\n    \n)\n\n  \n)\n\n\n)\n\n\n\n# Define server function required to create the scatterplot\n\nserver \n-\n \nfunction\n(\ninput\n,\n output\n)\n \n{\n\n\n  \n# Create scatterplot\n\n  output\n$\nscatterplot \n-\n renderPlot\n({\n\n    ggplot\n(\ndata \n=\n movies\n,\n aes_string\n(\nx \n=\n input\n$\nx\n,\n y \n=\n input\n$\ny\n))\n \n+\n\n      geom_point\n()\n\n  \n})\n\n\n  \n# Create densityplot\n\n  output\n$\ndensityplot \n-\n renderPlot\n({\n\n    ggplot\n(\ndata \n=\n movies\n,\n aes_string\n(\nx \n=\n input\n$\nx\n))\n \n+\n\n      geom_density\n()\n\n  \n})\n\n\n\n}\n\n\n\n# Create the Shiny app object\n\nshinyApp\n(\nui \n=\n ui\n,\n server \n=\n server\n)\n\n\n\n\n\n\n\nUI Inputs\n\n\nShiny provides a wide selection of input widgets:\n\n\ncheckboxInput\n\n\nAdd a checkbox input to specify whether the data plotted should be shown in a data table.\n\n\n\n\nUI\n: Add an input widget that the user can interact with to check/uncheck the box\n\n\nUI\n: Add an output defining where the data table should appear\n\n\nServer\n: Add a reactive expression that creates the data table \nif\n the checkbox is checked\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\nlibrary\n(\nshiny\n)\n\n\nlibrary\n(\ndplyr\n)\n\n\nlibrary\n(\nDT\n)\n\n\n\nload\n(\nurl\n(\nhttp://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata\n))\n\n\nn_total \n-\n \nnrow\n(\nmovies\n)\n\n\n\n# Define UI for application that plots features of movies\n\nui \n-\n fluidPage\n(\n\n\n  \n# Sidebar layout with a input and output definitions\n\n  sidebarLayout\n(\n\n\n    \n# Inputs\n\n    sidebarPanel\n(\n\n\n      \n# Text instructions\n\n      HTML\n(\npaste\n(\nEnter a value between 1 and\n,\n n_total\n)),\n\n\n      \n# Numeric input for sample size\n\n      numericInput\n(\ninputId \n=\n \nn\n,\n\n                   label \n=\n \nSample size:\n,\n\n                   min \n=\n \n1\n,\n\n                   max \n=\n n_total\n,\n\n                   value \n=\n \n30\n,\n\n                   step \n=\n \n1\n)\n\n\n    \n),\n\n\n    \n# Output: Show data table\n\n    mainPanel\n(\n\n      DT\n::\ndataTableOutput\n(\noutputId \n=\n \nmoviestable\n)\n\n    \n)\n\n  \n)\n\n\n)\n\n\n\n# Define server function required to create the scatterplot\n\nserver \n-\n \nfunction\n(\ninput\n,\n output\n)\n \n{\n\n\n  \n# Create data table\n\n  output\n$\nmoviestable \n-\n DT\n::\nrenderDataTable\n({\n\n    req\n(\ninput\n$\nn\n)\n\n    movies_sample \n-\n movies \n%\n%\n\n      sample_n\n(\ninput\n$\nn\n)\n \n%\n%\n\n      select\n(\ntitle\n:\nstudio\n)\n\n    DT\n::\ndatatable\n(\ndata \n=\n movies_sample\n,\n \n                  options \n=\n \nlist\n(\npageLength \n=\n \n10\n),\n \n                  rownames \n=\n \nFALSE\n)\n\n  \n})\n\n\n\n}\n\n\n\n# Create a Shiny app object\n\nshinyApp\n(\nui \n=\n ui\n,\n server \n=\n server\n)\n\n\n\n\n\n\n\n\n\n\n\nThe \nreq()\n function\n\n\nIf you delete the numeric value from the checkbox, you will encounter an error: \nError: size is not a numeric or integer vector\n. In order to avoid such errors, which users of your app could very easily encounter, we need to hold back the output from being calculated if the input is missing. The \nreq\n function\n is the simplest and best way to do this, it ensures that values are available (\"truthy\") before proceeding with a calculation or action. If any of the given values is not truthy, the operation is stopped by raising a \"silent\" exception (not logged by Shiny, nor displayed in the Shiny app's UI).\n\n\n\n\nselectInput\n: Multiple Selection\n\n\nThe following app can be used to display movies from selected studios. There are 211 unique studios represented in this dataset, we need a better way to select than to scroll through such a long list, and we address that with the \nselectize\n option, which will suggest names of studios as you type them.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\nlibrary\n(\nshiny\n)\n\n\nlibrary\n(\nggplot2\n)\n\n\nlibrary\n(\ndplyr\n)\n\n\nlibrary\n(\nDT\n)\n\n\nload\n(\nurl\n(\nhttp://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata\n))\n\nall_studios \n-\n \nsort\n(\nunique\n(\nmovies\n$\nstudio\n))\n\n\n\n# UI\n\nui \n-\n fluidPage\n(\n\n    sidebarLayout\n(\n\n\n    \n# Input(s)\n\n    sidebarPanel\n(\n\n      selectInput\n(\ninputId \n=\n \nstudio\n,\n\n                  label \n=\n \nSelect studio:\n,\n\n                  choices \n=\n all_studios\n,\n\n                  selected \n=\n \n20th Century Fox\n,\n\n                  multiple \n=\n \nTRUE\n,\n\n                  selectize \n=\n \nTRUE\n)\n\n    \n),\n\n\n    \n# Output(s)\n\n    mainPanel\n(\n\n      DT\n::\ndataTableOutput\n(\noutputId \n=\n \nmoviestable\n)\n\n    \n)\n\n  \n)\n\n\n)\n\n\n\n# Server\n\nserver \n-\n \nfunction\n(\ninput\n,\n output\n)\n \n{\n\n\n  \n# Create data table\n\n  output\n$\nmoviestable \n-\n DT\n::\nrenderDataTable\n({\n\n    req\n(\ninput\n$\nstudio\n)\n\n    movies_from_selected_studios \n-\n movies \n%\n%\n\n      filter\n(\nstudio \n%in%\n input\n$\nstudio\n)\n \n%\n%\n\n      select\n(\ntitle\n:\nstudio\n)\n\n    DT\n::\ndatatable\n(\ndata \n=\n movies_from_selected_studios\n,\n \n                  options \n=\n \nlist\n(\npageLength \n=\n \n10\n),\n \n                  rownames \n=\n \nFALSE\n)\n\n  \n})\n\n\n\n}\n\n\n\n# Create a Shiny app object\n\nshinyApp\n(\nui \n=\n ui\n,\n server \n=\n server\n)\n\n\n\n\n\n\n\n\n\ndateRangeInput\n\n\nThe following app is coded to show the selected movies between two given dates using \ndateRangeInput\n. This input will yield a vector (\ninput$date\n) of length two, the first element is the start date and the second is the end date. \n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\nlibrary\n(\nshiny\n)\n\n\nlibrary\n(\ndplyr\n)\n\n\nlibrary\n(\nggplot2\n)\n\n\nload\n(\nurl\n(\nhttp://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata\n))\n\n\nmin_date \n-\n \nmin\n(\nmovies\n$\nthtr_rel_date\n)\n\nmax_date \n-\n \nmax\n(\nmovies\n$\nthtr_rel_date\n)\n\n\n\n# UI\n\nui \n-\n fluidPage\n(\n\n    sidebarLayout\n(\n\n\n    \n# Input(s)\n\n    sidebarPanel\n(\n\n\n      \n# Explanatory text\n\n      HTML\n(\npaste0\n(\nMovies released between the following dates will be plotted. \n\n\n                  Pick dates between \n,\n min_date\n,\n \n and \n,\n max_date\n,\n \n.\n)),\n\n\n      \n# Break for visual separation\n\n      br\n(),\n br\n(),\n\n\n      \n# Date input\n\n      dateRangeInput\n(\ninputId \n=\n \ndate\n,\n\n                label \n=\n \nSelect dates:\n,\n\n                start \n=\n \n2013-01-01\n,\n\n                end\n=\n \n2014-01-01\n,\n\n                startview \n=\n \nyear\n,\n\n                min \n=\n min_date\n,\n max \n=\n max_date\n)\n\n    \n),\n\n\n    \n# Output(s)\n\n    mainPanel\n(\n\n      plotOutput\n(\noutputId \n=\n \nscatterplot\n)\n\n    \n)\n\n  \n)\n\n\n)\n\n\n\n# Server\n\nserver \n-\n \nfunction\n(\ninput\n,\n output\n)\n \n{\n\n\n  \n# Create the plot\n\n  output\n$\nscatterplot \n-\n renderPlot\n({\n\n    req\n(\ninput\n$\ndate\n)\n\n    movies_selected_date \n-\n movies \n%\n%\n\n      filter\n(\nthtr_rel_date \n=\n \nas.POSIXct\n(\ninput\n$\ndate\n[\n1\n])\n \n thtr_rel_date \n=\n \nas.POSIXct\n(\ninput\n$\ndate\n[\n2\n]))\n\n    ggplot\n(\ndata \n=\n movies_selected_date\n,\n aes\n(\nx \n=\n critics_score\n,\n y \n=\n audience_score\n,\n color \n=\n mpaa_rating\n))\n \n+\n\n      geom_point\n()\n\n  \n})\n\n\n\n}\n\n\n# Create a Shiny app object\n\nshinyApp\n(\nui \n=\n ui\n,\n server \n=\n server\n)\n\n\n\n\n\n\n\n\n\nRendering Functions\n\n\nShiny provides a wide selection of input widgets, each of which works with a render function:\n\n\nrenderTable\n\n\nAdd a table beneath the plot displaying summary statistics for a new variable: \nscore_ratio = audience_score / critics_score\n.\n\n\n\n\nCalculate the new variable\n\n\nUI\n: Add an input widget that the user can interact with to check boxes for selected title types\n\n\nUI\n: Add an output defining where the summary table should appear\n\n\nServer\n: Add a reactive expression that creates the summary table\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n(\n...\n)\n\n\n\n# Create new variable:\n\n\n# ratio of critics and audience scores\n\nmovies \n-\n movies \n%\n%\n\n mutate\n(\nscore_ratio \n=\n audience_score \n/\n critics_score\n)\n\n\n\n(\n...\n)\n\n\n\n# Subset for title types\n\ncheckboxGroupInput\n(\ninputId \n=\n \nselected_title_type\n,\n\n label \n=\n \nSelect title type:\n,\n\n choices \n=\n \nlevels\n(\nmovies\n$\ntitle_type\n),\n\n selected \n=\n \nlevels\n(\nmovies\n$\ntitle_type\n))\n\n\n\n(\n...\n)\n\n\nmainPanel\n(\n\n \n# Show scatterplot\n\n plotOutput\n(\noutputId \n=\n \nscatterplot\n),\n\n \n# Show data table\n\n tableOutput\n(\noutputId \n=\n \nsummarytable\n)\n\n\n)\n\n\n\n(\n...\n)\n\n\noutput\n$\nsummarytable \n-\n renderTable\n(\n\n \n{\nmovies \n%\n%\n\n filter\n(\ntitle_type \n%in%\n input\n$\nselected_title_type\n)\n \n%\n%\n\n group_by\n(\nmpaa_rating\n)\n \n%\n%\n\n summarise\n(\nMean \n=\n \nmean\n(\nscore_ratio\n),\n SD \n=\n sd\n(\nscore_ratio\n),\n n \n=\n n\n())},\n\n striped \n=\n \nTRUE\n,\n spacing \n=\n \nl\n,\n align \n=\n \nlccr\n,\n digits \n=\n \n4\n,\n width \n=\n \n90%\n,\n\n caption \n=\n \nScore ratio (audience / critics\n scores) summary statistics by\n\n\nMPAA rating.\n\n\n)\n\n\n\n(\n...\n)\n\n\n\n\n\n\n\nrenderText\n\n\nIn this app the user selects \nx\n and \ny\n variables for the scatterplot and also a \ntextOutput\n which prints the correlation between the two selected variables as well as some informational text.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\nlibrary\n(\nshiny\n)\n\n\nlibrary\n(\nggplot2\n)\n\n\nload\n(\nurl\n(\nhttp://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata\n))\n\n\n\n# UI\n\nui \n-\n fluidPage\n(\n\n  sidebarLayout\n(\n\n\n    \n# Input(s)\n\n    sidebarPanel\n(\n\n\n      \n# Select variable for y-axis\n\n      selectInput\n(\ninputId \n=\n \ny\n,\n \n                  label \n=\n \nY-axis:\n,\n\n                  choices \n=\n \nc\n(\nimdb_rating\n,\n \nimdb_num_votes\n,\n \ncritics_score\n,\n \naudience_score\n,\n \nruntime\n),\n \n                  selected \n=\n \naudience_score\n),\n\n\n      \n# Select variable for x-axis\n\n      selectInput\n(\ninputId \n=\n \nx\n,\n \n                  label \n=\n \nX-axis:\n,\n\n                  choices \n=\n \nc\n(\nimdb_rating\n,\n \nimdb_num_votes\n,\n \ncritics_score\n,\n \naudience_score\n,\n \nruntime\n),\n \n                  selected \n=\n \ncritics_score\n)\n\n    \n),\n\n\n    \n# Outputs\n\n    mainPanel\n(\n\n      plotOutput\n(\noutputId \n=\n \nscatterplot\n),\n\n      textOutput\n(\noutputId \n=\n \ncorrelation\n)\n\n    \n)\n\n  \n)\n\n\n)\n\n\n\n# Server\n\nserver \n-\n \nfunction\n(\ninput\n,\n output\n)\n \n{\n\n\n  \n# Create scatterplot object the plotOutput function is expecting\n\n  output\n$\nscatterplot \n-\n renderPlot\n({\n\n    ggplot\n(\ndata \n=\n movies\n,\n aes_string\n(\nx \n=\n input\n$\nx\n,\n y \n=\n input\n$\ny\n))\n \n+\n\n      geom_point\n()\n\n  \n})\n\n\n  \n# Create text output stating the correlation between the two ploted \n\n  output\n$\ncorrelation \n-\n renderText\n({\n\n    r \n-\n \nround\n(\ncor\n(\nmovies\n[,\n input\n$\nx\n],\n movies\n[,\n input\n$\ny\n],\n use \n=\n \npairwise\n),\n \n3\n)\n\n    \npaste0\n(\nCorrelation = \n,\n r\n,\n \n. Note: If the relationship between the two variables is not linear, the correlation coefficient will not be meaningful.\n)\n\n  \n})\n\n\n}\n\n\n\n# Create a Shiny app object\n\nshinyApp\n(\nui \n=\n ui\n,\n server \n=\n server\n)\n\n\n\n\n\n\n\n\n\nRecap of Output/Rendering Functions\n\n\n\n\nShiny has a variety of \nrender*\n functions with corresponding \n*Ourput\n functions to create and display outputs\n\n\nrender*\n functions can take on multiple arguments, the first being the expression for the desired output\n\n\nThe expression in the \nrender*\n function should be wrapped in curly braces\n\n\n\n\nUI Outputs\n\n\nplotOutput\n\n\nSelect points on the plot via brushing, and report the selected points in a data table underneath the plot. Brushing means that the user will be able to draw a rectangle in the plotting area and drag it around.\n\n\n\n\nUI\n: Add functionality to \nplotOutput\n to select points via brushing\n\n\nUI\n: Add an output defining where the data table should appear\n\n\nServer\n: Add a reactive expression that creates the data table for the selected points\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n(\n...\n)\n\n\n\n# Show scatterplot with brushing capability\n\nplotOutput\n(\noutputId \n=\n \nscatterplot\n,\n brush \n=\n \nplot_brush\n)\n\n\n\n(\n...\n)\n\n\n\n# Show data table\n\nDT\n::\ndataTableOutput\n(\noutputId \n=\n \nmoviestable\n)\n\n\n\n(\n...\n)\n\n\n \n# Print data table\n\n output\n$\nmoviestable \n-\n DT\n::\nrenderDataTable\n({\n\n brushedPoints\n(\nmovies\n,\n input\n$\nplot_brush\n)\n \n%\n%\n\n select\n(\ntitle\n,\n audience_score\n,\n critics_score\n)\n\n \n})\n\n\n \n(\n...\n)\n\n\n\n\n\n\n\n\n\nIn addition to brushing, users can also interact with plots by hovering over them as in the following example. \n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n# Load packages\n\n\nlibrary\n(\nshiny\n)\n\n\nlibrary\n(\nggplot2\n)\n\n\nlibrary\n(\ntidyverse\n)\n\n\nlibrary\n(\nDT\n)\n\n\n\n# Load data\n\n\nload\n(\nurl\n(\nhttp://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata\n))\n\n\n\n# Define UI for application that plots features of movies\n\nui \n-\n fluidPage\n(\n\n\n  br\n(),\n\n\n  \n# Sidebar layout with a input and output definitions\n\n  sidebarLayout\n(\n\n    \n# Inputs\n\n    sidebarPanel\n(\n\n      \n# Select variable for y-axis\n\n      selectInput\n(\ninputId \n=\n \ny\n,\n label \n=\n \nY-axis:\n,\n\n                  choices \n=\n \nc\n(\nimdb_rating\n,\n \nimdb_num_votes\n,\n \ncritics_score\n,\n \naudience_score\n,\n \nruntime\n),\n\n                  selected \n=\n \naudience_score\n),\n\n      \n# Select variable for x-axis\n\n      selectInput\n(\ninputId \n=\n \nx\n,\n label \n=\n \nX-axis:\n,\n\n                  choices \n=\n \nc\n(\nimdb_rating\n,\n \nimdb_num_votes\n,\n \ncritics_score\n,\n \naudience_score\n,\n \nruntime\n),\n\n                  selected \n=\n \ncritics_score\n)\n\n    \n),\n\n\n    \n# Output:\n\n    mainPanel\n(\n\n      \n# Show scatterplot with brushing capability\n\n      plotOutput\n(\noutputId \n=\n \nscatterplot\n,\n hover \n=\n \nplot_hover\n),\n\n      \n# Show data table\n\n      dataTableOutput\n(\noutputId \n=\n \nmoviestable\n),\n\n      br\n()\n\n    \n)\n\n  \n)\n\n\n)\n\n\n\n# Define server function required to create the scatterplot\n\nserver \n-\n \nfunction\n(\ninput\n,\n output\n)\n \n{\n\n\n  \n# Create scatterplot object the plotOutput function is expecting\n\n  output\n$\nscatterplot \n-\n renderPlot\n({\n\n    ggplot\n(\ndata \n=\n movies\n,\n aes_string\n(\nx \n=\n input\n$\nx\n,\n y \n=\n input\n$\ny\n))\n \n+\n\n      geom_point\n()\n\n  \n})\n\n\n  \n# Create data table\n\n  output\n$\nmoviestable \n-\n DT\n::\nrenderDataTable\n({\n\n    nearPoints\n(\nmovies\n,\n input\n$\nplot_hover\n)\n \n%\n%\n \n      select\n(\ntitle\n,\n audience_score\n,\n critics_score\n)\n\n  \n})\n\n\n\n}\n\n\n\n# Create a Shiny app object\n\nshinyApp\n(\nui \n=\n ui\n,\n server \n=\n server\n)\n\n\n\n\n\n\n\n\n\nverbatimtextOutput\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\nlibrary\n(\nshiny\n)\n\n\nlibrary\n(\ndplyr\n)\n\n\nlibrary\n(\nggplot2\n)\n\n\nload\n(\nurl\n(\nhttp://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata\n))\n\n\n\n# UI\n\nui \n-\n fluidPage\n(\n\n  sidebarLayout\n(\n\n\n    \n# Input(s)\n\n    sidebarPanel\n(\n\n\n      \n# Select variable for y-axis\n\n      selectInput\n(\ninputId \n=\n \ny\n,\n\n                  label \n=\n \nY-axis:\n,\n\n                  choices \n=\n \nc\n(\nimdb_rating\n,\n \nimdb_num_votes\n,\n \ncritics_score\n,\n \naudience_score\n,\n \nruntime\n),\n\n                  selected \n=\n \naudience_score\n),\n\n\n      \n# Select variable for x-axis\n\n      selectInput\n(\ninputId \n=\n \nx\n,\n\n                  label \n=\n \nX-axis:\n,\n\n                  choices \n=\n \nc\n(\nimdb_rating\n,\n \nimdb_num_votes\n,\n \ncritics_score\n,\n \naudience_score\n,\n \nruntime\n),\n\n                  selected \n=\n \ncritics_score\n)\n\n\n    \n),\n\n\n    \n# Output(s)\n\n    mainPanel\n(\n\n      plotOutput\n(\noutputId \n=\n \nscatterplot\n),\n\n      textOutput\n(\noutputId \n=\n \navg_x\n),\n \n# avg of x\n\n      textOutput\n(\noutputId \n=\n \navg_y\n),\n \n# avg of y\n\n      verbatimTextOutput\n(\noutputId \n=\n \nlmoutput\n)\n \n# regression output\n\n    \n)\n\n  \n)\n\n\n)\n\n\n\n# Server\n\nserver \n-\n \nfunction\n(\ninput\n,\n output\n)\n \n{\n\n\n  \n# Create scatterplot\n\n  output\n$\nscatterplot \n-\n renderPlot\n({\n\n    ggplot\n(\ndata \n=\n movies\n,\n aes_string\n(\nx \n=\n input\n$\nx\n,\n y \n=\n input\n$\ny\n))\n \n+\n\n      geom_point\n()\n\n  \n})\n\n\n  \n# Calculate average of x\n\n  output\n$\navg_x \n-\n renderText\n({\n\n    avg_x \n-\n movies \n%\n%\n pull\n(\ninput\n$\nx\n)\n \n%\n%\n \nmean\n()\n \n%\n%\n \nround\n(\n2\n)\n\n    \npaste\n(\nAverage\n,\n input\n$\nx\n,\n \n=\n,\n avg_x\n)\n\n  \n})\n\n\n  \n# Calculate average of y\n\n  output\n$\navg_y \n-\n renderText\n({\n\n    avg_y \n-\n movies \n%\n%\n pull\n(\ninput\n$\ny\n)\n \n%\n%\n \nmean\n()\n \n%\n%\n \nround\n(\n2\n)\n\n    \npaste\n(\nAverage\n,\n input\n$\ny\n,\n \n=\n,\n avg_y\n)\n\n  \n})\n\n\n  \n# Create regression output\n\n  output\n$\nlmoutput \n-\n renderPrint\n({\n\n    x \n-\n movies \n%\n%\n pull\n(\ninput\n$\nx\n)\n\n    y \n-\n movies \n%\n%\n pull\n(\ninput\n$\ny\n)\n\n    summ \n-\n \nsummary\n(\nlm\n(\ny \n~\n x\n,\n data \n=\n movies\n))\n \n    \nprint\n(\nsumm\n,\n digits \n=\n \n3\n,\n signif.stars \n=\n \nFALSE\n)\n\n  \n})\n\n\n\n}\n\n\n\n# Create a Shiny app object\n\nshinyApp\n(\nui \n=\n ui\n,\n server \n=\n server\n)\n\n\n\n\n\n\n\n\n\nhtmlOutput\n\n\nIn the previous example the app reported averages of selected \nx\n and \ny\n variables as two separate outputs. An alternative approach would be to combine them into a single, multi-line output. For this purpose, in the next example values calculated in app chunk in the \npaste()\n command are used to create customized HTML output with specified formatting obtaining the same result.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\nlibrary\n(\nshiny\n)\n\n\nlibrary\n(\ndplyr\n)\n\n\nlibrary\n(\nggplot2\n)\n\n\nload\n(\nurl\n(\nhttp://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata\n))\n\n\n\n# UI\n\nui \n-\n fluidPage\n(\n\n  sidebarLayout\n(\n\n\n    \n# Input(s)\n\n    sidebarPanel\n(\n\n\n      \n# Select variable for y-axis\n\n      selectInput\n(\ninputId \n=\n \ny\n,\n\n                  label \n=\n \nY-axis:\n,\n\n                  choices \n=\n \nc\n(\nimdb_rating\n,\n \nimdb_num_votes\n,\n \ncritics_score\n,\n \naudience_score\n,\n \nruntime\n),\n\n                  selected \n=\n \naudience_score\n),\n\n\n      \n# Select variable for x-axis\n\n      selectInput\n(\ninputId \n=\n \nx\n,\n\n                  label \n=\n \nX-axis:\n,\n\n                  choices \n=\n \nc\n(\nimdb_rating\n,\n \nimdb_num_votes\n,\n \ncritics_score\n,\n \naudience_score\n,\n \nruntime\n),\n\n                  selected \n=\n \ncritics_score\n)\n\n\n    \n),\n\n\n    \n# Output(s)\n\n    mainPanel\n(\n\n      plotOutput\n(\noutputId \n=\n \nscatterplot\n),\n\n      htmlOutput\n(\noutputId \n=\n \navgs\n),\n\n      verbatimTextOutput\n(\noutputId \n=\n \nlmoutput\n)\n \n# regression output\n\n    \n)\n\n  \n)\n\n\n)\n\n\n\n# Server\n\nserver \n-\n \nfunction\n(\ninput\n,\n output\n)\n \n{\n\n\n  \n# Create scatterplot\n\n  output\n$\nscatterplot \n-\n renderPlot\n({\n\n    ggplot\n(\ndata \n=\n movies\n,\n aes_string\n(\nx \n=\n input\n$\nx\n,\n y \n=\n input\n$\ny\n))\n \n+\n\n      geom_point\n()\n\n  \n})\n\n\n  \n# Calculate average of x\n\n  output\n$\navgs \n-\n renderUI\n({\n\n    avg_x \n-\n movies \n%\n%\n pull\n(\ninput\n$\nx\n)\n \n%\n%\n \nmean\n()\n \n%\n%\n \nround\n(\n2\n)\n\n    str_x \n-\n \npaste\n(\nAverage\n,\n input\n$\nx\n,\n \n=\n,\n avg_x\n)\n\n    avg_y \n-\n movies \n%\n%\n pull\n(\ninput\n$\ny\n)\n \n%\n%\n \nmean\n()\n \n%\n%\n \nround\n(\n2\n)\n\n    str_y \n-\n \npaste\n(\nAverage\n,\n input\n$\ny\n,\n \n=\n,\n avg_y\n)\n\n    HTML\n(\npaste\n(\nstr_x\n,\n str_y\n,\n sep \n=\n \nbr/\n))\n\n  \n})\n\n\n  \n# Create regression output\n\n  output\n$\nlmoutput \n-\n renderPrint\n({\n\n    x \n-\n movies \n%\n%\n pull\n(\ninput\n$\nx\n)\n\n    y \n-\n movies \n%\n%\n pull\n(\ninput\n$\ny\n)\n\n    \nprint\n(\nsummary\n(\nlm\n(\ny \n~\n x\n,\n data \n=\n movies\n)),\n digits \n=\n \n3\n,\n signif.stars \n=\n \nFALSE\n)\n\n  \n})\n\n\n\n}\n\n\n\n# Create a Shiny app object\n\nshinyApp\n(\nui \n=\n ui\n,\n server \n=\n server\n)\n\n\n\n\n\n\n\nDownload data with \ndownloadButton\n\n\nIn this app you get to specify the file type and the variables included in the file you will download. For downloading from a Shiny app we use the \ndownloadHandler\n function in the server and \ndownloadButton\n or \ndownloadLink\n function in the UI.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\nlibrary\n(\nshiny\n)\n\n\nlibrary\n(\ndplyr\n)\n\n\nlibrary\n(\nreadr\n)\n\n\nload\n(\nurl\n(\nhttp://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata\n))\n\n\n\n# UI\n\nui \n-\n fluidPage\n(\n\n  sidebarLayout\n(\n\n\n    \n# Input(s)\n\n    sidebarPanel\n(\n\n\n      \n# Select filetype\n\n      radioButtons\n(\ninputId \n=\n \nfiletype\n,\n\n                   label \n=\n \nSelect filetype:\n,\n\n                   choices \n=\n \nc\n(\ncsv\n,\n \ntsv\n),\n\n                   selected \n=\n \ncsv\n),\n\n\n      \n# Select variables to download\n\n      checkboxGroupInput\n(\ninputId \n=\n \nselected_var\n,\n\n                  label \n=\n \nSelect variables:\n,\n\n                  choices \n=\n \nnames\n(\nmovies\n),\n\n                  selected \n=\n \nc\n(\ntitle\n))\n\n\n    \n),\n\n\n    \n# Output(s)\n\n    mainPanel\n(\n\n      HTML\n(\nSelect filetype and variables, then hit \nDownload data\n.\n),\n\n      downloadButton\n(\ndownload_data\n,\n \nDownload data\n)\n\n    \n)\n\n  \n)\n\n\n)\n\n\n\n# Server\n\nserver \n-\n \nfunction\n(\ninput\n,\n output\n)\n \n{\n\n\n  \n# Download file\n\n  output\n$\ndownload_data \n-\n downloadHandler\n(\n\n    filename \n=\n \nfunction\n()\n \n{\n\n      \npaste0\n(\nmovies.\n,\n input\n$\nfiletype\n)\n\n      \n},\n\n    content \n=\n \nfunction\n(\nfile\n)\n \n{\n \n      \nif\n(\ninput\n$\nfiletype \n==\n \ncsv\n){\n \n        write_csv\n(\nmovies \n%\n%\n select\n(\ninput\n$\nselected_var\n),\n \nfile\n)\n \n        \n}\n\n      \nif\n(\ninput\n$\nfiletype \n==\n \ntsv\n){\n \n        write_tsv\n(\nmovies \n%\n%\n select\n(\ninput\n$\nselected_var\n),\n \nfile\n)\n \n        \n}\n\n    \n}\n\n  \n)\n\n\n\n}\n\n\n\n# Create a Shiny app object\n\nshinyApp\n(\nui \n=\n ui\n,\n server \n=\n server\n)", 
            "title": "Inputs, outputs, and rendering functions"
        }, 
        {
            "location": "/R-with-Shiny/input-output/#reactive-flow", 
            "text": "Suppose you have a  slinderInput in your app with the  inputId=\"alpha\" . The value of this input is stored in  input$alpha  so when the user moves around the slider the value of the  alpha input is updated in th einput list. Reactivity automatically occurs when an input value is used to render an output object.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51 library ( shiny )  library ( ggplot2 )  load ( url ( http://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata ))  # Define UI for application that plots features of movies \nui  -  fluidPage ( \n\n   # Sidebar layout with a input and output definitions \n  sidebarLayout ( \n\n     # Inputs \n    sidebarPanel ( \n\n       # Select variable for y-axis \n      selectInput ( inputId  =   y ,  \n                  label  =   Y-axis: , \n                  choices  =   c ( imdb_rating ,   imdb_num_votes ,   critics_score ,   audience_score ,   runtime ),  \n                  selected  =   audience_score ), \n\n       # Select variable for x-axis \n      selectInput ( inputId  =   x ,  \n                  label  =   X-axis: , \n                  choices  =   c ( imdb_rating ,   imdb_num_votes ,   critics_score ,   audience_score ,   runtime ),  \n                  selected  =   critics_score ), \n\n       # Set alpha level \n      sliderInput ( inputId  =   alpha ,  \n                  label  =   Alpha: ,  \n                  min  =   0 ,  max  =   1 ,  \n                  value  =   0.5 ) \n     ), \n\n     # Outputs \n    mainPanel ( \n      plotOutput ( outputId  =   scatterplot ) \n     ) \n   )  )  # Define server function required to create the scatterplot \nserver  -   function ( input ,  output )   { \n\n   # Create scatterplot object the plotOutput function is expecting \n  output $ scatterplot  -  renderPlot ({ \n    ggplot ( data  =  movies ,  aes_string ( x  =  input $ x ,  y  =  input $ y ))   + \n      geom_point ( alpha  =  input $ alpha ) \n   })  }  # Create the Shiny app object \nshinyApp ( ui  =  ui ,  server  =  server )     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53 library ( shiny )  library ( ggplot2 )  load ( url ( http://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata ))  # Define UI for application that plots features of movies \nui  -  fluidPage ( \n\n   # Sidebar layout with a input and output definitions \n  sidebarLayout ( \n\n     # Inputs \n    sidebarPanel ( \n\n       # Select variable for y-axis \n      selectInput ( inputId  =   y , \n                  label  =   Y-axis: , \n                  choices  =   c ( imdb_rating ,   imdb_num_votes ,   critics_score ,   audience_score ,   runtime ), \n                  selected  =   audience_score ), \n\n       # Select variable for x-axis \n      selectInput ( inputId  =   x , \n                  label  =   X-axis: , \n                  choices  =   c ( imdb_rating ,   imdb_num_votes ,   critics_score ,   audience_score ,   runtime ), \n                  selected  =   critics_score ) \n     ), \n\n     # Outputs \n    mainPanel ( \n      plotOutput ( outputId  =   scatterplot ), \n      plotOutput ( outputId  =   densityplot ,  height  =   200 ) \n     ) \n   )  )  # Define server function required to create the scatterplot \nserver  -   function ( input ,  output )   { \n\n   # Create scatterplot \n  output $ scatterplot  -  renderPlot ({ \n    ggplot ( data  =  movies ,  aes_string ( x  =  input $ x ,  y  =  input $ y ))   + \n      geom_point () \n   }) \n\n   # Create densityplot \n  output $ densityplot  -  renderPlot ({ \n    ggplot ( data  =  movies ,  aes_string ( x  =  input $ x ))   + \n      geom_density () \n   })  }  # Create the Shiny app object \nshinyApp ( ui  =  ui ,  server  =  server )", 
            "title": "Reactive Flow"
        }, 
        {
            "location": "/R-with-Shiny/input-output/#ui-inputs", 
            "text": "Shiny provides a wide selection of input widgets:", 
            "title": "UI Inputs"
        }, 
        {
            "location": "/R-with-Shiny/input-output/#checkboxinput", 
            "text": "Add a checkbox input to specify whether the data plotted should be shown in a data table.   UI : Add an input widget that the user can interact with to check/uncheck the box  UI : Add an output defining where the data table should appear  Server : Add a reactive expression that creates the data table  if  the checkbox is checked    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55 library ( shiny )  library ( dplyr )  library ( DT )  load ( url ( http://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata )) \n\nn_total  -   nrow ( movies )  # Define UI for application that plots features of movies \nui  -  fluidPage ( \n\n   # Sidebar layout with a input and output definitions \n  sidebarLayout ( \n\n     # Inputs \n    sidebarPanel ( \n\n       # Text instructions \n      HTML ( paste ( Enter a value between 1 and ,  n_total )), \n\n       # Numeric input for sample size \n      numericInput ( inputId  =   n , \n                   label  =   Sample size: , \n                   min  =   1 , \n                   max  =  n_total , \n                   value  =   30 , \n                   step  =   1 ) \n\n     ), \n\n     # Output: Show data table \n    mainPanel ( \n      DT :: dataTableOutput ( outputId  =   moviestable ) \n     ) \n   )  )  # Define server function required to create the scatterplot \nserver  -   function ( input ,  output )   { \n\n   # Create data table \n  output $ moviestable  -  DT :: renderDataTable ({ \n    req ( input $ n ) \n    movies_sample  -  movies  % % \n      sample_n ( input $ n )   % % \n      select ( title : studio ) \n    DT :: datatable ( data  =  movies_sample ,  \n                  options  =   list ( pageLength  =   10 ),  \n                  rownames  =   FALSE ) \n   })  }  # Create a Shiny app object \nshinyApp ( ui  =  ui ,  server  =  server )      The  req()  function  If you delete the numeric value from the checkbox, you will encounter an error:  Error: size is not a numeric or integer vector . In order to avoid such errors, which users of your app could very easily encounter, we need to hold back the output from being calculated if the input is missing. The  req  function  is the simplest and best way to do this, it ensures that values are available (\"truthy\") before proceeding with a calculation or action. If any of the given values is not truthy, the operation is stopped by raising a \"silent\" exception (not logged by Shiny, nor displayed in the Shiny app's UI).", 
            "title": "checkboxInput"
        }, 
        {
            "location": "/R-with-Shiny/input-output/#selectinput-multiple-selection", 
            "text": "The following app can be used to display movies from selected studios. There are 211 unique studios represented in this dataset, we need a better way to select than to scroll through such a long list, and we address that with the  selectize  option, which will suggest names of studios as you type them.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46 library ( shiny )  library ( ggplot2 )  library ( dplyr )  library ( DT )  load ( url ( http://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata )) \nall_studios  -   sort ( unique ( movies $ studio ))  # UI \nui  -  fluidPage ( \n    sidebarLayout ( \n\n     # Input(s) \n    sidebarPanel ( \n      selectInput ( inputId  =   studio , \n                  label  =   Select studio: , \n                  choices  =  all_studios , \n                  selected  =   20th Century Fox , \n                  multiple  =   TRUE , \n                  selectize  =   TRUE ) \n     ), \n\n     # Output(s) \n    mainPanel ( \n      DT :: dataTableOutput ( outputId  =   moviestable ) \n     ) \n   )  )  # Server \nserver  -   function ( input ,  output )   { \n\n   # Create data table \n  output $ moviestable  -  DT :: renderDataTable ({ \n    req ( input $ studio ) \n    movies_from_selected_studios  -  movies  % % \n      filter ( studio  %in%  input $ studio )   % % \n      select ( title : studio ) \n    DT :: datatable ( data  =  movies_from_selected_studios ,  \n                  options  =   list ( pageLength  =   10 ),  \n                  rownames  =   FALSE ) \n   })  }  # Create a Shiny app object \nshinyApp ( ui  =  ui ,  server  =  server )", 
            "title": "selectInput: Multiple Selection"
        }, 
        {
            "location": "/R-with-Shiny/input-output/#daterangeinput", 
            "text": "The following app is coded to show the selected movies between two given dates using  dateRangeInput . This input will yield a vector ( input$date ) of length two, the first element is the start date and the second is the end date.    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53 library ( shiny )  library ( dplyr )  library ( ggplot2 )  load ( url ( http://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata )) \n\nmin_date  -   min ( movies $ thtr_rel_date ) \nmax_date  -   max ( movies $ thtr_rel_date )  # UI \nui  -  fluidPage ( \n    sidebarLayout ( \n\n     # Input(s) \n    sidebarPanel ( \n\n       # Explanatory text \n      HTML ( paste0 ( Movies released between the following dates will be plotted.                     Pick dates between  ,  min_date ,    and  ,  max_date ,   . )), \n\n       # Break for visual separation \n      br (),  br (), \n\n       # Date input \n      dateRangeInput ( inputId  =   date , \n                label  =   Select dates: , \n                start  =   2013-01-01 , \n                end =   2014-01-01 , \n                startview  =   year , \n                min  =  min_date ,  max  =  max_date ) \n     ), \n\n     # Output(s) \n    mainPanel ( \n      plotOutput ( outputId  =   scatterplot ) \n     ) \n   )  )  # Server \nserver  -   function ( input ,  output )   { \n\n   # Create the plot \n  output $ scatterplot  -  renderPlot ({ \n    req ( input $ date ) \n    movies_selected_date  -  movies  % % \n      filter ( thtr_rel_date  =   as.POSIXct ( input $ date [ 1 ])    thtr_rel_date  =   as.POSIXct ( input $ date [ 2 ])) \n    ggplot ( data  =  movies_selected_date ,  aes ( x  =  critics_score ,  y  =  audience_score ,  color  =  mpaa_rating ))   + \n      geom_point () \n   })  }  # Create a Shiny app object \nshinyApp ( ui  =  ui ,  server  =  server )", 
            "title": "dateRangeInput"
        }, 
        {
            "location": "/R-with-Shiny/input-output/#rendering-functions", 
            "text": "Shiny provides a wide selection of input widgets, each of which works with a render function:", 
            "title": "Rendering Functions"
        }, 
        {
            "location": "/R-with-Shiny/input-output/#rendertable", 
            "text": "Add a table beneath the plot displaying summary statistics for a new variable:  score_ratio = audience_score / critics_score .   Calculate the new variable  UI : Add an input widget that the user can interact with to check boxes for selected title types  UI : Add an output defining where the summary table should appear  Server : Add a reactive expression that creates the summary table    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37 ( ... )  # Create new variable:  # ratio of critics and audience scores \nmovies  -  movies  % % \n mutate ( score_ratio  =  audience_score  /  critics_score )  ( ... )  # Subset for title types \ncheckboxGroupInput ( inputId  =   selected_title_type , \n label  =   Select title type: , \n choices  =   levels ( movies $ title_type ), \n selected  =   levels ( movies $ title_type ))  ( ... ) \n\nmainPanel ( \n  # Show scatterplot \n plotOutput ( outputId  =   scatterplot ), \n  # Show data table \n tableOutput ( outputId  =   summarytable )  )  ( ... ) \n\noutput $ summarytable  -  renderTable ( \n  { movies  % % \n filter ( title_type  %in%  input $ selected_title_type )   % % \n group_by ( mpaa_rating )   % % \n summarise ( Mean  =   mean ( score_ratio ),  SD  =  sd ( score_ratio ),  n  =  n ())}, \n striped  =   TRUE ,  spacing  =   l ,  align  =   lccr ,  digits  =   4 ,  width  =   90% , \n caption  =   Score ratio (audience / critics  scores) summary statistics by  MPAA rating.  )  ( ... )", 
            "title": "renderTable"
        }, 
        {
            "location": "/R-with-Shiny/input-output/#rendertext", 
            "text": "In this app the user selects  x  and  y  variables for the scatterplot and also a  textOutput  which prints the correlation between the two selected variables as well as some informational text.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50 library ( shiny )  library ( ggplot2 )  load ( url ( http://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata ))  # UI \nui  -  fluidPage ( \n  sidebarLayout ( \n\n     # Input(s) \n    sidebarPanel ( \n\n       # Select variable for y-axis \n      selectInput ( inputId  =   y ,  \n                  label  =   Y-axis: , \n                  choices  =   c ( imdb_rating ,   imdb_num_votes ,   critics_score ,   audience_score ,   runtime ),  \n                  selected  =   audience_score ), \n\n       # Select variable for x-axis \n      selectInput ( inputId  =   x ,  \n                  label  =   X-axis: , \n                  choices  =   c ( imdb_rating ,   imdb_num_votes ,   critics_score ,   audience_score ,   runtime ),  \n                  selected  =   critics_score ) \n     ), \n\n     # Outputs \n    mainPanel ( \n      plotOutput ( outputId  =   scatterplot ), \n      textOutput ( outputId  =   correlation ) \n     ) \n   )  )  # Server \nserver  -   function ( input ,  output )   { \n\n   # Create scatterplot object the plotOutput function is expecting \n  output $ scatterplot  -  renderPlot ({ \n    ggplot ( data  =  movies ,  aes_string ( x  =  input $ x ,  y  =  input $ y ))   + \n      geom_point () \n   }) \n\n   # Create text output stating the correlation between the two ploted  \n  output $ correlation  -  renderText ({ \n    r  -   round ( cor ( movies [,  input $ x ],  movies [,  input $ y ],  use  =   pairwise ),   3 ) \n     paste0 ( Correlation =  ,  r ,   . Note: If the relationship between the two variables is not linear, the correlation coefficient will not be meaningful. ) \n   })  }  # Create a Shiny app object \nshinyApp ( ui  =  ui ,  server  =  server )", 
            "title": "renderText"
        }, 
        {
            "location": "/R-with-Shiny/input-output/#recap-of-outputrendering-functions", 
            "text": "Shiny has a variety of  render*  functions with corresponding  *Ourput  functions to create and display outputs  render*  functions can take on multiple arguments, the first being the expression for the desired output  The expression in the  render*  function should be wrapped in curly braces", 
            "title": "Recap of Output/Rendering Functions"
        }, 
        {
            "location": "/R-with-Shiny/input-output/#ui-outputs", 
            "text": "", 
            "title": "UI Outputs"
        }, 
        {
            "location": "/R-with-Shiny/input-output/#plotoutput", 
            "text": "Select points on the plot via brushing, and report the selected points in a data table underneath the plot. Brushing means that the user will be able to draw a rectangle in the plotting area and drag it around.   UI : Add functionality to  plotOutput  to select points via brushing  UI : Add an output defining where the data table should appear  Server : Add a reactive expression that creates the data table for the selected points    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19 ( ... )  # Show scatterplot with brushing capability \nplotOutput ( outputId  =   scatterplot ,  brush  =   plot_brush )  ( ... )  # Show data table \nDT :: dataTableOutput ( outputId  =   moviestable )  ( ... ) \n\n  # Print data table \n output $ moviestable  -  DT :: renderDataTable ({ \n brushedPoints ( movies ,  input $ plot_brush )   % % \n select ( title ,  audience_score ,  critics_score ) \n  }) \n\n  ( ... )     In addition to brushing, users can also interact with plots by hovering over them as in the following example.    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58 # Load packages  library ( shiny )  library ( ggplot2 )  library ( tidyverse )  library ( DT )  # Load data  load ( url ( http://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata ))  # Define UI for application that plots features of movies \nui  -  fluidPage ( \n\n  br (), \n\n   # Sidebar layout with a input and output definitions \n  sidebarLayout ( \n     # Inputs \n    sidebarPanel ( \n       # Select variable for y-axis \n      selectInput ( inputId  =   y ,  label  =   Y-axis: , \n                  choices  =   c ( imdb_rating ,   imdb_num_votes ,   critics_score ,   audience_score ,   runtime ), \n                  selected  =   audience_score ), \n       # Select variable for x-axis \n      selectInput ( inputId  =   x ,  label  =   X-axis: , \n                  choices  =   c ( imdb_rating ,   imdb_num_votes ,   critics_score ,   audience_score ,   runtime ), \n                  selected  =   critics_score ) \n     ), \n\n     # Output: \n    mainPanel ( \n       # Show scatterplot with brushing capability \n      plotOutput ( outputId  =   scatterplot ,  hover  =   plot_hover ), \n       # Show data table \n      dataTableOutput ( outputId  =   moviestable ), \n      br () \n     ) \n   )  )  # Define server function required to create the scatterplot \nserver  -   function ( input ,  output )   { \n\n   # Create scatterplot object the plotOutput function is expecting \n  output $ scatterplot  -  renderPlot ({ \n    ggplot ( data  =  movies ,  aes_string ( x  =  input $ x ,  y  =  input $ y ))   + \n      geom_point () \n   }) \n\n   # Create data table \n  output $ moviestable  -  DT :: renderDataTable ({ \n    nearPoints ( movies ,  input $ plot_hover )   % %  \n      select ( title ,  audience_score ,  critics_score ) \n   })  }  # Create a Shiny app object \nshinyApp ( ui  =  ui ,  server  =  server )", 
            "title": "plotOutput"
        }, 
        {
            "location": "/R-with-Shiny/input-output/#verbatimtextoutput", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69 library ( shiny )  library ( dplyr )  library ( ggplot2 )  load ( url ( http://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata ))  # UI \nui  -  fluidPage ( \n  sidebarLayout ( \n\n     # Input(s) \n    sidebarPanel ( \n\n       # Select variable for y-axis \n      selectInput ( inputId  =   y , \n                  label  =   Y-axis: , \n                  choices  =   c ( imdb_rating ,   imdb_num_votes ,   critics_score ,   audience_score ,   runtime ), \n                  selected  =   audience_score ), \n\n       # Select variable for x-axis \n      selectInput ( inputId  =   x , \n                  label  =   X-axis: , \n                  choices  =   c ( imdb_rating ,   imdb_num_votes ,   critics_score ,   audience_score ,   runtime ), \n                  selected  =   critics_score ) \n\n     ), \n\n     # Output(s) \n    mainPanel ( \n      plotOutput ( outputId  =   scatterplot ), \n      textOutput ( outputId  =   avg_x ),   # avg of x \n      textOutput ( outputId  =   avg_y ),   # avg of y \n      verbatimTextOutput ( outputId  =   lmoutput )   # regression output \n     ) \n   )  )  # Server \nserver  -   function ( input ,  output )   { \n\n   # Create scatterplot \n  output $ scatterplot  -  renderPlot ({ \n    ggplot ( data  =  movies ,  aes_string ( x  =  input $ x ,  y  =  input $ y ))   + \n      geom_point () \n   }) \n\n   # Calculate average of x \n  output $ avg_x  -  renderText ({ \n    avg_x  -  movies  % %  pull ( input $ x )   % %   mean ()   % %   round ( 2 ) \n     paste ( Average ,  input $ x ,   = ,  avg_x ) \n   }) \n\n   # Calculate average of y \n  output $ avg_y  -  renderText ({ \n    avg_y  -  movies  % %  pull ( input $ y )   % %   mean ()   % %   round ( 2 ) \n     paste ( Average ,  input $ y ,   = ,  avg_y ) \n   }) \n\n   # Create regression output \n  output $ lmoutput  -  renderPrint ({ \n    x  -  movies  % %  pull ( input $ x ) \n    y  -  movies  % %  pull ( input $ y ) \n    summ  -   summary ( lm ( y  ~  x ,  data  =  movies ))  \n     print ( summ ,  digits  =   3 ,  signif.stars  =   FALSE ) \n   })  }  # Create a Shiny app object \nshinyApp ( ui  =  ui ,  server  =  server )", 
            "title": "verbatimtextOutput"
        }, 
        {
            "location": "/R-with-Shiny/input-output/#htmloutput", 
            "text": "In the previous example the app reported averages of selected  x  and  y  variables as two separate outputs. An alternative approach would be to combine them into a single, multi-line output. For this purpose, in the next example values calculated in app chunk in the  paste()  command are used to create customized HTML output with specified formatting obtaining the same result.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64 library ( shiny )  library ( dplyr )  library ( ggplot2 )  load ( url ( http://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata ))  # UI \nui  -  fluidPage ( \n  sidebarLayout ( \n\n     # Input(s) \n    sidebarPanel ( \n\n       # Select variable for y-axis \n      selectInput ( inputId  =   y , \n                  label  =   Y-axis: , \n                  choices  =   c ( imdb_rating ,   imdb_num_votes ,   critics_score ,   audience_score ,   runtime ), \n                  selected  =   audience_score ), \n\n       # Select variable for x-axis \n      selectInput ( inputId  =   x , \n                  label  =   X-axis: , \n                  choices  =   c ( imdb_rating ,   imdb_num_votes ,   critics_score ,   audience_score ,   runtime ), \n                  selected  =   critics_score ) \n\n     ), \n\n     # Output(s) \n    mainPanel ( \n      plotOutput ( outputId  =   scatterplot ), \n      htmlOutput ( outputId  =   avgs ), \n      verbatimTextOutput ( outputId  =   lmoutput )   # regression output \n     ) \n   )  )  # Server \nserver  -   function ( input ,  output )   { \n\n   # Create scatterplot \n  output $ scatterplot  -  renderPlot ({ \n    ggplot ( data  =  movies ,  aes_string ( x  =  input $ x ,  y  =  input $ y ))   + \n      geom_point () \n   }) \n\n   # Calculate average of x \n  output $ avgs  -  renderUI ({ \n    avg_x  -  movies  % %  pull ( input $ x )   % %   mean ()   % %   round ( 2 ) \n    str_x  -   paste ( Average ,  input $ x ,   = ,  avg_x ) \n    avg_y  -  movies  % %  pull ( input $ y )   % %   mean ()   % %   round ( 2 ) \n    str_y  -   paste ( Average ,  input $ y ,   = ,  avg_y ) \n    HTML ( paste ( str_x ,  str_y ,  sep  =   br/ )) \n   }) \n\n   # Create regression output \n  output $ lmoutput  -  renderPrint ({ \n    x  -  movies  % %  pull ( input $ x ) \n    y  -  movies  % %  pull ( input $ y ) \n     print ( summary ( lm ( y  ~  x ,  data  =  movies )),  digits  =   3 ,  signif.stars  =   FALSE ) \n   })  }  # Create a Shiny app object \nshinyApp ( ui  =  ui ,  server  =  server )", 
            "title": "htmlOutput"
        }, 
        {
            "location": "/R-with-Shiny/input-output/#download-data-with-downloadbutton", 
            "text": "In this app you get to specify the file type and the variables included in the file you will download. For downloading from a Shiny app we use the  downloadHandler  function in the server and  downloadButton  or  downloadLink  function in the UI.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56 library ( shiny )  library ( dplyr )  library ( readr )  load ( url ( http://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata ))  # UI \nui  -  fluidPage ( \n  sidebarLayout ( \n\n     # Input(s) \n    sidebarPanel ( \n\n       # Select filetype \n      radioButtons ( inputId  =   filetype , \n                   label  =   Select filetype: , \n                   choices  =   c ( csv ,   tsv ), \n                   selected  =   csv ), \n\n       # Select variables to download \n      checkboxGroupInput ( inputId  =   selected_var , \n                  label  =   Select variables: , \n                  choices  =   names ( movies ), \n                  selected  =   c ( title )) \n\n     ), \n\n     # Output(s) \n    mainPanel ( \n      HTML ( Select filetype and variables, then hit  Download data . ), \n      downloadButton ( download_data ,   Download data ) \n     ) \n   )  )  # Server \nserver  -   function ( input ,  output )   { \n\n   # Download file \n  output $ download_data  -  downloadHandler ( \n    filename  =   function ()   { \n       paste0 ( movies. ,  input $ filetype ) \n       }, \n    content  =   function ( file )   {  \n       if ( input $ filetype  ==   csv ){  \n        write_csv ( movies  % %  select ( input $ selected_var ),   file )  \n         } \n       if ( input $ filetype  ==   tsv ){  \n        write_tsv ( movies  % %  select ( input $ selected_var ),   file )  \n         } \n     } \n   )  }  # Create a Shiny app object \nshinyApp ( ui  =  ui ,  server  =  server )", 
            "title": "Download data with downloadButton"
        }, 
        {
            "location": "/R-with-Shiny/reactive/", 
            "text": "Reactive Elements\n\n\nThere are three kinds of objects in reactive programming:\n\n\n\n\nReactive Sources\n: user input that comes through a browser interface, typically.\n\n\nReactive Endpoints\n: something that appears in the user's browser window, such as a plot or a table of values. A reactive source can be connected to multiple endpoints, and vice versa.\n\n\nReactive Conductors\n: reactive component between a source and an endpoint. It can be a dependetn (child) and have dependents (parent) while \nsources can only be parents\n and \nendpoints can only be children\n\n\n\n\n\n\nWe can create a reactive data set using the \nreactive()\n function which creates a \ncached expression\n that knows it is out of date when input changes. Remember to check the availability of the predefined input with the \nreq()\n function before doing any calculations that depends on it and surround the expression with curly braces. When you refer to a reactive data set you need to use parentheses after its name, that is, a cached expression, meaning that it only rerun when its inputs change.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\nlibrary\n(\nshiny\n)\n\n\nlibrary\n(\ndplyr\n)\n\n\nlibrary\n(\nreadr\n)\n\n\nload\n(\nurl\n(\nhttp://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata\n))\n\n\n\n# UI\n\nui \n-\n fluidPage\n(\n\n  sidebarLayout\n(\n\n\n    \n# Input(s)\n\n    sidebarPanel\n(\n\n\n      \n# Select filetype\n\n      radioButtons\n(\ninputId \n=\n \nfiletype\n,\n\n                   label \n=\n \nSelect filetype:\n,\n\n                   choices \n=\n \nc\n(\ncsv\n,\n \ntsv\n),\n\n                   selected \n=\n \ncsv\n),\n\n\n      \n# Select variables to download\n\n      checkboxGroupInput\n(\ninputId \n=\n \nselected_var\n,\n\n                         label \n=\n \nSelect variables:\n,\n\n                         choices \n=\n \nnames\n(\nmovies\n),\n\n                         selected \n=\n \nc\n(\ntitle\n))\n\n\n    \n),\n\n\n    \n# Output(s)\n\n    mainPanel\n(\n\n      DT\n::\ndataTableOutput\n(\noutputId \n=\n \nmoviestable\n),\n\n      downloadButton\n(\ndownload_data\n,\n \nDownload data\n)\n\n    \n)\n\n  \n)\n\n\n)\n\n\n\n# Server\n\nserver \n-\n \nfunction\n(\ninput\n,\n output\n)\n \n{\n\n\n  \n# Create reactive data frame\n\n  movies_selected \n-\n reactive\n({\n\n    req\n(\ninput\n$\nselected_var\n)\n\n    movies \n%\n%\n select\n(\ninput\n$\nselected_var\n)\n\n  \n})\n\n\n  \n# Create data table\n\n  output\n$\nmoviestable \n-\n DT\n::\nrenderDataTable\n({\n\n    req\n(\ninput\n$\nselected_var\n)\n\n    DT\n::\ndatatable\n(\ndata \n=\n movies_selected\n(),\n \n                  options \n=\n \nlist\n(\npageLength \n=\n \n10\n),\n \n                  rownames \n=\n \nFALSE\n)\n\n  \n})\n\n\n  \n# Download file\n\n  output\n$\ndownload_data \n-\n downloadHandler\n(\n\n    filename \n=\n \nfunction\n()\n \n{\n\n      \npaste0\n(\nmovies.\n,\n input\n$\nfiletype\n)\n\n    \n},\n\n    content \n=\n \nfunction\n(\nfile\n)\n \n{\n \n      \nif\n(\ninput\n$\nfiletype \n==\n \ncsv\n){\n \n        write_csv\n(\nmovies_selected\n(),\n \nfile\n)\n \n      \n}\n\n      \nif\n(\ninput\n$\nfiletype \n==\n \ntsv\n){\n \n        write_tsv\n(\nmovies_selected\n(),\n \nfile\n)\n \n      \n}\n\n    \n}\n\n  \n)\n\n\n\n}\n\n\n\n# Create a Shiny app object\n\nshinyApp\n(\nui \n=\n ui\n,\n server \n=\n server\n)\n\n\n\n\n\n\n\n\n\nTip\n\n\nThe obvious choice for creating a text output would be \nrenderText\n but if you want to get a little fancier including some HTML to use some text decoration, like bolding and line breaks in the text output, we need a rendering function that generates HTML, which is \nrenderUI\n.\n\n\n\n\n\n\nWhy Using Reactives?\n\n\nBy using a reactive expression for the subsetted data frame, we were able to get away with subsetting once and then using the result twice.\n\n\nIn general, reactive conductors let you not repeat yourself (i.e. avoid copy-and-paste code) and decompose large, complex calculations into smaller pieces to make them more understandable. This benefits are similar to decomposing a large complex R script into a series of small functions that build on each other.\n\n\nFunctions vs Reactives\n\n\nEach time you call a function, R will revaluate it. However, reactive expressions are lazy, they only get executed when their input changes. This means that even if you call a reactive expression multiple times, it only re-executes when its input(s) change(s).\n\n\nUsing many reactive expressions in your app can create a complicated dependency structure. The \nreactlog\n is a graphical representation of this dependency structure, and it also gives you very detailed information about what's happening under the hood as Shiny evaluates your application. To view the \nreactlog\n:\n\n\n\n\nIn a fresh R session and run \noptions(shiny.reactlog = TRUE)\n \n\n\nThen, launch your app as you normally would\n\n\nIn the app, pres \nCtrl+F3\n\n\n\n\n  1\n  2\n  3\n  4\n  5\n  6\n  7\n  8\n  9\n 10\n 11\n 12\n 13\n 14\n 15\n 16\n 17\n 18\n 19\n 20\n 21\n 22\n 23\n 24\n 25\n 26\n 27\n 28\n 29\n 30\n 31\n 32\n 33\n 34\n 35\n 36\n 37\n 38\n 39\n 40\n 41\n 42\n 43\n 44\n 45\n 46\n 47\n 48\n 49\n 50\n 51\n 52\n 53\n 54\n 55\n 56\n 57\n 58\n 59\n 60\n 61\n 62\n 63\n 64\n 65\n 66\n 67\n 68\n 69\n 70\n 71\n 72\n 73\n 74\n 75\n 76\n 77\n 78\n 79\n 80\n 81\n 82\n 83\n 84\n 85\n 86\n 87\n 88\n 89\n 90\n 91\n 92\n 93\n 94\n 95\n 96\n 97\n 98\n 99\n100\nlibrary\n(\nshiny\n)\n\n\nlibrary\n(\nggplot2\n)\n\n\nlibrary\n(\ndplyr\n)\n\n\nlibrary\n(\ntools\n)\n\n\nload\n(\nurl\n(\nhttp://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata\n))\n\n\n\n# Define UI for application that plots features of movies\n\nui \n-\n fluidPage\n(\n\n\n  \n# Application title\n\n  titlePanel\n(\nMovie browser\n),\n\n\n  \n# Sidebar layout with a input and output definitions\n\n  sidebarLayout\n(\n\n\n    \n# Inputs(s)\n\n    sidebarPanel\n(\n\n\n      \n# Select variable for y-axis\n\n      selectInput\n(\ninputId \n=\n \ny\n,\n \n                  label \n=\n \nY-axis:\n,\n\n                  choices \n=\n \nc\n(\nIMDB rating\n \n=\n \nimdb_rating\n,\n \n                              \nIMDB number of votes\n \n=\n \nimdb_num_votes\n,\n \n                              \nCritics Score\n \n=\n \ncritics_score\n,\n \n                              \nAudience Score\n \n=\n \naudience_score\n,\n \n                              \nRuntime\n \n=\n \nruntime\n),\n \n                  selected \n=\n \naudience_score\n),\n\n\n      \n# Select variable for x-axis\n\n      selectInput\n(\ninputId \n=\n \nx\n,\n \n                  label \n=\n \nX-axis:\n,\n\n                  choices \n=\n \nc\n(\nIMDB rating\n \n=\n \nimdb_rating\n,\n \n                              \nIMDB number of votes\n \n=\n \nimdb_num_votes\n,\n \n                              \nCritics Score\n \n=\n \ncritics_score\n,\n \n                              \nAudience Score\n \n=\n \naudience_score\n,\n \n                              \nRuntime\n \n=\n \nruntime\n),\n \n                  selected \n=\n \ncritics_score\n),\n\n\n      \n# Select variable for color\n\n      selectInput\n(\ninputId \n=\n \nz\n,\n \n                  label \n=\n \nColor by:\n,\n\n                  choices \n=\n \nc\n(\nTitle Type\n \n=\n \ntitle_type\n,\n \n                              \nGenre\n \n=\n \ngenre\n,\n \n                              \nMPAA Rating\n \n=\n \nmpaa_rating\n,\n \n                              \nCritics Rating\n \n=\n \ncritics_rating\n,\n \n                              \nAudience Rating\n \n=\n \naudience_rating\n),\n\n                  selected \n=\n \nmpaa_rating\n),\n\n\n      \n# Enter text for plot title\n\n      textInput\n(\ninputId \n=\n \nplot_title\n,\n \n                label \n=\n \nPlot title\n,\n \n                placeholder \n=\n \nEnter text for plot title\n),\n\n\n      \n# Select which types of movies to plot\n\n      checkboxGroupInput\n(\ninputId \n=\n \nselected_type\n,\n\n                         label \n=\n \nSelect movie type(s):\n,\n\n                         choices \n=\n \nc\n(\nDocumentary\n,\n \nFeature Film\n,\n \nTV Movie\n),\n\n                         selected \n=\n \nFeature Film\n)\n\n\n    \n),\n\n\n    \n# Output(s)\n\n    mainPanel\n(\n\n      plotOutput\n(\noutputId \n=\n \nscatterplot\n),\n\n      textOutput\n(\noutputId \n=\n \ndescription\n)\n\n    \n)\n\n  \n)\n\n\n)\n\n\n\n# Server\n\nserver \n-\n \nfunction\n(\ninput\n,\n output\n)\n \n{\n\n\n  \n# Create a subset of data filtering for selected title types\n\n  movies_subset \n-\n reactive\n({\n\n    req\n(\ninput\n$\nselected_type\n)\n\n    filter\n(\nmovies\n,\n title_type \n%in%\n input\n$\nselected_type\n)\n\n  \n})\n\n\n  \n# Convert plot_title toTitleCase\n\n  pretty_plot_title \n-\n reactive\n({\n\n    toTitleCase\n(\ninput\n$\nplot_title\n)\n\n  \n})\n\n\n  \n# Create scatterplot object the plotOutput function is expecting\n\n  output\n$\nscatterplot \n-\n renderPlot\n({\n\n    ggplot\n(\ndata \n=\n movies_subset\n(),\n \n           aes_string\n(\nx \n=\n input\n$\nx\n,\n y \n=\n input\n$\ny\n,\n color \n=\n input\n$\nz\n))\n \n+\n\n      geom_point\n()\n \n+\n\n      labs\n(\ntitle \n=\n pretty_plot_title\n())\n\n  \n})\n\n\n  \n# Create descriptive text\n\n  output\n$\ndescription \n-\n renderText\n({\n\n    \npaste0\n(\nThe plot above titled \n,\n pretty_plot_title\n(),\n \n visualizes the relationship between \n,\n input\n$\nx\n,\n \n and \n,\n input\n$\ny\n,\n \n, conditional on \n,\n input\n$\nz\n,\n \n.\n)\n\n  \n})\n\n\n\n}\n\n\n\n# Create the Shiny app object\n\nshinyApp\n(\nui \n=\n ui\n,\n server \n=\n server\n)\n\n\n\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\nlibrary\n(\nshiny\n)\n\n\nlibrary\n(\nggplot2\n)\n\n\nlibrary\n(\ndplyr\n)\n\n\nload\n(\nurl\n(\nhttp://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata\n))\n\n\n\n# UI\n\nui \n-\n fluidPage\n(\n\n  sidebarLayout\n(\n\n\n    \n# Input(s)\n\n    sidebarPanel\n(\n\n\n      \n# Select variable for y-axis\n\n      selectInput\n(\ninputId \n=\n \ny\n,\n \n                  label \n=\n \nY-axis:\n,\n\n                  choices \n=\n \nc\n(\nIMDB rating\n \n=\n \nimdb_rating\n,\n \n                              \nIMDB number of votes\n \n=\n \nimdb_num_votes\n,\n \n                              \nCritics Score\n \n=\n \ncritics_score\n,\n \n                              \nAudience Score\n \n=\n \naudience_score\n,\n \n                              \nRuntime\n \n=\n \nruntime\n),\n \n                  selected \n=\n \naudience_score\n),\n\n\n      \n# Select variable for x-axis\n\n      selectInput\n(\ninputId \n=\n \nx\n,\n \n                  label \n=\n \nX-axis:\n,\n\n                  choices \n=\n \nc\n(\nIMDB rating\n \n=\n \nimdb_rating\n,\n \n                              \nIMDB number of votes\n \n=\n \nimdb_num_votes\n,\n \n                              \nCritics Score\n \n=\n \ncritics_score\n,\n \n                              \nAudience Score\n \n=\n \naudience_score\n,\n \n                              \nRuntime\n \n=\n \nruntime\n),\n \n                  selected \n=\n \ncritics_score\n),\n\n\n      \n# Select variable for color\n\n      selectInput\n(\ninputId \n=\n \nz\n,\n \n                  label \n=\n \nColor by:\n,\n\n                  choices \n=\n \nc\n(\nTitle Type\n \n=\n \ntitle_type\n,\n \n                              \nGenre\n \n=\n \ngenre\n,\n \n                              \nMPAA Rating\n \n=\n \nmpaa_rating\n,\n \n                              \nCritics Rating\n \n=\n \ncritics_rating\n,\n \n                              \nAudience Rating\n \n=\n \naudience_rating\n),\n\n                  selected \n=\n \nmpaa_rating\n),\n\n\n      \n# Select which types of movies to plot\n\n      checkboxGroupInput\n(\ninputId \n=\n \nselected_type\n,\n\n                         label \n=\n \nSelect movie type(s):\n,\n\n                         choices \n=\n \nc\n(\nDocumentary\n,\n \nFeature Film\n,\n \nTV Movie\n),\n\n                         selected \n=\n \nFeature Film\n),\n\n\n      \n# Select sample size\n\n      numericInput\n(\ninputId \n=\n \nn_samp\n,\n \n                   label \n=\n \nSample size:\n,\n \n                   min \n=\n \n1\n,\n max \n=\n \nnrow\n(\nmovies\n),\n \n                   value \n=\n \n3\n)\n\n    \n),\n\n\n    \n# Output(s)\n\n    mainPanel\n(\n\n      plotOutput\n(\noutputId \n=\n \nscatterplot\n),\n\n      uiOutput\n(\noutputId \n=\n \nn\n)\n\n    \n)\n\n  \n)\n\n\n)\n\n\n\n# Server\n\nserver \n-\n \nfunction\n(\ninput\n,\n output\n)\n \n{\n\n\n  \n# Create a subset of data filtering for selected title types\n\n  movies_subset \n-\n reactive\n({\n\n    req\n(\ninput\n$\nselected_type\n)\n\n    filter\n(\nmovies\n,\n title_type \n%in%\n input\n$\nselected_type\n)\n\n  \n})\n\n\n  \n# Create new df that is n_samp obs from selected type movies\n\n  movies_sample \n-\n reactive\n({\n \n    req\n(\ninput\n$\nn_samp\n)\n\n    sample_n\n(\nmovies_subset\n(),\n input\n$\nn_samp\n)\n\n  \n})\n\n\n  \n# Create scatterplot object the plotOutput function is expecting\n\n  output\n$\nscatterplot \n-\n renderPlot\n({\n\n    ggplot\n(\ndata \n=\n movies_sample\n(),\n aes_string\n(\nx \n=\n input\n$\nx\n,\n y \n=\n input\n$\ny\n,\n color \n=\n input\n$\nz\n))\n \n+\n\n      geom_point\n()\n\n  \n})\n\n\n  \n# Print number of movies plotted\n\n  output\n$\nn \n-\n renderUI\n({\n\n    types \n-\n movies_sample\n()\n$\ntitle_type \n%\n%\n \n      \nfactor\n(\nlevels \n=\n input\n$\nselected_type\n)\n \n    counts \n-\n \ntable\n(\ntypes\n)\n\n    HTML\n(\npaste\n(\nThere are\n,\n counts\n,\n input\n$\nselected_type\n,\n \nmovies plotted in the plot above. \nbr\n))\n\n  \n})\n\n\n\n}\n\n\n\n# Create a Shiny app object\n\nshinyApp\n(\nui \n=\n ui\n,\n server \n=\n server\n)\n\n\n\n\n\n\n\n\n\nReactives and observers\n\n\nHere we discuss implementations of the three different types of reactive objects. As we go through the different implementations, it's recommended to think back to where they appear on the reactive flow chart.\n\n\n\n\n\n\nImplementation of Reactive Sources\n: An implementation of reactive sources is \nreactiveValues()\n. One example of this is user inputs. The input object is a reactive value that looks like a list and contains many individual reactive values that are set by input from the web browser.\n\n\nImplementation of Reactive\n: The implementation of reactive conductors is a \nreactive()\n expression that you can create with the reactive function. An example is the reactive data frame subsets created in the previous example. \n\n\nReactive expressions can access reactive values or other reactive expressions and they return a value. \n\n\nThey are useful for caching the results of any procedure that happens in response to user input.\n\n\n\n\n\n\nImplementation of Reactive\n: The implementation of reactive endpoints is \nobserve()\n. For example, an output object is a reactive observer. Actually, under the hood, a render function returns a reactive expression, and when you assing this reactive expression to an output value, Shiny automatically creates an observer that uses the reactive expression. \n\n\nObservers can access reactive sources and reactive expressions, but they don't return a value.\n\n\nInstead they are used for their side effects, which typically involves sending data to the web browser.\n\n\n\n\n\n\n\n\nReactives vs Observers\n\n\n\n\nSimilarities: they both store expressions that can be executed\n\n\nDifferences:\n\n\nReactive expressions return values, but observers don't\n\n\nObservers (and endpoints in general) eagerly respond to changes in their dependences, but reactive expressions (and conductors in general) do not\n\n\nReactive expressions must not have side effects, while observers are only useful for their side effects\n\n\n\n\n\n\nMost importantly:\n\n\nThe \nreactive()\n function is used when calculating values, without side effects\n\n\nThe \nobserve()\n function is used to perform actions, with side effects\n\n\nDo not use an \nobserve()\n function when calculating a value, and especially don't use \nreactive()\n for performing actions with side effects\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nreactive()\n\n\nobserve()\n\n\n\n\n\n\n\n\n\n\nPurpose\n\n\nCalculations\n\n\nActions\n\n\n\n\n\n\nSide effects\n\n\nForbidden\n\n\nAllowed\n\n\n\n\n\n\n\n\nStop-Trigger-Delay\n\n\nIsolating Reactions\n\n\nSuppose your app has an input widget where users can enter text for the title of the plot. However, you only want the title to update if any ot the other inputs that go into the plot change. You can achieve this by isolating the plot title such that when \ninput$x\n or \ninput$y\n changes, the plot, along with the title, will update. But when only the title input changes, the plot will not update.\n\n\n1\n2\n3\n4\n5\noutput\n$\nscatterplot \n-\n renderPlot\n({\n\n  ggplot\n(\ndata \n=\n movies_subset\n(),\n aes_string\n(\nx \n=\n input\n$\nx\n,\n y \n=\n input\n$\ny\n))\n \n+\n \n  geom_point\n()\n \n+\n\n  labs\n(\ntitle \n=\n isolate\n({\n input\n$\nplot_title \n})\n \n)\n\n\n})\n\n\n\n\n\n\n\nisolate()\n is then used to stop a reaction.\n\n\nTriggering Reactions\n\n\nWhy might one want to explicitly trigger a reaction? Somethimes you might want to wait for a specific action to be taken from the user, like clicking an \nactionButton\n, before calculating an expression or taking an action. A reactive value or expression that is used to trigger other calculations in this way is called an \nevent\n. \n\n\nThese events can be the \nfirst argument\n in the \nobserveEvent\n function. This arguments can be a simple reactive value like an input, a call to a reactive expression, or a complex expresion provided wrapped in curly braces. The \nsecond argument\n is the expression to call whenever the first argument is invalidated. This is similar to saying if event expression happens, call handler expression.\n\n\n1\nobserveEvent\n(\neventExpr\n,\n handlerExpr\n,\n \n...\n)\n\n\n\n\n\n\n\nSuppose your app allows for taking a random sample of the data based on a sample size numeric input. Suppose also that you want to add functionality for the userd to download the random sample they generated \nif\n they press an action button requesting to do so. In the UI we create an action button and in the server we condition the \nobserveEvent\n on the \ninputId\n of that action button. This way R knows to call the expression given in the second argument of \nobserveEvent\n when the user presses the action button. And finally we can delay reactions with \neventReactive\n, which takes similar arguments as \nobserveEvent\n.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n# UI\n\nactionButton\n(\ninputId \n=\n \nwrite_csv\n,\n label \n=\n \nWrite CSV\n)\n\n\n\n# Server\n\nobserveEvent\n(\ninput\n$\nwrite_csv\n,\n \n{\n\n            filename \n-\n \npaste0\n(\nmovies_\n,\n\n                               str_replace_all\n(\nSys.time\n(),\n \n:|\\ \n,\n \n_\n),\n\n                               \n.csv\n)\n\n            write_csv\n(\nmovies_sample\n(),\n path \n=\n filename\n)\n\n            \n}\n\n\n\n\n\n\n\nSuppose your goal is to change how users take random samples in your app (you only want them to get a new sample when an action button that says \"get new sample\" is pressed, not when other things like numeric input defining the size of the sample changes). In the event reactive function, the first argument is the input associated with the action button and the second argument is the sampling code. Then we add one more argument, \nignoreNull\n, which tells R what to do (or what not to do) when the event expression evaluates to Null. For example, what should the app do when the app is first launched and the user has not even interacted with the app yet? If this is set to FALSE, the app will initially perform the action or calculation and then the user can re-initiate it.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n# UI\n\nactionButton\n(\ninputId \n=\n \nget_new_sample\n,\n label \n=\n \nGet new sample\n)\n\n\n\n# Server\n\nmovies_sample \n-\n eventReactive\n(\ninput\n$\nget_new_sample\n,\n \n{\n\n            req\n(\ninput\n$\nn_samp\n)\n\n            sample_n\n(\nmovies_subset\n(),\n input\n$\nn_samp\n)\n\n            \n},\n\n            ignoreNULL \n=\n \nFALSE\n\n\n)\n\n\n\n\n\n\n\nobserveEvent()\n and \neventReactive()\n look and feel very similar. They have the same syntax, same arguments, but they're actually not the same at all!\n\n\n\n\nobserveEvent()\n is to perform an action in response to an event\n\n\neventReactive()\n is used to create a calculated value that only updates in response to an event\n\n\n\n\nThis pair of functions also seem similar to the observe/reactive pair, however, the main differences between them is that \nobserve()\n and \nreactive()\n functions automatically trigger on whatever they access while \nobserveEvent()\n and \neventReactive()\n functions need to be explicitly told what triggers them.\n\n\nIn the following example \nisolate()\n is used to prevent the plot title to be updated until some of the other inputs are updated:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\nlibrary\n(\nshiny\n)\n\n\nlibrary\n(\nggplot2\n)\n\n\nlibrary\n(\ntools\n)\n\n\nload\n(\nurl\n(\nhttp://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata\n))\n\n\n\n# UI\n\nui \n-\n fluidPage\n(\n\n  sidebarLayout\n(\n\n\n    \n# Input\n\n    sidebarPanel\n(\n\n\n      \n# Select variable for y-axis\n\n      selectInput\n(\ninputId \n=\n \ny\n,\n \n                  label \n=\n \nY-axis:\n,\n\n                  choices \n=\n \nc\n(\nIMDB rating\n \n=\n \nimdb_rating\n,\n \n                              \nIMDB number of votes\n \n=\n \nimdb_num_votes\n,\n \n                              \nCritics Score\n \n=\n \ncritics_score\n,\n \n                              \nAudience Score\n \n=\n \naudience_score\n,\n \n                              \nRuntime\n \n=\n \nruntime\n),\n \n                  selected \n=\n \naudience_score\n),\n\n\n      \n# Select variable for x-axis\n\n      selectInput\n(\ninputId \n=\n \nx\n,\n \n                  label \n=\n \nX-axis:\n,\n\n                  choices \n=\n \nc\n(\nIMDB rating\n \n=\n \nimdb_rating\n,\n \n                              \nIMDB number of votes\n \n=\n \nimdb_num_votes\n,\n \n                              \nCritics Score\n \n=\n \ncritics_score\n,\n \n                              \nAudience Score\n \n=\n \naudience_score\n,\n \n                              \nRuntime\n \n=\n \nruntime\n),\n \n                  selected \n=\n \ncritics_score\n),\n\n\n      \n# Select variable for color\n\n      selectInput\n(\ninputId \n=\n \nz\n,\n \n                  label \n=\n \nColor by:\n,\n\n                  choices \n=\n \nc\n(\nTitle Type\n \n=\n \ntitle_type\n,\n \n                              \nGenre\n \n=\n \ngenre\n,\n \n                              \nMPAA Rating\n \n=\n \nmpaa_rating\n,\n \n                              \nCritics Rating\n \n=\n \ncritics_rating\n,\n \n                              \nAudience Rating\n \n=\n \naudience_rating\n),\n\n                  selected \n=\n \nmpaa_rating\n),\n\n\n      \n# Set alpha level\n\n      sliderInput\n(\ninputId \n=\n \nalpha\n,\n \n                  label \n=\n \nAlpha:\n,\n \n                  min \n=\n \n0\n,\n max \n=\n \n1\n,\n \n                  value \n=\n \n0.5\n),\n\n\n      \n# Set point size\n\n      sliderInput\n(\ninputId \n=\n \nsize\n,\n \n                  label \n=\n \nSize:\n,\n \n                  min \n=\n \n0\n,\n max \n=\n \n5\n,\n \n                  value \n=\n \n2\n),\n\n\n      \n# Enter text for plot title\n\n      textInput\n(\ninputId \n=\n \nplot_title\n,\n \n                label \n=\n \nPlot title\n,\n \n                placeholder \n=\n \nEnter text to be used as plot title\n)\n\n\n    \n),\n\n\n    \n# Output:\n\n    mainPanel\n(\n\n      plotOutput\n(\noutputId \n=\n \nscatterplot\n)\n\n    \n)\n\n  \n)\n\n\n)\n\n\n\n# Define server function required to create the scatterplot-\n\nserver \n-\n \nfunction\n(\ninput\n,\n output\n,\n session\n)\n \n{\n\n\n  \n# Create scatterplot object the plotOutput function is expecting \n\n  output\n$\nscatterplot \n-\n renderPlot\n({\n\n    ggplot\n(\ndata \n=\n movies\n,\n aes_string\n(\nx \n=\n input\n$\nx\n,\n y \n=\n input\n$\ny\n,\n color \n=\n input\n$\nz\n))\n \n+\n\n      geom_point\n(\nalpha \n=\n input\n$\nalpha\n,\n size \n=\n input\n$\nsize\n)\n \n+\n\n      labs\n(\ntitle \n=\n isolate\n({\ntoTitleCase\n(\ninput\n$\nplot_title\n)})\n \n)\n\n  \n})\n\n\n\n}\n\n\n\n# Create a Shiny app object\n\nshinyApp\n(\nui \n=\n ui\n,\n server \n=\n server\n)\n\n\n\n\n\n\n\n\n\nIn the following example an \neventReactive()\n function is used to wait until an \nactionButton()\n is clicked to update the plot title:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\nlibrary\n(\nshiny\n)\n\n\nlibrary\n(\nggplot2\n)\n\n\nlibrary\n(\ntools\n)\n\n\nload\n(\nurl\n(\nhttp://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata\n))\n\n\n\n# UI\n\nui \n-\n fluidPage\n(\n\n  sidebarLayout\n(\n\n\n    \n# Input\n\n    sidebarPanel\n(\n\n\n      \n# Select variable for y-axis\n\n      selectInput\n(\ninputId \n=\n \ny\n,\n \n                  label \n=\n \nY-axis:\n,\n\n                  choices \n=\n \nc\n(\nIMDB rating\n \n=\n \nimdb_rating\n,\n \n                              \nIMDB number of votes\n \n=\n \nimdb_num_votes\n,\n \n                              \nCritics Score\n \n=\n \ncritics_score\n,\n \n                              \nAudience Score\n \n=\n \naudience_score\n,\n \n                              \nRuntime\n \n=\n \nruntime\n),\n \n                  selected \n=\n \naudience_score\n),\n\n\n      \n# Select variable for x-axis\n\n      selectInput\n(\ninputId \n=\n \nx\n,\n \n                  label \n=\n \nX-axis:\n,\n\n                  choices \n=\n \nc\n(\nIMDB rating\n \n=\n \nimdb_rating\n,\n \n                              \nIMDB number of votes\n \n=\n \nimdb_num_votes\n,\n \n                              \nCritics Score\n \n=\n \ncritics_score\n,\n \n                              \nAudience Score\n \n=\n \naudience_score\n,\n \n                              \nRuntime\n \n=\n \nruntime\n),\n \n                  selected \n=\n \ncritics_score\n),\n\n\n      \n# Select variable for color\n\n      selectInput\n(\ninputId \n=\n \nz\n,\n \n                  label \n=\n \nColor by:\n,\n\n                  choices \n=\n \nc\n(\nTitle Type\n \n=\n \ntitle_type\n,\n \n                              \nGenre\n \n=\n \ngenre\n,\n \n                              \nMPAA Rating\n \n=\n \nmpaa_rating\n,\n \n                              \nCritics Rating\n \n=\n \ncritics_rating\n,\n \n                              \nAudience Rating\n \n=\n \naudience_rating\n),\n\n                  selected \n=\n \nmpaa_rating\n),\n\n\n      \n# Set alpha level\n\n      sliderInput\n(\ninputId \n=\n \nalpha\n,\n \n                  label \n=\n \nAlpha:\n,\n \n                  min \n=\n \n0\n,\n max \n=\n \n1\n,\n \n                  value \n=\n \n0.5\n),\n\n\n      \n# Set point size\n\n      sliderInput\n(\ninputId \n=\n \nsize\n,\n \n                  label \n=\n \nSize:\n,\n \n                  min \n=\n \n0\n,\n max \n=\n \n5\n,\n \n                  value \n=\n \n2\n),\n\n\n      \n# Enter text for plot title\n\n      textInput\n(\ninputId \n=\n \nplot_title\n,\n \n                label \n=\n \nPlot title\n,\n \n                placeholder \n=\n \nEnter text to be used as plot title\n),\n\n\n      \n# Action button for plot title\n\n      actionButton\n(\ninputId \n=\n \nupdate_plot_title\n,\n \n          label \n=\n \nUpdate plot title\n)\n\n\n    \n),\n\n\n    \n# Output:\n\n    mainPanel\n(\n\n      plotOutput\n(\noutputId \n=\n \nscatterplot\n)\n\n    \n)\n\n  \n)\n\n\n)\n\n\n\n# Define server function required to create the scatterplot-\n\nserver \n-\n \nfunction\n(\ninput\n,\n output\n,\n session\n)\n \n{\n\n\n  \n# New plot title\n\n  new_plot_title \n-\n eventReactive\n(\ninput\n$\nupdate_plot_title\n,\n \n                       \n{\n toTitleCase\n(\ninput\n$\nplot_title\n)\n \n}\n\n                       \n)\n\n\n  \n# Create scatterplot object the plotOutput function is expecting \n\n  output\n$\nscatterplot \n-\n renderPlot\n({\n\n    ggplot\n(\ndata \n=\n movies\n,\n aes_string\n(\nx \n=\n input\n$\nx\n,\n y \n=\n input\n$\ny\n,\n color \n=\n input\n$\nz\n))\n \n+\n\n      geom_point\n(\nalpha \n=\n input\n$\nalpha\n,\n size \n=\n input\n$\nsize\n)\n \n+\n\n      labs\n(\ntitle \n=\n new_plot_title\n()\n \n)\n\n  \n})\n\n\n\n}\n\n\n\n# Create a Shiny app object\n\nshinyApp\n(\nui \n=\n ui\n,\n server \n=\n server\n)\n\n\n\n\n\n\n\n\n\nIn the following example, \nobserveEvent()\n is used to make things happen when an action button is clicked: a message is printed to the console stating how many records are shown and a table is generated with those records. The table output is only printed when action button is clicked, but not when other inputs that go into the creation of that output changes thanks to the \nisolate()\n function.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\nlibrary\n(\nshiny\n)\n\n\nload\n(\nurl\n(\nhttp://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata\n))\n\n\n\n# UI\n\nui \n-\n fluidPage\n(\n\n  sidebarLayout\n(\n\n\n    \n# Input\n\n    sidebarPanel\n(\n\n\n      \n# Numeric input for number of rows to show\n\n      numericInput\n(\ninputId \n=\n \nn_rows\n,\n\n                   label \n=\n \nHow many rows do you want to see?\n,\n\n                   value \n=\n \n10\n),\n\n\n      \n# Action button to show\n\n      actionButton\n(\ninputId \n=\n \nbutton\n,\n \n                   label \n=\n \nShow\n)\n\n\n    \n),\n\n\n    \n# Output:\n\n    mainPanel\n(\n\n      tableOutput\n(\noutputId \n=\n \ndatatable\n)\n\n    \n)\n\n  \n)\n\n\n)\n\n\n\n# Define server function required to create the scatterplot-\n\nserver \n-\n \nfunction\n(\ninput\n,\n output\n,\n session\n)\n \n{\n\n\n  \n# Print a message to the console every time button is pressed\n\n  observeEvent\n(\ninput\n$\nbutton\n,\n \n{\n\n    \ncat\n(\nShowing\n,\n input\n$\nn_rows\n,\n \nrows\\n\n)\n\n  \n})\n\n  \n# Take a reactive dependency on input$button, \n\n  \n# but not on any of the stuff inside the function\n\n  df \n-\n eventReactive\n(\ninput\n$\nbutton\n,\n \n{\n\n    \nhead\n(\nmovies\n,\n input\n$\nn_rows\n)\n\n  \n})\n\n  output\n$\ndatatable \n-\n renderTable\n({\n\n    df\n()\n\n  \n})\n\n\n\n}\n\n\n\n# Create a Shiny app object\n\nshinyApp\n(\nui \n=\n ui\n,\n server \n=\n server\n)\n\n\n\n\n\n\n\n\n\nReactivity Summary\n\n\n\n\nReactives are equivalent to no argument functions. Think about them as functions, think about them as variables that can depend on user input and other reactives.\n\n\nReactives are for reactive values and expressions, observers are for their side effects.\n\n\nDo not define a \nreactive()\n inside a \nrender*()\n function.\n\n\nBe careful with missing parentheses when calling reactive expressions.\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\nlibrary\n(\nshiny\n)\n\n\nui \n-\n fluidPage\n(\n\n  titlePanel\n(\nAdd 2\n),\n\n  sidebarLayout\n(\n\n    sidebarPanel\n(\n sliderInput\n(\nx\n,\n \nSelect x\n,\n min \n=\n \n1\n,\n max \n=\n \n50\n,\n value \n=\n \n30\n)\n \n),\n\n    mainPanel\n(\n textOutput\n(\nx_updated\n)\n \n)\n\n  \n)\n\n\n)\n\n\nadd_2 \n-\n \nfunction\n(\nx\n)\n \n{\n x \n+\n \n2\n \n}\n\n\nserver \n-\n \nfunction\n(\ninput\n,\n output\n)\n \n{\n\n  current_x        \n-\n reactive\n({\nadd_2\n(\ninput\n$\nx\n)})\n\n  output\n$\nx_updated \n-\n renderText\n({\n current_x\n()\n \n})\n\n\n}\n\n\nshinyApp\n(\nui\n,\n server\n)", 
            "title": "Reactive programming"
        }, 
        {
            "location": "/R-with-Shiny/reactive/#reactive-elements", 
            "text": "There are three kinds of objects in reactive programming:   Reactive Sources : user input that comes through a browser interface, typically.  Reactive Endpoints : something that appears in the user's browser window, such as a plot or a table of values. A reactive source can be connected to multiple endpoints, and vice versa.  Reactive Conductors : reactive component between a source and an endpoint. It can be a dependetn (child) and have dependents (parent) while  sources can only be parents  and  endpoints can only be children    We can create a reactive data set using the  reactive()  function which creates a  cached expression  that knows it is out of date when input changes. Remember to check the availability of the predefined input with the  req()  function before doing any calculations that depends on it and surround the expression with curly braces. When you refer to a reactive data set you need to use parentheses after its name, that is, a cached expression, meaning that it only rerun when its inputs change.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70 library ( shiny )  library ( dplyr )  library ( readr )  load ( url ( http://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata ))  # UI \nui  -  fluidPage ( \n  sidebarLayout ( \n\n     # Input(s) \n    sidebarPanel ( \n\n       # Select filetype \n      radioButtons ( inputId  =   filetype , \n                   label  =   Select filetype: , \n                   choices  =   c ( csv ,   tsv ), \n                   selected  =   csv ), \n\n       # Select variables to download \n      checkboxGroupInput ( inputId  =   selected_var , \n                         label  =   Select variables: , \n                         choices  =   names ( movies ), \n                         selected  =   c ( title )) \n\n     ), \n\n     # Output(s) \n    mainPanel ( \n      DT :: dataTableOutput ( outputId  =   moviestable ), \n      downloadButton ( download_data ,   Download data ) \n     ) \n   )  )  # Server \nserver  -   function ( input ,  output )   { \n\n   # Create reactive data frame \n  movies_selected  -  reactive ({ \n    req ( input $ selected_var ) \n    movies  % %  select ( input $ selected_var ) \n   }) \n\n   # Create data table \n  output $ moviestable  -  DT :: renderDataTable ({ \n    req ( input $ selected_var ) \n    DT :: datatable ( data  =  movies_selected (),  \n                  options  =   list ( pageLength  =   10 ),  \n                  rownames  =   FALSE ) \n   }) \n\n   # Download file \n  output $ download_data  -  downloadHandler ( \n    filename  =   function ()   { \n       paste0 ( movies. ,  input $ filetype ) \n     }, \n    content  =   function ( file )   {  \n       if ( input $ filetype  ==   csv ){  \n        write_csv ( movies_selected (),   file )  \n       } \n       if ( input $ filetype  ==   tsv ){  \n        write_tsv ( movies_selected (),   file )  \n       } \n     } \n   )  }  # Create a Shiny app object \nshinyApp ( ui  =  ui ,  server  =  server )     Tip  The obvious choice for creating a text output would be  renderText  but if you want to get a little fancier including some HTML to use some text decoration, like bolding and line breaks in the text output, we need a rendering function that generates HTML, which is  renderUI .", 
            "title": "Reactive Elements"
        }, 
        {
            "location": "/R-with-Shiny/reactive/#why-using-reactives", 
            "text": "By using a reactive expression for the subsetted data frame, we were able to get away with subsetting once and then using the result twice.  In general, reactive conductors let you not repeat yourself (i.e. avoid copy-and-paste code) and decompose large, complex calculations into smaller pieces to make them more understandable. This benefits are similar to decomposing a large complex R script into a series of small functions that build on each other.", 
            "title": "Why Using Reactives?"
        }, 
        {
            "location": "/R-with-Shiny/reactive/#functions-vs-reactives", 
            "text": "Each time you call a function, R will revaluate it. However, reactive expressions are lazy, they only get executed when their input changes. This means that even if you call a reactive expression multiple times, it only re-executes when its input(s) change(s).  Using many reactive expressions in your app can create a complicated dependency structure. The  reactlog  is a graphical representation of this dependency structure, and it also gives you very detailed information about what's happening under the hood as Shiny evaluates your application. To view the  reactlog :   In a fresh R session and run  options(shiny.reactlog = TRUE)    Then, launch your app as you normally would  In the app, pres  Ctrl+F3     1\n  2\n  3\n  4\n  5\n  6\n  7\n  8\n  9\n 10\n 11\n 12\n 13\n 14\n 15\n 16\n 17\n 18\n 19\n 20\n 21\n 22\n 23\n 24\n 25\n 26\n 27\n 28\n 29\n 30\n 31\n 32\n 33\n 34\n 35\n 36\n 37\n 38\n 39\n 40\n 41\n 42\n 43\n 44\n 45\n 46\n 47\n 48\n 49\n 50\n 51\n 52\n 53\n 54\n 55\n 56\n 57\n 58\n 59\n 60\n 61\n 62\n 63\n 64\n 65\n 66\n 67\n 68\n 69\n 70\n 71\n 72\n 73\n 74\n 75\n 76\n 77\n 78\n 79\n 80\n 81\n 82\n 83\n 84\n 85\n 86\n 87\n 88\n 89\n 90\n 91\n 92\n 93\n 94\n 95\n 96\n 97\n 98\n 99\n100 library ( shiny )  library ( ggplot2 )  library ( dplyr )  library ( tools )  load ( url ( http://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata ))  # Define UI for application that plots features of movies \nui  -  fluidPage ( \n\n   # Application title \n  titlePanel ( Movie browser ), \n\n   # Sidebar layout with a input and output definitions \n  sidebarLayout ( \n\n     # Inputs(s) \n    sidebarPanel ( \n\n       # Select variable for y-axis \n      selectInput ( inputId  =   y ,  \n                  label  =   Y-axis: , \n                  choices  =   c ( IMDB rating   =   imdb_rating ,  \n                               IMDB number of votes   =   imdb_num_votes ,  \n                               Critics Score   =   critics_score ,  \n                               Audience Score   =   audience_score ,  \n                               Runtime   =   runtime ),  \n                  selected  =   audience_score ), \n\n       # Select variable for x-axis \n      selectInput ( inputId  =   x ,  \n                  label  =   X-axis: , \n                  choices  =   c ( IMDB rating   =   imdb_rating ,  \n                               IMDB number of votes   =   imdb_num_votes ,  \n                               Critics Score   =   critics_score ,  \n                               Audience Score   =   audience_score ,  \n                               Runtime   =   runtime ),  \n                  selected  =   critics_score ), \n\n       # Select variable for color \n      selectInput ( inputId  =   z ,  \n                  label  =   Color by: , \n                  choices  =   c ( Title Type   =   title_type ,  \n                               Genre   =   genre ,  \n                               MPAA Rating   =   mpaa_rating ,  \n                               Critics Rating   =   critics_rating ,  \n                               Audience Rating   =   audience_rating ), \n                  selected  =   mpaa_rating ), \n\n       # Enter text for plot title \n      textInput ( inputId  =   plot_title ,  \n                label  =   Plot title ,  \n                placeholder  =   Enter text for plot title ), \n\n       # Select which types of movies to plot \n      checkboxGroupInput ( inputId  =   selected_type , \n                         label  =   Select movie type(s): , \n                         choices  =   c ( Documentary ,   Feature Film ,   TV Movie ), \n                         selected  =   Feature Film ) \n\n     ), \n\n     # Output(s) \n    mainPanel ( \n      plotOutput ( outputId  =   scatterplot ), \n      textOutput ( outputId  =   description ) \n     ) \n   )  )  # Server \nserver  -   function ( input ,  output )   { \n\n   # Create a subset of data filtering for selected title types \n  movies_subset  -  reactive ({ \n    req ( input $ selected_type ) \n    filter ( movies ,  title_type  %in%  input $ selected_type ) \n   }) \n\n   # Convert plot_title toTitleCase \n  pretty_plot_title  -  reactive ({ \n    toTitleCase ( input $ plot_title ) \n   }) \n\n   # Create scatterplot object the plotOutput function is expecting \n  output $ scatterplot  -  renderPlot ({ \n    ggplot ( data  =  movies_subset (),  \n           aes_string ( x  =  input $ x ,  y  =  input $ y ,  color  =  input $ z ))   + \n      geom_point ()   + \n      labs ( title  =  pretty_plot_title ()) \n   }) \n\n   # Create descriptive text \n  output $ description  -  renderText ({ \n     paste0 ( The plot above titled  ,  pretty_plot_title (),    visualizes the relationship between  ,  input $ x ,    and  ,  input $ y ,   , conditional on  ,  input $ z ,   . ) \n   })  }  # Create the Shiny app object \nshinyApp ( ui  =  ui ,  server  =  server )      1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96 library ( shiny )  library ( ggplot2 )  library ( dplyr )  load ( url ( http://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata ))  # UI \nui  -  fluidPage ( \n  sidebarLayout ( \n\n     # Input(s) \n    sidebarPanel ( \n\n       # Select variable for y-axis \n      selectInput ( inputId  =   y ,  \n                  label  =   Y-axis: , \n                  choices  =   c ( IMDB rating   =   imdb_rating ,  \n                               IMDB number of votes   =   imdb_num_votes ,  \n                               Critics Score   =   critics_score ,  \n                               Audience Score   =   audience_score ,  \n                               Runtime   =   runtime ),  \n                  selected  =   audience_score ), \n\n       # Select variable for x-axis \n      selectInput ( inputId  =   x ,  \n                  label  =   X-axis: , \n                  choices  =   c ( IMDB rating   =   imdb_rating ,  \n                               IMDB number of votes   =   imdb_num_votes ,  \n                               Critics Score   =   critics_score ,  \n                               Audience Score   =   audience_score ,  \n                               Runtime   =   runtime ),  \n                  selected  =   critics_score ), \n\n       # Select variable for color \n      selectInput ( inputId  =   z ,  \n                  label  =   Color by: , \n                  choices  =   c ( Title Type   =   title_type ,  \n                               Genre   =   genre ,  \n                               MPAA Rating   =   mpaa_rating ,  \n                               Critics Rating   =   critics_rating ,  \n                               Audience Rating   =   audience_rating ), \n                  selected  =   mpaa_rating ), \n\n       # Select which types of movies to plot \n      checkboxGroupInput ( inputId  =   selected_type , \n                         label  =   Select movie type(s): , \n                         choices  =   c ( Documentary ,   Feature Film ,   TV Movie ), \n                         selected  =   Feature Film ), \n\n       # Select sample size \n      numericInput ( inputId  =   n_samp ,  \n                   label  =   Sample size: ,  \n                   min  =   1 ,  max  =   nrow ( movies ),  \n                   value  =   3 ) \n     ), \n\n     # Output(s) \n    mainPanel ( \n      plotOutput ( outputId  =   scatterplot ), \n      uiOutput ( outputId  =   n ) \n     ) \n   )  )  # Server \nserver  -   function ( input ,  output )   { \n\n   # Create a subset of data filtering for selected title types \n  movies_subset  -  reactive ({ \n    req ( input $ selected_type ) \n    filter ( movies ,  title_type  %in%  input $ selected_type ) \n   }) \n\n   # Create new df that is n_samp obs from selected type movies \n  movies_sample  -  reactive ({  \n    req ( input $ n_samp ) \n    sample_n ( movies_subset (),  input $ n_samp ) \n   }) \n\n   # Create scatterplot object the plotOutput function is expecting \n  output $ scatterplot  -  renderPlot ({ \n    ggplot ( data  =  movies_sample (),  aes_string ( x  =  input $ x ,  y  =  input $ y ,  color  =  input $ z ))   + \n      geom_point () \n   }) \n\n   # Print number of movies plotted \n  output $ n  -  renderUI ({ \n    types  -  movies_sample () $ title_type  % %  \n       factor ( levels  =  input $ selected_type )  \n    counts  -   table ( types ) \n    HTML ( paste ( There are ,  counts ,  input $ selected_type ,   movies plotted in the plot above.  br )) \n   })  }  # Create a Shiny app object \nshinyApp ( ui  =  ui ,  server  =  server )", 
            "title": "Functions vs Reactives"
        }, 
        {
            "location": "/R-with-Shiny/reactive/#reactives-and-observers", 
            "text": "Here we discuss implementations of the three different types of reactive objects. As we go through the different implementations, it's recommended to think back to where they appear on the reactive flow chart.    Implementation of Reactive Sources : An implementation of reactive sources is  reactiveValues() . One example of this is user inputs. The input object is a reactive value that looks like a list and contains many individual reactive values that are set by input from the web browser.  Implementation of Reactive : The implementation of reactive conductors is a  reactive()  expression that you can create with the reactive function. An example is the reactive data frame subsets created in the previous example.   Reactive expressions can access reactive values or other reactive expressions and they return a value.   They are useful for caching the results of any procedure that happens in response to user input.    Implementation of Reactive : The implementation of reactive endpoints is  observe() . For example, an output object is a reactive observer. Actually, under the hood, a render function returns a reactive expression, and when you assing this reactive expression to an output value, Shiny automatically creates an observer that uses the reactive expression.   Observers can access reactive sources and reactive expressions, but they don't return a value.  Instead they are used for their side effects, which typically involves sending data to the web browser.", 
            "title": "Reactives and observers"
        }, 
        {
            "location": "/R-with-Shiny/reactive/#reactives-vs-observers", 
            "text": "Similarities: they both store expressions that can be executed  Differences:  Reactive expressions return values, but observers don't  Observers (and endpoints in general) eagerly respond to changes in their dependences, but reactive expressions (and conductors in general) do not  Reactive expressions must not have side effects, while observers are only useful for their side effects    Most importantly:  The  reactive()  function is used when calculating values, without side effects  The  observe()  function is used to perform actions, with side effects  Do not use an  observe()  function when calculating a value, and especially don't use  reactive()  for performing actions with side effects         reactive()  observe()      Purpose  Calculations  Actions    Side effects  Forbidden  Allowed", 
            "title": "Reactives vs Observers"
        }, 
        {
            "location": "/R-with-Shiny/reactive/#stop-trigger-delay", 
            "text": "", 
            "title": "Stop-Trigger-Delay"
        }, 
        {
            "location": "/R-with-Shiny/reactive/#isolating-reactions", 
            "text": "Suppose your app has an input widget where users can enter text for the title of the plot. However, you only want the title to update if any ot the other inputs that go into the plot change. You can achieve this by isolating the plot title such that when  input$x  or  input$y  changes, the plot, along with the title, will update. But when only the title input changes, the plot will not update.  1\n2\n3\n4\n5 output $ scatterplot  -  renderPlot ({ \n  ggplot ( data  =  movies_subset (),  aes_string ( x  =  input $ x ,  y  =  input $ y ))   +  \n  geom_point ()   + \n  labs ( title  =  isolate ({  input $ plot_title  })   )  })    isolate()  is then used to stop a reaction.", 
            "title": "Isolating Reactions"
        }, 
        {
            "location": "/R-with-Shiny/reactive/#triggering-reactions", 
            "text": "Why might one want to explicitly trigger a reaction? Somethimes you might want to wait for a specific action to be taken from the user, like clicking an  actionButton , before calculating an expression or taking an action. A reactive value or expression that is used to trigger other calculations in this way is called an  event .   These events can be the  first argument  in the  observeEvent  function. This arguments can be a simple reactive value like an input, a call to a reactive expression, or a complex expresion provided wrapped in curly braces. The  second argument  is the expression to call whenever the first argument is invalidated. This is similar to saying if event expression happens, call handler expression.  1 observeEvent ( eventExpr ,  handlerExpr ,   ... )    Suppose your app allows for taking a random sample of the data based on a sample size numeric input. Suppose also that you want to add functionality for the userd to download the random sample they generated  if  they press an action button requesting to do so. In the UI we create an action button and in the server we condition the  observeEvent  on the  inputId  of that action button. This way R knows to call the expression given in the second argument of  observeEvent  when the user presses the action button. And finally we can delay reactions with  eventReactive , which takes similar arguments as  observeEvent .   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 # UI \nactionButton ( inputId  =   write_csv ,  label  =   Write CSV )  # Server \nobserveEvent ( input $ write_csv ,   { \n            filename  -   paste0 ( movies_ , \n                               str_replace_all ( Sys.time (),   :|\\  ,   _ ), \n                                .csv ) \n            write_csv ( movies_sample (),  path  =  filename ) \n             }    Suppose your goal is to change how users take random samples in your app (you only want them to get a new sample when an action button that says \"get new sample\" is pressed, not when other things like numeric input defining the size of the sample changes). In the event reactive function, the first argument is the input associated with the action button and the second argument is the sampling code. Then we add one more argument,  ignoreNull , which tells R what to do (or what not to do) when the event expression evaluates to Null. For example, what should the app do when the app is first launched and the user has not even interacted with the app yet? If this is set to FALSE, the app will initially perform the action or calculation and then the user can re-initiate it.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 # UI \nactionButton ( inputId  =   get_new_sample ,  label  =   Get new sample )  # Server \nmovies_sample  -  eventReactive ( input $ get_new_sample ,   { \n            req ( input $ n_samp ) \n            sample_n ( movies_subset (),  input $ n_samp ) \n             }, \n            ignoreNULL  =   FALSE  )    observeEvent()  and  eventReactive()  look and feel very similar. They have the same syntax, same arguments, but they're actually not the same at all!   observeEvent()  is to perform an action in response to an event  eventReactive()  is used to create a calculated value that only updates in response to an event   This pair of functions also seem similar to the observe/reactive pair, however, the main differences between them is that  observe()  and  reactive()  functions automatically trigger on whatever they access while  observeEvent()  and  eventReactive()  functions need to be explicitly told what triggers them.  In the following example  isolate()  is used to prevent the plot title to be updated until some of the other inputs are updated:   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82 library ( shiny )  library ( ggplot2 )  library ( tools )  load ( url ( http://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata ))  # UI \nui  -  fluidPage ( \n  sidebarLayout ( \n\n     # Input \n    sidebarPanel ( \n\n       # Select variable for y-axis \n      selectInput ( inputId  =   y ,  \n                  label  =   Y-axis: , \n                  choices  =   c ( IMDB rating   =   imdb_rating ,  \n                               IMDB number of votes   =   imdb_num_votes ,  \n                               Critics Score   =   critics_score ,  \n                               Audience Score   =   audience_score ,  \n                               Runtime   =   runtime ),  \n                  selected  =   audience_score ), \n\n       # Select variable for x-axis \n      selectInput ( inputId  =   x ,  \n                  label  =   X-axis: , \n                  choices  =   c ( IMDB rating   =   imdb_rating ,  \n                               IMDB number of votes   =   imdb_num_votes ,  \n                               Critics Score   =   critics_score ,  \n                               Audience Score   =   audience_score ,  \n                               Runtime   =   runtime ),  \n                  selected  =   critics_score ), \n\n       # Select variable for color \n      selectInput ( inputId  =   z ,  \n                  label  =   Color by: , \n                  choices  =   c ( Title Type   =   title_type ,  \n                               Genre   =   genre ,  \n                               MPAA Rating   =   mpaa_rating ,  \n                               Critics Rating   =   critics_rating ,  \n                               Audience Rating   =   audience_rating ), \n                  selected  =   mpaa_rating ), \n\n       # Set alpha level \n      sliderInput ( inputId  =   alpha ,  \n                  label  =   Alpha: ,  \n                  min  =   0 ,  max  =   1 ,  \n                  value  =   0.5 ), \n\n       # Set point size \n      sliderInput ( inputId  =   size ,  \n                  label  =   Size: ,  \n                  min  =   0 ,  max  =   5 ,  \n                  value  =   2 ), \n\n       # Enter text for plot title \n      textInput ( inputId  =   plot_title ,  \n                label  =   Plot title ,  \n                placeholder  =   Enter text to be used as plot title ) \n\n     ), \n\n     # Output: \n    mainPanel ( \n      plotOutput ( outputId  =   scatterplot ) \n     ) \n   )  )  # Define server function required to create the scatterplot- \nserver  -   function ( input ,  output ,  session )   { \n\n   # Create scatterplot object the plotOutput function is expecting  \n  output $ scatterplot  -  renderPlot ({ \n    ggplot ( data  =  movies ,  aes_string ( x  =  input $ x ,  y  =  input $ y ,  color  =  input $ z ))   + \n      geom_point ( alpha  =  input $ alpha ,  size  =  input $ size )   + \n      labs ( title  =  isolate ({ toTitleCase ( input $ plot_title )})   ) \n   })  }  # Create a Shiny app object \nshinyApp ( ui  =  ui ,  server  =  server )     In the following example an  eventReactive()  function is used to wait until an  actionButton()  is clicked to update the plot title:   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91 library ( shiny )  library ( ggplot2 )  library ( tools )  load ( url ( http://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata ))  # UI \nui  -  fluidPage ( \n  sidebarLayout ( \n\n     # Input \n    sidebarPanel ( \n\n       # Select variable for y-axis \n      selectInput ( inputId  =   y ,  \n                  label  =   Y-axis: , \n                  choices  =   c ( IMDB rating   =   imdb_rating ,  \n                               IMDB number of votes   =   imdb_num_votes ,  \n                               Critics Score   =   critics_score ,  \n                               Audience Score   =   audience_score ,  \n                               Runtime   =   runtime ),  \n                  selected  =   audience_score ), \n\n       # Select variable for x-axis \n      selectInput ( inputId  =   x ,  \n                  label  =   X-axis: , \n                  choices  =   c ( IMDB rating   =   imdb_rating ,  \n                               IMDB number of votes   =   imdb_num_votes ,  \n                               Critics Score   =   critics_score ,  \n                               Audience Score   =   audience_score ,  \n                               Runtime   =   runtime ),  \n                  selected  =   critics_score ), \n\n       # Select variable for color \n      selectInput ( inputId  =   z ,  \n                  label  =   Color by: , \n                  choices  =   c ( Title Type   =   title_type ,  \n                               Genre   =   genre ,  \n                               MPAA Rating   =   mpaa_rating ,  \n                               Critics Rating   =   critics_rating ,  \n                               Audience Rating   =   audience_rating ), \n                  selected  =   mpaa_rating ), \n\n       # Set alpha level \n      sliderInput ( inputId  =   alpha ,  \n                  label  =   Alpha: ,  \n                  min  =   0 ,  max  =   1 ,  \n                  value  =   0.5 ), \n\n       # Set point size \n      sliderInput ( inputId  =   size ,  \n                  label  =   Size: ,  \n                  min  =   0 ,  max  =   5 ,  \n                  value  =   2 ), \n\n       # Enter text for plot title \n      textInput ( inputId  =   plot_title ,  \n                label  =   Plot title ,  \n                placeholder  =   Enter text to be used as plot title ), \n\n       # Action button for plot title \n      actionButton ( inputId  =   update_plot_title ,  \n          label  =   Update plot title ) \n\n     ), \n\n     # Output: \n    mainPanel ( \n      plotOutput ( outputId  =   scatterplot ) \n     ) \n   )  )  # Define server function required to create the scatterplot- \nserver  -   function ( input ,  output ,  session )   { \n\n   # New plot title \n  new_plot_title  -  eventReactive ( input $ update_plot_title ,  \n                        {  toTitleCase ( input $ plot_title )   } \n                        ) \n\n   # Create scatterplot object the plotOutput function is expecting  \n  output $ scatterplot  -  renderPlot ({ \n    ggplot ( data  =  movies ,  aes_string ( x  =  input $ x ,  y  =  input $ y ,  color  =  input $ z ))   + \n      geom_point ( alpha  =  input $ alpha ,  size  =  input $ size )   + \n      labs ( title  =  new_plot_title ()   ) \n   })  }  # Create a Shiny app object \nshinyApp ( ui  =  ui ,  server  =  server )     In the following example,  observeEvent()  is used to make things happen when an action button is clicked: a message is printed to the console stating how many records are shown and a table is generated with those records. The table output is only printed when action button is clicked, but not when other inputs that go into the creation of that output changes thanks to the  isolate()  function.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48 library ( shiny )  load ( url ( http://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata ))  # UI \nui  -  fluidPage ( \n  sidebarLayout ( \n\n     # Input \n    sidebarPanel ( \n\n       # Numeric input for number of rows to show \n      numericInput ( inputId  =   n_rows , \n                   label  =   How many rows do you want to see? , \n                   value  =   10 ), \n\n       # Action button to show \n      actionButton ( inputId  =   button ,  \n                   label  =   Show ) \n\n     ), \n\n     # Output: \n    mainPanel ( \n      tableOutput ( outputId  =   datatable ) \n     ) \n   )  )  # Define server function required to create the scatterplot- \nserver  -   function ( input ,  output ,  session )   { \n\n   # Print a message to the console every time button is pressed \n  observeEvent ( input $ button ,   { \n     cat ( Showing ,  input $ n_rows ,   rows\\n ) \n   }) \n   # Take a reactive dependency on input$button,  \n   # but not on any of the stuff inside the function \n  df  -  eventReactive ( input $ button ,   { \n     head ( movies ,  input $ n_rows ) \n   }) \n  output $ datatable  -  renderTable ({ \n    df () \n   })  }  # Create a Shiny app object \nshinyApp ( ui  =  ui ,  server  =  server )", 
            "title": "Triggering Reactions"
        }, 
        {
            "location": "/R-with-Shiny/reactive/#reactivity-summary", 
            "text": "Reactives are equivalent to no argument functions. Think about them as functions, think about them as variables that can depend on user input and other reactives.  Reactives are for reactive values and expressions, observers are for their side effects.  Do not define a  reactive()  inside a  render*()  function.  Be careful with missing parentheses when calling reactive expressions.    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18 library ( shiny ) \n\nui  -  fluidPage ( \n  titlePanel ( Add 2 ), \n  sidebarLayout ( \n    sidebarPanel (  sliderInput ( x ,   Select x ,  min  =   1 ,  max  =   50 ,  value  =   30 )   ), \n    mainPanel (  textOutput ( x_updated )   ) \n   )  ) \n\nadd_2  -   function ( x )   {  x  +   2   } \n\nserver  -   function ( input ,  output )   { \n  current_x         -  reactive ({ add_2 ( input $ x )}) \n  output $ x_updated  -  renderText ({  current_x ()   })  } \n\nshinyApp ( ui ,  server )", 
            "title": "Reactivity Summary"
        }, 
        {
            "location": "/R-with-Shiny/custom/", 
            "text": "Shiny comes with a list of functions saved under tags that allow us to access HTML tags and use them to add static (as opposed to reactive) content to our apps. The tags objects in Shiny is a list of a 110 simple functions for constructing HMTL documents. Each of the elements in this list is a function that maps to an HTML tag.\n\n\n1\n2\ntags\n$\nb\n(\nThis is my first app\n)\n\n\nb\nThis is my first app\n/\nb\n\n\n\n\n\n\n\nThe most common used tags are wrapped in their own functions and you can use them without the \ntags$\n list. These are functions like \na()\n for anchor text, \nbr()\n for line break, \ncode()\n for displaying code in monospace form and the heading functions (\nh1()\n, \nh2()\n, etc.).\n\n\nThe following app does a bunch of things: allows users to customize the plot, subsets and samples the data, and reports simple summary statistics and displays the data table. HTML tags are used to add headers and visual separators to make it a bit more clear, at a first glance, what's happening in the app.\n\n\n  1\n  2\n  3\n  4\n  5\n  6\n  7\n  8\n  9\n 10\n 11\n 12\n 13\n 14\n 15\n 16\n 17\n 18\n 19\n 20\n 21\n 22\n 23\n 24\n 25\n 26\n 27\n 28\n 29\n 30\n 31\n 32\n 33\n 34\n 35\n 36\n 37\n 38\n 39\n 40\n 41\n 42\n 43\n 44\n 45\n 46\n 47\n 48\n 49\n 50\n 51\n 52\n 53\n 54\n 55\n 56\n 57\n 58\n 59\n 60\n 61\n 62\n 63\n 64\n 65\n 66\n 67\n 68\n 69\n 70\n 71\n 72\n 73\n 74\n 75\n 76\n 77\n 78\n 79\n 80\n 81\n 82\n 83\n 84\n 85\n 86\n 87\n 88\n 89\n 90\n 91\n 92\n 93\n 94\n 95\n 96\n 97\n 98\n 99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\nlibrary\n(\nshiny\n)\n\n\nlibrary\n(\nggplot2\n)\n\n\nlibrary\n(\nstringr\n)\n\n\nlibrary\n(\ndplyr\n)\n\n\nlibrary\n(\nDT\n)\n\n\nlibrary\n(\ntools\n)\n\n\nload\n(\nurl\n(\nhttp://s3.amazonaws.com/assets.datacamp.com/production/course_4850/datasets/movies.Rdata\n))\n\n\n\n# Define UI for application that plots features of movies\n\nui \n-\n fluidPage\n(\n\n\n  \n# Sidebar layout with a input and output definitions\n\n  sidebarLayout\n(\n\n\n    \n# Inputs\n\n    sidebarPanel\n(\n\n\n      h3\n(\nPlotting\n),\n      \n# Third level header: Plotting\n\n\n      \n# Select variable for y-axis \n\n      selectInput\n(\ninputId \n=\n \ny\n,\n \n                  label \n=\n \nY-axis:\n,\n\n                  choices \n=\n \nc\n(\nIMDB rating\n \n=\n \nimdb_rating\n,\n \n                              \nIMDB number of votes\n \n=\n \nimdb_num_votes\n,\n \n                              \nCritics Score\n \n=\n \ncritics_score\n,\n \n                              \nAudience Score\n \n=\n \naudience_score\n,\n \n                              \nRuntime\n \n=\n \nruntime\n),\n \n                  selected \n=\n \naudience_score\n),\n\n\n      \n# Select variable for x-axis \n\n      selectInput\n(\ninputId \n=\n \nx\n,\n \n                  label \n=\n \nX-axis:\n,\n\n                  choices \n=\n \nc\n(\nIMDB rating\n \n=\n \nimdb_rating\n,\n \n                              \nIMDB number of votes\n \n=\n \nimdb_num_votes\n,\n \n                              \nCritics Score\n \n=\n \ncritics_score\n,\n \n                              \nAudience Score\n \n=\n \naudience_score\n,\n \n                              \nRuntime\n \n=\n \nruntime\n),\n \n                  selected \n=\n \ncritics_score\n),\n\n\n      \n# Select variable for color\n\n      selectInput\n(\ninputId \n=\n \nz\n,\n \n                  label \n=\n \nColor by:\n,\n\n                  choices \n=\n \nc\n(\nTitle Type\n \n=\n \ntitle_type\n,\n \n                              \nGenre\n \n=\n \ngenre\n,\n \n                              \nMPAA Rating\n \n=\n \nmpaa_rating\n,\n \n                              \nCritics Rating\n \n=\n \ncritics_rating\n,\n \n                              \nAudience Rating\n \n=\n \naudience_rating\n),\n\n                  selected \n=\n \nmpaa_rating\n),\n\n\n      hr\n(),\n      \n# Horizontal line for visual separation\n\n\n      \n# Set alpha level\n\n      sliderInput\n(\ninputId \n=\n \nalpha\n,\n \n                  label \n=\n \nAlpha:\n,\n \n                  min \n=\n \n0\n,\n max \n=\n \n1\n,\n \n                  value \n=\n \n0.5\n),\n\n\n      \n# Set point size\n\n      sliderInput\n(\ninputId \n=\n \nsize\n,\n \n                  label \n=\n \nSize:\n,\n \n                  min \n=\n \n0\n,\n max \n=\n \n5\n,\n \n                  value \n=\n \n2\n),\n\n\n      \n# Enter text for plot title\n\n      textInput\n(\ninputId \n=\n \nplot_title\n,\n \n                label \n=\n \nPlot title\n,\n \n                placeholder \n=\n \nEnter text to be used as plot title\n),\n\n\n      hr\n(),\n      \n# Horizontal line for visual separation\n\n\n      h3\n(\nSampling and subsetting\n),\n      \n# Third level header: Sampling and subsetting\n\n\n      \n# Select which types of movies to plot\n\n      checkboxGroupInput\n(\ninputId \n=\n \nselected_type\n,\n\n                         label \n=\n \nSelect movie type(s):\n,\n\n                         choices \n=\n \nc\n(\nDocumentary\n,\n \nFeature Film\n,\n \nTV Movie\n),\n\n                         selected \n=\n \nFeature Film\n),\n\n\n      \n# Select sample size\n\n      numericInput\n(\ninputId \n=\n \nn_samp\n,\n \n                   label \n=\n \nSample size:\n,\n \n                   min \n=\n \n1\n,\n max \n=\n \nnrow\n(\nmovies\n),\n \n                   value \n=\n \n50\n),\n\n\n      hr\n(),\n      \n# Horizontal line for visual separation\n\n\n      \n# Show data table\n\n      checkboxInput\n(\ninputId \n=\n \nshow_data\n,\n\n                    label \n=\n \nShow data table\n,\n\n                    value \n=\n \nTRUE\n)\n\n\n    \n),\n\n\n    \n# Output:\n\n    mainPanel\n(\n\n\n      \n# Show scatterplot\n\n      h3\n(\nScatterplot\n),\n          \n# Third level header: Scatterplot\n\n      plotOutput\n(\noutputId \n=\n \nscatterplot\n),\n\n      br\n(),\n          \n# Single line break for a little bit of visual separation\n\n\n      \n# Print number of obs plotted\n\n      h4\n(\nuiOutput\n(\noutputId \n=\n \nn\n)),\n    \n# Fourth level header\n\n      br\n(),\n br\n(),\n    \n# Two line breaks for a little bit of visual separation\n\n\n      \n# Show data table\n\n      h3\n(\nData table\n),\n          \n# Third level header: Data table\n\n      DT\n::\ndataTableOutput\n(\noutputId \n=\n \nmoviestable\n)\n\n    \n)\n\n  \n)\n\n\n)\n\n\n\n# Define server function required to create the scatterplot\n\nserver \n-\n \nfunction\n(\ninput\n,\n output\n,\n session\n)\n \n{\n\n\n  \n# Create a subset of data filtering for selected title types\n\n  movies_subset \n-\n reactive\n({\n\n    req\n(\ninput\n$\nselected_type\n)\n \n# ensure availablity of value before proceeding\n\n    filter\n(\nmovies\n,\n title_type \n%in%\n input\n$\nselected_type\n)\n\n  \n})\n\n\n  \n# Update the maximum allowed n_samp for selected type movies\n\n  observe\n({\n\n    updateNumericInput\n(\nsession\n,\n \n                       inputId \n=\n \nn_samp\n,\n\n                       value \n=\n \nmin\n(\n50\n,\n \nnrow\n(\nmovies_subset\n())),\n\n                       max \n=\n \nnrow\n(\nmovies_subset\n())\n\n    \n)\n\n  \n})\n\n\n  \n# Create new df that is n_samp obs from selected type movies\n\n  movies_sample \n-\n reactive\n({\n \n    req\n(\ninput\n$\nn_samp\n)\n \n# ensure availablity of value before proceeding\n\n    sample_n\n(\nmovies_subset\n(),\n input\n$\nn_samp\n)\n\n  \n})\n\n\n  \n# Create scatterplot object the plotOutput function is expecting \n\n  output\n$\nscatterplot \n-\n renderPlot\n({\n\n    ggplot\n(\ndata \n=\n movies_sample\n(),\n aes_string\n(\nx \n=\n input\n$\nx\n,\n y \n=\n input\n$\ny\n,\n\n                                              color \n=\n input\n$\nz\n))\n \n+\n\n      geom_point\n(\nalpha \n=\n input\n$\nalpha\n,\n size \n=\n input\n$\nsize\n)\n \n+\n\n      labs\n(\nx \n=\n toTitleCase\n(\nstr_replace_all\n(\ninput\n$\nx\n,\n \n_\n,\n \n \n)),\n\n           y \n=\n toTitleCase\n(\nstr_replace_all\n(\ninput\n$\ny\n,\n \n_\n,\n \n \n)),\n\n           color \n=\n toTitleCase\n(\nstr_replace_all\n(\ninput\n$\nz\n,\n \n_\n,\n \n \n)),\n\n           title \n=\n toTitleCase\n(\ninput\n$\nplot_title\n))\n\n  \n})\n\n\n  \n# Print number of movies plotted \n\n  output\n$\nn \n-\n renderUI\n({\n\n    types \n-\n movies_sample\n()\n$\ntitle_type \n%\n%\n \n      \nfactor\n(\nlevels \n=\n input\n$\nselected_type\n)\n \n    counts \n-\n \ntable\n(\ntypes\n)\n\n\n    HTML\n(\npaste\n(\nThere are\n,\n counts\n,\n input\n$\nselected_type\n,\n \nmovies in this dataset. \nbr\n))\n\n  \n})\n\n\n  \n# Print data table if checked\n\n  output\n$\nmoviestable \n-\n DT\n::\nrenderDataTable\n(\n\n    \nif\n(\ninput\n$\nshow_data\n){\n\n      DT\n::\ndatatable\n(\ndata \n=\n movies_sample\n()[,\n \n1\n:\n7\n],\n \n                    options \n=\n \nlist\n(\npageLength \n=\n \n10\n),\n \n                    rownames \n=\n \nFALSE\n)\n\n    \n}\n\n  \n)\n\n\n  \n}\n\n\n\n# Create Shiny app object\n\nshinyApp\n(\nui \n=\n ui\n,\n server \n=\n server\n)", 
            "title": "Customizing appearance"
        }
    ]
}